<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#21516;&#29702;&#35770;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28608;&#21169;&#26426;&#21046;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#26500;&#24615;&#21644;&#24930;&#36895;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36739;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06448</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#21516;&#29702;&#35770;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#19982;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory. (arXiv:2310.06448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#21516;&#29702;&#35770;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28608;&#21169;&#26426;&#21046;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#26500;&#24615;&#21644;&#24930;&#36895;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36739;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22266;&#26377;&#30340;&#24322;&#26500;&#24615;&#20197;&#21450;&#21560;&#24341;&#39640;&#36136;&#37327;&#30340;&#23458;&#25143;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#28608;&#21169;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28608;&#21169;&#26426;&#21046;&#36890;&#24120;&#22312;&#20256;&#32479;&#30340;&#21516;&#27493;&#32858;&#21512;&#20013;&#20351;&#29992;&#65292;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#24930;&#36895;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#21516;&#29702;&#35770;&#30340;&#26032;&#22411;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#28608;&#21169;&#26426;&#21046;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#35757;&#32451;&#36718;&#25968;&#26469;&#26368;&#22823;&#21270;&#20219;&#21153;&#21457;&#24067;&#32773;&#30340;&#25928;&#29992;&#65292;&#32771;&#34385;&#21040;&#26102;&#38388;&#24310;&#36831;&#21644;&#27979;&#35797;&#20934;&#30830;&#24615;&#31561;&#22240;&#32032;&#12290;&#22312;&#24322;&#27493;&#26041;&#26696;&#20013;&#65292;&#32771;&#34385;&#21040;&#23458;&#25143;&#31471;&#36136;&#37327;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32858;&#21512;&#26435;&#37325;&#21644;&#35775;&#38382;&#25511;&#21046;&#31639;&#27861;&#65292;&#20197;&#20419;&#36827;&#24322;&#27493;&#32858;&#21512;&#12290;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#27604;&#20854;&#20182;&#26041;&#27861;&#39640;&#20986;3.12%&#21644;5.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenges posed by the heterogeneity inherent in federated learning (FL) and to attract high-quality clients, various incentive mechanisms have been employed. However, existing incentive mechanisms are typically utilized in conventional synchronous aggregation, resulting in significant straggler issues. In this study, we propose a novel asynchronous FL framework that integrates an incentive mechanism based on contract theory. Within the incentive mechanism, we strive to maximize the utility of the task publisher by adaptively adjusting clients' local model training epochs, taking into account factors such as time delay and test accuracy. In the asynchronous scheme, considering client quality, we devise aggregation weights and an access control algorithm to facilitate asynchronous aggregation. Through experiments conducted on the MNIST dataset, the simulation results demonstrate that the test accuracy achieved by our framework is 3.12% and 5.84% higher than that achieved
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#27491;&#20998;&#31867;&#27169;&#22411;&#30340;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#20934;&#30830;&#23376;&#38598;&#21644;&#23545;&#20854;&#36827;&#34892;&#20462;&#27491;&#30340;&#35268;&#21017;&#21015;&#34920;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06446</link><description>&lt;p&gt;
&#29992;&#20110;&#20462;&#27491;&#20998;&#31867;&#27169;&#22411;&#30340;&#35268;&#21017;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Rule Mining for Correcting Classification Models. (arXiv:2310.06446v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#27491;&#20998;&#31867;&#27169;&#22411;&#30340;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#20934;&#30830;&#23376;&#38598;&#21644;&#23545;&#20854;&#36827;&#34892;&#20462;&#27491;&#30340;&#35268;&#21017;&#21015;&#34920;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#25110;&#20462;&#27491;&#65292;&#20197;&#30830;&#20445;&#39044;&#27979;&#20934;&#30830;&#24615;&#22987;&#32456;&#20445;&#25345;&#39640;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#27169;&#22411;&#20462;&#27491;&#21487;&#33021;&#25913;&#21464;&#39044;&#27979;&#32467;&#26524;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#27169;&#22411;&#26159;&#22797;&#26434;&#31995;&#32479;&#25110;&#36719;&#20214;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#24076;&#26395;&#33021;&#22815;&#25511;&#21046;&#20462;&#27491;&#30340;&#35268;&#33539;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#20102;&#35299;&#21738;&#20123;&#36755;&#20837;&#30340;&#23376;&#38598;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20462;&#27491;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#20197;&#33719;&#21462;&#25551;&#36848;&#19981;&#20934;&#30830;&#23376;&#38598;&#21644;&#22914;&#20309;&#36827;&#34892;&#20462;&#27491;&#30340;&#20840;&#38754;&#35268;&#21017;&#21015;&#34920;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20462;&#27491;&#35268;&#21017;&#25366;&#25496;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#39057;&#32321;&#39033;&#38598;&#25366;&#25496;&#21644;&#29420;&#29305;&#30340;&#20462;&#27491;&#35268;&#21017;&#20462;&#21098;&#25216;&#26415;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35813;&#31639;&#27861;&#25214;&#21040;&#20102;&#21508;&#31181;&#35268;&#21017;&#65292;&#26377;&#21161;&#20110;&#25910;&#38598;&#19981;&#20805;&#20998;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#30452;&#25509;&#20462;&#27491;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models need to be continually updated or corrected to ensure that the prediction accuracy remains consistently high. In this study, we consider scenarios where developers should be careful to change the prediction results by the model correction, such as when the model is part of a complex system or software. In such scenarios, the developers want to control the specification of the corrections. To achieve this, the developers need to understand which subpopulations of the inputs get inaccurate predictions by the model. Therefore, we propose correction rule mining to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. We also develop an efficient correction rule mining algorithm that is a combination of frequent itemset mining and a unique pruning technique for correction rules. We observed that the proposed algorithm found various rules which help to collect data insufficiently learned, directly correct model outputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#31574;&#30053;&#30340;&#39592;&#39612;&#22320;&#38754;&#23454;&#20917;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#30340;&#19978;&#19979;&#25991;&#12289;&#31616;&#21333;&#24615;&#21644;&#23436;&#25972;&#24615;&#32447;&#32034;&#36827;&#34892;&#20154;-&#25968;&#25454;&#22238;&#36335;&#30340;GT&#25552;&#21462;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#39592;&#39612;GT&#32570;&#20047;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06437</link><description>&lt;p&gt;
&#39592;&#39612;&#22320;&#38754;&#23454;&#20917;&#25552;&#21462;&#65306;&#26041;&#27861;&#35770;&#12289;&#27880;&#37322;&#24037;&#20855;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks. (arXiv:2310.06437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#31574;&#30053;&#30340;&#39592;&#39612;&#22320;&#38754;&#23454;&#20917;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#30340;&#19978;&#19979;&#25991;&#12289;&#31616;&#21333;&#24615;&#21644;&#23436;&#25972;&#24615;&#32447;&#32034;&#36827;&#34892;&#20154;-&#25968;&#25454;&#22238;&#36335;&#30340;GT&#25552;&#21462;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#39592;&#39612;GT&#32570;&#20047;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39592;&#39612;&#22320;&#38754;&#23454;&#20917;&#65288;GT&#65289;&#23545;&#20110;&#30417;&#30563;&#24335;&#39592;&#39612;&#25552;&#21462;&#26041;&#27861;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#27969;&#34892;&#20043;&#19979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30475;&#21040;&#39592;&#39612; GT &#19981;&#20165;&#29992;&#20110;&#29992;&#20110;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#39592;&#39612;&#26816;&#27979;&#22120;&#65292;&#36824;&#29992;&#20110;&#35780;&#20272;&#19982;&#39592;&#39612;&#30456;&#20851;&#30340;&#20462;&#21098;&#21644;&#21305;&#37197;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24418;&#29366;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#22312;&#39592;&#39612; GT &#30340;&#32570;&#20047;&#21644; GT &#26631;&#20934;&#30340;&#19981;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24456;&#38590;&#20844;&#24179;&#22320;&#35780;&#20272;&#21644;&#22797;&#29616;&#22522;&#20110; CNN &#30340;&#39592;&#39612;&#26816;&#27979;&#22120;&#21644;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20108;&#20803;&#24418;&#29366;&#21644;&#33258;&#28982;&#22270;&#20687;&#20013;&#25552;&#21462;&#39592;&#39612; GT &#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#24314;&#31435;&#22312;&#35786;&#26029;&#24615;&#20551;&#35774;&#30340;&#25193;&#23637;&#29702;&#35770;&#19978;&#65292;&#36825;&#20351;&#24471;&#20154;-&#25968;&#25454;&#22238;&#36335;&#30340; GT &#25552;&#21462;&#21487;&#20197;&#22522;&#20110;&#30446;&#26631;&#30340;&#19978;&#19979;&#25991;&#12289;&#31616;&#21333;&#24615;&#21644;&#23436;&#25972;&#24615;&#30340;&#32447;&#32034;&#26469;&#36827;&#34892;&#32534;&#30721;&#12290;&#20351;&#29992;&#36825;&#20010;&#31574;&#30053;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;SkeView&#65292;&#29992;&#20110;&#29983;&#25104;17&#20010;&#29616;&#26377;&#24418;&#29366;&#25968;&#25454;&#38598;&#30340;&#39592;&#39612; GT&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN) but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target's context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape 
&lt;/p&gt;</description></item><item><title>&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#21547;&#26377;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAPS&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#33293;&#24323;&#38500;&#26368;&#22823;softmax&#27010;&#29575;&#20043;&#22806;&#30340;&#25152;&#26377;&#27010;&#29575;&#20540;&#65292;&#20943;&#23569;&#20102;&#38750;&#19968;&#33268;&#24615;&#35780;&#20998;&#23545;&#27010;&#29575;&#20540;&#30340;&#20381;&#36182;&#24615;&#65292;&#36798;&#21040;&#20102;&#29983;&#25104;&#23567;&#23610;&#23544;&#38598;&#21512;&#21644;&#20256;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.06430</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#25490;&#21517;&#23454;&#29616;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Deep Classifier via Label Ranking. (arXiv:2310.06430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06430
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#21547;&#26377;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAPS&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#33293;&#24323;&#38500;&#26368;&#22823;softmax&#27010;&#29575;&#20043;&#22806;&#30340;&#25152;&#26377;&#27010;&#29575;&#20540;&#65292;&#20943;&#23569;&#20102;&#38750;&#19968;&#33268;&#24615;&#35780;&#20998;&#23545;&#27010;&#29575;&#20540;&#30340;&#20381;&#36182;&#24615;&#65292;&#36798;&#21040;&#20102;&#29983;&#25104;&#23567;&#23610;&#23544;&#38598;&#21512;&#21644;&#20256;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#21547;&#26377;&#25152;&#38656;&#35206;&#30422;&#20445;&#35777;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#30340;&#39044;&#27979;&#27010;&#29575;&#36890;&#24120;&#26159;&#38169;&#35823;&#26657;&#20934;&#30340;&#65292;&#23548;&#33268;&#31526;&#21512;&#39044;&#27979;&#20013;&#30340;&#22823;&#22411;&#39044;&#27979;&#38598;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#24573;&#30053;&#27010;&#29575;&#20540;&#21487;&#20197;&#32531;&#35299;&#38169;&#35823;&#26657;&#20934;&#27010;&#29575;&#20540;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25490;&#24207;&#33258;&#36866;&#24212;&#39044;&#27979;&#38598;&#8221;&#65288;SAPS&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33293;&#24323;&#38500;&#26368;&#22823;softmax&#27010;&#29575;&#20043;&#22806;&#30340;&#25152;&#26377;&#27010;&#29575;&#20540;&#12290;SAPS&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#20445;&#30041;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23613;&#37327;&#20943;&#23569;&#38750;&#19968;&#33268;&#24615;&#35780;&#20998;&#23545;&#27010;&#29575;&#20540;&#30340;&#20381;&#36182;&#24615;&#12290;&#36825;&#26679;&#65292;SAPS&#21487;&#20197;&#29983;&#25104;&#23567;&#23610;&#23544;&#30340;&#38598;&#21512;&#65292;&#24182;&#20256;&#36798;&#27599;&#20010;&#23454;&#20363;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;SAPS&#30340;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#26399;&#26395;&#20540;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. In this paper, we empirically and theoretically show that disregarding the probabilities' value will mitigate the undesirable effect of miscalibrated probability values. Then, we propose a novel algorithm named $\textit{Sorted Adaptive prediction sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce sets of small size and communicate instance-wise uncertainty. Theoretically, we provide a finite-sample coverage guarantee of SAPS and show that the expected value of se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#23545;&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#23398;&#20064;&#65292;&#21363;&#20351;&#22312;&#38750;&#20445;&#23432;&#30340;&#21487;&#36870;&#31995;&#32479;&#20013;&#20063;&#33021;&#20445;&#25345;&#33021;&#37327;&#65292;&#24182;&#19988;&#33021;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.06427</link><description>&lt;p&gt;
TANGO: &#26102;&#38388;&#21453;&#28436;&#28508;&#22312;&#22270;ODE&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems. (arXiv:2310.06427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#23545;&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#23398;&#20064;&#65292;&#21363;&#20351;&#22312;&#38750;&#20445;&#23432;&#30340;&#21487;&#36870;&#31995;&#32479;&#20013;&#20063;&#33021;&#20445;&#25345;&#33021;&#37327;&#65292;&#24182;&#19988;&#33021;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#22312;&#29289;&#29702;&#27169;&#25311;&#21644;&#26448;&#26009;&#24314;&#27169;&#20013;&#12290;&#22312;&#32431;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22914;Hamiltonian&#31070;&#32463;&#32593;&#32476;&#20005;&#26684;&#36981;&#24490;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#24341;&#20837;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;&#24471;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#24182;&#19981;&#20005;&#26684;&#36981;&#23432;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#65292;&#22914;&#24102;&#26377;&#25705;&#25830;&#30340;&#24377;&#31783;&#31995;&#32479;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#29289;&#29702;&#21407;&#29702;&#65306;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#65292;&#23427;&#25551;&#36848;&#20102;&#24403;&#31995;&#32479;&#30340;&#21160;&#21147;&#22312;&#26102;&#38388;&#19978;&#20498;&#36716;&#26102;&#65292;&#21160;&#21147;&#23398;&#24212;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#20173;&#28982;&#26377;&#21161;&#20110;&#20445;&#25345;&#20445;&#23432;&#31995;&#32479;&#30340;&#33021;&#37327;&#65292;&#24182;&#21516;&#26102;&#20026;&#38750;&#20445;&#23432;&#30340;&#21487;&#36870;&#31995;&#32479;&#25552;&#20379;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20026;&#20102;&#27880;&#20837;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#27491;&#21017;&#21270;&#39033;&#20316;&#20026;&#36719;&#32422;&#26463;&#65292;&#23558;&#27491;&#21521;&#21644;&#36870;&#21521;&#21160;&#21147;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall remain invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this paper, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06417</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#23398;&#20064;&#20013;&#30340;&#25299;&#25169;&#27010;&#25324;&#30340;&#27969;&#21160;&#25193;&#25955;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#26041;&#31243;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#20998;&#26512;GNN&#21160;&#21147;&#23398;&#12289;&#24418;&#24335;&#21270;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#35777;&#26126;&#26550;&#26500;&#36873;&#25321;&#30340;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#12290;&#22270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;GNN&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;&#24403;&#21069;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#20551;&#35774;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#22270;&#25299;&#25169;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22270;&#25193;&#25955;&#26041;&#31243;&#22312;&#19981;&#21516;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#30340;&#22806;&#25512;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#36808;&#20986;&#20102;&#35299;&#26512;GNN&#27010;&#25324;&#24615;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#19978;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#25351;&#25968;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#65292;&#23427;&#20513;&#23548;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;&#28508;&#22312;&#22270;&#36827;&#34892;&#29305;&#24449;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#21270;&#23398;&#31995;&#32479;&#30340;&#20998;&#31163;&#20849;&#27832;&#28151;&#21512;&#29289;&#30340;&#27969;&#31243;&#22270;&#65292;&#24182;&#33021;&#23558;&#36229;&#36807;99&#65285;&#30340;&#26448;&#26009;&#20998;&#31163;&#20026;&#32431;&#32452;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.06415</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25581;&#31034;&#20102;&#26080;&#20808;&#39564;&#30693;&#35782;&#19979;&#20998;&#31163;&#20849;&#27832;&#28151;&#21512;&#29289;&#30340;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge. (arXiv:2310.06415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#21270;&#23398;&#31995;&#32479;&#30340;&#20998;&#31163;&#20849;&#27832;&#28151;&#21512;&#29289;&#30340;&#27969;&#31243;&#22270;&#65292;&#24182;&#33021;&#23558;&#36229;&#36807;99&#65285;&#30340;&#26448;&#26009;&#20998;&#31163;&#20026;&#32431;&#32452;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#24037;&#31243;&#20013;&#30340;&#36807;&#31243;&#32508;&#21512;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#30001;&#20110;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12289;&#36830;&#32493;&#21442;&#25968;&#21644;&#27867;&#21270;&#30340;&#38656;&#27714;&#12290;&#36817;&#24180;&#26469;&#65292;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#27969;&#31243;&#22270;&#32508;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#27010;&#24565;&#65292;&#20294;&#20391;&#37325;&#20110;&#21333;&#19968;&#21270;&#23398;&#31995;&#32479;&#20013;&#30340;&#29421;&#31364;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27969;&#31243;&#22270;&#32508;&#21512;&#30340;&#36890;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#20010;&#20195;&#29702;&#22312;&#20998;&#31163;&#20108;&#20803;&#20849;&#27832;&#28151;&#21512;&#29289;&#30340;&#19968;&#33324;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#23398;&#20250;&#35774;&#35745;&#22810;&#20010;&#21270;&#23398;&#31995;&#32479;&#30340;&#25509;&#36817;&#26368;&#20248;&#30340;&#27969;&#31243;&#22270;&#65292;&#32771;&#34385;&#19981;&#21516;&#30340;&#36827;&#26009;&#32452;&#25104;&#21644;&#27010;&#24565;&#26041;&#27861;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#35813;&#20195;&#29702;&#33021;&#23558;&#36229;&#36807;99&#65285;&#30340;&#26448;&#26009;&#20998;&#31163;&#20026;&#32431;&#32452;&#20998;&#65292;&#24182;&#33258;&#20027;&#23398;&#20064;&#22522;&#26412;&#30340;&#36807;&#31243;&#24037;&#31243;&#24072;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Process synthesis in chemical engineering is a complex planning problem due to vast search spaces, continuous parameters and the need for generalization. Deep reinforcement learning agents, trained without prior knowledge, have shown to outperform humans in various complex planning problems in recent years. Existing work on reinforcement learning for flowsheet synthesis shows promising concepts, but focuses on narrow problems in a single chemical system, limiting its practicality. We present a general deep reinforcement learning approach for flowsheet synthesis. We demonstrate the adaptability of a single agent to the general task of separating binary azeotropic mixtures. Without prior knowledge, it learns to craft near-optimal flowsheets for multiple chemical systems, considering different feed compositions and conceptual approaches. On average, the agent can separate more than 99% of the involved materials into pure components, while autonomously learning fundamental process engineer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;Lo-Hi&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21069;&#23548;&#20248;&#21270;&#21644;&#21629;&#20013;&#35782;&#21035;&#20004;&#20010;&#20219;&#21153;&#65292;&#20026;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#20999;&#23454;&#21487;&#34892;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#23545;&#20110;&#21629;&#20013;&#35782;&#21035;&#20219;&#21153;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20915;&#39030;&#28857;&#26368;&#23567;k-Cut&#38382;&#39064;&#30340;&#26032;&#22411;&#20998;&#23376;&#25286;&#20998;&#31639;&#27861;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#29616;&#23454;&#19988;&#36807;&#20110;&#20048;&#35266;&#12290;</title><link>http://arxiv.org/abs/2310.06399</link><description>&lt;p&gt;
Lo-Hi: &#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#33647;&#29289;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Lo-Hi: Practical ML Drug Discovery Benchmark. (arXiv:2310.06399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;Lo-Hi&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21069;&#23548;&#20248;&#21270;&#21644;&#21629;&#20013;&#35782;&#21035;&#20004;&#20010;&#20219;&#21153;&#65292;&#20026;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#20999;&#23454;&#21487;&#34892;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#23545;&#20110;&#21629;&#20013;&#35782;&#21035;&#20219;&#21153;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20915;&#39030;&#28857;&#26368;&#23567;k-Cut&#38382;&#39064;&#30340;&#26032;&#22411;&#20998;&#23376;&#25286;&#20998;&#31639;&#27861;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#29616;&#23454;&#19988;&#36807;&#20110;&#20048;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#26032;&#33647;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#33647;&#29289;&#21457;&#29616;&#30340;&#24076;&#26395;&#20043;&#19968;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#27491;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#22312;MoleculeNet&#31561;&#22522;&#20934;&#27979;&#35797;&#20013;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#20999;&#23454;&#38469;&#65292;&#24182;&#19988;&#19982;&#23454;&#38469;&#24212;&#29992;&#27169;&#22411;&#30456;&#24046;&#22826;&#22823;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#29992;&#30340;Lo-Hi&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#21069;&#23548;&#20248;&#21270;&#65288;Lo&#65289;&#21644;&#21629;&#20013;&#35782;&#21035;&#65288;Hi&#65289;&#65292;&#23545;&#24212;&#20110;&#30495;&#23454;&#30340;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#12290;&#23545;&#20110;Hi&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35299;&#20915;&#24179;&#34913;&#39030;&#28857;&#26368;&#23567;k-Cut&#38382;&#39064;&#30340;&#26032;&#22411;&#20998;&#23376;&#25286;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#21738;&#31181;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#19981;&#20999;&#23454;&#38469;&#24182;&#19988;&#36807;&#20110;&#20048;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical \emph{Lo-Hi} benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.  Review: https://openreview.net/forum?id=H2Yb28qGLV  Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023  Lo-Hi split
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20445;&#23432;&#21704;&#23494;&#39039;&#31070;&#32463;&#27969;&#26500;&#24314;&#20102;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25239;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06396</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65306;&#19968;&#31181;&#21704;&#23494;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach. (arXiv:2310.06396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20445;&#23432;&#21704;&#23494;&#39039;&#31070;&#32463;&#27969;&#26500;&#24314;&#20102;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25239;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#24433;&#21709;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#30340;&#25200;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#19981;&#21516;&#30340;&#31070;&#32463;&#27969;&#23548;&#20986;&#30340;GNNs&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#19982;&#22810;&#31181;&#31283;&#23450;&#24615;&#27010;&#24565;&#65288;&#22914;BIBO&#31283;&#23450;&#24615;&#12289;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#12289;&#32467;&#26500;&#31283;&#23450;&#24615;&#21644;&#20445;&#23432;&#31283;&#23450;&#24615;&#65289;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#24182;&#19981;&#33021;&#20445;&#35777;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#21463;&#29289;&#29702;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;&#20855;&#26377;&#20445;&#23432;&#21704;&#23494;&#39039;&#31070;&#32463;&#27969;&#30340;GNNs&#26469;&#26500;&#24314;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#30340;&#32593;&#32476;&#12290;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#65292;&#36890;&#36807;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#27604;&#36739;&#20102;&#19981;&#21516;&#31070;&#32463;&#27969;GNNs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#20855;&#26377;&#26446;&#20122;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#30340;&#20445;&#23432;&#21704;&#23494;&#39039;&#27969;&#30340;GNNs&#22312;&#23545;&#25239;&#25200;&#21160;&#26041;&#38754;&#22823;&#24133;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experim
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#28165;&#21333;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#36328;&#22269;&#21442;&#32771;&#20316;&#29289;&#31867;&#22411;&#30417;&#27979;&#25968;&#25454;&#24211;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#33719;&#21462;&#21487;&#38752;&#39640;&#36136;&#37327;&#21442;&#32771;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06393</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#28165;&#21333;&#21019;&#24314;&#21487;&#38752;&#30340;&#36328;&#22269;&#21442;&#32771;&#20316;&#29289;&#31867;&#22411;&#30417;&#27979;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring. (arXiv:2310.06393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#28165;&#21333;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#36328;&#22269;&#21442;&#32771;&#20316;&#29289;&#31867;&#22411;&#30417;&#27979;&#25968;&#25454;&#24211;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#33719;&#21462;&#21487;&#38752;&#39640;&#36136;&#37327;&#21442;&#32771;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#39134;&#36895;&#21457;&#23637;&#21450;&#20854;&#22312;&#22320;&#29699;&#35266;&#27979;&#25361;&#25112;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#39046;&#22495;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25552;&#21319;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#21069;&#21463;&#21040;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#21644;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#20294;&#32570;&#20047;&#36275;&#22815;&#30340;&#21442;&#32771;&#25968;&#25454;&#29616;&#22312;&#26500;&#25104;&#20102;&#26032;&#30340;&#29942;&#39048;&#12290;&#30001;&#20110;&#21019;&#24314;&#36825;&#31181;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#26159;&#19968;&#39033;&#26114;&#36149;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#65292;&#24517;&#39035;&#24819;&#20986;&#26032;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#19978;&#33719;&#21462;&#21487;&#38752;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25968;&#25454;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;E URO C ROPS&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20316;&#29289;&#31867;&#22411;&#20998;&#31867;&#30340;&#21442;&#32771;&#25968;&#25454;&#38598;&#65292;&#23427;&#32858;&#21512;&#24182;&#21327;&#35843;&#20102;&#19981;&#21516;&#22269;&#23478;&#35843;&#26597;&#30340;&#34892;&#25919;&#25968;&#25454;&#65292;&#30446;&#30340;&#26159;&#23454;&#29616;&#36328;&#22269;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With leaps in machine learning techniques and their applicationon Earth observation challenges has unlocked unprecedented performance across the domain. While the further development of these methods was previously limited by the availability and volume of sensor data and computing resources, the lack of adequate reference data is now constituting new bottlenecks. Since creating such ground-truth information is an expensive and error-prone task, new ways must be devised to source reliable, high-quality reference data on large scales. As an example, we showcase E URO C ROPS, a reference dataset for crop type classification that aggregates and harmonizes administrative data surveyed in different countries with the goal of transnational interoperability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;CAST&#65289;&#65292;&#36890;&#36807;&#35268;&#33539;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65292;&#24357;&#34917;&#20102;&#33258;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#24369;&#28857;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06380</link><description>&lt;p&gt;
CAST&#65306;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CAST: Cluster-Aware Self-Training for Tabular Data. (arXiv:2310.06380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;CAST&#65289;&#65292;&#36890;&#36807;&#35268;&#33539;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65292;&#24357;&#34917;&#20102;&#33258;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#24369;&#28857;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#22810;&#21151;&#33021;&#24615;&#32780;&#21463;&#21040;&#21560;&#24341;&#65292;&#28982;&#32780;&#23427;&#23481;&#26131;&#21463;&#21040;&#26377;&#22122;&#38899;&#30340;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;&#20960;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25104;&#21151;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#21066;&#24369;&#20102;&#33258;&#35757;&#32451;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#33258;&#35757;&#32451;&#31639;&#27861;&#25110;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#29305;&#23450;&#30340;&#20462;&#25913;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19982;&#22312;&#34920;&#26684;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#32676;&#38598;&#20551;&#35774;&#65292;&#21363;&#30456;&#20114;&#25509;&#36817;&#30340;&#25968;&#25454;&#26679;&#26412;&#24448;&#24448;&#23646;&#20110;&#21516;&#19968;&#31867;&#12290;&#22312;&#27492;&#20551;&#35774;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#65288;CAST&#65289;&#26041;&#27861;&#12290;CAST&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#26222;&#36941;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#36827;&#29616;&#26377;&#30340;&#33258;&#35757;&#32451;&#31639;&#27861;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#24133;&#20462;&#25913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35268;&#33539;&#20102;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#65292;&#21363;&#20266;&#26631;&#31614;&#30340;&#20540;&#65292;&#24378;&#21046;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#23545;&#20266;&#26631;&#31614;&#36827;&#34892;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Fourier&#31070;&#32463;&#25805;&#20316;&#31526;(FNO)&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;FNO&#29256;&#26412;&#30340;He&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#27169;&#24335;&#25130;&#26029;&#21644;&#23494;&#38598;&#36830;&#25509;&#32593;&#32476;&#30456;&#20284;&#30340;&#29305;&#28857;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#36127;&#21021;&#22987;&#21270;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06379</link><description>&lt;p&gt;
Fourier&#31070;&#32463;&#25805;&#20316;&#31526;&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#65306;&#37325;&#26032;&#23457;&#35270;&#28151;&#27788;&#36793;&#32536;
&lt;/p&gt;
&lt;p&gt;
Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos. (arXiv:2310.06379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Fourier&#31070;&#32463;&#25805;&#20316;&#31526;(FNO)&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;FNO&#29256;&#26412;&#30340;He&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#27169;&#24335;&#25130;&#26029;&#21644;&#23494;&#38598;&#36830;&#25509;&#32593;&#32476;&#30456;&#20284;&#30340;&#29305;&#28857;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#36127;&#21021;&#22987;&#21270;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Fourier&#31070;&#32463;&#25805;&#20316;&#31526;(FNO)&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#12290;&#24314;&#31435;&#20102;&#19968;&#20010;&#38024;&#23545;FNO&#30340;&#24179;&#22343;&#22330;&#29702;&#35770;&#65292;&#20174;&#8220;&#28151;&#27788;&#36793;&#32536;&#8221;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#38543;&#26426;FNO&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#34892;&#20026;&#34920;&#29616;&#20986;&#19982;FNO&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#36825;&#26159;&#30001;&#27169;&#24335;&#25130;&#26029;&#24341;&#36215;&#30340;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#19982;&#23494;&#38598;&#36830;&#25509;&#32593;&#32476;&#30456;&#20284;&#30340;&#29305;&#28857;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;FNO&#29256;&#26412;&#30340;He&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#36127;&#21021;&#22987;&#21270;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#21021;&#22987;&#21270;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#24471;32&#23618;FNO&#30340;&#35757;&#32451;&#31283;&#23450;&#65292;&#26080;&#38656;&#39069;&#22806;&#25216;&#26415;&#25110;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the initialization bias of the Fourier neural operator (FNO). A mean-field theory for FNO is established, analyzing the behavior of the random FNO from an ``edge of chaos'' perspective. We uncover that the forward and backward propagation behaviors exhibit characteristics unique to FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Building upon this observation, we also propose a FNO version of the He initialization scheme to mitigate the negative initialization bias leading to training instability. Experimental results demonstrate the effectiveness of our initialization scheme, enabling stable training of a 32-layer FNO without the need for additional techniques or significant performance degradation.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#21464;&#21270;&#24182;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#65292;&#35299;&#20915;&#21518;&#38376;&#25915;&#20987;&#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.06372</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21464;&#21270;&#20026;&#40065;&#26834;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data. (arXiv:2310.06372v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#21464;&#21270;&#24182;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#65292;&#35299;&#20915;&#21518;&#38376;&#25915;&#20987;&#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#23545;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#65292;&#23427;&#20204;&#22312;&#27169;&#22411;&#20013;&#31192;&#23494;&#24341;&#20837;&#38544;&#34255;&#21151;&#33021;&#12290;&#36825;&#20123;&#21518;&#38376;&#22312;&#23545;&#24178;&#20928;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#26102;&#20445;&#25345;&#27785;&#40664;&#65292;&#30001;&#20110;&#38544;&#34109;&#30340;&#34892;&#20026;&#32780;&#36991;&#20813;&#34987;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#36755;&#20837;&#25968;&#25454;&#20013;&#20986;&#29616;&#29305;&#23450;&#30340;&#35302;&#21457;&#27169;&#24335;&#65292;&#21518;&#38376;&#23601;&#20250;&#28608;&#27963;&#65292;&#23548;&#33268;&#27169;&#22411;&#25191;&#34892;&#20854;&#38544;&#34255;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#25163;&#21160;&#26816;&#26597;&#65292;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21040;&#36825;&#31181;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#23454;&#29616;&#23545;&#21487;&#33021;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#19978;&#21019;&#24314;&#21512;&#25104;&#21464;&#21270;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#35302;&#21457;&#27169;&#24335;&#20855;&#26377;&#22266;&#26377;&#30340;&#24377;&#24615;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#29983;&#25104;&#26041;&#27861;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#20445;&#25345;&#20219;&#21153;&#19978;&#30340;&#24635;&#20307;&#24615;&#33021;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that maintain their general performance on the task while exhib
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21306;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#20943;&#23567;&#35823;&#24046;&#21644;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#21363;&#20351;&#38544;&#31169;&#39044;&#31639;&#26377;&#38480;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#21487;&#25913;&#21892;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06371</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#21306;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Partition-based differentially private synthetic data generation. (arXiv:2310.06371v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21306;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#20943;&#23567;&#35823;&#24046;&#21644;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#21363;&#20351;&#38544;&#31169;&#39044;&#31639;&#26377;&#38480;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#21487;&#25913;&#21892;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25688;&#35201;&#32479;&#35745;&#30456;&#27604;&#65292;&#31169;&#23494;&#21512;&#25104;&#25968;&#25454;&#20849;&#20139;&#26356;&#21463;&#38738;&#30544;&#65292;&#22240;&#20026;&#23427;&#20445;&#30041;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#20998;&#24067;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#36873;&#25321;-&#24230;&#37327;-&#29983;&#25104;&#33539;&#24335;&#65292;&#20294;&#27979;&#37327;&#22823;&#39046;&#22495;&#36793;&#32536;&#20173;&#28982;&#20250;&#23548;&#33268;&#24456;&#22823;&#35823;&#24046;&#65292;&#36845;&#20195;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#20173;&#28982;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35823;&#24046;&#65292;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#21363;&#20351;&#38544;&#31169;&#39044;&#31639;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#23637;&#29616;&#20986;&#25913;&#21892;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#31169;&#23494;&#21512;&#25104;&#25968;&#25454;&#20849;&#20139;&#30340;&#39318;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private synthetic data sharing is preferred as it keeps the distribution and nuances of original data compared to summary statistics. The state-of-the-art methods adopt a select-measure-generate paradigm, but measuring large domain marginals still results in much error and allocating privacy budget iteratively is still difficult. To address these issues, our method employs a partition-based approach that effectively reduces errors and improves the quality of synthetic data, even with a limited privacy budget. Results from our experiments demonstrate the superiority of our method over existing approaches. The synthetic data produced using our approach exhibits improved quality and utility, making it a preferable choice for private synthetic data sharing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#26144;&#23556;&#21040;&#40654;&#26364;&#26354;&#38754;&#19978;&#30340;&#23616;&#37096;&#24179;&#22374;&#22352;&#26631;&#65292;&#23454;&#29616;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#31283;&#23450;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.06369</link><description>&lt;p&gt;
&#20960;&#20309;&#23545;&#40784;&#36716;&#31227;&#32534;&#30721;&#22120;&#29992;&#20110;&#24402;&#32435;&#36716;&#31227;&#22238;&#24402;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks. (arXiv:2310.06369v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#26144;&#23556;&#21040;&#40654;&#26364;&#26354;&#38754;&#19978;&#30340;&#23616;&#37096;&#24179;&#22374;&#22352;&#26631;&#65292;&#23454;&#29616;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#31283;&#23450;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#22788;&#29702;&#21487;&#33021;&#19982;&#20854;&#20182;&#20016;&#23500;&#25968;&#25454;&#30456;&#20851;&#30340;&#23569;&#37327;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25193;&#23637;&#36716;&#31227;&#23398;&#20064;&#26041;&#26696;&#21040;&#22238;&#24402;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#20998;&#20960;&#20309;&#30340;&#26032;&#22411;&#36716;&#31227;&#25216;&#26415;&#65292;&#21363;&#20960;&#20309;&#23545;&#40784;&#36716;&#31227;&#32534;&#30721;&#22120;&#65288;GATE&#65289;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#35299;&#37322;&#20026;&#23384;&#22312;&#20110;&#40654;&#26364;&#26354;&#38754;&#19978;&#30340;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#31181;&#36866;&#24403;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#30830;&#20445;&#27599;&#20010;&#20219;&#24847;&#28857;&#26144;&#23556;&#21040;&#37325;&#21472;&#21306;&#22495;&#20013;&#30340;&#23616;&#37096;&#24179;&#22374;&#22352;&#26631;&#65292;&#20174;&#32780;&#23454;&#29616;&#20174;&#28304;&#25968;&#25454;&#21040;&#30446;&#26631;&#25968;&#25454;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#36825;&#20063;&#20316;&#20026;&#27169;&#22411;&#22312;&#22806;&#25512;&#21306;&#22495;&#34892;&#20026;&#33391;&#22909;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GATE&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that GATE outperforms conventional methods and exhibits stable behavior in both the latent space 
&lt;/p&gt;</description></item><item><title>DrugCLIP&#23558;&#34394;&#25311;&#31579;&#36873;&#36716;&#21270;&#20026;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#26469;&#33258;&#22823;&#37327;&#25104;&#23545;&#30340;&#34507;&#30333;&#36136;&#21475;&#34955;&#21644;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#34394;&#25311;&#31579;&#36873;&#30340;&#21152;&#36895;&#12290;&#36825;&#39033;&#26032;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#34507;&#30333;&#36136;&#20998;&#23376;&#30340;&#29305;&#23450;&#32467;&#21512;&#20146;&#21644;&#21147;&#35780;&#20998;&#65292;&#32780;&#26159;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;&#29983;&#29289;&#23398;&#30693;&#35782;&#21551;&#21457;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.06367</link><description>&lt;p&gt;
DrugCLIP: &#29992;&#20110;&#34394;&#25311;&#31579;&#36873;&#30340;&#23545;&#27604;&#34507;&#30333;&#36136;-&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening. (arXiv:2310.06367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06367
&lt;/p&gt;
&lt;p&gt;
DrugCLIP&#23558;&#34394;&#25311;&#31579;&#36873;&#36716;&#21270;&#20026;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#26469;&#33258;&#22823;&#37327;&#25104;&#23545;&#30340;&#34507;&#30333;&#36136;&#21475;&#34955;&#21644;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#34394;&#25311;&#31579;&#36873;&#30340;&#21152;&#36895;&#12290;&#36825;&#39033;&#26032;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#34507;&#30333;&#36136;&#20998;&#23376;&#30340;&#29305;&#23450;&#32467;&#21512;&#20146;&#21644;&#21147;&#35780;&#20998;&#65292;&#32780;&#26159;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;&#29983;&#29289;&#23398;&#30693;&#35782;&#21551;&#21457;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#31579;&#36873;&#26159;AI&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23427;&#20174;&#24222;&#22823;&#30340;&#21270;&#21512;&#29289;&#25968;&#25454;&#24211;&#20013;&#35782;&#21035;&#20986;&#19982;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#32467;&#21512;&#30340;&#28508;&#22312;&#33647;&#29289;&#12290;&#20256;&#32479;&#30340;&#23545;&#25509;&#26041;&#27861;&#32791;&#26102;&#19988;&#21482;&#33021;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#20351;&#29992;&#21463;&#38480;&#30340;&#25628;&#32034;&#24211;&#12290;&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#36827;&#34892;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#30001;&#20110;&#23545;&#21487;&#38752;&#32467;&#21512;&#20146;&#21644;&#21147;&#26631;&#31614;&#30340;&#26377;&#38480;&#25968;&#25454;&#20381;&#36182;&#24615;&#24378;&#65292;&#23578;&#26410;&#36229;&#36234;&#23545;&#25509;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;DrugCLIP&#65292;&#23558;&#34394;&#25311;&#31579;&#36873;&#37325;&#26032;&#34920;&#36848;&#20026;&#23494;&#38598;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#27627;&#19981;&#20381;&#36182;&#20110;&#26174;&#24335;&#32467;&#21512;&#20146;&#21644;&#21147;&#35780;&#20998;&#30340;&#22823;&#37327;&#25104;&#23545;&#25968;&#25454;&#20013;&#30340;&#34507;&#30333;&#36136;&#21475;&#34955;&#21644;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#30693;&#35782;&#21551;&#21457;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;-&#20998;&#23376;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual screening, which identifies potential drugs from vast compound databases to bind with a particular protein pocket, is a critical step in AI-assisted drug discovery. Traditional docking methods are highly time-consuming, and can only work with a restricted search library in real-life applications. Recent supervised learning approaches using scoring functions for binding-affinity prediction, although promising, have not yet surpassed docking methods due to their strong dependency on limited data with reliable binding-affinity labels. In this paper, we propose a novel contrastive learning framework, DrugCLIP, by reformulating virtual screening as a dense retrieval task and employing contrastive learning to align representations of binding protein pockets and molecules from a large quantity of pairwise data without explicit binding-affinity scores. We also introduce a biological-knowledge inspired data augmentation strategy to learn better protein-molecule representations. Extensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26680;&#24515;-&#20013;&#38388;-&#22806;&#22260;&#65288;CIP&#65289;&#25351;&#25968;&#30340;&#26032;&#30340;&#23450;&#37327;&#24230;&#37327;&#25351;&#26631;&#65292;&#36890;&#36807;&#23545;&#37051;&#22495;&#21644;&#26368;&#30701;&#36335;&#24452;&#20013;&#24515;&#24615;&#24230;&#37327;&#25968;&#25454;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#33410;&#28857;&#22312;&#32593;&#32476;&#20013;&#25198;&#28436;&#26680;&#24515;&#33410;&#28857;&#21644;&#22806;&#22260;&#33410;&#28857;&#30340;&#31243;&#24230;&#12290;&#20316;&#32773;&#36890;&#36807;&#27979;&#35797;&#22312;12&#20010;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06358</link><description>&lt;p&gt;
Core-Intermediate-Peripheral Index: &#22522;&#20110;&#37051;&#22495;&#21644;&#26368;&#30701;&#36335;&#24452;&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#22240;&#23376;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and Shortest Paths-based Centrality Metrics. (arXiv:2310.06358v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26680;&#24515;-&#20013;&#38388;-&#22806;&#22260;&#65288;CIP&#65289;&#25351;&#25968;&#30340;&#26032;&#30340;&#23450;&#37327;&#24230;&#37327;&#25351;&#26631;&#65292;&#36890;&#36807;&#23545;&#37051;&#22495;&#21644;&#26368;&#30701;&#36335;&#24452;&#20013;&#24515;&#24615;&#24230;&#37327;&#25968;&#25454;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#33410;&#28857;&#22312;&#32593;&#32476;&#20013;&#25198;&#28436;&#26680;&#24515;&#33410;&#28857;&#21644;&#22806;&#22260;&#33410;&#28857;&#30340;&#31243;&#24230;&#12290;&#20316;&#32773;&#36890;&#36807;&#27979;&#35797;&#22312;12&#20010;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22235;&#20010;&#20027;&#35201;&#30340;&#37051;&#22495;&#21644;&#26368;&#30701;&#36335;&#24452;&#20013;&#24515;&#24615;&#24230;&#37327;&#65288;&#24230;&#12289;&#29305;&#24449;&#21521;&#37327;&#12289;&#20171;&#25968;&#21644;&#25509;&#36817;&#24230;&#65289;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#37327;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#26680;&#24515;-&#20013;&#38388;-&#22806;&#22260;&#65288;CIP&#65289;&#25351;&#25968;&#65292;&#20197;&#25429;&#25417;&#33410;&#28857;&#22312;&#32593;&#32476;&#20013;&#33021;&#22815;&#25198;&#28436;&#26680;&#24515;&#33410;&#28857;&#65288;&#20855;&#26377;&#20219;&#20309;&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#36739;&#39640;&#20540;&#30340;&#32593;&#32476;&#20013;&#24515;&#65289;&#21644;&#22806;&#22260;&#33410;&#28857;&#65288;&#20855;&#26377;&#20219;&#20309;&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#36739;&#20302;&#20540;&#30340;&#32593;&#32476;&#36793;&#32536;&#33410;&#28857;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#31243;&#24230;&#12290;&#25105;&#20204;&#23545;&#20013;&#24515;&#24615;&#24230;&#37327;&#25968;&#25454;&#38598;&#30340;&#36716;&#32622;&#30697;&#38453;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65288;&#22522;&#20110;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#24046;&#26059;&#36716;&#65289;&#65292;&#20551;&#35774;&#23384;&#22312;&#20004;&#20010;&#22240;&#23376;&#65288;&#26680;&#24515;&#21644;&#22806;&#22260;&#65289;&#26469;&#39537;&#21160;&#33410;&#28857;&#30456;&#23545;&#20110;&#20013;&#24515;&#24615;&#24230;&#37327;&#25152;&#20135;&#29983;&#30340;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#22810;&#26679;&#21270;&#30340;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform factor analysis on the raw data of the four major neighborhood and shortest paths-based centrality metrics (Degree, Eigenvector, Betweeenness and Closeness) and propose a novel quantitative measure called the Core-Intermediate-Peripheral (CIP) Index to capture the extent with which a node could play the role of a core node (nodes at the center of a network with larger values for any centrality metric) vis-a-vis a peripheral node (nodes that exist at the periphery of a network with lower values for any centrality metric). We conduct factor analysis (varimax-based rotation of the Eigenvectors) on the transpose matrix of the raw centrality metrics dataset, with the node ids as features, under the hypothesis that there are two factors (core and peripheral) that drive the values incurred by the nodes with respect to the centrality metrics. We test our approach on a diverse suite of 12 complex real-world networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19968;&#33268;&#24615;&#31574;&#30053;&#19982;Q-Learning &#65288;CPQL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20174;&#21453;&#21521;&#25193;&#25955;&#36712;&#36857;&#21040;&#26399;&#26395;&#31574;&#30053;&#30340;&#26144;&#23556;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#25351;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06343</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#31574;&#30053;&#25552;&#21319;&#36830;&#32493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Boosting Continuous Control with Consistency Policy. (arXiv:2310.06343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19968;&#33268;&#24615;&#31574;&#30053;&#19982;Q-Learning &#65288;CPQL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20174;&#21453;&#21521;&#25193;&#25955;&#36712;&#36857;&#21040;&#26399;&#26395;&#31574;&#30053;&#30340;&#26144;&#23556;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#25351;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#24378;&#22823;&#34920;&#36798;&#33021;&#21147;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#24102;&#26469;&#20102;&#20960;&#20010;&#25361;&#25112;&#65306;1&#65289;&#23545;&#22823;&#37327;&#25193;&#25955;&#27493;&#39588;&#30340;&#38656;&#27714;&#20351;&#24471;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#65307;2&#65289;&#22914;&#20309;&#25552;&#20379;&#20934;&#30830;&#25351;&#23548;&#20197;&#23454;&#29616;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#25913;&#36827;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#21463;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25928;&#29575;&#26041;&#27861;&#65292;&#31216;&#20026;&#19968;&#33268;&#24615;&#31574;&#30053;&#19982;Q-Learning&#65288;CPQL&#65289;&#65292;&#23427;&#36890;&#36807;&#21333;&#27493;&#20174;&#22122;&#22768;&#20013;&#23548;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24314;&#31435;&#20174;&#21453;&#21521;&#25193;&#25955;&#36712;&#36857;&#21040;&#26399;&#26395;&#31574;&#30053;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#21516;&#26102;&#35299;&#20915;&#20102;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#26102;&#30340;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#25351;&#23548;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CPQL&#21487;&#20197;&#36890;&#36807;&#20934;&#30830;&#25351;&#23548;&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement l
&lt;/p&gt;</description></item><item><title>Upcycled-FL&#26159;&#19968;&#31181;&#20943;&#23569;&#20449;&#24687;&#27844;&#28431;&#21644;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27599;&#20010;&#20598;&#25968;&#36845;&#20195;&#20013;&#24212;&#29992;&#19968;&#38454;&#36817;&#20284;&#65292;&#20351;&#24471;&#19968;&#21322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26356;&#26032;&#19981;&#20250;&#27844;&#28431;&#20449;&#24687;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2310.06341</link><description>&lt;p&gt;
&#20943;&#23569;&#20449;&#24687;&#27844;&#28431;&#21644;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Reduced Information Leakage and Computation. (arXiv:2310.06341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06341
&lt;/p&gt;
&lt;p&gt;
Upcycled-FL&#26159;&#19968;&#31181;&#20943;&#23569;&#20449;&#24687;&#27844;&#28431;&#21644;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27599;&#20010;&#20598;&#25968;&#36845;&#20195;&#20013;&#24212;&#29992;&#19968;&#38454;&#36817;&#20284;&#65292;&#20351;&#24471;&#19968;&#21322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26356;&#26032;&#19981;&#20250;&#27844;&#28431;&#20449;&#24687;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#20844;&#20849;&#27169;&#22411;&#12290;&#23613;&#31649;&#26412;&#22320;&#25968;&#25454;&#27809;&#26377;&#30452;&#25509;&#26292;&#38706;&#65292;&#20294;&#20173;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#20449;&#24687;&#21487;&#20197;&#20174;&#20013;&#38388;&#35745;&#31639;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30456;&#21516;&#25968;&#25454;&#22312;&#36845;&#20195;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#36825;&#31181;&#20449;&#24687;&#27844;&#28431;&#20250;&#19981;&#26029;&#31215;&#32047;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#24456;&#38590;&#24179;&#34913;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;Upcycled-FL&#65292;&#23427;&#22312;&#27599;&#20010;&#20598;&#25968;&#36845;&#20195;&#20013;&#37117;&#24212;&#29992;&#20102;&#19968;&#38454;&#36817;&#20284;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#19968;&#21322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26356;&#26032;&#19981;&#20250;&#36896;&#25104;&#20449;&#24687;&#27844;&#28431;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Upcycled-FL&#30340;&#25910;&#25947;&#65288;&#36895;&#29575;&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#28982;&#21518;&#24212;&#29992;&#25200;&#21160;&#26426;&#21046;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36229;&#22768;&#22270;&#20687;&#20013;&#32467;&#33410;&#24322;&#36136;&#22806;&#35266;&#23548;&#33268;&#30340;&#38590;&#20197;&#36827;&#34892;&#36880;&#20010;&#32467;&#33410;&#26816;&#26597;&#30340;&#38382;&#39064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#33410;&#37325;&#26032;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;&#25968;&#30334;&#20010;&#20083;&#33146;&#36229;&#22768;&#35270;&#39057;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06339</link><description>&lt;p&gt;
&#33258;&#21160;&#35782;&#21035;&#21644;&#21306;&#20998;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#32467;&#33410;&#65292;&#20197;&#20415;&#36827;&#34892;&#36880;&#32467;&#33410;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination. (arXiv:2310.06339v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36229;&#22768;&#22270;&#20687;&#20013;&#32467;&#33410;&#24322;&#36136;&#22806;&#35266;&#23548;&#33268;&#30340;&#38590;&#20197;&#36827;&#34892;&#36880;&#20010;&#32467;&#33410;&#26816;&#26597;&#30340;&#38382;&#39064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#33410;&#37325;&#26032;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;&#25968;&#30334;&#20010;&#20083;&#33146;&#36229;&#22768;&#35270;&#39057;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#26159;&#20581;&#24247;&#31579;&#26597;&#20013;&#37325;&#35201;&#30340;&#35786;&#26029;&#25216;&#26415;&#65292;&#20855;&#26377;&#26080;&#21019;&#12289;&#32463;&#27982;&#12289;&#26080;&#36752;&#23556;&#31561;&#20248;&#28857;&#65292;&#22240;&#27492;&#22312;&#32467;&#33410;&#30340;&#35786;&#26029;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36229;&#22768;&#22270;&#20687;&#20013;&#65292;&#21333;&#20010;&#32467;&#33410;&#22312;&#19981;&#21516;&#30340;&#20999;&#38754;&#35270;&#22270;&#19979;&#21487;&#33021;&#21576;&#29616;&#20986;&#24322;&#36136;&#30340;&#22806;&#35266;&#65292;&#36825;&#20351;&#24471;&#36880;&#20010;&#32467;&#33410;&#26816;&#26597;&#21464;&#24471;&#22256;&#38590;&#12290;&#36229;&#22768;&#26816;&#26597;&#36890;&#24120;&#20381;&#36182;&#20110;&#36229;&#22768;&#24072;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#20020;&#24202;&#32463;&#39564;&#12290;&#36229;&#22768;&#24072;&#36890;&#24120;&#36890;&#36807;&#26816;&#26597;&#32467;&#33410;&#29305;&#24449;&#21644;&#21608;&#22260;&#32467;&#26500;&#65288;&#22914;&#33146;&#20307;&#21644;&#23548;&#31649;&#65289;&#26469;&#21306;&#20998;&#19981;&#21516;&#30340;&#32467;&#33410;&#65292;&#36825;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#25968;&#30334;&#20010;&#20083;&#33146;&#36229;&#22768;&#35270;&#39057;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32467;&#33410;&#37325;&#26032;&#35782;&#21035;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25552;&#21462;&#22120;&#65292;&#21487;&#20197;&#20174;&#36755;&#20837;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#21450;&#23454;&#26102;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#29305;&#24449;&#21521;&#37327;&#25353;&#32467;&#33410;&#20998;&#32452;&#12290;&#35813;&#31995;&#32479;&#33719;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound is a vital diagnostic technique in health screening, with the advantages of non-invasive, cost-effective, and radiation free, and therefore is widely applied in the diagnosis of nodules. However, it relies heavily on the expertise and clinical experience of the sonographer. In ultrasound images, a single nodule might present heterogeneous appearances in different cross-sectional views which makes it hard to perform per-nodule examination. Sonographers usually discriminate different nodules by examining the nodule features and the surrounding structures like gland and duct, which is cumbersome and time-consuming. To address this problem, we collected hundreds of breast ultrasound videos and built a nodule reidentification system that consists of two parts: an extractor based on the deep learning model that can extract feature vectors from the input video clips and a real-time clustering algorithm that automatically groups feature vectors by nodules. The system obtains satisfa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23398;&#20064;&#24050;&#30693;&#39592;&#26550;&#30340;&#26377;&#30028;&#24230;&#22810;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#36825;&#23545;&#20110;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.06333</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#24050;&#30693;&#39592;&#26550;&#30340;&#26377;&#30028;&#24230;&#22810;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning bounded-degree polytrees with known skeleton. (arXiv:2310.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23398;&#20064;&#24050;&#30693;&#39592;&#26550;&#30340;&#26377;&#30028;&#24230;&#22810;&#26641;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#36825;&#23545;&#20110;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#30340;&#23398;&#20064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#39640;&#32500;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#31867;&#20016;&#23500;&#30340;&#22810;&#26641;&#65288;polytrees&#65289;&#8212;&#8212;&#26377;&#30028;&#24230;&#22810;&#26641;&#65292;&#24314;&#31435;&#20102;&#39640;&#25928;&#36866;&#24403;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#26377;&#30028;&#24230;&#22810;&#26641;&#26159;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#23376;&#31867;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#22270;&#27169;&#22411;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;Bhattacharyya&#31561;&#20154;&#65288;2021&#65289;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#22312;&#24050;&#30693;&#26080;&#21521;&#22270;&#65288;&#39592;&#26550;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;1-&#22810;&#26641;&#24674;&#22797;&#20102;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#20182;&#20204;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20869;&#23398;&#20064;&#20219;&#20309;&#26377;&#30028;&#24230;&#30340;$d$-&#22810;&#26641;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#19982;&#20449;&#24687;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19979;&#30028;&#32467;&#21512;&#36215;&#26469;&#65292;&#34920;&#26126;&#23545;&#32500;&#24230;&#21644;&#30446;&#26631;&#31934;&#24230;&#21442;&#25968;&#30340;&#20381;&#36182;&#20960;&#20046;&#26159;&#32039;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#65288;ARC&#65289;&#26469;&#23450;&#20041;&#36866;&#24403;&#30340;&#23545;&#20934;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#22312;WiFi&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;CSI&#25968;&#25454;&#19978;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06328</link><description>&lt;p&gt;
&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#23450;&#20041;CSI&#25968;&#25454;&#30340;&#23545;&#20934;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Exploit the antenna response consistency to define the alignment criteria for CSI data. (arXiv:2310.06328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#65288;ARC&#65289;&#26469;&#23450;&#20041;&#36866;&#24403;&#30340;&#23545;&#20934;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#22312;WiFi&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;CSI&#25968;&#25454;&#19978;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#29992;&#20110;&#22522;&#20110;WiFi&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30001;&#20110;&#33021;&#22815;&#35299;&#20915;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#32780;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;SSL&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#27604;&#23398;&#20064;&#65292;&#31227;&#26893;&#21040;CSI&#25968;&#25454;&#19978;&#24448;&#24448;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#23545;&#20934;&#26631;&#20934;&#19981;&#24403;&#65292;&#36825;&#30772;&#22351;&#20102;&#29305;&#24449;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;``Anetenna Response Consistency (ARC)''&#20316;&#20026;&#23450;&#20041;&#21512;&#36866;&#23545;&#20934;&#26631;&#20934;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;ARC&#30340;&#35774;&#35745;&#22312;&#20445;&#30041;&#36755;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#23545;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20174;CSI&#25968;&#25454;&#32467;&#26500;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;ARC&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26368;&#20248;&#35299;&#23548;&#33268;&#20102;&#20174;&#36755;&#20837;CSI&#25968;&#25454;&#21040;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#21160;&#20316;&#21521;&#37327;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \textbf{A}netenna \textbf{R}esponse \textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#65292;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;0.427&#30340;&#24471;&#20998;&#65292;&#22312;Kaggle&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#31454;&#36187;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;</title><link>http://arxiv.org/abs/2310.06322</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#19977;&#31181;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Predicting Three Types of Freezing of Gait Events Using Deep Learning Models. (arXiv:2310.06322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#65292;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;0.427&#30340;&#24471;&#20998;&#65292;&#22312;Kaggle&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#31454;&#36187;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20923;&#32467;&#27493;&#24577;&#26159;&#24085;&#37329;&#26862;&#30149;&#30340;&#30151;&#29366;&#20043;&#19968;&#65292;&#24739;&#32773;&#22312;&#34892;&#36208;&#26102;&#20250;&#21608;&#26399;&#24615;&#22320;&#20986;&#29616;&#19981;&#33021;&#36808;&#27493;&#25110;&#36716;&#36523;&#30340;&#24773;&#20917;&#12290;&#34429;&#28982;&#21307;&#23398;&#19987;&#23478;&#24050;&#32463;&#21457;&#29616;&#20102;&#22810;&#31181;&#24341;&#21457;&#21644;&#32531;&#35299;&#20923;&#32467;&#27493;&#24577;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#28508;&#22312;&#21407;&#22240;&#21644;&#39044;&#27979;&#27169;&#22411;&#20173;&#22312;&#30740;&#31350;&#20013;&#12290;&#30446;&#21069;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#27169;&#22411;&#21487;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#39640;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#23545;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#31867;&#22411;&#30340;&#20855;&#20307;&#35828;&#26126;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#21152;&#19978;&#21452;&#21521;LSTM&#23618;&#21644;&#19981;&#21516;&#29305;&#24449;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20923;&#32467;&#27493;&#24577;&#20107;&#20214;&#12290;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;0.427&#30340;&#24471;&#20998;&#65292;&#22312;&#30001;&#36808;&#20811;&#23572;&#183;J&#183;&#31119;&#20811;&#26031;&#22522;&#37329;&#20250;&#20027;&#21150;&#30340;Kaggle&#20923;&#32467;&#27493;&#24577;&#39044;&#27979;&#31454;&#36187;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#24847;&#35782;&#21040;&#20102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Freezing of gait is a Parkinson's Disease symptom that episodically inflicts a patient with the inability to step or turn while walking. While medical experts have discovered various triggers and alleviating actions for freezing of gait, the underlying causes and prediction models are still being explored today. Current freezing of gait prediction models that utilize machine learning achieve high sensitivity and specificity in freezing of gait predictions based on time-series data; however, these models lack specifications on the type of freezing of gait events. We develop various deep learning models using the transformer encoder architecture plus Bidirectional LSTM layers and different feature sets to predict the three different types of freezing of gait events. The best performing model achieves a score of 0.427 on testing data, which would rank top 5 in Kaggle's Freezing of Gait prediction competition, hosted by THE MICHAEL J. FOX FOUNDATION. However, we also recognize overfitting 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27169;&#25311;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#30340;&#20117;&#25511;&#21046;&#30340;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#20004;&#30456;&#27969;&#21160;&#12290;&#32593;&#32476;&#36890;&#36807;&#26102;&#38388;&#21464;&#21270;&#25511;&#21046;&#23545;&#35299;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24314;&#31435;&#20102;&#25511;&#21046;&#21040;&#29366;&#24577;&#30340;&#22238;&#24402;&#12290;&#32593;&#32476;&#32467;&#26500;&#37319;&#29992;&#20004;&#20010;&#24179;&#34892;&#30340;U-Net&#32467;&#26500;&#65292;&#36755;&#20837;&#20026;&#20117;&#25511;&#21046;&#65292;&#36755;&#20986;&#20026;&#31995;&#32479;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.06319</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27169;&#25311;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#25511;&#21046;&#30340;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls. (arXiv:2310.06319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27169;&#25311;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#30340;&#20117;&#25511;&#21046;&#30340;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#20004;&#30456;&#27969;&#21160;&#12290;&#32593;&#32476;&#36890;&#36807;&#26102;&#38388;&#21464;&#21270;&#25511;&#21046;&#23545;&#35299;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24314;&#31435;&#20102;&#25511;&#21046;&#21040;&#29366;&#24577;&#30340;&#22238;&#24402;&#12290;&#32593;&#32476;&#32467;&#26500;&#37319;&#29992;&#20004;&#20010;&#24179;&#34892;&#30340;U-Net&#32467;&#26500;&#65292;&#36755;&#20837;&#20026;&#20117;&#25511;&#21046;&#65292;&#36755;&#20986;&#20026;&#31995;&#32479;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27169;&#25311;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#30340;&#20117;&#25511;&#21046;&#30340;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#20004;&#30456;&#27969;&#21160;&#12290;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;PICNN&#37117;&#26159;&#38024;&#23545;&#21442;&#25968;&#21040;&#29366;&#24577;&#26144;&#23556;&#36827;&#34892;&#24037;&#20316;&#30340;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#36890;&#36807;&#26102;&#38388;&#21464;&#21270;&#25511;&#21046;&#23545;&#35299;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24314;&#31435;&#20102;&#25511;&#21046;&#21040;&#29366;&#24577;&#30340;&#22238;&#24402;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#26377;&#38480;&#20307;&#31215;&#26041;&#26696;&#23545;&#27969;&#21160;&#26041;&#31243;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#21046;&#23450;&#31526;&#21512;&#36136;&#37327;&#23432;&#24658;&#23450;&#24459;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;Neumann&#36793;&#30028;&#26465;&#20214;&#26080;&#32541;&#22320;&#34701;&#20837;&#21322;&#31163;&#25955;&#21270;&#26041;&#31243;&#20013;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#12290;&#32593;&#32476;&#32467;&#26500;&#30001;&#20004;&#20010;&#24179;&#34892;&#30340;U-Net&#32467;&#26500;&#32452;&#25104;&#65292;&#32593;&#32476;&#36755;&#20837;&#20026;&#20117;&#25511;&#21046;&#65292;&#36755;&#20986;&#20026;&#31995;&#32479;&#29366;&#24577;&#12290;&#20026;&#20102;&#25429;&#25417;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#26102;&#21464;&#20851;&#31995;&#65292;&#32593;&#32476;&#34987;&#35774;&#35745;&#25104;&#27169;&#25311;&#31163;&#25955;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#26041;&#31243;&#12290;&#25105;&#20204;&#36880;&#27493;&#35757;&#32451;&#32593;&#32476;&#30340;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#27833;&#21387;&#21644;...
&lt;/p&gt;
&lt;p&gt;
A physics-informed convolutional neural network is proposed to simulate two phase flow in porous media with time-varying well controls. While most of PICNNs in existing literatures worked on parameter-to-state mapping, our proposed network parameterizes the solution with time-varying controls to establish a control-to-state regression. Firstly, finite volume scheme is adopted to discretize flow equations and formulate loss function that respects mass conservation laws. Neumann boundary conditions are seamlessly incorporated into the semi-discretized equations so no additional loss term is needed. The network architecture comprises two parallel U-Net structures, with network inputs being well controls and outputs being the system states. To capture the time-dependent relationship between inputs and outputs, the network is well designed to mimic discretized state space equations. We train the network progressively for every timestep, enabling it to simultaneously predict oil pressure and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#25513;&#34109;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;(MIM)&#65292;&#36890;&#36807;&#20462;&#22797;&#25513;&#34109;&#30340; Mpox &#22270;&#20687;&#26469;&#23398;&#20064; Mpox &#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#20462;&#22797;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#21028;&#26029;&#36755;&#20837;&#26159;&#21542;&#23646;&#20110; Mpox&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#24178;&#25200;&#12289;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#38750; Mpox &#22270;&#20687;&#20197;&#21450;&#26080;&#27861;&#26816;&#27979;&#24322;&#24120;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06318</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25513;&#34109;&#22270;&#20687;&#20462;&#22797;&#22312;&#40065;&#26834;&#26816;&#27979; Mpox &#21644;&#38750; Mpox &#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Masked Image Inpainting for Robust Detection of Mpox and Non-Mpox. (arXiv:2310.06318v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#25513;&#34109;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;(MIM)&#65292;&#36890;&#36807;&#20462;&#22797;&#25513;&#34109;&#30340; Mpox &#22270;&#20687;&#26469;&#23398;&#20064; Mpox &#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#20462;&#22797;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#21028;&#26029;&#36755;&#20837;&#26159;&#21542;&#23646;&#20110; Mpox&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#24178;&#25200;&#12289;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#38750; Mpox &#22270;&#20687;&#20197;&#21450;&#26080;&#27861;&#26816;&#27979;&#24322;&#24120;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340; Mpox &#35786;&#26029;&#25216;&#26415;&#65292;Mpox &#30149;&#20363;&#19981;&#26029;&#22686;&#21152;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979; Mpox &#21644;&#38750; Mpox &#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#32467;&#26524;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#30340;&#24178;&#25200;&#65292;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#38750; Mpox &#22270;&#20687;&#65292;&#24182;&#19988;&#26080;&#27861;&#26816;&#27979;&#24322;&#24120;&#36755;&#20837;&#12290;&#36825;&#20123;&#32570;&#28857;&#20351;&#24471;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25513;&#34109;&#12289;&#20462;&#22797;&#21644;&#27979;&#37327;&#8221;&#65288;MIM&#65289;&#26041;&#27861;&#12290;&#22312; MIM &#30340;&#27969;&#31243;&#20013;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20165;&#36890;&#36807;&#20462;&#22797;&#25513;&#34109;&#30340; Mpox &#22270;&#20687;&#26469;&#23398;&#20064; Mpox &#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;MIM&#36890;&#36807;&#27979;&#37327;&#20462;&#22797;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#36755;&#20837;&#26159;&#21542;&#23646;&#20110; Mpox&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#30001;&#20110; MIM &#20165;&#27169;&#25311; Mpox &#22270;&#20687;&#65292;&#23427;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26080;&#27861;&#20934;&#30830;&#20462;&#22797;&#38750; Mpox &#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the lack of efficient mpox diagnostic technology, mpox cases continue to increase. Recently, the great potential of deep learning models in detecting mpox and non-mpox has been proven. However, existing models learn image representations via image classification, which results in they may be easily susceptible to interference from real-world noise, require diverse non-mpox images, and fail to detect abnormal input. These drawbacks make classification models inapplicable in real-world settings. To address these challenges, we propose "Mask, Inpainting, and Measure" (MIM). In MIM's pipeline, a generative adversarial network only learns mpox image representations by inpainting the masked mpox images. Then, MIM determines whether the input belongs to mpox by measuring the similarity between the inpainted image and the original image. The underlying intuition is that since MIM solely models mpox images, it struggles to accurately inpaint non-mpox images in real-world settings. Withou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06312</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discovering Mixtures of Structural Causal Models from Time Series Data. (arXiv:2310.06312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06312
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#12289;&#27668;&#20505;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#20195;&#25216;&#26415;&#21487;&#20197;&#22788;&#29702;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#28789;&#27963;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21270;&#20551;&#35774;&#65292;&#21363;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25918;&#26494;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#20174;&#26469;&#28304;&#20110;&#19981;&#21516;&#22240;&#26524;&#27169;&#22411;&#28151;&#21512;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#12290;&#25105;&#20204;&#25512;&#26029;&#20102;&#28508;&#22312;&#30340;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#31471;&#23545;&#31471;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26368;&#22823;&#21270;&#20102;&#25968;&#25454;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#24403;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21046;&#36896;&#19994;&#30340;AI&#23413;&#21270;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26356;&#26032;AI&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#25345;&#32493;&#25913;&#36827;&#20915;&#31574;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;CBEAL&#30340;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22320;&#25351;&#23548;&#25968;&#25454;&#33719;&#21462;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#26524;&#30340;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.06306</link><description>&lt;p&gt;
&#38754;&#21521;&#21046;&#36896;&#19994;&#30340;AI&#23413;&#21270;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing. (arXiv:2310.06306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21046;&#36896;&#19994;&#30340;AI&#23413;&#21270;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26356;&#26032;AI&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#25345;&#32493;&#25913;&#36827;&#20915;&#31574;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;CBEAL&#30340;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22320;&#25351;&#23548;&#25968;&#25454;&#33719;&#21462;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#26524;&#30340;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#24863;&#30693;&#21644;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#38459;&#30861;&#20102;&#31163;&#32447;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;AI&#27169;&#22411;&#20250;&#36890;&#36807;&#27969;&#24335;&#25968;&#25454;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#20197;&#25345;&#32493;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32422;&#26463;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#36873;&#25321;&#29992;&#20110;&#26356;&#26032;&#30340;&#20248;&#36136;&#27969;&#24335;&#26679;&#26412;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#25991;&#29486;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#20851;&#27880;&#19981;&#36275;&#25110;&#36807;&#24230;&#34920;&#31034;&#30340;&#21306;&#22495;&#26469;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#21046;&#36896;&#32972;&#26223;&#19979;&#24179;&#34913;&#36825;&#20123;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;AI&#23398;&#20064;&#21040;&#30340;&#19968;&#20123;&#33719;&#21462;&#20934;&#21017;&#21487;&#20197;&#21160;&#24577;&#36866;&#24212;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#22987;&#32456;&#22788;&#29702;&#39057;&#32321;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;CBEAL&#65292;&#19987;&#38376;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#20195;&#29702;&#30340;&#26435;&#37325;&#26681;&#25454;&#20915;&#31574;&#26377;&#25928;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;CBEAL&#21487;&#20197;&#20248;&#21270;&#22320;&#25351;&#23548;&#25968;&#25454;&#33719;&#21462;&#65292;&#23454;&#29616;&#25968;&#25454;&#25928;&#26524;&#30340;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online sensing and computational resources in Industrial Cyber-physical Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data quality, such as imbalanced classes, hinder AI models trained offline. To address this, AI models are updated online with streaming data for continuous improvement. Supervised learning models, however, face challenges in selecting quality streaming samples for updates due to annotation constraints. Active learning methods in literature offer solutions by focusing on under-represented or well-represented regions. Balancing these strategies in changing manufacturing contexts is challenging. Some acquisition criteria learned by AI dynamically adapt but may not consistently handle frequent changes. We introduce an ensemble active learning method, CBEAL, employing active learning agents specifically for exploration or exploitation. Weights of agents are adjusted based on agent decision effectiveness. CBEAL optimally guides data acquisition, minim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#30740;&#31350;&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#65292;&#22312;&#20004;&#20010;&#38544;&#34255;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#27491;&#21017;&#30340;$k$-gons&#26159;&#20020;&#30028;&#28857;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#29702;&#35770;&#34920;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20915;&#23450;&#20102;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#30456;&#21464;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20063;&#20915;&#23450;&#20102;SGD&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;SGD&#23398;&#20064;&#36712;&#36857;&#21463;&#39034;&#24207;&#23398;&#20064;&#26426;&#21046;&#24433;&#21709;&#30340;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.06301</link><description>&lt;p&gt;
&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#20013;&#30340;&#21160;&#21147;&#23398;&#19982;&#36125;&#21494;&#26031;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition. (arXiv:2310.06301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#30740;&#31350;&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#65292;&#22312;&#20004;&#20010;&#38544;&#34255;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#27491;&#21017;&#30340;$k$-gons&#26159;&#20020;&#30028;&#28857;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#29702;&#35770;&#34920;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20915;&#23450;&#20102;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#30456;&#21464;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#20020;&#30028;&#28857;&#20063;&#20915;&#23450;&#20102;SGD&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;SGD&#23398;&#20064;&#36712;&#36857;&#21463;&#39034;&#24207;&#23398;&#20064;&#26426;&#21046;&#24433;&#21709;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#65288;SLT&#65289;&#30740;&#31350;&#36229;&#21472;&#21152;&#30340;&#29609;&#20855;&#27169;&#22411;&#65288;TMS&#65289;&#20013;&#30340;&#30456;&#21464;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#29702;&#35770;&#25439;&#22833;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#24182;&#22312;&#20004;&#20010;&#38544;&#34255;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#65292;&#27491;&#21017;&#30340;$k$-gons&#26159;&#20020;&#30028;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#29702;&#35770;&#65292;&#34920;&#26126;&#36825;&#20123;$k$-gons&#30340;&#26412;&#22320;&#23398;&#20064;&#31995;&#25968;&#65288;&#20960;&#20309;&#19981;&#21464;&#37327;&#65289;&#20915;&#23450;&#20102;&#36125;&#21494;&#26031;&#21518;&#39564;&#20316;&#20026;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#20989;&#25968;&#30340;&#30456;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#21516;&#26679;&#30340;$k$-gon&#20020;&#30028;&#28857;&#20063;&#20915;&#23450;&#20102;SGD&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#24471;&#20986;&#30340;&#32467;&#35770;&#25903;&#25345;&#20102;SGD&#23398;&#20064;&#36712;&#36857;&#21463;&#39034;&#24207;&#23398;&#20064;&#26426;&#21046;&#24433;&#21709;&#30340;&#29468;&#24819;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;TMS&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;SGD&#36824;&#26159;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#20174;&#39640;&#25439;&#22833;&#21644;&#20302;&#22797;&#26434;&#24615;&#30340;&#21306;&#22495;&#21521;&#20302;&#25439;&#22833;&#21644;&#39640;&#22797;&#26434;&#24615;&#30340;&#21306;&#22495;&#30340;&#26053;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Gem5Pred&#65292;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;Gem5&#27169;&#25311;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#25351;&#20196;&#31867;&#22411;&#23545;&#27169;&#25311;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#22238;&#24402;&#27169;&#22411;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;0.546&#65292;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;0.696&#12290;&#36825;&#20123;&#27169;&#22411;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2310.06290</link><description>&lt;p&gt;
Gem5Pred: Gem5&#27169;&#25311;&#26102;&#38388;&#30340;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gem5Pred: Predictive Approaches For Gem5 Simulation Time. (arXiv:2310.06290v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Gem5Pred&#65292;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;Gem5&#27169;&#25311;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#25351;&#20196;&#31867;&#22411;&#23545;&#27169;&#25311;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#22238;&#24402;&#27169;&#22411;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;0.546&#65292;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;0.696&#12290;&#36825;&#20123;&#27169;&#22411;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gem5&#26159;&#19968;&#20010;&#24320;&#28304;&#12289;&#28789;&#27963;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#27169;&#25311;&#22120;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30828;&#20214;&#27169;&#25311;&#20013;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;Gem5&#27169;&#25311;&#31243;&#24207;&#36890;&#24120;&#32791;&#26102;&#36739;&#38271;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#27169;&#25311;&#26102;&#38388;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#23384;&#22312;&#12290;&#38024;&#23545;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#21019;&#24314;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#23545;&#19981;&#21516;&#25351;&#20196;&#31867;&#22411;&#23545;Gem5&#27169;&#25311;&#26102;&#38388;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#20511;&#21161;CodeBERT&#25191;&#34892;&#22522;&#20110;&#24320;&#21457;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#20248;&#36234;&#30340;&#22238;&#24402;&#27169;&#22411;&#23454;&#29616;&#20102;0.546&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#65292;&#32780;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#20998;&#31867;&#27169;&#22411;&#35760;&#24405;&#20102;0.696&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#26410;&#26469;&#23545;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#65292;&#21516;&#26102;&#20063;&#20316;&#20026;&#21518;&#32493;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gem5, an open-source, flexible, and cost-effective simulator, is widely recognized and utilized in both academic and industry fields for hardware simulation. However, the typically time-consuming nature of simulating programs on Gem5 underscores the need for a predictive model that can estimate simulation time. As of now, no such dataset or model exists. In response to this gap, this paper makes a novel contribution by introducing a unique dataset specifically created for this purpose. We also conducted analysis of the effects of different instruction types on the simulation time in Gem5. After this, we employ three distinct models leveraging CodeBERT to execute the prediction task based on the developed dataset. Our superior regression model achieves a Mean Absolute Error (MAE) of 0.546, while our top-performing classification model records an Accuracy of 0.696. Our models establish a foundation for future investigations on this topic, serving as benchmarks against which subsequent mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#22909;&#12289;&#26356;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20272;&#35745;&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#39640;&#26031;&#21327;&#26041;&#24046;&#30340;&#35889;&#35823;&#24046;&#21644;&#26377;&#30028;$k$&#38454;&#30697;&#30340;&#37325;&#23614;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.06289</link><description>&lt;p&gt;
&#26356;&#22909;&#12289;&#26356;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20272;&#35745;&#30340;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Better and Simpler Lower Bounds for Differentially Private Statistical Estimation. (arXiv:2310.06289v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#22909;&#12289;&#26356;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20272;&#35745;&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#39640;&#26031;&#21327;&#26041;&#24046;&#30340;&#35889;&#35823;&#24046;&#21644;&#26377;&#30028;$k$&#38454;&#30697;&#30340;&#37325;&#23614;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20004;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#39640;&#32500;&#31169;&#26377;&#20272;&#35745;&#20219;&#21153;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#19979;&#30028;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#29992;&#20110;&#20272;&#35745;&#39640;&#26031;&#21327;&#26041;&#24046;&#30340;&#35889;&#35823;&#24046;$\alpha$&#65292;&#20219;&#24847;$\alpha \le O(1)$&#65292;&#38656;&#35201;$\tilde{\Omega} \left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$&#20010;&#26679;&#26412;&#65292;&#36825;&#26159;&#32039;&#20945;&#30340;&#65292;&#20165;&#24046;&#23545;&#25968;&#22240;&#23376;&#12290;&#36825;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;$\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$&#26102;&#24314;&#31435;&#30340;&#32467;&#26524;&#35201;&#22909;&#65292;&#24182;&#19988;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#31616;&#21333;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#26377;&#30028;$k$&#38454;&#30697;&#30340;&#37325;&#23614;&#20998;&#24067;&#30340;&#22343;&#20540;&#20272;&#35745;&#65292;&#38656;&#35201;$\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$&#20010;&#26679;&#26412;&#12290;&#36825;&#19982;&#24050;&#30693;&#30340;&#19978;&#30028;&#30456;&#21563;&#21512;&#65292;&#24182;&#25913;&#36827;&#20102;&#27492;&#38382;&#39064;&#30340;&#24050;&#30693;&#26368;&#20339;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#20165;&#36866;&#29992;&#20110;&#32431;&#31929;&#30340;&#24046;&#20998;&#38544;&#31169;&#25110;$k = 2$&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36981;&#24490;&#25351;&#32441;&#26041;&#24335;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide improved lower bounds for two well-known high-dimensional private estimation tasks. First, we prove that for estimating the covariance of a Gaussian up to spectral error $\alpha$ with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ samples for any $\alpha \le O(1)$, which is tight up to logarithmic factors. This improves over previous work which established this for $\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$, and is also simpler than previous work. Next, we prove that for estimating the mean of a heavy-tailed distribution with bounded $k$th moments with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ samples. This matches known upper bounds and improves over the best known lower bound for this problem, which only hold for pure differential privacy, or when $k = 2$. Our techniques follow the method of fingerpr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06286</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#34892;&#20026;&#25233;&#21046;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#31216;&#20026;&#34394;&#25311;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65288;DAQ&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35843;&#33410;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#36890;&#36807;&#34394;&#25311;&#29609;&#23478;&#65292;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25152;&#25552;&#20986;&#30340;DAQ&#23558;&#20960;&#31181;Q&#23398;&#20064;&#30340;&#21464;&#20307;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#25511;&#21046;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#20363;&#22914;maxmin Q&#23398;&#20064;&#21644;minmax Q&#23398;&#20064;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#12290;&#36890;&#36807;&#34394;&#25311;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;DAQ&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65292;&#20174;&#32508;&#21512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;DAQ&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#23454;&#35777;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;DAQ&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
&lt;/p&gt;</description></item><item><title>MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.06282</link><description>&lt;p&gt;
MuseChat:&#19968;&#31181;&#35270;&#39057;&#23545;&#35805;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06282
&lt;/p&gt;
&lt;p&gt;
MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MuseChat&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#20010;&#29420;&#29305;&#30340;&#24179;&#21488;&#19981;&#20165;&#25552;&#20379;&#20114;&#21160;&#29992;&#25143;&#21442;&#19982;&#65292;&#36824;&#20026;&#36755;&#20837;&#30340;&#35270;&#39057;&#25552;&#20379;&#20102;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25913;&#36827;&#21644;&#20010;&#24615;&#21270;&#20182;&#20204;&#30340;&#38899;&#20048;&#36873;&#25321;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20197;&#21069;&#30340;&#31995;&#32479;&#20027;&#35201;&#24378;&#35843;&#20869;&#23481;&#30340;&#20860;&#23481;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#20307;&#20559;&#22909;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#20363;&#22914;&#65292;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#25552;&#20379;&#22522;&#26412;&#30340;&#38899;&#20048;-&#35270;&#39057;&#37197;&#23545;&#65292;&#25110;&#32773;&#24102;&#26377;&#38899;&#20048;&#25551;&#36848;&#30340;&#37197;&#23545;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35805;&#21512;&#25104;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#36718;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#20013;&#65292;&#29992;&#25143;&#25552;&#20132;&#19968;&#20010;&#35270;&#39057;&#32473;&#31995;&#32479;&#65292;&#31995;&#32479;&#20250;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#38468;&#24102;&#35299;&#37322;&#12290;&#20043;&#21518;&#65292;&#29992;&#25143;&#20250;&#34920;&#36798;&#20182;&#20204;&#23545;&#38899;&#20048;&#30340;&#20559;&#22909;&#65292;&#31995;&#32479;&#20250;&#21576;&#29616;&#19968;&#20010;&#25913;&#36827;&#21518;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23433;&#20840;&#24615;&#30340;&#24895;&#26223;&#65292;&#20197;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06278</link><description>&lt;p&gt;
BC4LLM&#65306;&#24403;&#21306;&#22359;&#38142;&#36935;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models. (arXiv:2310.06278v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23433;&#20840;&#24615;&#30340;&#24895;&#26223;&#65292;&#20197;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27491;&#22312;&#37325;&#26032;&#22609;&#36896;&#31038;&#20250;&#30340;&#29983;&#20135;&#26041;&#24335;&#21644;&#29983;&#20135;&#21147;&#65292;&#24182;&#25913;&#21464;&#31185;&#23398;&#30740;&#31350;&#30340;&#33539; paradigm&#12290;&#20854;&#20013;&#65292;&#20197;ChatGPT&#20026;&#20195;&#34920;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#24418;&#24335;&#26381;&#21153;&#20110;&#20154;&#20204;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#21672;&#35810;&#12289;&#21307;&#30103;&#21644;&#25945;&#32946;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#20445;&#35777;AIGC&#23398;&#20064;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;AI&#35757;&#32451;&#20013;&#20063;&#23384;&#22312;&#38544;&#24739;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;LLMs&#29983;&#25104;&#30340;&#20869;&#23481;&#24456;&#38590;&#35782;&#21035;&#21644;&#36861;&#36394;&#65292;&#38590;&#20197;&#36328;&#24179;&#21488;&#30456;&#20114;&#35748;&#21487;&#12290;&#22312;&#20197;LLMs&#20026;&#21160;&#21147;&#30340;AI&#26102;&#20195;&#21363;&#23558;&#21040;&#26469;&#20043;&#38469;&#65292;&#19978;&#36848;&#20449;&#24687;&#23433;&#20840;&#38382;&#39064;&#23558;&#34987;&#26080;&#38480;&#25918;&#22823;&#65292;&#24433;&#21709;&#27599;&#20010;&#20154;&#30340;&#29983;&#27963;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#20026;LLMs&#36171;&#20104;&#21331;&#36234;&#30340;&#23433;&#20840;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06272</link><description>&lt;p&gt;
&#35753;&#27169;&#22411;&#35828;&#23494;&#25991;: &#36890;&#36807;&#23884;&#20837;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#35752;&#35770;&#21644;&#36777;&#35770;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30001;&#20110;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#32780;&#25104;&#20026;&#26126;&#26174;&#30340;&#20132;&#27969;&#36873;&#25321;&#65292;&#20294;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26102;&#38656;&#35201;&#36827;&#34892;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#21487;&#33021;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#19968;&#20010;&#26631;&#35760;&#26469;&#20195;&#34920;&#27169;&#22411;&#22312;&#25972;&#20010;&#35789;&#27719;&#34920;&#20013;&#30340;&#20449;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#65288;&#36890;&#36807;&#23884;&#20837;&#34920;&#31034;&#36827;&#34892;&#20132;&#27969;&#30340;&#32593;&#32476;&#27169;&#22411;&#21327;&#35758;&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;LLMs&#20013;&#21435;&#38500;&#20102;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#23427;&#20204;&#36890;&#36807;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#30340;&#26399;&#26395;&#26469;&#20256;&#36798;&#23427;&#20204;&#30340;&#20449;&#24565;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20559;&#31163;&#33258;&#28982;&#35821;&#35328;&#65292;CIPHER&#22312;&#19981;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#20043;&#38388;&#30340;&#23618;&#27425;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#32570;&#20047;&#25506;&#32034;&#25152;&#23548;&#33268;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#21644;&#26368;&#22823;&#21270;&#20445;&#23432;&#20272;&#35745;&#20540;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06268</link><description>&lt;p&gt;
&#26377;&#38480;&#25506;&#32034;&#26465;&#20214;&#19979;&#30340;&#21452;&#23618;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bi-Level Offline Policy Optimization with Limited Exploration. (arXiv:2310.06268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#20043;&#38388;&#30340;&#23618;&#27425;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#32570;&#20047;&#25506;&#32034;&#25152;&#23548;&#33268;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#21644;&#26368;&#22823;&#21270;&#20445;&#23432;&#20272;&#35745;&#20540;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24050;&#30693;&#25968;&#25454;&#38598;&#23398;&#20064;&#33391;&#22909;&#31574;&#30053;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#32570;&#20047;&#36275;&#22815;&#30340;&#25506;&#32034;&#65292;&#23548;&#33268;&#20102;&#20998;&#24067;&#20559;&#31227;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32467;&#26500;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#27169;&#25311;&#20102;&#31574;&#30053;&#65288;&#19978;&#23618;&#65289;&#21644;&#20540;&#20989;&#25968;&#65288;&#19979;&#23618;&#65289;&#20043;&#38388;&#30340;&#23618;&#27425;&#24615;&#20132;&#20114;&#12290;&#19979;&#23618;&#30340;&#37325;&#28857;&#26159;&#26500;&#24314;&#19968;&#20010;&#32622;&#20449;&#21306;&#38388;&#65292;&#20197;&#20445;&#25345;&#26435;&#37325;&#24179;&#22343;Bellman&#35823;&#24046;&#36275;&#22815;&#23567;&#65292;&#21516;&#26102;&#25511;&#21046;&#30001;&#20998;&#24067;&#19981;&#21305;&#37197;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#38543;&#21518;&#65292;&#22312;&#19978;&#23618;&#65292;&#31574;&#30053;&#26088;&#22312;&#26368;&#22823;&#21270;&#19979;&#23618;&#24418;&#25104;&#30340;&#32622;&#20449;&#21306;&#38388;&#20013;&#30340;&#20445;&#23432;&#20272;&#35745;&#20540;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#34920;&#36848;&#20445;&#30041;&#20102;&#38544;&#24335;&#24341;&#23548;&#30340;&#25506;&#32034;&#25968;&#25454;&#20998;&#24067;&#30340;&#26368;&#22823;&#28789;&#27963;&#24615;&#65292;&#20351;&#24471;&#27169;&#22411;&#25512;&#24191;&#30340;&#33021;&#21147;&#24471;&#20197;&#21457;&#25381;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it
&lt;/p&gt;</description></item><item><title>CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06266</link><description>&lt;p&gt;
CodeFuse-13B: &#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06266
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22240;&#20854;&#22312;&#36719;&#20214;&#24037;&#31243;&#20840;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#38750;&#33521;&#35821;&#36755;&#20837;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#30340;&#25928;&#26524;&#20173;&#28982;&#36828;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeFuse-13B&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;LLM&#12290;&#23427;&#19987;&#20026;&#21253;&#21547;&#33521;&#25991;&#21644;&#20013;&#25991;&#25552;&#31034;&#30340;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#24182;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;CodeFuse&#36890;&#36807;&#21033;&#29992;&#30001;&#31243;&#24207;&#20998;&#26512;&#22120;&#31934;&#24515;&#31579;&#36873;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#30340;&#39640;&#36136;&#37327;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20351;&#29992;&#22330;&#26223;&#12289;&#24037;&#19994;&#26631;&#20934;&#22522;&#20934;HumanEval-x&#65292;&#20197;&#21450;&#19987;&#20026;&#20013;&#25991;&#25552;&#31034;&#35774;&#35745;&#30340;CodeFuseEval&#12290;&#20026;&#20102;&#35780;&#20272;CodeFuse&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#31215;&#26497;&#25910;&#38598;&#20102;AntGroup&#36719;&#20214;&#24320;&#21457;&#22242;&#38431;&#30340;&#23453;&#36149;&#20154;&#24037;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#27491;&#24120;&#22270;&#35757;&#32451;&#30340;&#24322;&#24120;&#22270;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#25554;&#20540;&#30340;&#20266;&#24322;&#24120;&#22270;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.06261</link><description>&lt;p&gt;
&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#29992;&#20110;&#24322;&#24120;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Discriminative Modeling for Anomalous Graph Detection. (arXiv:2310.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#27491;&#24120;&#22270;&#35757;&#32451;&#30340;&#24322;&#24120;&#22270;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#25554;&#20540;&#30340;&#20266;&#24322;&#24120;&#22270;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20165;&#22522;&#20110;&#27491;&#24120;&#22270;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#24322;&#24120;&#22270;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#20998;&#23376;&#12289;&#29983;&#29289;&#21644;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21306;&#20998;&#24314;&#27169;&#26694;&#26550;&#29992;&#20110;&#24322;&#24120;&#22270;&#26816;&#27979;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20174;&#32473;&#23450;&#30340;&#27491;&#24120;&#22270;&#21644;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#29983;&#25104;&#30340;&#20266;&#24322;&#24120;&#22270;&#20013;&#30340;&#21028;&#21035;&#22120;&#65288;&#20998;&#31867;&#22120;&#65289;&#65292;&#20174;&#32780;&#20351;&#24471;&#29983;&#25104;&#30340;&#20266;&#24322;&#24120;&#22270;&#22312;&#27491;&#24120;&#22270;&#21644;&#30495;&#23454;&#24322;&#24120;&#22270;&#20043;&#38388;&#25554;&#20540;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#31181;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#30340;&#31639;&#27861;&#29992;&#20110;&#24322;&#24120;&#22270;&#26816;&#27979;&#12290;&#36825;&#19977;&#31181;&#31639;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#22522;&#32447;&#22312;&#20061;&#20010;&#27969;&#34892;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#65288;&#20854;&#20013;&#22235;&#20010;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#65292;&#20116;&#20010;&#25968;&#25454;&#38598;&#35268;&#27169;&#36866;&#20013;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of detecting anomalous graphs using a machine learning model trained on only normal graphs, which has many applications in molecule, biology, and social network data analysis. We present a self-discriminative modeling framework for anomalous graph detection. The key idea, mathematically and numerically illustrated, is to learn a discriminator (classifier) from the given normal graphs together with pseudo-anomalous graphs generated by a model jointly trained, where we never use any true anomalous graphs and we hope that the generated pseudo-anomalous graphs interpolate between normal ones and (real) anomalous ones. Under the framework, we provide three algorithms with different computational efficiencies and stabilities for anomalous graph detection. The three algorithms are compared with several state-of-the-art graph-level anomaly detection baselines on nine popular graph datasets (four with small size and five with moderate size) and show significant im
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.06253</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning. (arXiv:2310.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#30340;&#26174;&#24335;&#27169;&#22411;&#20351;&#20195;&#29702;&#26356;&#33410;&#32422;&#26679;&#26412;&#12289;&#36866;&#24212;&#24615;&#26356;&#24378;&#21644;&#26356;&#26131;&#35299;&#37322;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;MBRL&#20195;&#29702;&#30340;&#33021;&#21147;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#20294;&#22914;&#20309;&#26368;&#22909;&#22320;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;MBRL&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29615;&#22659;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#28982;&#21518;&#20351;&#29992;&#27169;&#22411;&#30830;&#23450;&#26368;&#26377;&#30410;&#30340;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#36890;&#24120;&#19982;&#21160;&#20316;&#36136;&#37327;&#19981;&#30456;&#20851;&#65292;&#23558;&#26681;&#26412;&#21407;&#22240;&#24402;&#32467;&#20026;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#23398;&#20064;&#19982;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#20043;&#38388;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#12290;&#38543;&#30528;MBRL&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#26029;&#25104;&#29087;&#65292;&#28044;&#29616;&#20986;&#20102;&#19968;&#20123;&#20114;&#30456;&#20851;&#32852;&#30340;&#35299;&#20915;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#31867;&#21035;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35762;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#32467;&#26500;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#27934;&#23519;&#21644;&#39044;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23618;&#21322;&#20223;&#23556;&#36755;&#20837;&#36716;&#25442;&#26469;&#25552;&#20379;&#39044;&#27979;&#35268;&#21017;&#24182;&#25214;&#21040;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.06251</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#31687;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Learning: A Tutorial. (arXiv:2310.06251v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35762;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#32467;&#26500;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#27934;&#23519;&#21644;&#39044;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23618;&#21322;&#20223;&#23556;&#36755;&#20837;&#36716;&#25442;&#26469;&#25552;&#20379;&#39044;&#27979;&#35268;&#21017;&#24182;&#25214;&#21040;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#23545;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22238;&#39038;&#65292;&#20197;&#25581;&#31034;&#23545;&#32467;&#26500;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#27934;&#23519;&#12290;&#19982;&#22823;&#22810;&#25968;&#32479;&#35745;&#27169;&#22411;&#24120;&#29992;&#30340;&#27973;&#23618;&#21152;&#24615;&#32467;&#26500;&#19981;&#21516;&#65292;&#28145;&#24230;&#23398;&#20064;&#20351;&#29992;&#22810;&#23618;&#21322;&#20223;&#23556;&#36755;&#20837;&#36716;&#25442;&#26469;&#25552;&#20379;&#39044;&#27979;&#35268;&#21017;&#12290;&#24212;&#29992;&#36825;&#20123;&#36716;&#25442;&#23618;&#23548;&#33268;&#19968;&#32452;&#23646;&#24615;&#65288;&#25110;&#29305;&#24449;&#65289;&#65292;&#21487;&#20197;&#24212;&#29992;&#27010;&#29575;&#32479;&#35745;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#36798;&#21040;&#21487;&#25193;&#23637;&#30340;&#39044;&#27979;&#35268;&#21017;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20854;&#20013;&#31232;&#30095;&#27491;&#21017;&#21270;&#25214;&#21040;&#20102;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is to provide a review of deep learning methods which provide insight into structured high-dimensional data. Rather than using shallow additive architectures common to most statistical models, deep learning uses layers of semi-affine input transformations to provide a predictive rule. Applying these layers of transformations leads to a set of attributes (or, features) to which probabilistic statistical methods can be applied. Thus, the best of both worlds can be achieved: scalable prediction rules fortified with uncertainty quantification, where sparse regularization finds the features.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39640;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#35299;&#32806;&#31995;&#25968;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20302;&#22797;&#26434;&#24230;&#19979;&#30340;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#12289;&#31895;&#31890;&#24230;&#30456;&#20851;&#22343;&#34913;&#21644;&#30456;&#20851;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#22312;&#20122;&#32447;&#24615;&#21518;&#24724;&#26041;&#38754;&#34920;&#29616;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06243</link><description>&lt;p&gt;
&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65306;&#20248;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Multi-Agent RL: An Optimization Perspective. (arXiv:2310.06243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39640;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#35299;&#32806;&#31995;&#25968;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20302;&#22797;&#26434;&#24230;&#19979;&#30340;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#12289;&#31895;&#31890;&#24230;&#30456;&#20851;&#22343;&#34913;&#21644;&#30456;&#20851;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#22312;&#20122;&#32447;&#24615;&#21518;&#24724;&#26041;&#38754;&#34920;&#29616;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#65292;&#38024;&#23545;&#19968;&#33324;&#21644;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#65288;Markov Games&#65292;MGs&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#20026;&#20102;&#25214;&#21040;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#22810;&#26234;&#33021;&#20307;&#35299;&#32806;&#31995;&#25968;&#65288;MADC&#65289;&#30340;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#30830;&#20445;&#22312;&#20302;MADC&#26465;&#20214;&#19979;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#12289;&#31895;&#31890;&#24230;&#30456;&#20851;&#22343;&#34913;&#21644;&#30456;&#20851;&#22343;&#34913;&#30340;&#27169;&#22411;&#22522;&#20110;&#21644;&#26080;&#27169;&#22411;MARL&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#19982;&#29616;&#26377;&#24037;&#20316;&#30456;&#23218;&#32654;&#30340;&#20122;&#32447;&#24615;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#22343;&#34913;&#27714;&#35299;&#39044;&#27979;&#22120;&#19982;&#19968;&#20010;&#27714;&#35299;&#27599;&#20010;&#30830;&#23450;&#24615;&#32852;&#21512;&#31574;&#30053;&#30340;&#27491;&#21017;&#21270;&#25910;&#30410;&#30340;&#20869;&#32622;&#20248;&#21270;&#23376;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#36991;&#20813;&#20102;&#22312;&#25968;&#25454;&#30456;&#20851;&#32422;&#26463;&#26465;&#20214;&#19979;&#27714;&#35299;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#25110;&#25191;&#34892;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#36125;&#21494;&#26031;&#26041;&#27861;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25289;&#26684;&#26391;&#26085;&#25551;&#36848;&#29289;&#29702;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#21202;&#35753;&#24503;&#21464;&#25442;&#33258;&#21160;&#25552;&#21462;&#21704;&#23494;&#39039;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.06241</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#25289;&#26684;&#26391;&#26085;&#36125;&#21494;&#26031;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data. (arXiv:2310.06241v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#36125;&#21494;&#26031;&#26041;&#27861;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25289;&#26684;&#26391;&#26085;&#25551;&#36848;&#29289;&#29702;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#21202;&#35753;&#24503;&#21464;&#25442;&#33258;&#21160;&#25552;&#21462;&#21704;&#23494;&#39039;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#39044;&#27979;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#38656;&#35201;&#23545;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#26041;&#31243;&#21457;&#29616;&#30340;&#26694;&#26550;&#25512;&#24191;&#21040;&#29289;&#29702;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#21644;&#25289;&#26684;&#26391;&#26085;&#30340;&#21457;&#29616;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25289;&#26684;&#26391;&#26085;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#36125;&#21494;&#26031;&#26041;&#27861;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25289;&#26684;&#26391;&#26085;&#25551;&#36848;&#29289;&#29702;&#31995;&#32479;&#30340;&#26367;&#20195;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;(a)&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#25289;&#26684;&#26391;&#26085;&#25551;&#36848;&#65292;(b)&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26469;&#37327;&#21270;&#30001;&#20110;&#26377;&#38480;&#25968;&#25454;&#32780;&#23548;&#33268;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;(c)&#36890;&#36807;&#21202;&#35753;&#24503;&#21464;&#25442;&#33258;&#21160;&#25552;&#21462;&#20174;&#23398;&#20064;&#21040;&#30340;&#25289;&#26684;&#26391;&#26085;&#24471;&#21040;&#30340;&#21704;&#23494;&#39039;&#25551;&#36848;&#65292;(d)&#25552;&#20379;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35266;&#27979;&#31995;&#32479;&#25551;&#36848;&#12290;&#28041;&#21450;&#20845;&#20010;&#19981;&#21516;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#20004;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning and predicting the dynamics of physical systems requires a profound understanding of the underlying physical laws. Recent works on learning physical laws involve generalizing the equation discovery frameworks to the discovery of Hamiltonian and Lagrangian of physical systems. While the existing methods parameterize the Lagrangian using neural networks, we propose an alternate framework for learning interpretable Lagrangian descriptions of physical systems from limited data using the sparse Bayesian approach. Unlike existing neural network-based approaches, the proposed approach (a) yields an interpretable description of Lagrangian, (b) exploits Bayesian learning to quantify the epistemic uncertainty due to limited data, (c) automates the distillation of Hamiltonian from the learned Lagrangian using Legendre transformation, and (d) provides ordinary (ODE) and partial differential equation (PDE) based descriptions of the observed systems. Six different examples involving both di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26469;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#21508;&#31181;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.06238</link><description>&lt;p&gt;
&#35299;&#20915;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65306;&#20026;&#26080;&#20559;&#38382;&#31572;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering. (arXiv:2310.06238v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;MUSIC-AVQA&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26469;&#20445;&#35777;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#21508;&#31181;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#21449;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#65292;&#25512;&#21160;&#20102;&#22810;&#27169;&#24577;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20219;&#20309;&#27169;&#24577;&#20013;&#23384;&#22312;&#30340;&#24378;&#28872;&#20559;&#35265;&#20250;&#23548;&#33268;&#27169;&#22411;&#24573;&#35270;&#20854;&#20182;&#27169;&#24577;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#26377;&#25928;&#22320;&#36328;&#36234;&#36825;&#20123;&#22810;&#26679;&#21270;&#27169;&#24577;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#21463;&#21040;&#25439;&#23475;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#23457;&#26597;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#31181;&#38382;&#39064;&#31867;&#22411;&#65292;&#36873;&#25321;&#20855;&#26377;&#26126;&#26174;&#31572;&#26696;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20114;&#34917;&#30340;&#35270;&#39057;&#21644;&#38382;&#39064;&#65292;&#30830;&#20445;&#27809;&#26377;&#31572;&#26696;&#26377;&#26126;&#26174;&#30340;&#20559;&#26012;&#20998;&#24067;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20108;&#20803;&#38382;&#39064;&#65292;&#25105;&#20204;&#21162;&#21147;&#30830;&#20445;&#27599;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#20004;&#20010;&#31572;&#26696;&#20960;&#20046;&#22343;&#21248;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MUSIC-AVQA v2.0&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23427;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#30456;&#20449;&#33021;&#22815;&#26356;&#22909;&#22320;&#20419;&#36827;AVQA&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adapter&#37325;&#32452;&#65288;ARC&#65289;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#20174;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#36866;&#24212;&#21442;&#25968;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#24341;&#20837;&#21442;&#25968;&#20849;&#20139;&#26041;&#26696;&#65292;&#21033;&#29992;&#23545;&#31216;&#30340;&#25237;&#24433;&#25805;&#20316;&#26469;&#26500;&#24314;&#20849;&#20139;&#30340;&#29942;&#39048;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#37325;&#26032;&#32553;&#25918;&#31995;&#25968;&#26469;&#26377;&#25928;&#37325;&#26032;&#32452;&#21512;&#23618;&#36866;&#24212;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.06234</link><description>&lt;p&gt;
&#22823;&#22411;Vision Transformer&#30340;&#39640;&#25928;&#36866;&#24212;&#24615;&#36890;&#36807;Adapter&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing. (arXiv:2310.06234v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adapter&#37325;&#32452;&#65288;ARC&#65289;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#20174;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#36866;&#24212;&#21442;&#25968;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#24341;&#20837;&#21442;&#25968;&#20849;&#20139;&#26041;&#26696;&#65292;&#21033;&#29992;&#23545;&#31216;&#30340;&#25237;&#24433;&#25805;&#20316;&#26469;&#26500;&#24314;&#20849;&#20139;&#30340;&#29942;&#39048;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#37325;&#26032;&#32553;&#25918;&#31995;&#25968;&#26469;&#26377;&#25928;&#37325;&#26032;&#32452;&#21512;&#23618;&#36866;&#24212;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23481;&#37327;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#24335;&#65292;&#23558;&#28966;&#28857;&#20174;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#36716;&#21521;&#20102;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#36866;&#24212;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#36731;&#37327;&#32423;&#30340;&#36866;&#37197;&#22120;&#21450;&#20854;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#38656;&#35201;&#26356;&#26032;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Adapter&#37325;&#32452;&#65288;ARC&#65289;&#31574;&#30053;&#65292;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#36866;&#24212;&#21442;&#25968;&#30340;&#21487;&#37325;&#29992;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#21442;&#25968;&#20849;&#20139;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#31216;&#30340;&#21521;&#19979;/&#21521;&#19978;&#25237;&#24433;&#26469;&#26500;&#24314;&#29942;&#39048;&#25805;&#20316;&#65292;&#36825;&#20123;&#25805;&#20316;&#22312;&#19981;&#21516;&#23618;&#20043;&#38388;&#20849;&#20139;&#12290;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#37325;&#26032;&#32553;&#25918;&#31995;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#26032;&#32452;&#21512;&#23618;&#36866;&#24212;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adapti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#39062;&#30340;&#31232;&#30095;&#24863;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#23454;&#29616;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#30340;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#38408;&#20540;&#20989;&#25968;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#24182;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#30340;&#31639;&#27861;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24674;&#22797;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.06233</link><description>&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;&#31232;&#30095;&#24863;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#23454;&#29616;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers. (arXiv:2310.06233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#39062;&#30340;&#31232;&#30095;&#24863;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#23454;&#29616;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#30340;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#38408;&#20540;&#20989;&#25968;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#24182;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#30340;&#31639;&#27861;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24674;&#22797;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#36731;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#20013;&#30001;l1&#33539;&#25968;&#20135;&#29983;&#30340;&#20559;&#24046;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#38750;&#20984;&#30340;&#26367;&#20195;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26367;&#20195;&#20989;&#25968;/&#27491;&#21017;&#21270;&#22120;&#65292;&#34429;&#28982;&#23427;&#20204;&#37117;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38750;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#38408;&#20540;&#20989;&#25968;&#21487;&#33021;&#27809;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#34920;&#36798;&#24335;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#36845;&#20195;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29983;&#25104;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#38408;&#20540;&#20989;&#25968;&#30340;&#31232;&#30095;&#24863;&#24212;&#27491;&#21017;&#21270;&#22120;&#12290;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#24212;&#29992;&#20110;&#20302; Tubal Rank &#24352;&#37327;&#34917;&#20840;&#65292;&#24182;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#30340;&#26041;&#27861;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#24207;&#21015;&#26159;&#26377;&#30028;&#30340;&#65292;&#20219;&#20309;&#26497;&#38480;&#28857;&#37117;&#26159;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24674;&#22797;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To alleviate the bias generated by the l1-norm in the low-rank tensor completion problem, nonconvex surrogates/regularizers have been suggested to replace the tensor nuclear norm, although both can achieve sparsity. However, the thresholding functions of these nonconvex regularizers may not have closed-form expressions and thus iterations are needed, which increases the computational loads. To solve this issue, we devise a framework to generate sparsity-inducing regularizers with closed-form thresholding functions. These regularizers are applied to low-tubal-rank tensor completion, and efficient algorithms based on the alternating direction method of multipliers are developed. Furthermore, convergence of our methods is analyzed and it is proved that the generated sequences are bounded and any limit point is a stationary point. Experimental results using synthetic and real-world datasets show that the proposed algorithms outperform the state-of-the-art methods in terms of restoration pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#19987;&#26377;&#30340;MRI&#32959;&#30244;&#21644;&#30149;&#29702;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#38754;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#37197;&#32622;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#25104;&#21151;&#29575;&#65292;&#24378;&#35843;&#20102;&#23545;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#36843;&#20999;&#38656;&#35201;&#65292;&#24182;&#24314;&#35758;&#37325;&#26032;&#35780;&#20272;&#24403;&#21069;&#32852;&#21512;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#23433;&#20840;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.06227</link><description>&lt;p&gt;
&#25506;&#32034;&#21307;&#23398;&#22270;&#20687;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploring adversarial attacks in federated learning for medical imaging. (arXiv:2310.06227v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#19987;&#26377;&#30340;MRI&#32959;&#30244;&#21644;&#30149;&#29702;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#38754;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#37197;&#32622;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#25104;&#21151;&#29575;&#65292;&#24378;&#35843;&#20102;&#23545;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#36843;&#20999;&#38656;&#35201;&#65292;&#24182;&#24314;&#35758;&#37325;&#26032;&#35780;&#20272;&#24403;&#21069;&#32852;&#21512;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#23433;&#20840;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26694;&#26550;&#65292;&#20294;&#20063;&#26292;&#38706;&#20102;&#31995;&#32479;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#19987;&#26377;&#30340;MRI&#32959;&#30244;&#21644;&#30149;&#29702;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#24050;&#30693;&#23041;&#32961;&#22330;&#26223;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#37197;&#32622;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#25915;&#20987;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#36843;&#20999;&#38656;&#35201;&#65292;&#24182;&#24314;&#35758;&#23545;&#24403;&#21069;&#32852;&#21512;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#23433;&#20840;&#21327;&#35758;&#36827;&#34892;&#20851;&#38190;&#37325;&#26032;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning offers a privacy-preserving framework for medical image analysis but exposes the system to adversarial attacks. This paper aims to evaluate the vulnerabilities of federated learning networks in medical image analysis against such attacks. Employing domain-specific MRI tumor and pathology imaging datasets, we assess the effectiveness of known threat scenarios in a federated learning environment. Our tests reveal that domain-specific configurations can increase the attacker's success rate significantly. The findings emphasize the urgent need for effective defense mechanisms and suggest a critical re-evaluation of current security protocols in federated medical image analysis systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.06225</link><description>&lt;p&gt;
GPT-4&#20316;&#20026;&#20892;&#23398;&#21161;&#25163;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#20892;&#19994;&#32771;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#65292;LLM&#30340;&#24615;&#33021;&#19982;&#35757;&#32451;&#26377;&#32032;&#30340;&#20154;&#31867;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#65292;&#22240;&#27492;&#21512;&#29702;&#22320;&#20351;&#29992;&#20154;&#31867;&#32771;&#35797;&#65288;&#20363;&#22914;&#35748;&#35777;&#32771;&#35797;&#65289;&#26469;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;LLM&#65288;&#22914;Llama 2&#21644;GPT&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#36816;&#29992;&#20102;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#21644;ER&#65288;&#38598;&#21512;&#32454;&#21270;&#65289;&#25216;&#26415;&#65292;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23637;&#31034;LLM&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#26469;&#33258;&#24052;&#35199;&#12289;&#21360;&#24230;&#21644;&#32654;&#22269;&#19977;&#20010;&#26368;&#22823;&#30340;&#20892;&#19994;&#29983;&#20135;&#22269;&#30340;&#20892;&#19994;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#22312;&#32771;&#35797;&#20013;&#21462;&#24471;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#20013;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#20027;&#35201;&#21253;&#25324;&#23545;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#30340;&#26816;&#27979;&#21644;&#24320;&#25918;&#19990;&#30028;&#34920;&#31034;&#23398;&#20064;&#12290;OOD&#26816;&#27979;&#19987;&#27880;&#20110;&#35782;&#21035;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#23454;&#20363;&#65292;&#32780;ORL&#21017;&#25193;&#23637;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06221</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#26816;&#27979;&#21644;&#23398;&#20064;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65306;&#31639;&#27861;&#21644;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory. (arXiv:2310.06221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#20013;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#20027;&#35201;&#21253;&#25324;&#23545;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#30340;&#26816;&#27979;&#21644;&#24320;&#25918;&#19990;&#30028;&#34920;&#31034;&#23398;&#20064;&#12290;OOD&#26816;&#27979;&#19987;&#27880;&#20110;&#35782;&#21035;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#23454;&#20363;&#65292;&#32780;ORL&#21017;&#25193;&#23637;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#21644;&#24773;&#22659;&#30340;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#36129;&#29486;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#22312;&#22266;&#23450;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#38598;&#21512;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#36825;&#31181;&#26465;&#20214;&#34987;&#31216;&#20026;&#23553;&#38381;&#19990;&#30028;&#35774;&#32622;&#12290;&#23613;&#31649;&#36825;&#31181;&#20551;&#35774;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#26377;&#25928;&#65292;&#20294;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21364;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#26032;&#30340;&#31867;&#21035;&#25110;&#25968;&#25454;&#31867;&#22411;&#21487;&#33021;&#21160;&#24577;&#32780;&#24847;&#22806;&#22320;&#20986;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#30340;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#20851;&#38190;&#27493;&#39588;&#65306;&#26410;&#30693;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#21644;&#24320;&#25918;&#19990;&#30028;&#34920;&#31034;&#23398;&#20064;&#65288;ORL&#65289;&#12290;OOD&#26816;&#27979;&#19987;&#27880;&#20110;&#35782;&#21035;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#36825;&#20010;&#36807;&#31243;&#38477;&#20302;&#20102;&#23545;&#38476;&#29983;&#36755;&#20837;&#36827;&#34892;&#36807;&#24230;&#33258;&#20449;&#21644;&#38169;&#35823;&#39044;&#27979;&#30340;&#39118;&#38505;&#12290;&#22312;&#36229;&#36234;OOD&#26816;&#27979;&#30340;&#21516;&#26102;&#65292;ORL&#25193;&#23637;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis makes considerable contributions to the realm of machine learning, specifically in the context of open-world scenarios where systems face previously unseen data and contexts. Traditional machine learning models are usually trained and tested within a fixed and known set of classes, a condition known as the closed-world setting. While this assumption works in controlled environments, it falls short in real-world applications where new classes or categories of data can emerge dynamically and unexpectedly. To address this, our research investigates two intertwined steps essential for open-world machine learning: Out-of-distribution (OOD) Detection and Open-world Representation Learning (ORL). OOD detection focuses on identifying instances from unknown classes that fall outside the model's training distribution. This process reduces the risk of making overly confident, erroneous predictions about unfamiliar inputs. Moving beyond OOD detection, ORL extends the capabilities of th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#65288;SUBP&#65289;&#26041;&#27861;&#65292;&#22312;1xN&#31232;&#30095;CNN&#20013;&#23454;&#29616;&#20102;&#22810;&#32447;&#31243;&#21152;&#36895;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35757;&#32451;&#25104;&#26412;&#26114;&#36149;&#12289;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#22823;&#12289;&#27169;&#22411;&#36136;&#37327;&#27425;&#20248;&#20197;&#21450;&#32447;&#31243;&#36127;&#36733;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06218</link><description>&lt;p&gt;
SUBP&#65306;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#29992;&#20110;1xN&#31232;&#30095;CNN&#30340;&#22810;&#32447;&#31243;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration. (arXiv:2310.06218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#65288;SUBP&#65289;&#26041;&#27861;&#65292;&#22312;1xN&#31232;&#30095;CNN&#20013;&#23454;&#29616;&#20102;&#22810;&#32447;&#31243;&#21152;&#36895;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35757;&#32451;&#25104;&#26412;&#26114;&#36149;&#12289;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#22823;&#12289;&#27169;&#22411;&#36136;&#37327;&#27425;&#20248;&#20197;&#21450;&#32447;&#31243;&#36127;&#36733;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#30340;&#31232;&#30095;&#24615;&#30740;&#31350;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#21387;&#32553;&#21644;&#21152;&#36895;&#27169;&#22411;&#12290;&#36890;&#36807;&#32422;&#26463;&#36755;&#20986;&#36890;&#36947;&#19978;&#30340;N&#20010;&#36830;&#32493;&#26435;&#37325;&#20026;&#32452;&#20869;&#38750;&#38646;&#65292;&#26368;&#36817;&#30340;1xN&#31232;&#30095;&#32593;&#32476;&#22240;&#20854;&#19977;&#20010;&#31361;&#20986;&#20248;&#21183;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65306;1&#65289;&#36890;&#36807;&#19968;&#31181;&#8220;&#22359;&#31232;&#30095;&#34892;&#8221;&#30697;&#38453;&#22823;&#37327;&#33410;&#30465;&#23384;&#20648;&#31354;&#38388;&#12290;2&#65289;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;3&#65289;&#22312;&#20855;&#26377;&#39640;&#32423;&#30690;&#37327;&#25193;&#23637;&#30340;CPU&#19978;&#26174;&#33879;&#21152;&#36895;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38656;&#35201;&#22522;&#20110;&#31264;&#23494;&#39044;&#35757;&#32451;&#26435;&#37325;&#36873;&#25321;&#21644;&#24494;&#35843;1xN&#31232;&#30095;&#26435;&#37325;&#65292;&#23548;&#33268;&#35757;&#32451;&#25104;&#26412;&#26114;&#36149;&#12289;&#20869;&#23384;&#35775;&#38382;&#24320;&#38144;&#22823;&#12289;&#27169;&#22411;&#36136;&#37327;&#27425;&#20248;&#20197;&#21450;&#19981;&#24179;&#34913;&#30340;&#32447;&#31243;&#36127;&#36733;&#65288;&#36755;&#20986;&#36890;&#36947;&#19978;&#30340;&#19981;&#21516;&#31232;&#30095;&#24615;&#65289;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36719;&#22343;&#21248;&#22359;&#21098;&#26525;&#8221;&#65288;SUBP&#65289;&#26041;&#27861;&#26469;&#35757;&#32451;&#19968;&#20010;u
&lt;/p&gt;
&lt;p&gt;
The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \emph{\textbf{S}oft \textbf{U}niform \textbf{B}lock \textbf{P}runing} (SUBP) approach to train a u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#19978;&#30340;&#20998;&#24067;&#24335;&#22810;&#32423;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#35328;&#30340;&#20998;&#24067;&#24335;&#22810;&#32423;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06217</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32593;&#32476;&#19978;&#30340;&#32852;&#21512;&#22810;&#32423;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Level Optimization over Decentralized Networks. (arXiv:2310.06217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#19978;&#30340;&#20998;&#24067;&#24335;&#22810;&#32423;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#35328;&#30340;&#20998;&#24067;&#24335;&#22810;&#32423;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#32423;&#20248;&#21270;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20026;&#35299;&#20915;&#35768;&#22810;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#65288;&#22914;&#20803;&#23398;&#20064;&#12289;&#22810;&#20154;&#28216;&#25103;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23884;&#22871;&#32452;&#21512;&#20248;&#21270;&#65289;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#19978;&#30340;&#20998;&#24067;&#24335;&#22810;&#32423;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#21482;&#33021;&#19982;&#23427;&#20204;&#30340;&#30452;&#25509;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#12290;&#36825;&#20010;&#35774;&#32622;&#26159;&#30001;&#22823;&#35268;&#27169;&#31995;&#32479;&#20013;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#38656;&#27714;&#25152;&#39537;&#21160;&#30340;&#65292;&#20854;&#20013;&#38598;&#20013;&#20248;&#21270;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#35328;&#30340;&#20998;&#24067;&#24335;&#22810;&#32423;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#32593;&#32476;&#20195;&#29702;&#21487;&#20197;&#22312;&#21333;&#20010;&#26102;&#38388;&#23610;&#24230;&#20869;&#35299;&#20915;&#19981;&#21516;&#23618;&#27425;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#20849;&#20139;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#19982;&#32593;&#32476;&#35268;&#27169;&#32447;&#24615;&#22686;&#38271;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-level optimization has gained increasing attention in recent years, as it provides a powerful framework for solving complex optimization problems that arise in many fields, such as meta-learning, multi-player games, reinforcement learning, and nested composition optimization. In this paper, we study the problem of distributed multi-level optimization over a network, where agents can only communicate with their immediate neighbors. This setting is motivated by the need for distributed optimization in large-scale systems, where centralized optimization may not be practical or feasible. To address this problem, we propose a novel gossip-based distributed multi-level optimization algorithm that enables networked agents to solve optimization problems at different levels in a single timescale and share information through network propagation. Our algorithm achieves optimal sample complexity, scaling linearly with the network size, and demonstrates state-of-the-art performance on variou
&lt;/p&gt;</description></item><item><title>GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06213</link><description>&lt;p&gt;
GeoLLM: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06213
&lt;/p&gt;
&lt;p&gt;
GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#24120;&#24120;&#20381;&#36182;&#20110;&#20840;&#29699;&#33539;&#22260;&#21487;&#29992;&#30340;&#21355;&#26143;&#22270;&#20687;&#31561;&#39044;&#27979;&#21464;&#37327;&#65292;&#36825;&#21487;&#33021;&#35201;&#20040;&#24456;&#26114;&#36149;&#65292;&#35201;&#20040;&#32570;&#20047;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20114;&#32852;&#32593;&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#21253;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;LLM&#20013;&#23884;&#20837;&#20102;&#26377;&#20851;&#20301;&#32622;&#30340;&#26174;&#33879;&#31354;&#38388;&#20449;&#24687;&#65292;&#20294;&#20165;&#20351;&#29992;&#22320;&#29702;&#22352;&#26631;&#26469;&#26597;&#35810;LLM&#23545;&#20110;&#39044;&#27979;&#20154;&#21475;&#23494;&#24230;&#31561;&#20851;&#38190;&#25351;&#26631;&#26159;&#26080;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;LLM&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22269;&#38469;&#31038;&#21306;&#20851;&#24515;&#30340;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#30340;&#27979;&#37327;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these task
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#27491;&#30340;&#20998;&#31867;&#22120;&#24323;&#26435;&#26041;&#27861;&#65292;&#19981;&#36896;&#25104;&#20260;&#23475;&#30340;&#21516;&#26102;&#36798;&#21040;&#19968;&#23450;&#31243;&#24230;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#21644;&#20195;&#29702;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#30340;&#24323;&#26435;&#20915;&#31574;&#65292;&#20998;&#26512;&#20102;&#24323;&#26435;&#29575;&#19982;&#19981;&#20844;&#24179;&#23481;&#24525;&#24230;&#21644;&#20934;&#30830;&#24615;&#32422;&#26463;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.06205</link><description>&lt;p&gt;
&#20844;&#27491;&#30340;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#19981;&#36896;&#25104;&#20260;&#23475;&#22320;&#24323;&#26435;
&lt;/p&gt;
&lt;p&gt;
Fair Classifiers that Abstain without Harm. (arXiv:2310.06205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06205
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#27491;&#30340;&#20998;&#31867;&#22120;&#24323;&#26435;&#26041;&#27861;&#65292;&#19981;&#36896;&#25104;&#20260;&#23475;&#30340;&#21516;&#26102;&#36798;&#21040;&#19968;&#23450;&#31243;&#24230;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#21644;&#20195;&#29702;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#30340;&#24323;&#26435;&#20915;&#31574;&#65292;&#20998;&#26512;&#20102;&#24323;&#26435;&#29575;&#19982;&#19981;&#20844;&#24179;&#23481;&#24525;&#24230;&#21644;&#20934;&#30830;&#24615;&#32422;&#26463;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#20998;&#31867;&#22120;&#21521;&#20154;&#31867;&#25512;&#36831;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#20351;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#39044;&#27979;&#26576;&#20123;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#24323;&#26435;&#20998;&#31867;&#22120;&#34987;&#28608;&#21169;&#20197;&#22312;&#20445;&#25345;&#27599;&#20010;&#23376;&#32676;&#20307;&#30340;&#21407;&#22987;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19968;&#32452;&#25351;&#23450;&#31243;&#24230;&#30340;&#32676;&#20307;&#20844;&#24179;&#23450;&#20041;&#65288;&#21363;&#26080;&#20260;&#23475;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#36807;&#31243;&#65292;&#20026;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20998;&#37197;&#24323;&#26435;&#20915;&#31574;&#20197;&#28385;&#36275;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#12290;&#20026;&#20102;&#23558;&#24323;&#26435;&#20915;&#31574;&#25512;&#24191;&#21040;&#27979;&#35797;&#26679;&#26412;&#65292;&#25105;&#20204;&#25509;&#30528;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#26469;&#23398;&#20064;&#24323;&#26435;&#20915;&#31574;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25972;&#25968;&#35268;&#21010;&#36807;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#30830;&#23450;&#22312;&#19981;&#21516;&#30340;&#19981;&#20844;&#24179;&#23481;&#24525;&#24230;&#21644;&#20934;&#30830;&#24615;&#32422;&#26463;&#27700;&#24179;&#19979;&#23454;&#29616;&#26080;&#20260;&#23475;&#25152;&#21487;&#33021;&#30340;&#24323;&#26435;&#29575;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#30830;&#23450;&#20102;&#36825;&#20123;&#29702;&#35770;&#20851;&#31995;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each sub-population (i.e. no harm) while achieving a set of group fairness definitions to a user specified degree. To this end, we design an Integer Programming (IP) procedure that assigns abstention decisions for each training sample to satisfy a set of constraints. To generalize the abstaining decisions to test samples, we then train a surrogate model to learn the abstaining decisions based on the IP solutions in an end-to-end manner. We analyze the feasibility of the IP procedure to determine the possible abstention rate for different levels of unfairness tolerance and accuracy constraint for achieving no harm. To the best of our knowledge, this work is the first to identify the theoretical relationships
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.06200</link><description>&lt;p&gt;
&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#21450;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#31070;&#32463;&#20803;&#35299;&#37322;&#20013;&#21363;&#26102;&#35843;&#20248;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#23433;&#20840;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#24182;&#27809;&#26377;&#21516;&#27493;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;LLM&#12290;&#25105;&#20204;&#22312;&#21069;&#20154;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#22914;&#20309;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29983;&#25104;&#35299;&#37322;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#26356;&#33258;&#28982;&#30340;&#26041;&#24335;&#37325;&#26032;&#26684;&#24335;&#21270;&#35299;&#37322;&#25552;&#31034;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#25105;&#20204;&#26032;&#25552;&#31034;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayesian&#20809;&#35889;&#24402;&#19968;&#21270;&#30028;&#38480;&#65292;&#29992;&#20110;&#23545;&#25239;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#12290;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#19988;&#26356;&#32039;&#23494;&#22320;&#19982;&#26631;&#20934;&#27867;&#21270;&#30340;&#30028;&#38480;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.06182</link><description>&lt;p&gt;
PAC-Bayesian&#20809;&#35889;&#24402;&#19968;&#21270;&#30028;&#23545;&#25239;&#40065;&#26834;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization. (arXiv:2310.06182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayesian&#20809;&#35889;&#24402;&#19968;&#21270;&#30028;&#38480;&#65292;&#29992;&#20110;&#23545;&#25239;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#12290;&#19982;&#29616;&#26377;&#30028;&#38480;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#19988;&#26356;&#32039;&#23494;&#22320;&#19982;&#26631;&#20934;&#27867;&#21270;&#30340;&#30028;&#38480;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#25239;&#24615;&#40065;&#26834;&#27867;&#21270;&#22312;&#24314;&#31435;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#31639;&#27861;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#23545;&#40065;&#26834;&#27867;&#21270;&#30340;&#29702;&#35770;&#20445;&#35777;&#38750;&#24120;&#26377;&#24847;&#20041;&#12290;&#26412;&#25991;&#22522;&#20110;PAC-Bayes&#26041;&#27861;(Neyshabur&#31561;&#20154;&#65292;2017&#24180;)&#30340;&#22522;&#20110;&#33539;&#25968;&#30340;&#22797;&#26434;&#24615;&#23637;&#24320;&#30740;&#31350;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23558;&#22312;&#26631;&#20934;&#24773;&#20917;&#19979;&#30340;&#20027;&#35201;&#26500;&#25104;&#35201;&#32032;&#65292;&#21363;&#26435;&#37325;&#25200;&#21160;&#30028;&#65292;&#25193;&#23637;&#21040;&#40065;&#26834;&#24773;&#20917;&#19979;&#12290;&#29616;&#26377;&#30340;&#23581;&#35797;&#20005;&#37325;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#24378;&#20551;&#35774;&#65292;&#23548;&#33268;&#30028;&#38480;&#19981;&#20005;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20026;DNNs&#25552;&#20379;&#20102;&#19968;&#31181;&#20809;&#35889;&#24402;&#19968;&#21270;&#30340;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#12290;&#19982;&#29616;&#26377;&#30340;&#30028;&#38480;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#20854;&#27425;&#65292;&#23427;&#30340;&#30028;&#38480;&#30456;&#24403;&#32039;&#23494;&#65292;&#19982;&#26631;&#20934;&#27867;&#21270;&#30340;&#30028;&#38480;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#35777;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;(AutoSTPP)&#65292;&#25193;&#23637;&#20102;AutoInt&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#26102;&#31354;&#28857;&#36807;&#31243;(STPP)&#30340;&#35745;&#31639;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06179</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Integration for Spatiotemporal Neural Point Processes. (arXiv:2310.06179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;(AutoSTPP)&#65292;&#25193;&#23637;&#20102;AutoInt&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#26102;&#31354;&#28857;&#36807;&#31243;(STPP)&#30340;&#35745;&#31639;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36830;&#32493;&#26102;&#38388;&#30340;&#28857;&#36807;&#31243;&#23545;&#20110;&#35768;&#22810;&#31163;&#25955;&#20107;&#20214;&#39044;&#27979;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26102;&#31354;&#28857;&#36807;&#31243;&#65288;STPPs&#65289;&#30340;&#31215;&#20998;&#38382;&#39064;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#36827;&#34892;&#19977;&#37325;&#31215;&#20998;&#35745;&#31639;&#12290;&#29616;&#26377;&#30340;STPP&#31215;&#20998;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#24378;&#24230;&#20989;&#25968;&#20855;&#26377;&#21442;&#25968;&#24418;&#24335;&#65292;&#36825;&#32570;&#20047;&#28789;&#27963;&#24615;&#65307;&#35201;&#20040;&#29992;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#36817;&#20284;&#24378;&#24230;&#65292;&#36825;&#24341;&#20837;&#20102;&#25968;&#20540;&#35823;&#24046;&#12290;Omi&#31561;&#20154;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#31215;&#20998;&#26041;&#27861;AutoInt&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#31215;&#20998;&#28789;&#27963;&#30340;&#24378;&#24230;&#20989;&#25968;&#65292;&#20294;&#35813;&#26041;&#27861;&#21482;&#20851;&#27880;1D&#26102;&#38388;&#28857;&#36807;&#31243;&#12290;&#26412;&#25991;&#23558;AutoInt&#26041;&#27861;&#25193;&#23637;&#33267;3D STPP&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65306;AutoSTPP&#65288;&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#25193;&#23637;&#20043;&#21069;&#30340;&#24037;&#20316;&#20250;&#36807;&#20110;&#32422;&#26463;&#24378;&#24230;&#20989;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prov
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;msGeMM&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#65292;&#21487;&#20197;&#20351;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36817;2.5&#20493;&#12290;&#35813;&#31639;&#27861;&#38656;&#35201;&#29305;&#27530;&#30340;CUDA&#26680;&#24515;&#26469;&#23454;&#29616;&#20174;&#23567;&#22411;&#26597;&#25214;&#34920;&#20013;&#28155;&#21152;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06178</link><description>&lt;p&gt;
&#20511;&#21161;msGeMM&#22686;&#21152;AI GeMM&#30340;&#24615;&#33021;&#36817;2.5&#20493;&#30340;Look-Up mAI GeMM
&lt;/p&gt;
&lt;p&gt;
Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM. (arXiv:2310.06178v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;msGeMM&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#65292;&#21487;&#20197;&#20351;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36817;2.5&#20493;&#12290;&#35813;&#31639;&#27861;&#38656;&#35201;&#29305;&#27530;&#30340;CUDA&#26680;&#24515;&#26469;&#23454;&#29616;&#20174;&#23567;&#22411;&#26597;&#25214;&#34920;&#20013;&#28155;&#21152;&#20803;&#32032;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;HPC&#24212;&#29992;&#20013;&#38656;&#35201;&#21452;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#65292;&#32780;fp8&#25110;int4&#31561;&#26356;&#20302;&#31934;&#24230;&#30340;&#25968;&#25454;&#31867;&#22411;&#24050;&#32463;&#36275;&#22815;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#65292;&#32780;&#19988;&#36136;&#37327;&#30456;&#24403;&#12290;&#22312;&#27492;&#36235;&#21183;&#19979;&#65292;&#20687;NVIDIA&#21644;AMD&#36825;&#26679;&#30340;GPU&#20379;&#24212;&#21830;&#36890;&#36807;&#24352;&#37327;&#26680;&#24515;&#25552;&#20379;&#20102;&#23545;fp16&#12289;fp8&#21644;int8 GeMM&#25805;&#20316;&#30340;&#30828;&#20214;&#25903;&#25345;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;msGeMM&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#35777;&#26126;&#20102;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#31867;&#22411;&#30340;AI&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#32422;2.5&#20493;&#30340;&#20056;&#27861;&#21644;&#21152;&#27861;&#25351;&#20196;&#12290;&#23454;&#29616;&#27492;&#31639;&#27861;&#30340;&#39640;&#25928;&#29575;&#38656;&#35201;&#20855;&#22791;&#19982;&#24352;&#37327;&#26680;&#24515;&#30456;&#21516;&#36895;&#29575;&#20174;&#23567;&#22411;&#26597;&#25214;&#34920;&#20013;&#28155;&#21152;&#20803;&#32032;&#30340;&#29305;&#27530;CUDA&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DockGame&#30340;&#26032;&#39062;&#28216;&#25103;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#32858;&#21018;&#24615;&#34507;&#30333;&#36136;&#23545;&#25509;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#34507;&#30333;&#36136;&#23545;&#25509;&#35270;&#20026;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#21512;&#20316;&#28216;&#25103;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#28216;&#25103;&#28508;&#21147;&#21644;&#36890;&#36807;&#26799;&#24230;&#26356;&#26032;&#35745;&#31639;&#22343;&#34913;&#26469;&#39044;&#27979;&#26368;&#32456;&#30340;&#32452;&#35013;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.06177</link><description>&lt;p&gt;
DockGame: &#22810;&#32858;&#21018;&#24615;&#34507;&#30333;&#36136;&#23545;&#25509;&#30340;&#21512;&#20316;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
DockGame: Cooperative Games for Multimeric Rigid Protein Docking. (arXiv:2310.06177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DockGame&#30340;&#26032;&#39062;&#28216;&#25103;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#32858;&#21018;&#24615;&#34507;&#30333;&#36136;&#23545;&#25509;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#34507;&#30333;&#36136;&#23545;&#25509;&#35270;&#20026;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#21512;&#20316;&#28216;&#25103;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#28216;&#25103;&#28508;&#21147;&#21644;&#36890;&#36807;&#26799;&#24230;&#26356;&#26032;&#35745;&#31639;&#22343;&#34913;&#26469;&#39044;&#27979;&#26368;&#32456;&#30340;&#32452;&#35013;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;&#32452;&#35013;&#23545;&#22823;&#22810;&#25968;&#29983;&#29289;&#36807;&#31243;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#32452;&#25104;&#34507;&#30333;&#36136;&#39044;&#27979;&#32452;&#35013;&#32467;&#26500;&#65288;&#31216;&#20026;&#34507;&#30333;&#36136;&#23545;&#25509;&#20219;&#21153;&#65289;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22823;&#22810;&#25968;&#23545;&#25509;&#30340;&#20256;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20108;&#36827;&#21046;&#23545;&#25509;&#19978;&#65292;&#36981;&#24490;&#25628;&#32034;&#12289;&#22238;&#24402;&#25110;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#12290;&#26412;&#25991;&#38024;&#23545;&#23569;&#26377;&#30740;&#31350;&#30340;&#22810;&#32858;&#20307;&#65288;&#21363;&#20004;&#20010;&#25110;&#26356;&#22810;&#34507;&#30333;&#36136;&#65289;&#23545;&#25509;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;DockGame&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#28216;&#25103;&#29702;&#35770;&#26694;&#26550;&#29992;&#20110;&#23545;&#25509;&#8212;&#8212;&#25105;&#20204;&#23558;&#34507;&#30333;&#36136;&#23545;&#25509;&#35270;&#20026;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#21512;&#20316;&#28216;&#25103;&#65292;&#26368;&#32456;&#30340;&#32452;&#35013;&#32467;&#26500;&#26500;&#25104;&#19982;&#24213;&#23618;&#28216;&#25103;&#28508;&#21147;&#31283;&#23450;&#22343;&#34913;&#12290;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#30495;&#23454;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#26041;&#27861;&#8212;&#8212;i)&#36890;&#36807;&#29289;&#29702;&#33021;&#37327;&#20989;&#25968;&#25351;&#23548;&#23398;&#20064;&#26367;&#20195;&#28216;&#25103;&#28508;&#21147;&#24182;&#36890;&#36807;&#21516;&#26102;&#26799;&#24230;&#26356;&#26032;&#35745;&#31639;&#22343;&#34913;&#65292;&#20197;&#21450;ii)&#25506;&#32034;&#23545;&#25509;&#38382;&#39064;&#30340;&#21462;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.06171</link><description>&lt;p&gt;
&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20869;&#23384;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#35757;&#32451;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#36755;&#20986;&#32467;&#26524;&#36827;&#34892;&#30828;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#38169;&#35823;&#30340;&#32047;&#31215;&#29616;&#35937;&#65292;&#20445;&#35777;&#20102;&#31574;&#30053;&#25928;&#26524;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#19987;&#23478;&#28436;&#31034;&#22823;&#22823;&#31616;&#21270;&#20102;&#31574;&#30053;&#21512;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#27169;&#20223;&#31574;&#30053;&#26469;&#35828;&#65292;&#36828;&#31163;&#35757;&#32451;&#26679;&#26412;&#30340;&#38169;&#35823;&#23588;&#20026;&#20851;&#38190;&#12290;&#21363;&#20351;&#22312;&#31574;&#30053;&#30340;&#34892;&#21160;&#36755;&#20986;&#20013;&#20986;&#29616;&#32597;&#35265;&#30340;&#38169;&#35823;&#65292;&#30001;&#20110;&#36825;&#20123;&#38169;&#35823;&#20250;&#23548;&#33268;&#19981;&#29087;&#24713;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#31574;&#30053;&#22312;&#36825;&#20123;&#29366;&#24577;&#19979;&#20173;&#26356;&#23481;&#26131;&#20986;&#38169;&#65292;&#26368;&#32456;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#31616;&#21333;&#30340;&#30417;&#30563;&#24335;&#8220;&#34892;&#20026;&#20811;&#38534;&#8221;&#26041;&#27861;&#65292;&#33021;&#22815;&#26041;&#20415;&#22320;&#20165;&#36890;&#36807;&#39044;&#20808;&#35760;&#24405;&#30340;&#28436;&#31034;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#25269;&#28040;&#38169;&#35823;&#32047;&#31215;&#29616;&#35937;&#30340;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#30340;&#8220;&#20869;&#23384;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;&#8221;(MCNN)&#36755;&#20986;&#34987;&#24378;&#21046;&#32422;&#26463;&#22312;&#19982;&#20856;&#22411;&#30340;&#8220;&#20869;&#23384;&#8221;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#26126;&#30830;&#25351;&#23450;&#30340;&#20801;&#35768;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;MCNN&#31574;&#30053;&#23548;&#33268;&#30340;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#20445;&#35777;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;9&#20010;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#20351;&#29992;MCNNs&#65292;&#37319;&#29992;MLP&#12289;Transformer&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;3D&#20132;&#20114;&#29615;&#22659;&#65292;&#35843;&#26597;&#25968;&#25454;&#25910;&#38598;&#23545;&#26080;&#30417;&#30563;&#28145;&#24230;&#24863;&#30693;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25506;&#32034;&#27169;&#24335;&#19981;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;&#25968;&#25454;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#28145;&#24230;&#23436;&#25972;&#24615;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.06164</link><description>&lt;p&gt;
DEUX: &#27963;&#36291;&#25506;&#32034;&#29992;&#20110;&#23398;&#20064;&#26080;&#30417;&#30563;&#28145;&#24230;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
DEUX: Active Exploration for Learning Unsupervised Depth Perception. (arXiv:2310.06164v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;3D&#20132;&#20114;&#29615;&#22659;&#65292;&#35843;&#26597;&#25968;&#25454;&#25910;&#38598;&#23545;&#26080;&#30417;&#30563;&#28145;&#24230;&#24863;&#30693;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#25506;&#32034;&#27169;&#24335;&#19981;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;&#25968;&#25454;&#65292;&#36827;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#28145;&#24230;&#23436;&#25972;&#24615;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#27169;&#22411;&#36890;&#24120;&#22312;&#39044;&#23450;&#20041;&#30340;&#30456;&#26426;&#36712;&#36857;&#19978;&#30340;&#38750;&#20132;&#20114;&#24335;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#24120;&#24120;&#20250;&#24341;&#20837;&#19982;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#36873;&#25321;&#30340;&#29305;&#23450;&#30456;&#26426;&#36335;&#24452;&#30456;&#20851;&#30340;&#31995;&#32479;&#20559;&#24046;&#65292;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#20174;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#35270;&#35282;&#65292;&#36890;&#36807;&#21033;&#29992;3D&#20132;&#20114;&#29615;&#22659;&#26469;&#35843;&#26597;&#25968;&#25454;&#25910;&#38598;&#23545;&#23398;&#20064;&#28145;&#24230;&#23436;&#25972;&#24615;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#20010;&#20351;&#29992;&#20256;&#32479;&#23548;&#33322;&#25216;&#26415;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#28145;&#24230;&#23436;&#25972;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#29616;&#26377;&#30340;&#25506;&#32034;&#27169;&#24335;&#19981;&#19968;&#23450;&#25552;&#20379;&#20219;&#21153;&#20855;&#20307;&#25968;&#25454;&#28857;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23436;&#25972;&#24615;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20809;&#24230;&#37325;&#24314;&#30456;&#20851;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#30452;&#25509;&#31215;&#26497;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#28145;&#24230;&#23436;&#25972;&#24615;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;DEpth Uncert&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth perception models are typically trained on non-interactive datasets with predefined camera trajectories. However, this often introduces systematic biases into the learning process correlated to specific camera paths chosen during data acquisition. In this paper, we investigate the role of how data is collected for learning depth completion, from a robot navigation perspective, by leveraging 3D interactive environments. First, we evaluate four depth completion models trained on data collected using conventional navigation techniques. Our key insight is that existing exploration paradigms do not necessarily provide task-specific data points to achieve competent unsupervised depth completion learning. We then find that data collected with respect to photometric reconstruction has a direct positive influence on model performance. As a result, we develop an active, task-informed, depth uncertainty-based motion planning approach for learning depth completion, which we call DEpth Uncert
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#40723;&#21169;&#28145;&#24230;&#27169;&#22411;&#21033;&#29992;&#26356;&#22810;&#26679;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#20943;&#36731;&#31616;&#21333;&#24615;&#20559;&#24046;&#24102;&#26469;&#30340;OOD&#25512;&#24191;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#38382;&#39064;&#21644;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06161</link><description>&lt;p&gt;
&#20943;&#36731;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31616;&#21333;&#24615;&#20559;&#24046;&#20197;&#25913;&#21892;OOD&#25512;&#24191;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness. (arXiv:2310.06161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#40723;&#21169;&#28145;&#24230;&#27169;&#22411;&#21033;&#29992;&#26356;&#22810;&#26679;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#20943;&#36731;&#31616;&#21333;&#24615;&#20559;&#24046;&#24102;&#26469;&#30340;OOD&#25512;&#24191;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#38382;&#39064;&#21644;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#30693;&#23384;&#22312;&#31616;&#21333;&#24615;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#20542;&#21521;&#20110;&#23398;&#20064;&#8220;&#31616;&#21333;&#8221;&#29305;&#24449;&#32780;&#19981;&#26159;&#26356;&#8220;&#22797;&#26434;&#8221;&#30340;&#29305;&#24449;&#65292;&#21363;&#20351;&#21518;&#32773;&#21487;&#33021;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#31616;&#21333;&#24615;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#20855;&#26377;&#36739;&#24046;&#30340;OOD&#25512;&#24191;&#30340;&#20559;&#35265;&#24615;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#40723;&#21169;&#27169;&#22411;&#20351;&#29992;&#26356;&#22810;&#26679;&#30340;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#26465;&#20214;&#20114;&#20449;&#24687;&#23545;&#20854;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#24471;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#35774;&#32622;&#21644;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#31616;&#21333;&#24615;&#20559;&#24046;&#65292;&#20419;&#20351;&#26356;&#22810;&#29305;&#24449;&#34987;&#20351;&#29992;&#65292;&#22686;&#24378;&#20102;OOD&#25512;&#24191;&#65292;&#24182;&#25552;&#39640;&#20102;&#23376;&#32452;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#34917;&#20805;&#36825;&#20123;&#32467;&#26524;&#19982;&#23545;&#27491;&#21017;&#21270;&#25928;&#26524;&#21450;&#20854;OOD&#25512;&#24191;&#29305;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learning 'simple' features over more 'complex' ones, even when the latter may be more informative. Simplicity bias can lead to the model making biased predictions which have poor out-of-distribution (OOD) generalization. To address this, we propose a framework that encourages the model to use a more diverse set of features to make predictions. We first train a simple model, and then regularize the conditional mutual information with respect to it to obtain the final model. We demonstrate the effectiveness of this framework in various problem settings and real-world applications, showing that it effectively addresses simplicity bias and leads to more features being used, enhances OOD generalization, and improves subgroup robustness and fairness. We complement these results with theoretical analyses of the effect of the regularization and its OOD generalization properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32553;&#25918;&#26799;&#24230;&#19979;&#38477;&#65288;ScaledGD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24658;&#23450;&#36895;&#29575;&#19979;&#25910;&#25947;&#65292;&#32780;&#19981;&#21463;&#20302;&#31209;&#23545;&#35937;&#26465;&#20214;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20302;&#36845;&#20195;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.06159</link><description>&lt;p&gt;
&#32463;&#36807;&#32553;&#25918;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#65292;&#29978;&#33267;&#36807;&#21442;&#25968;&#21270;&#30340;&#21487;&#35777;&#26126;&#21152;&#36895;&#30340;&#30149;&#24577;&#20302;&#31209;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization. (arXiv:2310.06159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32553;&#25918;&#26799;&#24230;&#19979;&#38477;&#65288;ScaledGD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24658;&#23450;&#36895;&#29575;&#19979;&#25910;&#25947;&#65292;&#32780;&#19981;&#21463;&#20302;&#31209;&#23545;&#35937;&#26465;&#20214;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20302;&#36845;&#20195;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#36935;&#21040;&#30340;&#35768;&#22810;&#38382;&#39064;&#21487;&#20197;&#24402;&#32435;&#20026;&#20174;&#19981;&#23436;&#25972;&#19988;&#21487;&#33021;&#25439;&#22351;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#20272;&#35745;&#20302;&#31209;&#23545;&#35937;&#65288;&#20363;&#22914;&#30697;&#38453;&#21644;&#24352;&#37327;&#65289;&#12290;&#36890;&#36807;&#30697;&#38453;&#21644;&#24352;&#37327;&#20998;&#35299;&#30340;&#35270;&#35282;&#65292;&#20854;&#20013;&#19968;&#31181;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31616;&#21333;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#22914;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30452;&#25509;&#24674;&#22797;&#20302;&#31209;&#22240;&#23376;&#65292;&#36825;&#26679;&#21487;&#20197;&#23454;&#29616;&#23567;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;GD&#30340;&#25910;&#25947;&#36895;&#29575;&#32447;&#24615;&#22320;&#20381;&#36182;&#20110;&#20302;&#31209;&#23545;&#35937;&#30340;&#26465;&#20214;&#25968;&#65292;&#26377;&#26102;&#29978;&#33267;&#26159;&#20108;&#27425;&#30340;&#65292;&#22240;&#27492;&#24403;&#38382;&#39064;&#30149;&#24577;&#26102;&#65292;GD&#30340;&#25910;&#25947;&#38750;&#24120;&#32531;&#24930;&#12290;&#26412;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#31216;&#20026;&#32553;&#25918;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;ScaledGD&#65289;&#65292;&#23427;&#33021;&#22815;&#20197;&#24658;&#23450;&#36895;&#29575;&#32447;&#24615;&#22320;&#25910;&#25947;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20302;&#31209;&#23545;&#35937;&#30340;&#26465;&#20214;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26799;&#24230;&#19979;&#38477;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27599;&#27425;&#36845;&#20195;&#25104;&#26412;&#36739;&#20302;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#40065;&#26834;&#20027;&#25104;&#20998;&#20272;&#35745;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#27979;&#22320;&#32447;&#21644;&#27969;&#21160;&#26469;&#25551;&#36848;&#21487;&#24494;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#21644;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;iable&#27969;&#24418;&#19978;&#36827;&#34892;&#32479;&#35745;&#21644;&#38477;&#38454;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.06157</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#30340;Eikonal&#26041;&#31243;&#65306;&#21487;&#24494;&#27969;&#24418;&#19978;&#30340;&#27979;&#22320;&#36317;&#31163;&#21644;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds. (arXiv:2310.06157v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#27979;&#22320;&#32447;&#21644;&#27969;&#21160;&#26469;&#25551;&#36848;&#21487;&#24494;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#21644;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;iable&#27969;&#24418;&#19978;&#36827;&#34892;&#32479;&#35745;&#21644;&#38477;&#38454;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21457;&#29616;&#30340;&#27969;&#24418;&#25552;&#20379;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#36825;&#20123;&#27969;&#24418;&#19978;&#30340;&#27979;&#22320;&#32447;&#23450;&#20041;&#20102;&#23616;&#37096;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#65292;&#24182;&#25552;&#20379;&#20102;&#36317;&#31163;&#30340;&#27010;&#24565;&#65292;&#36825;&#23545;&#20110;&#38477;&#38454;&#24314;&#27169;&#12289;&#32479;&#35745;&#25512;&#26029;&#21644;&#25554;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#34920;&#31034;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#22330;&#21644;&#27979;&#22320;&#27969;&#21160;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;Eikonal&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#22914;&#20309;&#24433;&#21709;&#36317;&#31163;&#22330;&#65292;&#24182;&#21033;&#29992;&#27979;&#22320;&#27969;&#21160;&#30452;&#25509;&#33719;&#24471;&#20840;&#23616;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#22312;&#21487;&#24494;&#27969;&#24418;&#19978;&#36827;&#34892;&#32479;&#35745;&#21644;&#38477;&#38454;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifolds discovered by machine learning models provide a compact representation of the underlying data. Geodesics on these manifolds define locally length-minimising curves and provide a notion of distance, which are key for reduced-order modelling, statistical inference, and interpolation. In this work, we propose a model-based parameterisation for distance fields and geodesic flows on manifolds, exploiting solutions of a manifold-augmented Eikonal equation. We demonstrate how the geometry of the manifold impacts the distance field, and exploit the geodesic flow to obtain globally length-minimising curves directly. This work opens opportunities for statistics and reduced-order modelling on differentiable manifolds.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;DiscDiff&#65292;&#29992;&#20110;&#31163;&#25955;DNA&#24207;&#21015;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#31163;&#25955;DNA&#24207;&#21015;&#23884;&#20837;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#31163;&#25955;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FReD&#65292;&#29992;&#20110;&#35780;&#20272;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.06150</link><description>&lt;p&gt;
DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Model for DNA Sequence Generation. (arXiv:2310.06150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06150
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;DiscDiff&#65292;&#29992;&#20110;&#31163;&#25955;DNA&#24207;&#21015;&#29983;&#25104;&#12290;&#36890;&#36807;&#23558;&#31163;&#25955;DNA&#24207;&#21015;&#23884;&#20837;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#31163;&#25955;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FReD&#65292;&#29992;&#20110;&#35780;&#20272;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36816;&#29992;&#65292;&#20026;&#21512;&#25104;DNA&#24207;&#21015;&#29983;&#25104;&#39046;&#22495;&#25171;&#24320;&#20102;&#26032;&#30340;&#21069;&#26223;&#12290;&#34429;&#28982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#36825;&#20010;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#27169;&#24335;&#23849;&#28291;&#31561;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#26377;&#21069;&#26223;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#21463;&#36825;&#20123;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#31163;&#25955;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;DiscDiff&#12290;&#36890;&#36807;&#23558;&#31163;&#25955;DNA&#24207;&#21015;&#31616;&#21333;&#22320;&#23884;&#20837;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#29983;&#25104;&#31163;&#25955;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;Fr\'echet&#37325;&#24314;&#36317;&#31163;&#65288;FReD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The harnessing of machine learning, especially deep generative models, has opened up promising avenues in the field of synthetic DNA sequence generation. Whilst Generative Adversarial Networks (GANs) have gained traction for this application, they often face issues such as limited sample diversity and mode collapse. On the other hand, Diffusion Models are a promising new class of generative models that are not burdened with these problems, enabling them to reach the state-of-the-art in domains such as image generation. In light of this, we propose a novel latent diffusion model, DiscDiff, tailored for discrete DNA sequence generation. By simply embedding discrete DNA sequences into a continuous latent space using an autoencoder, we are able to leverage the powerful generative abilities of continuous diffusion models for the generation of discrete data. Additionally, we introduce Fr\'echet Reconstruction Distance (FReD) as a new metric to measure the sample quality of DNA sequence gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#12289;MAML&#21644;Reptile&#22312;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#39046;&#22495;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#24403;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20165;&#23545;&#39044;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20934;&#32447;&#21487;&#33021;&#27604;&#26356;&#22797;&#26434;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.06148</link><description>&lt;p&gt;
&#29702;&#35299;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques. (arXiv:2310.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#12289;MAML&#21644;Reptile&#22312;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#39046;&#22495;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#24403;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20165;&#23545;&#39044;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20934;&#32447;&#21487;&#33021;&#27604;&#26356;&#22797;&#26434;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#12290;&#20803;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#25552;&#39640;&#36825;&#20123;&#32593;&#32476;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#20803;&#23398;&#20064;&#25216;&#26415;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35780;&#20272;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#20219;&#21153;&#26102;&#65292;&#19982;&#22797;&#26434;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;MAML&#65289;&#30456;&#27604;&#65292;&#20165;&#23545;&#39044;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20934;&#32447;&#21487;&#33021;&#26356;&#26377;&#25928;&#12290;&#36825;&#19968;&#28857;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;MAML&#30340;&#23398;&#20064;&#34892;&#20026;&#19982;&#24494;&#35843;&#30340;&#34892;&#20026;&#31867;&#20284;&#65306;&#20004;&#32773;&#37117;&#20381;&#36182;&#20110;&#37325;&#22797;&#20351;&#29992;&#24050;&#23398;&#20064;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#24494;&#35843;&#12289;MAML&#21644;&#21478;&#19968;&#31181;&#21517;&#20026;Reptile&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;MAML&#21644;Reptile&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;LLM&#26102;&#20195;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;RLHF&#12289;Prompting&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.06147</link><description>&lt;p&gt;
&#22312;LLM&#26102;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#20160;&#20040;&#26159;&#24517;&#35201;&#30340;&#65311;&#20160;&#20040;&#26159;&#38656;&#35201;&#30340;&#65311;&#24378;&#21270;&#23398;&#20064;&#23545;RLHF&#12289;Prompting&#31561;&#30340;&#35270;&#35282;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond. (arXiv:2310.06147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06147
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;LLM&#26102;&#20195;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;RLHF&#12289;Prompting&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#21462;&#24471;&#20102;ChatGPT&#21644;GPT-4&#31561;&#25104;&#21151;&#20135;&#21697;&#12290;&#23427;&#20204;&#22312;&#36981;&#24490;&#25351;&#20196;&#24182;&#25552;&#20379;&#26080;&#23475;&#12289;&#26377;&#24110;&#21161;&#21644;&#35802;&#23454;&#65288;3H&#65289;&#22238;&#31572;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#20027;&#35201;&#24402;&#21151;&#20110;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#19982;LLM&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;RL&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#36890;&#36807;&#35752;&#35770;RL&#22312;&#20309;&#26102;&#12289;&#20309;&#22320;&#21644;&#22914;&#20309;&#20248;&#31168;&#65292;&#35299;&#37322;&#36825;&#19968;&#25216;&#26415;&#30340;&#31070;&#31192;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#39046;&#22495;&#65292;&#36825;&#20123;&#39046;&#22495;&#21487;&#33021;&#20250;&#20174;RLHF&#30740;&#31350;&#20013;&#33719;&#30410;&#25110;&#20026;&#20854;&#20570;&#20986;&#36129;&#29486;&#12290;&#37325;&#28857;&#20869;&#23481;&#65306;1. RLHF&#26159;&#24102;&#26377;&#31163;&#32447;&#31034;&#33539;&#25968;&#25454;&#30340;&#22312;&#32447;&#36870;&#21521;RL&#12290;2. RLHF&#27604;SFT&#26356;&#22909;&#65292;&#22240;&#20026;&#27169;&#20223;&#23398;&#20064;&#65288;&#21644;&#36870;&#21521;RL&#65289;&#27604;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#26356;&#22909;&#65292;&#33021;&#22815;&#32531;&#35299;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;3. RLHF&#20013;&#30340;RM&#27493;&#39588;&#20135;&#29983;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#20195;&#29702;&#65292;&#36825;&#26679;&#30340;&#35265;&#35299;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;LLM&#20219;&#21153;&#65292;&#20363;&#22914;&#25552;&#31034;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.  Highlighted Takeaways:  1. RLHF is Online Inverse RL with Offline Demonstration Data.  2. RLHF $&gt;$ SFT because Imitation Learning (and Inverse RL) $&gt;$ Behavior Cloning (BC) by alleviating the problem of compounding error.  3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evalua
&lt;/p&gt;</description></item><item><title>HydraViT&#26159;&#19968;&#31181;&#23558;&#21464;&#21387;&#22120;&#19982;&#22810;&#20998;&#25903;&#36755;&#20986;&#27169;&#22359;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22810;&#26631;&#31614;&#30142;&#30149;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06143</link><description>&lt;p&gt;
HydraViT:&#33258;&#36866;&#24212;&#22810;&#20998;&#25903;&#21464;&#21387;&#22120;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22810;&#26631;&#31614;&#30142;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease Classification from Chest X-ray Images. (arXiv:2310.06143v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06143
&lt;/p&gt;
&lt;p&gt;
HydraViT&#26159;&#19968;&#31181;&#23558;&#21464;&#21387;&#22120;&#19982;&#22810;&#20998;&#25903;&#36755;&#20986;&#27169;&#22359;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22810;&#26631;&#31614;&#30142;&#30149;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#23556;&#32447;&#26159;&#35782;&#21035;&#33016;&#37096;&#30142;&#30149;&#30340;&#19968;&#31181;&#37325;&#35201;&#35786;&#26029;&#24037;&#20855;&#65292;&#30001;&#20110;&#30149;&#29702;&#24322;&#24120;&#22312;&#32954;&#37096;&#30340;&#39640;&#25935;&#24863;&#24615;&#65292;&#20294;&#30001;&#20110;&#30149;&#29702;&#30340;&#22823;&#23567;&#21644;&#20301;&#32622;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#30149;&#29702;&#30340;&#35270;&#35273;&#30456;&#20284;&#24615;&#21644;&#20849;&#21516;&#20986;&#29616;&#65292;&#20197;&#22270;&#20687;&#20026;&#39537;&#21160;&#30340;&#35786;&#26029;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#20026;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#21306;&#22495;&#24448;&#24448;&#21344;&#25454;&#35786;&#26029;&#22270;&#20687;&#30340;&#30456;&#23545;&#23567;&#37096;&#20998;&#65292;&#25152;&#20197;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22522;&#20110;&#30340;&#20998;&#31867;&#27169;&#22411;&#21463;&#21040;&#20102;&#23427;&#20204;&#30340;&#23616;&#37096;&#24615;&#20559;&#35265;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#34429;&#28982;&#20043;&#21069;&#26366;&#23558;CNNs&#19982;&#27880;&#24847;&#22270;&#25110;&#31354;&#38388;&#25513;&#30721;&#30456;&#32467;&#21512;&#65292;&#20197;&#25351;&#23548;&#23545;&#28508;&#22312;&#20851;&#38190;&#21306;&#22495;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#30149;&#29702;&#31354;&#38388;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#19979;&#23398;&#20064;&#23450;&#20301;&#25351;&#23548;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#25552;&#39640;&#22810;&#26631;&#31614;&#20998;&#31867;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;HydraViT&#65292;&#23427;&#23558;&#21464;&#21387;&#22120;&#39592;&#24178;&#32593;&#32476;&#19982;&#20855;&#26377;&#23398;&#20064;&#26435;&#37325;&#30340;&#22810;&#20998;&#25903;&#36755;&#20986;&#27169;&#22359;&#32467;&#21512;&#36215;&#26469;&#21327;&#21516;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chest X-ray is an essential diagnostic tool in the identification of chest diseases given its high sensitivity to pathological abnormalities in the lungs. However, image-driven diagnosis is still challenging due to heterogeneity in size and location of pathology, as well as visual similarities and co-occurrence of separate pathology. Since disease-related regions often occupy a relatively small portion of diagnostic images, classification models based on traditional convolutional neural networks (CNNs) are adversely affected given their locality bias. While CNNs were previously augmented with attention maps or spatial masks to guide focus on potentially critical regions, learning localization guidance under heterogeneity in the spatial distribution of pathology is challenging. To improve multi-label classification performance, here we propose a novel method, HydraViT, that synergistically combines a transformer backbone with a multi-branch output module with learned weighting. The tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21464;&#37327;&#19982;&#23427;&#20204;&#30340;&#20027;&#25104;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#25551;&#36848;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20195;&#25968;&#20844;&#24335;&#12290;&#36825;&#20010;&#20844;&#24335;&#21487;&#20197;&#24212;&#29992;&#20110;&#20248;&#21270;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#22240;&#23376;&#20998;&#26512;&#20013;&#30340;&#25104;&#20998;&#25968;&#30446;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.06139</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#21464;&#37327;&#21450;&#20854;&#20027;&#25104;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Correlation between Random Variables and their Principal Components. (arXiv:2310.06139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21464;&#37327;&#19982;&#23427;&#20204;&#30340;&#20027;&#25104;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#25551;&#36848;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20195;&#25968;&#20844;&#24335;&#12290;&#36825;&#20010;&#20844;&#24335;&#21487;&#20197;&#24212;&#29992;&#20110;&#20248;&#21270;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#22240;&#23376;&#20998;&#26512;&#20013;&#30340;&#25104;&#20998;&#25968;&#30446;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35797;&#22270;&#25214;&#21040;&#25551;&#36848;&#38543;&#26426;&#21464;&#37327;&#19982;&#20195;&#34920;&#23427;&#20204;&#30340;&#20027;&#25104;&#20998;&#20043;&#38388;&#30456;&#20851;&#31995;&#25968;&#30340;&#20195;&#25968;&#20844;&#24335;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#20174;&#19982;&#21333;&#20010;&#38543;&#26426;&#21464;&#37327;&#30456;&#20851;&#30340;&#36873;&#23450;&#32479;&#35745;&#25968;&#25454;&#20986;&#21457;&#65292;&#23558;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#22312;&#32447;&#24615;&#20195;&#25968;&#35821;&#35328;&#20013;&#34920;&#31034;&#20026;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#30340;&#31561;&#25928;&#24418;&#24335;&#65292;&#20351;&#29992;&#21521;&#37327;&#21644;&#30697;&#38453;&#30340;&#27010;&#24565;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#39044;&#26399;&#30340;&#20844;&#24335;&#12290;&#25214;&#21040;&#30340;&#20844;&#24335;&#19982;&#22240;&#23376;&#20998;&#26512;&#20013;&#29992;&#20110;&#35745;&#31639;&#22240;&#23376;&#36733;&#33655;&#30340;&#20844;&#24335;&#30456;&#21516;&#12290;&#35752;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#23558;&#35813;&#20844;&#24335;&#24212;&#29992;&#20110;&#20248;&#21270;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#30340;&#20027;&#25104;&#20998;&#25968;&#30446;&#65292;&#20197;&#21450;&#22240;&#23376;&#20998;&#26512;&#20013;&#30340;&#22240;&#23376;&#25968;&#30446;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article attempts to find an algebraic formula describing the correlation coefficients between random variables and the principal components representing them. As a result of the analysis, starting from selected statistics relating to individual random variables, the equivalents of these statistics relating to a set of random variables were presented in the language of linear algebra, using the concepts of vector and matrix. This made it possible, in subsequent steps, to derive the expected formula. The formula found is identical to the formula used in Factor Analysis to calculate factor loadings. The discussion showed that it is possible to apply this formula to optimize the number of principal components in Principal Component Analysis, as well as to optimize the number of factors in Factor Analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LTrajDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#22122;&#22768;&#31227;&#21160;&#25968;&#25454;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#24067;&#23616;&#24207;&#21015;&#65292;&#20811;&#26381;&#20102;&#30001;&#22024;&#26434;&#25968;&#25454;&#12289;&#19981;&#23436;&#25972;&#36712;&#36857;&#21644;&#29615;&#22659;&#22240;&#32032;&#23548;&#33268;&#30340;&#35270;&#35273;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2310.06138</link><description>&lt;p&gt;
&#20174;&#22024;&#26434;&#30340;&#31227;&#21160;&#27169;&#24577;&#20013;&#39044;&#27979;&#24067;&#23616;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Layout Sequence Prediction From Noisy Mobile Modality. (arXiv:2310.06138v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LTrajDiff&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#22122;&#22768;&#31227;&#21160;&#25968;&#25454;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#24067;&#23616;&#24207;&#21015;&#65292;&#20811;&#26381;&#20102;&#30001;&#22024;&#26434;&#25968;&#25454;&#12289;&#19981;&#23436;&#25972;&#36712;&#36857;&#21644;&#29615;&#22659;&#22240;&#32032;&#23548;&#33268;&#30340;&#35270;&#35273;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#22312;&#29702;&#35299;&#34892;&#20154;&#31227;&#21160;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#20381;&#36182;&#20110;&#35270;&#35273;&#27169;&#24577;&#30340;&#38271;&#12289;&#23436;&#25972;&#19988;&#20934;&#30830;&#35266;&#27979;&#21040;&#30340;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#20986;&#29616;&#30456;&#26426;&#36974;&#25377;&#12289;&#36951;&#28431;&#23545;&#35937;&#25110;&#30001;&#20110;&#29615;&#22659;&#22240;&#32032;&#23548;&#33268;&#23545;&#35937;&#19981;&#22312;&#35270;&#32447;&#33539;&#22260;&#20869;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#36712;&#36857;&#19981;&#23436;&#25972;&#25110;&#22122;&#22768;&#36739;&#22823;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LTrajDiff&#65292;&#23558;&#34987;&#36974;&#25377;&#25110;&#19981;&#22312;&#35270;&#32447;&#33539;&#22260;&#20869;&#30340;&#29289;&#20307;&#19982;&#23436;&#20840;&#21487;&#35265;&#36712;&#36857;&#30340;&#29289;&#20307;&#21516;&#31561;&#37325;&#35201;&#12290;LTrajDiff&#21033;&#29992;&#31227;&#21160;&#25163;&#26426;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20811;&#26381;&#19981;&#22312;&#35270;&#32447;&#33539;&#22260;&#20869;&#30340;&#32422;&#26463;&#65292;&#20294;&#24341;&#20837;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#27169;&#24577;&#34701;&#21512;&#12289;&#22122;&#22768;&#25968;&#25454;&#21644;&#32570;&#20047;&#31354;&#38388;&#24067;&#23616;&#21644;&#29289;&#20307;&#23610;&#23544;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#37319;&#29992;&#20174;&#31895;&#21040;&#31934;&#30340;&#25193;&#25955;&#31574;&#30053;&#65292;&#20174;&#22024;&#26434;&#30340;&#31227;&#21160;&#25968;&#25454;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#24067;&#23616;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#33258;&#21160;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#36719;&#31561;&#21464;&#24615;&#30340;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#36793;&#32536;&#20284;&#28982;&#26469;&#23454;&#29616;&#23618;&#38388;&#23545;&#31216;&#24615;&#30340;&#33258;&#21160;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.06131</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#33258;&#21160;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Layer-wise Equivariances Automatically using Gradients. (arXiv:2310.06131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#33258;&#21160;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#36719;&#31561;&#21464;&#24615;&#30340;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#36793;&#32536;&#20284;&#28982;&#26469;&#23454;&#29616;&#23618;&#38388;&#23545;&#31216;&#24615;&#30340;&#33258;&#21160;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#23558;&#31561;&#21464;&#24615;&#23545;&#31216;&#24615;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#31216;&#24615;&#25552;&#20379;&#20102;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#22266;&#23450;&#30828;&#32422;&#26463;&#65292;&#38656;&#35201;&#20107;&#20808;&#25351;&#23450;&#65292;&#24182;&#19988;&#19981;&#33021;&#36866;&#24212;&#25913;&#21464;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20801;&#35768;&#28789;&#27963;&#30340;&#23545;&#31216;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#33258;&#21160;&#22320;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23545;&#31216;&#24615;&#21644;&#30456;&#20851;&#30340;&#26435;&#37325;&#36830;&#25509;&#32467;&#26500;&#26377;&#20004;&#20010;&#22256;&#38590;&#12290;&#39318;&#20808;&#65292;&#23427;&#38656;&#35201;&#26377;&#25928;&#28789;&#27963;&#30340;&#23618;&#38388;&#31561;&#21464;&#24615;&#21442;&#25968;&#21270;&#12290;&#20854;&#27425;&#65292;&#23545;&#31216;&#24615;&#20316;&#20026;&#32422;&#26463;&#65292;&#22240;&#27492;&#19981;&#20250;&#34987;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#40723;&#21169;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36719;&#31561;&#21464;&#24615;&#30340;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36793;&#32536;&#20284;&#28982;&#26469;&#23398;&#20064;&#23618;&#38388;&#31561;&#21464;&#24615;&#30340;&#25968;&#37327;&#65292;&#20854;&#20013;&#36793;&#32536;&#20284;&#28982;&#26159;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#20272;&#35745;&#30340;&#12290;&#36825;&#20010;&#30446;&#26631;&#24179;&#34913;&#20102;&#25968;&#25454;&#25311;&#21512;&#21644;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#20351;&#23618;&#38388;&#23545;&#31216;&#24615;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in dee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#22495;Conformer&#27169;&#22411;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#27169;&#22411;&#65292;&#22312;&#22024;&#26434;&#30340;&#28151;&#21709;&#22768;&#23398;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31163;&#25928;&#26524;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06125</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#28151;&#21709;&#22768;&#23398;&#29615;&#22659;&#20013;&#65292;&#20851;&#20110;&#21333;&#22768;&#36947;&#35821;&#38899;&#20998;&#31163;&#30340;&#26102;&#22495;Conformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments. (arXiv:2310.06125v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#22495;Conformer&#27169;&#22411;&#30340;&#21333;&#22768;&#36947;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#27169;&#22411;&#65292;&#22312;&#22024;&#26434;&#30340;&#28151;&#21709;&#22768;&#23398;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31163;&#25928;&#26524;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20998;&#31163;&#23545;&#22810;&#35828;&#35805;&#32773;&#25216;&#26415;&#30740;&#31350;&#20154;&#21592;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;&#21367;&#31215;&#22686;&#24378;&#36716;&#25442;&#22120;&#65288;conformer&#65289;&#22312;&#35768;&#22810;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#24471;&#21040;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#20998;&#31163;&#27169;&#22411;&#22823;&#22810;&#25968;&#26159;&#26102;&#22495;&#38899;&#39057;&#20998;&#31163;&#32593;&#32476;&#65288;TasNets&#65289;&#12290;&#19968;&#20123;&#25104;&#21151;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#21452;&#36890;&#36947;&#65288;DP&#65289;&#32593;&#32476;&#65292;&#23427;&#20204;&#25353;&#39034;&#24207;&#22788;&#29702;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;&#26102;&#22495;Conformer&#65288;TD-Conformers&#65289;&#26159;DP&#26041;&#27861;&#30340;&#19968;&#20010;&#31867;&#27604;&#65292;&#23427;&#20204;&#20063;&#25353;&#39034;&#24207;&#22788;&#29702;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#36739;&#30701;&#30340;&#20449;&#21495;&#38271;&#24230;&#19979;&#65292;&#30456;&#23545;&#20110;&#29305;&#24449;&#32500;&#24230;&#65292;Conformer&#22312;&#25511;&#21046;&#29305;&#24449;&#32500;&#24230;&#26102;&#26356;&#39640;&#25928;&#12290;&#25552;&#20986;&#20102;&#23376;&#37319;&#26679;&#23618;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#26368;&#20339;&#30340;TD-Conformer&#22312;WHAMR&#21644;WSJ0-2Mix&#22522;&#20934;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;14.6dB&#21644;21.2dB&#30340;SISDR&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech separation remains an important topic for multi-speaker technology researchers. Convolution augmented transformers (conformers) have performed well for many speech processing tasks but have been under-researched for speech separation. Most recent state-of-the-art (SOTA) separation models have been time-domain audio separation networks (TasNets). A number of successful models have made use of dual-path (DP) networks which sequentially process local and global information. Time domain conformers (TD-Conformers) are an analogue of the DP approach in that they also process local and global context sequentially but have a different time complexity function. It is shown that for realistic shorter signal lengths, conformers are more efficient when controlling for feature dimension. Subsampling layers are proposed to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#23427;&#21487;&#20197;&#20811;&#26381;&#22810;&#20219;&#21153;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#21033;&#29992;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#31561;&#26041;&#38754;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FTN&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30340;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06124</link><description>&lt;p&gt;
&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#29992;&#20110;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Factorized Tensor Networks for Multi-Task and Multi-Domain Learning. (arXiv:2310.06124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#23427;&#21487;&#20197;&#20811;&#26381;&#22810;&#20219;&#21153;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#21033;&#29992;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#31561;&#26041;&#38754;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FTN&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30340;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#20351;&#29992;&#21333;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;/&#39046;&#22495;&#65292;&#25110;&#32773;&#20808;&#21518;&#23398;&#20064;&#23427;&#20204;&#12290;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#20250;&#26159;&#21033;&#29992;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#25552;&#39640;&#32479;&#19968;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#23569;&#37327;&#38468;&#21152;&#21442;&#25968;&#23454;&#29616;&#19982;&#29420;&#31435;&#21333;&#20219;&#21153;/&#39046;&#22495;&#32593;&#32476;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;FTN&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#20923;&#32467;&#20027;&#24178;&#32593;&#32476;&#65292;&#24182;&#36880;&#27493;&#28155;&#21152;&#20219;&#21153;/&#39046;&#22495;&#29305;&#23450;&#30340;&#20302;&#31209;&#24352;&#37327;&#22240;&#23376;&#21040;&#20849;&#20139;&#30340;&#20923;&#32467;&#32593;&#32476;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;FTN&#38656;&#35201;&#36739;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#39046;&#22495;&#21644;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task and multi-domain learning methods seek to learn multiple tasks/domains, jointly or one after another, using a single unified network. The key challenge and opportunity is to exploit shared information across tasks and domains to improve the efficiency of the unified network. The efficiency can be in terms of accuracy, storage cost, computation, or sample complexity. In this paper, we propose a factorized tensor network (FTN) that can achieve accuracy comparable to independent single-task/domain networks with a small number of additional parameters. FTN uses a frozen backbone network from a source model and incrementally adds task/domain-specific low-rank tensor factors to the shared frozen network. This approach can adapt to a large number of target domains and tasks without catastrophic forgetting. Furthermore, FTN requires a significantly smaller number of task-specific parameters compared to existing methods. We performed experiments on widely used multi-domain and multi-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.06119</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#21644;&#24322;&#36136;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#20013;&#20844;&#24179;&#22522;&#20934;&#27979;&#35797;&#21644;&#25216;&#26415;&#26041;&#27861;&#36873;&#25321;&#30340;&#20105;&#35758;&#65292;&#24182;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#31995;&#32479;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#24433;&#21709;&#36825;&#20123;&#31995;&#32479;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;MTS&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#21644;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#33719;&#24471;&#20102;&#24456;&#22823;&#30340;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#21644;&#26102;&#31354;&#39044;&#27979;&#65288;STF&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#21644;&#25216;&#26415;&#26041;&#27861;&#30340;&#36873;&#25321;&#22312;&#30456;&#20851;&#24037;&#20316;&#20013;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#20123;&#20105;&#35758;&#26174;&#33879;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#36827;&#23637;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#20105;&#35758;&#65292;&#20197;&#25552;&#20379;&#23545;&#21462;&#24471;&#30340;&#36827;&#23637;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;&#20026;&#20102;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BasicTS&#65292;&#19968;&#20010;&#26088;&#22312;&#20844;&#24179;&#27604;&#36739;MTS&#39044;&#27979;&#30340;&#22522;&#20934;&#12290;BasicTS&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#27969;&#31243;&#21644;&#21512;&#29702;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#33021;&#22815;&#23545;30&#22810;&#31181;&#27969;&#34892;&#30340;MTS&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20844;&#27491;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.06117</link><description>&lt;p&gt;
&#36864;&#21518;&#19968;&#27493;&#65306;&#36890;&#36807;&#25277;&#35937;&#21796;&#36215;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#36864;&#21518;&#25552;&#31034;&#8221;&#30340;&#31616;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#21253;&#21547;&#20855;&#20307;&#32454;&#33410;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#25277;&#35937;&#65292;&#24471;&#20986;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#21644;&#21407;&#29702;&#26469;&#25351;&#23548;&#25512;&#29702;&#27493;&#39588;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#25512;&#29702;&#36335;&#24452;&#19978;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;PaLM-2L&#27169;&#22411;&#36827;&#34892;&#20102;&#36864;&#21518;&#25552;&#31034;&#23454;&#39564;&#65292;&#22312;&#21253;&#25324;STEM&#12289;&#30693;&#35782;&#38382;&#31572;&#21644;&#22810;&#36339;&#25512;&#29702;&#22312;&#20869;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35266;&#23519;&#21040;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20363;&#22914;&#65292;&#22312;MMLU&#29289;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#65292;&#36864;&#21518;&#25552;&#31034;&#21487;&#20197;&#23558;PaLM-2L&#30340;&#24615;&#33021;&#25552;&#21319;7%&#21644;11%&#65292;&#22312;TimeQA&#20219;&#21153;&#19978;&#25552;&#21319;27%&#65292;&#22312;MuSiQue&#20219;&#21153;&#19978;&#25552;&#21319;7%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19981;&#21487;&#30693;&#30340;PAC&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36328;&#36234;&#23481;&#37327;&#36825;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#29983;&#25104;&#21644;&#22312;&#32447;&#35775;&#38382;&#27169;&#22411;&#20043;&#38388;&#20197;&#21450;&#22312;&#32447;&#35775;&#38382;&#19979;&#30340;&#30830;&#23450;&#21644;&#38543;&#26426;MDP&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.06113</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#26159;&#22522;&#20110;&#19981;&#21487;&#30693;&#28608;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#21487;&#22788;&#29702;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
When is Agnostic Reinforcement Learning Statistically Tractable?. (arXiv:2310.06113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19981;&#21487;&#30693;&#30340;PAC&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#36328;&#36234;&#23481;&#37327;&#36825;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#29983;&#25104;&#21644;&#22312;&#32447;&#35775;&#38382;&#27169;&#22411;&#20043;&#38388;&#20197;&#21450;&#22312;&#32447;&#35775;&#38382;&#19979;&#30340;&#30830;&#23450;&#21644;&#38543;&#26426;MDP&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19981;&#21487;&#30693;&#30340;PAC&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#38656;&#35201;&#21644;&#19968;&#20010;&#26410;&#30693;&#30340;&#26377;&#21487;&#33021;&#26377;&#22823;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#36827;&#34892;&#22810;&#23569;&#36718;&#20114;&#21160;&#26469;&#23398;&#20064;&#19968;&#20010;&#20851;&#20110;&#928;&#30340;&#949;-&#27425;&#20248;&#31574;&#30053;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#31216;&#20026;&#8220;&#36328;&#36234;&#23481;&#37327;&#8221;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#24182;&#19988;&#19982;MDP&#30340;&#21160;&#24577;&#26080;&#20851;&#12290;&#36890;&#36807;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#26377;&#30028;&#30340;&#36328;&#36234;&#23481;&#37327;&#21487;&#20197;&#21051;&#30011;PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#32447;RL&#26469;&#35828;&#65292;&#24773;&#20917;&#26356;&#21152;&#24494;&#22937;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#20010;&#20855;&#26377;&#26377;&#30028;&#36328;&#36234;&#23481;&#37327;&#30340;&#31574;&#30053;&#31867;&#21035;&#928;&#65292;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#12290;&#36825;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#29983;&#25104;&#21644;&#22312;&#32447;&#35775;&#38382;&#27169;&#22411;&#20043;&#38388;&#20197;&#21450;&#22312;&#32447;&#35775;&#38382;&#19979;&#30340;&#30830;&#23450;/&#38543;&#26426;MDP&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#21487;&#23398;&#20064;&#24615;&#20043;&#38388;&#30340;&#20986;&#20046;&#24847;&#26009;&#30340;&#24046;&#24322;&#12290;&#22312;&#31215;&#26497;&#30340;&#26041;&#38754;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#22826;&#38451;&#8221;
&lt;/p&gt;
&lt;p&gt;
We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#20998;&#26512;&#20102;&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adv-NTK&#30340;AT&#31639;&#27861;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06112</link><description>&lt;p&gt;
&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#20998;&#26512;&#65306;&#19968;&#31181;NTK&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach. (arXiv:2310.06112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#20998;&#26512;&#20102;&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adv-NTK&#30340;AT&#31639;&#27861;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(AT)&#26159;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#40065;&#26834;&#24615;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23454;&#39564;&#35777;&#26126;&#23427;&#23384;&#22312;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#65292;&#21363;&#38271;&#26102;&#38388;&#30340;AT&#21487;&#33021;&#23545;DNNs&#30340;&#40065;&#26834;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;DNNs&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#29702;&#35770;&#38750;&#24179;&#20961;&#22320;&#25193;&#23637;&#21040;AT&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;AT&#35757;&#32451;&#30340;&#23485;DNN&#21487;&#20197;&#24456;&#22909;&#22320;&#36817;&#20284;&#20026;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;DNN&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24179;&#26041;&#25439;&#22833;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#32447;&#24615;&#21270;DNN&#30340;&#38381;&#24335;AT&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;AT&#36864;&#21270;&#29616;&#35937;&#65306;&#38271;&#26399;&#30340;AT&#23558;&#23548;&#33268;&#23485;DNN&#36864;&#21270;&#20026;&#27809;&#26377;AT&#30340;DNN&#65292;&#20174;&#32780;&#24341;&#36215;&#40065;&#26834;&#36807;&#25311;&#21512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Adv-NTK&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#38024;&#23545;&#26080;&#38480;&#23485;&#30340;DNNs&#30340;AT&#31639;&#27861;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Adv-NTK&#21487;&#20197;&#24110;&#21161;&#26080;&#38480;&#23485;&#30340;DNNs&#25552;&#21319;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#33879;&#30340;&#31867;&#21035;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#29992;&#25143;&#19982;LLM&#20132;&#20114;&#21512;&#20316;&#36827;&#34892;&#26631;&#27880;&#65292;&#24418;&#25104;&#20998;&#31867;&#25552;&#31034;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;82%&#12290;</title><link>http://arxiv.org/abs/2310.06111</link><description>&lt;p&gt;
BYOC: &#20351;&#29992;&#21512;&#33879;&#30340;&#31867;&#21035;&#25551;&#36848;&#20010;&#24615;&#21270;&#36827;&#34892;&#23569;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions. (arXiv:2310.06111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06111
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21512;&#33879;&#30340;&#31867;&#21035;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#29992;&#25143;&#19982;LLM&#20132;&#20114;&#21512;&#20316;&#36827;&#34892;&#26631;&#27880;&#65292;&#24418;&#25104;&#20998;&#31867;&#25552;&#31034;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;82%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#24102;&#26631;&#27880;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#26102;&#65292;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#25552;&#31034;&#24182;&#20351;&#29992;&#33021;&#23481;&#32435;&#35768;&#22810;&#31034;&#20363;&#30340;&#38271;&#19978;&#19979;&#25991;&#12290;&#32467;&#26524;&#65292;&#26222;&#36890;&#29992;&#25143;&#19981;&#33021;&#20026;&#33258;&#24049;&#26500;&#24314;&#20998;&#31867;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;LLM&#19981;&#26159;&#20351;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#32780;&#26159;&#30001;&#29992;&#25143;&#21644;LLM&#21512;&#33879;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#26174;&#33879;&#29305;&#24449;&#30340;&#25551;&#36848;&#36827;&#34892;&#25552;&#31034;&#12290;&#22312;&#29992;&#25143;&#26631;&#27880;&#27599;&#20010;&#23569;&#26679;&#26412;&#31034;&#20363;&#26102;&#65292;LLM&#20250;&#25552;&#20986;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#29992;&#25143;&#32473;&#20986;&#31572;&#26696;&#12290;&#31034;&#20363;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#34987;&#24635;&#32467;&#25104;&#20998;&#31867;&#25552;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#39640;&#20934;&#30830;&#29575;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;82%&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples. As a result, it is not possible for end-users to build classifiers for themselves. To address this issue, we propose a novel approach to few-shot text classification using an LLM. Rather than few-shot examples, the LLM is prompted with descriptions of the salient features of each class. These descriptions are coauthored by the user and the LLM interactively: while the user annotates each few-shot example, the LLM asks relevant questions that the user answers. Examples, questions, and answers are summarized to form the classification prompt. Our experiments show that our approach yields high accuracy classifiers, within 82% of the performance of models trained with s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#27934;&#23519;&#29616;&#35937;&#21487;&#33021;&#26159;&#30001;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#35757;&#32451;&#21160;&#24577;&#36807;&#28193;&#21040;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#36319;&#36394;&#36275;&#22815;&#30340;&#32479;&#35745;&#37327;&#65292;&#21457;&#29616;&#27934;&#23519;&#26159;&#22312;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#21518;&#65292;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#25214;&#21040;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20043;&#21518;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06110</link><description>&lt;p&gt;
&#20174;&#25042;&#24816;&#21040;&#20016;&#23500;&#35757;&#32451;&#21160;&#24577;&#30340;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Grokking as the Transition from Lazy to Rich Training Dynamics. (arXiv:2310.06110v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06110
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#27934;&#23519;&#29616;&#35937;&#21487;&#33021;&#26159;&#30001;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#35757;&#32451;&#21160;&#24577;&#36807;&#28193;&#21040;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#36319;&#36394;&#36275;&#22815;&#30340;&#32479;&#35745;&#37327;&#65292;&#21457;&#29616;&#27934;&#23519;&#26159;&#22312;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#21518;&#65292;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#25214;&#21040;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20043;&#21518;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#27934;&#23519;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25439;&#22833;&#22312;&#27979;&#35797;&#25439;&#22833;&#20043;&#21069;&#22823;&#24133;&#19979;&#38477;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#20174;&#25042;&#24816;&#30340;&#35757;&#32451;&#21160;&#24577;&#36716;&#21464;&#20026;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#26426;&#21046;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;Vanilla&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#22810;&#39033;&#24335;&#22238;&#24402;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#35813;&#35757;&#32451;&#23637;&#29616;&#20102;&#26080;&#27861;&#29992;&#29616;&#26377;&#29702;&#35770;&#35299;&#37322;&#30340;&#27934;&#23519;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35813;&#32593;&#32476;&#27979;&#35797;&#25439;&#22833;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#36319;&#36394;&#36825;&#20123;&#32479;&#35745;&#37327;&#25581;&#31034;&#20102;&#27934;&#23519;&#29616;&#35937;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32593;&#32476;&#39318;&#20808;&#23581;&#35797;&#20351;&#29992;&#21021;&#22987;&#29305;&#24449;&#25311;&#21512;&#26680;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#65292;&#25509;&#30528;&#22312;&#35757;&#32451;&#25439;&#22833;&#24050;&#32463;&#24456;&#20302;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21518;&#26399;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#25214;&#21040;&#20102;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27934;&#23519;&#20135;&#29983;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#29305;&#24449;&#23398;&#20064;&#30340;&#36895;&#29575;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#32553;&#25918;&#32593;&#32476;&#21442;&#25968;&#26469;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#26469;&#37327;&#21270;&#22522;&#20110;&#39118;&#38505;&#20915;&#31574;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20013;&#31163;&#25955;&#36755;&#20837;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#20943;&#36731;&#30456;&#20851;&#39118;&#38505;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#22788;&#29702;&#28041;&#21450;&#20998;&#31867;&#21644;&#31163;&#25955;&#29305;&#24449;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06105</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#39118;&#38505;&#20915;&#31574;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20013;&#31163;&#25955;&#36755;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making. (arXiv:2310.06105v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#26469;&#37327;&#21270;&#22522;&#20110;&#39118;&#38505;&#20915;&#31574;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#20013;&#31163;&#25955;&#36755;&#20837;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#20943;&#36731;&#30456;&#20851;&#39118;&#38505;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#22788;&#29702;&#28041;&#21450;&#20998;&#31867;&#21644;&#31163;&#25955;&#29305;&#24449;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#22312;&#22522;&#20110;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22312;&#21307;&#30103;&#12289;&#37329;&#34701;&#12289;&#21046;&#36896;&#21644;&#36136;&#37327;&#25511;&#21046;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#20943;&#36731;&#30456;&#20851;&#39118;&#38505;&#65292;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#25110;&#19981;&#30830;&#23450;&#24615;&#24212;&#35813;&#19982;&#31639;&#27861;&#30340;&#25972;&#20307;&#24615;&#33021;&#19968;&#36215;&#36827;&#34892;&#35780;&#20272;&#12290;&#26368;&#36817;&#20851;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#37327;&#21270;&#28304;&#20110;&#36755;&#20837;&#22122;&#22768;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#23545;&#36755;&#20837;&#22122;&#22768;&#30340;&#27491;&#24577;&#24615;&#20551;&#35774;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#20110;&#28041;&#21450;&#20998;&#31867;&#21644;&#31163;&#25955;&#29305;&#24449;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#37327;&#21270;DNN&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#28304;&#20110;&#36981;&#24490;&#24050;&#30693;&#26377;&#38480;&#31163;&#25955;&#20998;&#24067;&#30340;&#39044;&#27979;&#22120;&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#39044;&#27979;&#32467;&#26680;&#30149;&#24739;&#32773;&#27835;&#30103;&#32467;&#26524;&#30340;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Deep Neural Network (DNN) models in risk-based decision-making has attracted extensive attention with broad applications in medical, finance, manufacturing, and quality control. To mitigate prediction-related risks in decision making, prediction confidence or uncertainty should be assessed alongside the overall performance of algorithms. Recent studies on Bayesian deep learning helps quantify prediction uncertainty arises from input noises and model parameters. However, the normality assumption of input noise in these models limits their applicability to problems involving categorical and discrete feature variables in tabular datasets. In this paper, we propose a mathematical framework to quantify prediction uncertainty for DNN models. The prediction uncertainty arises from errors in predictors that follow some known finite discrete distribution. We then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#21464;&#20998;&#32972;&#38376;&#35843;&#25972;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21307;&#30103;&#25968;&#25454;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.06100</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#32972;&#38376;&#35843;&#25972;&#36827;&#34892;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
High Dimensional Causal Inference with Variational Backdoor Adjustment. (arXiv:2310.06100v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#21464;&#20998;&#32972;&#38376;&#35843;&#25972;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#21307;&#30103;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#38376;&#35843;&#25972;&#26159;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#32431;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#25968;&#37327;&#12290;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#32972;&#38376;&#35843;&#25972;&#21487;&#29992;&#20110;&#25511;&#21046;&#28151;&#26434;&#22240;&#32032;&#24182;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#21487;&#33021;&#24341;&#21457;&#19968;&#31995;&#21015;&#28508;&#22312;&#38382;&#39064;&#65306;&#21487;&#35745;&#31639;&#24615;&#12289;&#21487;&#36776;&#35782;&#24615;&#12289;&#20248;&#21270;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#32500;&#27835;&#30103;&#21644;&#28151;&#26434;&#22240;&#32032;&#30340;&#32972;&#38376;&#35843;&#25972;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#32972;&#38376;&#35843;&#25972;&#35270;&#20026;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#38656;&#20381;&#36182;&#20195;&#29702;&#21464;&#37327;&#21644;&#38544;&#34255;&#28151;&#26434;&#22240;&#32032;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#39640;&#32500;&#29615;&#22659;&#20013;&#20272;&#35745;&#24178;&#39044;&#27010;&#29575;&#65292;&#21253;&#25324;&#21322;&#21512;&#25104;X&#20809;&#21307;&#30103;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#32972;&#38376;&#35843;&#25972;&#30340;&#39318;&#27425;&#24212;&#29992;&#65292;&#20854;&#20013;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#37117;&#26159;&#39640;&#32500;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#20013;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#23545;&#25968;&#20284;&#28982;&#24230;&#35780;&#20272;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.06085</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#35757;&#32451;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantile-based Maximum Likelihood Training for Outlier Detection. (arXiv:2310.06085v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#20013;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#23545;&#25968;&#20284;&#28982;&#24230;&#35780;&#20272;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#24615;&#23398;&#20064;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#20998;&#31867;&#39044;&#27979;&#30495;&#23454;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#24120;&#20540;&#26041;&#38754;&#65292;&#23427;&#32463;&#24120;&#23548;&#33268;&#35823;&#25253;&#38451;&#24615;&#65292;&#36825;&#22312;&#33258;&#20027;&#39550;&#39542;&#21644;&#35270;&#39057;&#30417;&#35270;&#31995;&#32479;&#31561;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20197;&#24448;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#23581;&#35797;&#21253;&#25324;&#20351;&#29992;&#23454;&#38469;&#24322;&#24120;&#20540;&#25968;&#25454;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#25110;&#32773;&#36890;&#36807;&#21512;&#25104;&#24322;&#24120;&#20540;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#20687;&#32032;&#31354;&#38388;&#20013;&#23545;&#20869;&#28857;&#36827;&#34892;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#23398;&#20064;&#20869;&#28857;&#20998;&#24067;&#65292;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#25552;&#39640;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#35780;&#20272;&#30340;&#23545;&#25968;&#20284;&#28982;&#24230;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discriminative learning effectively predicts true object class for image classification. However, it often results in false positives for outliers, posing critical concerns in applications like autonomous driving and video surveillance systems. Previous attempts to address this challenge involved training image classifiers through contrastive learning using actual outlier data or synthesizing outliers for self-supervised learning. Furthermore, unsupervised generative modeling of inliers in pixel space has shown limited success for outlier detection. In this work, we introduce a quantile-based maximum likelihood objective for learning the inlier distribution to improve the outlier separation during inference. Our approach fits a normalizing flow to pre-trained discriminative features and detects the outliers according to the evaluated log-likelihood. The experimental evaluation demonstrates the effectiveness of our method as it surpasses the performance of the state-of-the-art unsupervi
&lt;/p&gt;</description></item><item><title>transformers&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#31867;&#27604;&#33258;&#28982;&#35821;&#35328;&#21644;&#21270;&#23398;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#23558;&#24102;&#26469;&#26356;&#22810;&#31361;&#30772;&#21644;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.06083</link><description>&lt;p&gt;
Transformers&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transformers and Large Language Models for Chemistry and Drug Discovery. (arXiv:2310.06083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06083
&lt;/p&gt;
&lt;p&gt;
transformers&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#31867;&#27604;&#33258;&#28982;&#35821;&#35328;&#21644;&#21270;&#23398;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#23558;&#24102;&#26469;&#26356;&#22810;&#31361;&#30772;&#21644;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#30340;&#21457;&#26126;&#65292;&#24341;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35768;&#22810;&#39046;&#22495;&#30340;&#38761;&#21629;&#65292;&#20197;&#21450;&#22312;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#31361;&#30772;&#12290;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#31867;&#27604;&#26159;&#22914;&#20309;&#21551;&#21457;&#20351;&#29992;Transformer&#26469;&#35299;&#20915;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#29942;&#39048;&#65292;&#22914;&#36870;&#21512;&#25104;&#35268;&#21010;&#21644;&#21270;&#23398;&#31354;&#38388;&#25506;&#32034;&#12290;&#36825;&#19968;&#38761;&#21629;&#22987;&#20110;&#33021;&#22815;&#20351;&#29992;&#19968;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#27604;&#22914;&#32447;&#24615;&#21270;&#30340;&#20998;&#23376;&#22270;&#65292;&#28982;&#21518;&#21457;&#23637;&#21040;&#21253;&#25324;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#22914;&#26469;&#33258;&#20998;&#26512;&#20202;&#22120;&#30340;&#20809;&#35889;&#12289;&#21512;&#25104;&#34892;&#21160;&#21644;&#20154;&#31867;&#35821;&#35328;&#12290;&#19968;&#31181;&#26032;&#30340;&#36235;&#21183;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#33021;&#22815;&#35299;&#20915;&#21270;&#23398;&#20013;&#36890;&#29992;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26159;&#30001;&#33258;&#28982;&#35821;&#35328;&#25552;&#20379;&#30340;&#12290;&#38543;&#30528;&#25105;&#20204;&#32487;&#32493;&#25506;&#32034;&#21644;&#24212;&#29992;&#36825;&#20123;&#33021;&#21147;&#65292;&#25105;&#20204;&#21487;&#20197;&#26399;&#24453;&#36827;&#19968;&#27493;&#30340;&#31361;&#30772;&#21644;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#21363;Ito&#38142;&#30340;Ito&#25193;&#25955;&#36924;&#36817;&#12290;&#19982;&#22823;&#22810;&#25968;&#30456;&#20851;&#35770;&#25991;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#38142;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#21644;&#29366;&#24577;&#30456;&#20851;&#30340;&#22122;&#22768;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Ito&#38142;&#19982;&#23545;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20043;&#38388;&#30340;W2-&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#22312;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#39318;&#27425;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.06081</link><description>&lt;p&gt;
&#29992;&#20110;&#37319;&#26679;&#12289;&#20248;&#21270;&#21644;&#25552;&#21319;&#30340;&#36890;&#29992;Ito&#38142;&#30340;Ito&#25193;&#25955;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting. (arXiv:2310.06081v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#21363;Ito&#38142;&#30340;Ito&#25193;&#25955;&#36924;&#36817;&#12290;&#19982;&#22823;&#22810;&#25968;&#30456;&#20851;&#35770;&#25991;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#38142;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#21644;&#29366;&#24577;&#30456;&#20851;&#30340;&#22122;&#22768;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Ito&#38142;&#19982;&#23545;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20043;&#38388;&#30340;W2-&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#22312;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#39318;&#27425;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#30456;&#24403;&#19968;&#33324;&#21644;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#21363;Ito&#38142;&#65292;&#20854;&#31867;&#20284;&#20110;&#26576;&#20123;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;Euler-Maruyama&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#38142;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#26694;&#26550;&#12290;&#19982;&#22823;&#22810;&#25968;&#30456;&#20851;&#35770;&#25991;&#20013;&#30340;&#27491;&#24577;&#21644;&#29366;&#24577;&#29420;&#31435;&#22122;&#22768;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#38142;&#20855;&#26377;&#20960;&#20046;&#20219;&#24847;&#21508;&#21521;&#21516;&#24615;&#21644;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38142;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#21487;&#20197;&#26159;&#31934;&#30830;&#30340;&#65292;&#20197;&#28085;&#30422;&#35832;&#22914;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#12289;&#37319;&#26679;&#12289;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25110;&#38543;&#26426;&#26799;&#24230;&#25552;&#21319;&#31561;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Ito&#38142;&#19982;&#23545;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20043;&#38388;&#30340;W2-&#36317;&#31163;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#25110;&#35206;&#30422;&#20102;&#22823;&#37096;&#20998;&#24050;&#30693;&#30340;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26159;&#31532;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, our chain's drift and diffusion coefficient can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of the Ito chain and the corresponding Stochastic Differential Equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23637;&#31034;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06077</link><description>&lt;p&gt;
&#21487;&#23637;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Time-Series Forecasting. (arXiv:2310.06077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23637;&#31034;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#12290;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#65292;&#22914;&#20844;&#20849;&#21355;&#29983;&#12289;&#32463;&#27982;&#21644;&#31038;&#20250;&#24212;&#29992;&#65292;&#28041;&#21450;&#21040;&#21453;&#39304;&#24490;&#29615;&#65292;&#20854;&#20013;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#30340;&#32467;&#26524;&#65292;&#36827;&#32780;&#25913;&#21464;&#30446;&#26631;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#23637;&#31034;&#24615;&#65292;&#24341;&#20837;&#20102;&#21487;&#33021;&#20986;&#29616;&#8220;&#33258;&#25105;&#25269;&#28040;&#8221;&#25110;&#8220;&#33258;&#25105;&#23454;&#29616;&#8221;&#30340;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#23545;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23637;&#31034;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21487;&#23637;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PeTS&#65289;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#35299;&#20915;&#20102;&#24403;&#21487;&#33021;&#23384;&#22312;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#20934;&#30830;&#39044;&#27979;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#29305;&#24449;&#23637;&#31034;&#24615;&#36716;&#31227;&#65288;FPS&#65289;&#65292;&#23427;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#39044;&#27979;&#20998;&#24067;&#30340;&#21464;&#21270;&#21644;&#38543;&#21518;&#30340;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.  In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subseque
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#65292;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30140;&#30171;&#36712;&#36857;&#65292;&#20197;&#24110;&#21161;&#24739;&#32773;&#31649;&#29702;&#38256;&#29366;&#32454;&#32990;&#36139;&#34880;&#65292;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#30340;&#20381;&#36182;&#21644;&#21103;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.06075</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#30340;&#30171;&#33510;&#39044;&#27979;&#65306;&#39044;&#38450;&#38463;&#29255;&#31867;&#33647;&#29289;&#25104;&#30270;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction. (arXiv:2310.06075v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#65292;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30140;&#30171;&#36712;&#36857;&#65292;&#20197;&#24110;&#21161;&#24739;&#32773;&#31649;&#29702;&#38256;&#29366;&#32454;&#32990;&#36139;&#34880;&#65292;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#65292;&#24182;&#20943;&#23569;&#23545;&#38463;&#29255;&#31867;&#33647;&#29289;&#30340;&#20381;&#36182;&#21644;&#21103;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38256;&#29366;&#32454;&#32990;&#36139;&#34880;&#65288;SCD&#65289;&#26159;&#19968;&#31181;&#24930;&#24615;&#36951;&#20256;&#24615;&#30142;&#30149;&#65292;&#29305;&#24449;&#20026;&#21453;&#22797;&#21457;&#20316;&#30340;&#24613;&#24615;&#30140;&#30171;&#12290;&#38463;&#29255;&#31867;&#33647;&#29289;&#36890;&#24120;&#29992;&#20110;&#31649;&#29702;&#36825;&#20123;&#30140;&#30171;&#21457;&#20316;&#65307;&#22312;&#36825;&#31181;&#30142;&#30149;&#20013;&#20351;&#29992;&#38463;&#29255;&#31867;&#33647;&#29289;&#31649;&#29702;&#30140;&#30171;&#30340;&#31243;&#24230;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#38382;&#39064;&#12290;&#21560;&#39135;&#25104;&#30270;&#30340;&#39118;&#38505;&#21644;&#38463;&#29255;&#31867;&#33647;&#29289;&#27835;&#30103;&#30340;&#21103;&#20316;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#23558;&#26469;&#26356;&#22810;&#30340;&#30140;&#30171;&#21457;&#20316;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#23558;&#26469;&#30340;&#30140;&#30171;&#21457;&#23637;&#36712;&#36857;&#23545;&#20110;&#24110;&#21161;&#24739;&#32773;&#31649;&#29702;&#20182;&#20204;&#30340;SCD&#20197;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#32780;&#19981;&#25439;&#23475;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#30140;&#30171;&#20027;&#35201;&#26159;&#30001;&#24739;&#32773;&#33258;&#34892;&#25253;&#21578;&#35760;&#24405;&#30340;&#65292;&#33719;&#24471;&#35768;&#22810;&#30140;&#30171;&#35760;&#24405;&#26469;&#35774;&#35745;&#39044;&#27979;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35299;&#20915;&#30140;&#30171;&#39044;&#27979;&#38382;&#39064;&#26102;&#65292;&#20165;&#20381;&#38752;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#26082;&#26114;&#36149;&#21448;&#30171;&#33510;&#65288;&#22240;&#20026;&#38656;&#35201;&#24739;&#32773;&#37197;&#21512;&#65289;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#30140;&#30171;&#39044;&#27979;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23545;&#36825;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#23545;&#20110;&#24739;&#32773;&#34920;&#22411;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by recurrent acute painful episodes. Opioids are often used to manage these painful episodes; the extent of their use in managing pain in this disorder is an issue of debate. The risk of addiction and side effects of these opioid treatments can often lead to more pain episodes in the future. Hence, it is crucial to forecast future patient pain trajectories to help patients manage their SCD to improve their quality of life without compromising their treatment. It is challenging to obtain many pain records to design forecasting models since it is mainly recorded by patients' self-report. Therefore, it is expensive and painful (due to the need for patient compliance) to solve pain forecasting problems in a purely supervised manner. In light of this challenge, we propose to solve the pain forecasting problem using self-supervised learning methods. Also, clustering such time-series data is crucial for patient phenotyping,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#26368;&#20248;&#25506;&#32034;&#21644;&#21482;&#38656;&#35201;&#30456;&#21516;&#35745;&#31639;&#25805;&#20316;&#30340;&#31639;&#27861;&#65311;</title><link>http://arxiv.org/abs/2310.06069</link><description>&lt;p&gt;
&#26368;&#20248;&#25506;&#32034;&#19981;&#27604;&#27748;&#26222;&#26862;&#37319;&#26679;&#26356;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#26368;&#20248;&#25506;&#32034;&#21644;&#21482;&#38656;&#35201;&#30456;&#21516;&#35745;&#31639;&#25805;&#20316;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#32452;&#33218;$\mathcal{Z}\subset \mathbb{R}^d$&#21644;&#26410;&#30693;&#21442;&#25968;&#21521;&#37327;$\theta_\ast\in\mathbb{R}^d$&#30340;&#24773;&#20917;&#19979;&#65292;&#32431;&#25506;&#32034;&#32447;&#24615;&#33218;&#38382;&#39064;&#26088;&#22312;&#36890;&#36807;&#23545;$x^{\top}\theta_{\ast}$&#30340;&#22122;&#22768;&#27979;&#37327;&#65292;&#36820;&#22238;$\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$&#65292;&#24182;&#20197;&#39640;&#27010;&#29575;&#25214;&#21040;&#27491;&#30830;&#35299;&#12290;&#29616;&#26377;&#30340;&#65288;&#28176;&#36817;&#65289;&#26368;&#20248;&#26041;&#27861;&#35201;&#27714;&#35201;&#20040;&#20026;&#27599;&#20010;&#33218;$z\in \mathcal{Z}$&#36827;&#34892;&#28508;&#22312;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#35201;&#20040;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#26126;&#30830;&#22320;&#32500;&#25252;&#19968;&#37096;&#20998;&#27491;&#22312;&#32771;&#34385;&#30340;$\mathcal{Z}$&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#19982;&#27969;&#34892;&#19988;&#31616;&#21333;&#30340;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#29992;&#20110;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#24773;&#20917;&#23436;&#20840;&#30456;&#21453;&#65292;&#21518;&#32773;&#21482;&#38656;&#35201;&#35775;&#38382;&#21518;&#39564;&#37319;&#26679;&#21644;argmax oracle&#65292;&#24182;&#19988;&#22312;&#20219;&#20309;&#26102;&#38388;&#28857;&#37117;&#19981;&#38656;&#35201;&#26522;&#20030;$\mathcal{Z}$&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24050;&#30693;&#27748;&#26222;&#26862;&#37319;&#26679;&#23545;&#20110;&#32431;&#25506;&#32034;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#31639;&#27861;&#33021;&#22815;&#36827;&#34892;&#26368;&#20248;&#25506;&#32034;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#30456;&#21516;&#30340;&#35745;&#31639;&#25805;&#20316;&#65311;
&lt;/p&gt;
&lt;p&gt;
Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20135;&#29983;&#30340;&#22686;&#24191;&#25968;&#25454;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#26089;&#26399;&#30315;&#30187;&#21457;&#20316;&#20449;&#21495;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#30340;&#20020;&#30028;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.06059</link><description>&lt;p&gt;
&#39044;&#35686;&#21644;&#38544;&#21464;&#21270;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#21450;&#20803;&#26631;&#31614;&#32416;&#27491;&#30340;&#26089;&#26399;&#39044;&#35686;
&lt;/p&gt;
&lt;p&gt;
Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting. (arXiv:2310.06059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20135;&#29983;&#30340;&#22686;&#24191;&#25968;&#25454;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#26089;&#26399;&#30315;&#30187;&#21457;&#20316;&#20449;&#21495;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#30340;&#20020;&#30028;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#30315;&#30187;&#24739;&#32773;&#36827;&#34892;&#26089;&#26399;&#39044;&#35686;&#23545;&#20110;&#20182;&#20204;&#30340;&#23433;&#20840;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#39044;&#38450;&#25110;&#20943;&#23569;&#30315;&#30187;&#21457;&#20316;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#36890;&#36807;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#23545;&#26089;&#26399;&#30315;&#30187;&#21457;&#20316;&#20449;&#21495;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#20803;&#26631;&#31614;&#32416;&#27491;&#26041;&#27861;&#65292;&#25105;&#20204;&#34701;&#21512;&#20102;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#20135;&#29983;&#30340;&#22686;&#24191;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#36807;&#28193;&#26102;&#38388;&#20998;&#24067;&#26469;&#20248;&#36873;&#36873;&#25321;&#28508;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25552;&#21462;&#30340;&#20020;&#30028;&#21160;&#21147;&#23398;&#29305;&#24449;&#20063;&#34987;&#38598;&#25104;&#21040;&#20803;&#32593;&#32476;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;LSTM&#23454;&#26045;&#20026;&#22522;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#39044;&#27979;&#21508;&#31181;&#38271;&#26399;&#31383;&#21475;&#65288;1-2&#31186;&#30340;&#36755;&#20837;&#25968;&#25454;&#65289;&#20869;&#30340;&#30315;&#30187;&#21457;&#20316;&#65292;&#24182;&#21457;&#29616;&#39044;&#27979;&#20934;&#30830;&#29575;&#20986;&#29616;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early warning for epilepsy patients is crucial for their safety and well-being, in terms of preventing or minimizing the severity of seizures. Through the patients' EEG data, we propose a meta learning framework for improving prediction on early ictal signals. To better utilize the meta label corrector method, we fuse the information from both the real data and the augmented data from the latent Stochastic differential equation(SDE). Besides, we also optimally select the latent dynamical system via distribution of transition time between real data and that from the latent SDE. In this way, the extracted tipping dynamical feature is also integrated into the meta network to better label the noisy data. To validate our method, LSTM is implemented as the baseline model. We conduct a series of experiments to predict seizure in various long-term window from 1-2 seconds input data and find surprisingly increment of prediction accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#21387;&#32553;&#25104;&#21487;&#37096;&#32626;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25913;&#21892;&#26816;&#27979;&#28789;&#25935;&#24230;&#12290;&#21387;&#32553;&#21518;&#30340;&#27169;&#22411;&#22312;&#20943;&#23567;&#22823;&#23567;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#20855;&#26377;&#19982;&#36739;&#22823;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06047</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation for Anomaly Detection. (arXiv:2310.06047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#21387;&#32553;&#25104;&#21487;&#37096;&#32626;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25913;&#21892;&#26816;&#27979;&#28789;&#25935;&#24230;&#12290;&#21387;&#32553;&#21518;&#30340;&#27169;&#22411;&#22312;&#20943;&#23567;&#22823;&#23567;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#20855;&#26377;&#19982;&#36739;&#22823;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24191;&#27867;&#29992;&#20110;&#35782;&#21035;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22823;&#23567;&#24120;&#24120;&#38480;&#21046;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#22411;&#36807;&#31243;&#65292;&#23558;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#21387;&#32553;&#25104;&#19968;&#31181;&#21487;&#37096;&#32626;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25913;&#21892;&#26816;&#27979;&#28789;&#25935;&#24230;&#12290;&#21387;&#32553;&#21518;&#30340;&#27169;&#22411;&#22312;&#26174;&#33879;&#20943;&#23569;&#22823;&#23567;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#21516;&#26102;&#65292;&#34920;&#29616;&#19982;&#36739;&#22823;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised deep learning techniques are widely used to identify anomalous behaviour. The performance of such methods is a product of the amount of training data and the model size. However, the size is often a limiting factor for the deployment on resource-constrained devices. We present a novel procedure based on knowledge distillation for compressing an unsupervised anomaly detection model into a supervised deployable one and we suggest a set of techniques to improve the detection sensitivity. Compressed models perform comparably to their larger counterparts while significantly reducing the size and memory footprint.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.06045</link><description>&lt;p&gt;
&#36890;&#36807;&#30830;&#23450;&#24615;&#23545;&#27969;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#20005;&#37325;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model. (arXiv:2310.06045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#32654;&#22269;&#26412;&#22303;&#20005;&#37325;&#22825;&#27668;&#65288;&#40857;&#21367;&#39118;&#12289;&#20912;&#38649;&#21644;&#22823;&#39118;&#38453;&#65289;&#30340;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21518;&#22788;&#29702;&#23545;&#27969;&#20801;&#35768;&#27169;&#22411;&#65288;CAM&#65289;&#30340;&#39044;&#27979;&#12290;CGANs&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#30830;&#23450;&#24615;CAM&#39044;&#27979;&#20013;&#21019;&#24314;&#21512;&#25104;&#38598;&#25104;&#25104;&#21592;&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;CNN&#22788;&#29702;&#20197;&#20272;&#35745;&#20005;&#37325;&#22825;&#27668;&#30340;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24555;&#36895;&#21047;&#26032;&#65288;HRRR&#65289;1-24&#23567;&#26102;&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#21450;&#26292;&#39118;&#39044;&#35686;&#20013;&#24515;&#65288;SPC&#65289;&#30340;&#20005;&#37325;&#22825;&#27668;&#25253;&#21578;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;2021&#24180;&#30340;HRRR&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#32771;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overcon
&lt;/p&gt;</description></item><item><title>DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.06020</link><description>&lt;p&gt;
DyST&#65306;&#38754;&#21521;&#23454;&#38469;&#35270;&#39057;&#30340;&#21160;&#24577;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06020
&lt;/p&gt;
&lt;p&gt;
DyST&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#22330;&#26223;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#20174;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#20102;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#30456;&#26426;&#21644;&#22330;&#26223;&#20869;&#23481;&#30340;&#29420;&#31435;&#25511;&#21046;&#35270;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#19990;&#30028;&#30340;&#35270;&#35273;&#29702;&#35299;&#36229;&#36234;&#20102;&#21333;&#20010;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#24179;&#38754;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#20013;&#25429;&#25417;&#21040;&#23454;&#38469;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;Dynamic Scene Transformer&#65288;DyST&#65289;&#27169;&#22411;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#30740;&#31350;&#25104;&#26524;&#65292;&#23398;&#20064;&#20102;&#21333;&#30446;&#23454;&#38469;&#35270;&#39057;&#30340;&#28508;&#22312;&#20998;&#35299;&#65292;&#21253;&#25324;&#22330;&#26223;&#20869;&#23481;&#12289;&#27599;&#20010;&#35270;&#35282;&#30340;&#22330;&#26223;&#21160;&#24577;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#36890;&#36807;&#22312;&#21333;&#30446;&#35270;&#39057;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;DySO&#19978;&#36827;&#34892;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36825;&#31181;&#20998;&#31163;&#12290;DyST&#23398;&#20064;&#21040;&#20102;&#21160;&#24577;&#22330;&#26223;&#30340;&#20855;&#20307;&#28508;&#22312;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#22330;&#26223;&#30340;&#30456;&#26426;&#21644;&#20869;&#23481;&#36827;&#34892;&#29420;&#31435;&#25511;&#21046;&#30340;&#35270;&#22270;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.06009</link><description>&lt;p&gt;
AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#20998;&#32780;&#27835;&#20043;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Dynamics in AI-Driven Disempowerment. (arXiv:2310.06009v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06009
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20844;&#21496;&#35797;&#22270;&#21019;&#36896;&#20986;&#22312;&#22823;&#37096;&#20998;&#32463;&#27982;&#20215;&#20540;&#24037;&#20316;&#19978;&#36229;&#36234;&#20154;&#31867;&#30340;AI&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;AI&#27169;&#22411;&#24050;&#32463;&#33258;&#21160;&#21270;&#21066;&#24369;&#20102;&#19968;&#20123;&#33402;&#26415;&#23478;&#12289;&#28436;&#21592;&#21644;&#20316;&#23478;&#30340;&#29983;&#35745;&#12290;&#20294;&#26159;&#22312;&#37027;&#20123;&#20248;&#20808;&#32771;&#34385;&#24403;&#21069;&#21361;&#23475;&#21644;&#26410;&#26469;&#21361;&#23475;&#20043;&#38388;&#23384;&#22312;&#30528;&#20869;&#35751;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#26469;&#30740;&#31350;&#36825;&#31181;&#19981;&#22242;&#32467;&#30340;&#21407;&#22240;&#21644;&#21518;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#22312;&#21382;&#21490;&#19978;&#65292;&#38754;&#20020;&#20849;&#21516;&#23041;&#32961;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#21457;&#29616;&#32852;&#21512;&#36215;&#26469;&#23545;&#25239;&#35813;&#23041;&#32961;&#26159;&#26377;&#21033;&#30340;&#65292;&#32780;&#35813;&#20849;&#21516;&#23041;&#32961;&#21448;&#21457;&#29616;&#20998;&#32780;&#27835;&#20043;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#29616;&#23454;&#21442;&#25968;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#20960;&#20010;&#39044;&#27979;&#65292;&#22312;&#21382;&#21490;&#32463;&#39564;&#35760;&#24405;&#20013;&#24471;&#21040;&#20102;&#21021;&#27493;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.  Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;&#23545;&#35757;&#32451;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#20869;&#23384;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#22120;&#65288;PaRO&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#27169;&#22411;&#35757;&#32451;&#30340;&#20998;&#23618;&#37325;&#21472;&#29615;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65288;HO-Ring&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06003</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39640;&#25928;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Rethinking Memory and Communication Cost for Efficient Large Language Model Training. (arXiv:2310.06003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;&#23545;&#35757;&#32451;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#20869;&#23384;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#22120;&#65288;PaRO&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#27169;&#22411;&#35757;&#32451;&#30340;&#20998;&#23618;&#37325;&#21472;&#29615;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65288;HO-Ring&#65289;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#22823;&#35268;&#27169;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#36890;&#36807;&#21508;&#31181;&#20998;&#29255;&#25216;&#26415;&#20943;&#23567;&#20869;&#23384;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#24040;&#22823;&#30340;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#24102;&#23485;&#21464;&#21270;&#30340;&#20844;&#20849;&#20113;&#29615;&#22659;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20869;&#23384;&#28040;&#32791;&#21644;&#36890;&#20449;&#24320;&#38144;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#20869;&#23384;&#21644;&#36890;&#20449;&#30340;&#37096;&#20998;&#20887;&#20313;&#20248;&#21270;&#22120;(PaRO)&#12290;PaRO&#36890;&#36807;&#23558;GPU&#38598;&#32676;&#20998;&#32452;&#21644;&#24341;&#20837;&#24494;&#23567;&#30340;&#32452;&#20869;&#20869;&#23384;&#20887;&#20313;&#65292;&#20943;&#23569;&#20102;&#32452;&#38388;&#36890;&#20449;&#30340;&#25968;&#37327;&#21644;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#37325;&#21472;&#29615;(HO-Ring)&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#22686;&#24378;&#22823;&#27169;&#22411;&#35757;&#32451;&#20013;&#33410;&#28857;&#20043;&#38388;&#25110;&#36328;&#20132;&#25442;&#26426;&#20043;&#38388;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;HO-Ring&#31639;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
As model sizes and training datasets continue to increase, large-scale model training frameworks reduce memory consumption by various sharding techniques. However, the huge communication overhead reduces the training efficiency, especially in public cloud environments with varying network bandwidths. In this paper, we rethink the impact of memory consumption and communication overhead on the training speed of large language model, and propose a memory-communication balanced \underline{Pa}rtial \underline{R}edundancy \underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of inter-group communication by grouping GPU clusters and introducing minor intra-group memory redundancy, thereby improving the training efficiency of the model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large model training. Our experiments demonstrate that the HO-Ring algorithm improves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#24490;&#29615;&#26368;&#20248;&#20256;&#36755;&#65288;LCOT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24490;&#29615;&#27010;&#29575;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290;LCOT&#26041;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#32447;&#24615;&#23884;&#20837;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#24490;&#29615;&#26368;&#20248;&#20256;&#36755;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20462;&#25913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;LCOT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06002</link><description>&lt;p&gt;
LCOT: &#32447;&#24615;&#24490;&#29615;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
LCOT: Linear circular optimal transport. (arXiv:2310.06002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#24490;&#29615;&#26368;&#20248;&#20256;&#36755;&#65288;LCOT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24490;&#29615;&#27010;&#29575;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290;LCOT&#26041;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#32447;&#24615;&#23884;&#20837;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#24490;&#29615;&#26368;&#20248;&#20256;&#36755;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20462;&#25913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;LCOT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#25903;&#25345;&#30340;&#27979;&#24230;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#22312;&#28041;&#21450;&#34920;&#31034;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24490;&#29615;&#27010;&#29575;&#27979;&#24230;&#65292;&#21363;&#25903;&#25345;&#22312;&#21333;&#20301;&#22278;&#19978;&#30340;&#27010;&#29575;&#27979;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#36739;&#39640;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#32447;&#24615;&#24490;&#29615;&#26368;&#20248;&#20256;&#36755;&#65288;LCOT&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#32447;&#24615;&#23884;&#20837;&#65292;&#20351;&#24471;&#21487;&#20197;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#24212;&#29992;&#20110;&#23884;&#20837;&#30340;&#27979;&#24230;&#65292;&#24182;&#26080;&#32541;&#22320;&#20462;&#25913;ML&#31639;&#27861;&#30340;&#22522;&#26412;&#24230;&#37327;&#20026;LCOT&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#25166;&#26681;&#20110;&#24490;&#29615;&#26368;&#20248;&#20256;&#36755;&#65288;COT&#65289;&#65292;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#30456;&#23545;&#20110;&#22266;&#23450;&#21442;&#32771;&#27979;&#24230;&#30340;COT&#24230;&#37327;&#30340;&#32447;&#24615;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23548;&#20986;&#20102;&#24490;&#29615;&#27010;&#29575;&#27979;&#24230;&#30340;&#25104;&#23545;&#27604;&#36739;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce a new computationally efficient metric for these measures, denoted as Linear Circular Optimal Transport (LCOT). The proposed metric comes with an explicit linear embedding that allows one to apply Machine Learning (ML) algorithms to the embedded measures and seamlessly modify the underlying metric for the ML algorithm to LCOT. We show that the proposed metric is rooted in the Circular Optimal Transport (COT) and can be considered the linearization of the COT metric with respect to a fixed reference measure. We provide a theoretical analysis of the proposed metric and derive the computational complexities for pairwise comparison of circular probability measures. Lastly, through a set of numer
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25903;&#25345;&#21521;&#37327;&#26426;&#36716;&#21270;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#27010;&#29575;&#20998;&#31867;&#22120;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.05997</link><description>&lt;p&gt;
&#25104;&#26412;&#25935;&#24863;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cost-sensitive probabilistic predictions for support vector machines. (arXiv:2310.05997v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25903;&#25345;&#21521;&#37327;&#26426;&#36716;&#21270;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#27010;&#29575;&#20998;&#31867;&#22120;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#20102;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#26159;&#26368;&#21463;&#20851;&#27880;&#21644;&#20351;&#29992;&#30340;&#20108;&#20803;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#12290;SVM&#30340;&#20998;&#31867;&#26159;&#22522;&#20110;&#24471;&#20998;&#36807;&#31243;&#30340;&#65292;&#24471;&#21040;&#30340;&#26159;&#30830;&#23450;&#24615;&#30340;&#20998;&#31867;&#35268;&#21017;&#65292;&#21487;&#20197;&#36716;&#21270;&#20026;&#27010;&#29575;&#35268;&#21017;&#65288;&#22312;&#29616;&#25104;&#30340;SVM&#24211;&#20013;&#23454;&#29616;&#65289;&#65292;&#20294;&#26412;&#36136;&#19978;&#24182;&#19981;&#26159;&#27010;&#29575;&#24615;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;SVM&#20013;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#35843;&#20248;&#34987;&#35748;&#20026;&#38656;&#35201;&#24456;&#39640;&#30340;&#35745;&#31639;&#37327;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#20449;&#24687;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#65292;&#27809;&#26377;&#29992;&#20110;&#26500;&#24314;&#27010;&#29575;&#20998;&#31867;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#29983;&#25104;SVM&#30340;&#27010;&#29575;&#36755;&#20986;&#12290;&#26032;&#26041;&#27861;&#20855;&#26377;&#20197;&#19979;&#19977;&#20010;&#29305;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#35774;&#35745;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;&#25935;&#24863;&#24615;&#65288;&#25110;&#30495;&#27491;&#20363;&#29575;&#65292;TPR&#65289;&#21644;&#29305;&#24322;&#24615;&#65288;&#30495;&#36127;&#20363;&#29575;&#65292;TNR&#65289;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#65292;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#25104;&#26412;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector machines (SVMs) are widely used and constitute one of the best examined and used machine learning models for two-class classification. Classification in SVM is based on a score procedure, yielding a deterministic classification rule, which can be transformed into a probabilistic rule (as implemented in off-the-shelf SVM libraries), but is not probabilistic in nature. On the other hand, the tuning of the regularization parameters in SVM is known to imply a high computational effort and generates pieces of information that are not fully exploited, not being used to build a probabilistic classification rule. In this paper we propose a novel approach to generate probabilistic outputs for the SVM. The new method has the following three properties. First, it is designed to be cost-sensitive, and thus the different importance of sensitivity (or true positive rate, TPR) and specificity (true negative rate, TNR) is readily accommodated in the model. As a result, the model can dea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#31185;&#23398;&#31639;&#27861;&#36827;&#34892;&#24739;&#32773;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#24739;&#32773;&#20998;&#31867;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.05996</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#31185;&#23398;&#31639;&#27861;&#25913;&#36827;&#24739;&#32773;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel Network Science Algorithm for Improving Triage of Patients. (arXiv:2310.05996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#31185;&#23398;&#31639;&#27861;&#36827;&#34892;&#24739;&#32773;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#24739;&#32773;&#20998;&#31867;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#20998;&#31867;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30149;&#24773;&#30340;&#32039;&#24613;&#31243;&#24230;&#30830;&#20445;&#21450;&#26102;&#21644;&#24688;&#24403;&#30340;&#25252;&#29702;&#12290;&#20256;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20154;&#30340;&#21028;&#26029;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#20027;&#35266;&#24615;&#21644;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#26159;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24320;&#21457;&#29992;&#20110;&#24739;&#32773;&#20998;&#31867;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24739;&#32773;&#20998;&#31867;&#31639;&#27861;&#30340;&#24320;&#21457;&#36807;&#31243;&#12290;&#23427;&#22522;&#20110;&#23545;&#24739;&#32773;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#20197;&#20135;&#29983;&#26377;&#20851;&#20854;&#20248;&#20808;&#32423;&#30340;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#26159;&#22312;&#21253;&#21547;&#30456;&#20851;&#24739;&#32773;&#20449;&#24687;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20363;&#22914;&#29983;&#21629;&#20307;&#24449;&#12289;&#30151;&#29366;&#21644;&#30149;&#21490;&#31561;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65292;&#35813;&#31639;&#27861;&#34987;&#35774;&#35745;&#20026;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#24739;&#32773;&#20998;&#31867;&#21040;&#20998;&#31867;&#31867;&#21035;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#26426;&#31185;&#23398;&#32435;&#20837;&#24739;&#32773;&#20998;&#31867;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#25913;&#21892;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patient triage plays a crucial role in healthcare, ensuring timely and appropriate care based on the urgency of patient conditions. Traditional triage methods heavily rely on human judgment, which can be subjective and prone to errors. Recently, a growing interest has been in leveraging artificial intelligence (AI) to develop algorithms for triaging patients. This paper presents the development of a novel algorithm for triaging patients. It is based on the analysis of patient data to produce decisions regarding their prioritization. The algorithm was trained on a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history. The algorithm was designed to accurately classify patients into triage categories through rigorous preprocessing and feature engineering. Experimental results demonstrate that our algorithm achieved high accuracy and performance, outperforming traditional triage methods. By incorporating computer science into the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05990</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#21160;&#21306;&#22495;&#24615;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65306;&#20266;&#26631;&#31614;&#27861;&#29992;&#20110;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis. (arXiv:2310.05990v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05990
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#26159;&#21487;&#39044;&#38450;&#30340;&#20027;&#35201;&#27515;&#20129;&#21644;&#27531;&#30142;&#21407;&#22240;&#20043;&#19968;&#12290;&#36825;&#20123;&#30142;&#30149;&#30340;&#35786;&#26029;&#36890;&#24120;&#22256;&#38590;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#20013;&#30340;&#21160;&#33033;&#20998;&#21106;&#24050;&#32463;&#28436;&#21464;&#25104;&#20026;&#19968;&#31181;&#36741;&#21161;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37327;&#26377;&#38480;&#19988;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#20998;&#21106;&#20219;&#21153;&#19968;&#30452;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#24615;&#33021;&#30340;&#24605;&#24819;&#12290;&#35813;&#26041;&#27861;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#23558;&#22522;&#32447;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;9&#65285;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#20102;3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronary Artery Diseases(CADs) though preventable are one of the leading causes of death and disability. Diagnosis of these diseases is often difficult and resource intensive. Segmentation of arteries in angiographic images has evolved as a tool for assistance, helping clinicians in making accurate diagnosis. However, due to the limited amount of data and the difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;R2SL&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#27719;&#24635;&#25968;&#25454;&#26469;&#25429;&#25417;&#21306;&#22495;&#32593;&#32476;&#34892;&#20026;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#30340;Huber&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;QoS&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05988</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#26041;&#27861;&#65306;&#21033;&#29992;&#21306;&#22495;&#32593;&#32476;&#30456;&#20284;&#24615;&#36827;&#34892;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Dual Latent State Learning Approach: Exploiting Regional Network Similarities for QoS Prediction. (arXiv:2310.05988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;R2SL&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#27719;&#24635;&#25968;&#25454;&#26469;&#25429;&#25417;&#21306;&#22495;&#32593;&#32476;&#34892;&#20026;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#30340;Huber&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;QoS&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#23450;&#21306;&#22495;&#20869;&#30340;&#20010;&#20307;&#23545;&#35937;&#65292;&#26080;&#35770;&#26159;&#29992;&#25143;&#36824;&#26159;&#26381;&#21153;&#65292;&#36890;&#24120;&#30001;&#20110;&#23427;&#20204;&#26469;&#33258;&#21516;&#19968;&#22478;&#24066;&#25110;&#33258;&#27835;&#31995;&#32479;&#65288;AS&#65289;&#65292;&#23637;&#29616;&#20986;&#30456;&#20284;&#30340;&#32593;&#32476;&#29366;&#24577;&#12290;&#23613;&#31649;&#23384;&#22312;&#21306;&#22495;&#32593;&#32476;&#30456;&#20284;&#24615;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#24573;&#35270;&#20102;&#20854;&#28508;&#21147;&#65292;&#23548;&#33268;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#32780;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#32593;&#32476;&#65288;R2SL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#22522;&#20110;&#20010;&#20307;&#23545;&#35937;&#30340;QoS&#39044;&#27979;&#25216;&#26415;&#30340;&#32570;&#28857;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;R2SL&#36890;&#36807;&#20174;&#20844;&#20849;&#21306;&#22495;&#27719;&#24635;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#21306;&#22495;&#32593;&#32476;&#28508;&#22312;&#29366;&#24577;&#65306;&#22478;&#24066;&#32593;&#32476;&#28508;&#22312;&#29366;&#24577;&#21644;AS&#32593;&#32476;&#28508;&#22312;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;R2SL&#37319;&#29992;&#20102;&#22686;&#24378;&#30340;Huber&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual objects, whether users or services, within a specific region often exhibit similar network states due to their shared origin from the same city or autonomous system (AS). Despite this regional network similarity, many existing techniques overlook its potential, resulting in subpar performance arising from challenges such as data sparsity and label imbalance. In this paper, we introduce the regional-based dual latent state learning network(R2SL), a novel deep learning framework designed to overcome the pitfalls of traditional individual object-based prediction techniques in Quality of Service (QoS) prediction. Unlike its predecessors, R2SL captures the nuances of regional network behavior by deriving two distinct regional network latent states: the city-network latent state and the AS-network latent state. These states are constructed utilizing aggregated data from common regions rather than individual object data. Furthermore, R2SL adopts an enhanced Huber loss function that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#24535;&#24895;&#32773;&#32593;&#32476;&#20013;&#20851;&#38190;&#29992;&#25143;&#30340;&#34892;&#20026;&#36235;&#21183;&#65292;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#26469;&#25581;&#31034;&#20851;&#38190;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#21644;&#39044;&#27979;&#26410;&#26469;&#34892;&#20026;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#26377;&#25928;&#20998;&#26512;&#20851;&#38190;&#29992;&#25143;&#34892;&#20026;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.05978</link><description>&lt;p&gt;
&#20998;&#26512;&#24535;&#24895;&#32773;&#32593;&#32476;&#20013;&#20851;&#38190;&#29992;&#25143;&#34892;&#20026;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Analyzing Key Users' behavior trends in Volunteer-Based Networks. (arXiv:2310.05978v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#24535;&#24895;&#32773;&#32593;&#32476;&#20013;&#20851;&#38190;&#29992;&#25143;&#30340;&#34892;&#20026;&#36235;&#21183;&#65292;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#26469;&#25581;&#31034;&#20851;&#38190;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#21644;&#39044;&#27979;&#26410;&#26469;&#34892;&#20026;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#26377;&#25928;&#20998;&#26512;&#20851;&#38190;&#29992;&#25143;&#34892;&#20026;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#30340;&#20351;&#29992;&#37327;&#26174;&#33879;&#22686;&#21152;&#24182;&#25345;&#32493;&#27969;&#34892;&#12290;&#22810;&#20010;&#31038;&#20132;&#24179;&#21488;&#23558;&#24535;&#24895;&#32773;&#20316;&#20026;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#36817;&#24180;&#26469;&#65292;&#24535;&#24895;&#32773;&#34892;&#20026;&#22312;&#24535;&#24895;&#32773;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#24535;&#24895;&#32773;&#31038;&#20132;&#32593;&#32476;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#27963;&#21160;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#65306;&#31532;&#19968;&#20010;&#31639;&#27861;&#25581;&#31034;&#20102;&#20851;&#38190;&#29992;&#25143;&#38543;&#26102;&#38388;&#30340;&#34892;&#20026;&#27169;&#24335;&#65307;&#31532;&#20108;&#20010;&#31639;&#27861;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20851;&#38190;&#29992;&#25143;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#21253;&#25324;&#26159;&#21542;&#20445;&#25345;&#27963;&#36291;&#25424;&#36192;&#32773;&#36523;&#20221;&#25110;&#25913;&#21464;&#34892;&#20026;&#25104;&#20026;&#20027;&#35201;&#25509;&#21463;&#32773;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#36825;&#20123;&#31639;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#34892;&#20026;&#39044;&#27979;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#19968;&#20010;P2P&#39135;&#21697;&#20998;&#20139;&#22312;&#32447;&#24179;&#21488;&#30340;240&#19975;&#29992;&#25143;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#20174;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#29992;&#25143;&#30340;&#34892;&#20026;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online social networks usage has increased significantly in the last decade and continues to grow in popularity. Multiple social platforms use volunteers as a central component. The behavior of volunteers in volunteer-based networks has been studied extensively in recent years. Here, we explore the development of volunteer-based social networks, primarily focusing on their key users' behaviors and activities. We developed two novel algorithms: the first reveals key user behavior patterns over time; the second utilizes machine learning methods to generate a forecasting model that can predict the future behavior of key users, including whether they will remain active donors or change their behavior to become mainly recipients, and vice-versa. These algorithms allowed us to analyze the factors that significantly influence behavior predictions.  To evaluate our algorithms, we utilized data from over 2.4 million users on a peer-to-peer food-sharing online platform. Using our algorithm, we i
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CFDBench&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#22235;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#12289;&#27969;&#20307;&#29289;&#29702;&#29305;&#24615;&#21644;&#22495;&#20960;&#20309;&#30340;&#25968;&#25454;&#65292;&#33021;&#24110;&#21161;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05963</link><description>&lt;p&gt;
CFDBench&#65306;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics. (arXiv:2310.05963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;CFDBench&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#22235;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#12289;&#27969;&#20307;&#29289;&#29702;&#29305;&#24615;&#21644;&#22495;&#20960;&#20309;&#30340;&#25968;&#25454;&#65292;&#33021;&#24110;&#21161;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#33021;&#23398;&#20064;&#35299;&#20915;&#25972;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#31639;&#23376;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#22312;&#31616;&#21333;&#30340;&#27969;&#21160;&#26041;&#31243;&#65288;&#22914;Burger&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19988;&#20165;&#32771;&#34385;&#20102;&#23545;&#19981;&#21516;&#21021;&#22987;&#26465;&#20214;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;CFDBench&#65292;&#19968;&#20010;&#38024;&#23545;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#20013;&#22235;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#39537;&#21160;&#33108;&#27969;&#21160;&#12289;&#22278;&#31649;&#20013;&#30340;&#23618;&#27969;&#36793;&#30028;&#23618;&#27969;&#21160;&#12289;&#36890;&#36807;&#21488;&#38454;&#30340;&#22365;&#27969;&#21160;&#21644;&#21608;&#26399;&#24615;&#30340;&#21345;&#38376;&#28065;&#34903;&#27969;&#21160;&#12290;&#27599;&#20010;&#27969;&#21160;&#38382;&#39064;&#37117;&#21253;&#25324;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#12289;&#27969;&#20307;&#29289;&#29702;&#29305;&#24615;&#21644;&#22495;&#20960;&#20309;&#30340;&#25968;&#25454;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;CFDBench&#20855;&#26377;&#20197;&#19979;&#20248;&#21183;&#65306;&#65288;1&#65289;&#32508;&#21512;&#12290;&#23427;&#21253;&#21547;&#24120;&#29992;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#22914;&#36895;&#24230;&#12289;&#21387;&#21147;&#21644;&#33108;&#20307;&#27604;&#20363;&#12290;&#65288;2&#65289;&#30495;&#23454;&#12290;&#38750;&#24120;&#36866;&#21512;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;R17 Type-II&#30721;&#26412;&#23384;&#22312;&#30340;&#31232;&#30095;&#32467;&#26500;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25913;&#36827;&#30721;&#26412;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20934;&#30830;&#36873;&#25321;&#20027;&#35201;&#30340;&#35282;&#24230;-&#26102;&#24310;&#22495;&#31471;&#21475;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#19979;&#34892;CSI&#65292;&#26377;&#25928;&#21033;&#29992;&#31232;&#30095;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.05962</link><description>&lt;p&gt;
&#12298;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25552;&#39640;R17 Type-II&#30721;&#26412;&#30340;&#24615;&#33021;&#12299;
&lt;/p&gt;
&lt;p&gt;
Improving the Performance of R17 Type-II Codebook with Deep Learning. (arXiv:2310.05962v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;R17 Type-II&#30721;&#26412;&#23384;&#22312;&#30340;&#31232;&#30095;&#32467;&#26500;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25913;&#36827;&#30721;&#26412;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20934;&#30830;&#36873;&#25321;&#20027;&#35201;&#30340;&#35282;&#24230;-&#26102;&#24310;&#22495;&#31471;&#21475;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#19979;&#34892;CSI&#65292;&#26377;&#25928;&#21033;&#29992;&#31232;&#30095;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
R17&#30340;Type-II&#30721;&#26412;&#21033;&#29992;&#19978;&#19979;&#34892;&#20449;&#36947;&#20043;&#38388;&#30340;&#35282;&#24230;-&#26102;&#24310;&#22495;&#37096;&#20998;&#20114;&#26131;&#24615;&#26469;&#36873;&#25321;&#27979;&#37327;&#21644;&#21453;&#39304;&#19979;&#34892;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#37096;&#20998;&#35282;&#24230;-&#26102;&#24310;&#22495;&#31471;&#21475;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;CSI&#21453;&#39304;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#31232;&#30095;&#32467;&#26500;&#30340;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25913;&#36827;R17 Type-II&#30721;&#26412;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#21040;&#19978;&#34892;&#20449;&#36947;&#30340;&#20302;&#20449;&#22122;&#27604;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20934;&#30830;&#36873;&#25321;&#20027;&#35201;&#30340;&#35282;&#24230;-&#26102;&#24310;&#22495;&#31471;&#21475;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#28966;&#28857;&#25439;&#22833;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#26681;&#25454;&#22522;&#31449;&#30340;R17 Type-II&#30721;&#26412;&#21453;&#39304;&#37325;&#24314;&#19979;&#34892;CSI&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#31232;&#30095;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#21152;&#26435;&#24555;&#25463;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Type-II codebook in Release 17 (R17) exploits the angular-delay-domain partial reciprocity between uplink and downlink channels to select part of angular-delay-domain ports for measuring and feeding back the downlink channel state information (CSI), where the performance of existing deep learning enhanced CSI feedback methods is limited due to the deficiency of sparse structures. To address this issue, we propose two new perspectives of adopting deep learning to improve the R17 Type-II codebook. Firstly, considering the low signal-to-noise ratio of uplink channels, deep learning is utilized to accurately select the dominant angular-delay-domain ports, where the focal loss is harnessed to solve the class imbalance problem. Secondly, we propose to adopt deep learning to reconstruct the downlink CSI based on the feedback of the R17 Type-II codebook at the base station, where the information of sparse structures can be effectively leveraged. Besides, a weighted shortcut module is desig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#21487;&#20197;&#36731;&#26494;&#25171;&#30772;&#21442;&#19982;&#32773;&#21311;&#21517;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23454;&#38469;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2310.05960</link><description>&lt;p&gt;
&#25351;&#32441;&#25915;&#20987;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#21435;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fingerprint Attack: Client De-Anonymization in Federated Learning. (arXiv:2310.05960v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#21487;&#20197;&#36731;&#26494;&#25171;&#30772;&#21442;&#19982;&#32773;&#21311;&#21517;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23454;&#38469;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22312;&#21442;&#19982;&#32773;&#19981;&#30456;&#20449;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#24444;&#27492;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36890;&#36807;&#30830;&#20445;&#21442;&#19982;&#32773;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#32463;&#36807;&#28151;&#27927;&#65292;&#23558;&#21442;&#19982;&#32773;&#36523;&#20221;&#19982;&#20854;&#25968;&#25454;&#20998;&#31163;&#65292;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#38544;&#31169;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#25351;&#32441;&#25915;&#20987;&#26469;&#26816;&#39564;&#36825;&#31181;&#38450;&#24481;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21311;&#21517;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#30340;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#26799;&#24230;&#32858;&#31867;&#21487;&#20197;&#36731;&#26494;&#22320;&#25171;&#30772;&#21311;&#21517;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#20379;&#23545;&#25105;&#20204;&#30340;&#25351;&#32441;&#25915;&#20987;&#30340;&#23454;&#38469;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning allows collaborative training without data sharing in settings where participants do not trust the central server and one another. Privacy can be further improved by ensuring that communication between the participants and the server is anonymized through a shuffle; decoupling the participant identity from their data. This paper seeks to examine whether such a defense is adequate to guarantee anonymity, by proposing a novel fingerprinting attack over gradients sent by the participants to the server. We show that clustering of gradients can easily break the anonymization in an empirical study of learning federated language models on two language corpora. We then show that training with differential privacy can provide a practical defense against our fingerprint attack.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20840;&#29699;&#22810;&#26679;&#30340;&#28369;&#22369;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#20998;&#21106;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28369;&#22369;&#26144;&#23556;&#20013;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12289;&#36807;&#25311;&#21512;&#21644;&#20302;&#26144;&#23556;&#31934;&#24230;&#30340;&#22256;&#25200;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.05959</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#33258;&#21160;&#21270;&#20840;&#29699;&#28369;&#22369;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automating global landslide detection with heterogeneous ensemble deep-learning classification. (arXiv:2310.05959v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20840;&#29699;&#22810;&#26679;&#30340;&#28369;&#22369;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#20998;&#21106;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28369;&#22369;&#26144;&#23556;&#20013;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#12289;&#36807;&#25311;&#21512;&#21644;&#20302;&#26144;&#23556;&#31934;&#24230;&#30340;&#22256;&#25200;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27668;&#20505;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#24050;&#32463;&#30475;&#21040;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#21450;&#20854;&#27425;&#29983;&#21518;&#26524;&#65292;&#21253;&#25324;&#28369;&#22369;&#30340;&#22686;&#21152;&#12290;&#28369;&#22369;&#23041;&#32961;&#22522;&#30784;&#35774;&#26045;&#65292;&#21253;&#25324;&#36947;&#36335;&#12289;&#38081;&#36335;&#12289;&#24314;&#31569;&#29289;&#21644;&#20154;&#31867;&#29983;&#21629;&#12290;&#22522;&#20110;&#21361;&#38505;&#30340;&#31354;&#38388;&#35268;&#21010;&#21644;&#39044;&#35686;&#31995;&#32479;&#26159;&#20943;&#23569;&#31038;&#20250;&#28369;&#22369;&#39118;&#38505;&#30340;&#32463;&#27982;&#26377;&#25928;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#37117;&#20381;&#36182;&#20110;&#20197;&#21069;&#28369;&#22369;&#20107;&#20214;&#30340;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#24456;&#23569;&#35265;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;(DL)&#27169;&#22411;&#24050;&#32463;&#24212;&#29992;&#20110;&#20351;&#29992;&#20013;&#33267;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#28369;&#22369;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#25935;&#24863;&#24615;&#38382;&#39064;&#12289;&#36807;&#25311;&#21512;&#21644;&#20302;&#26144;&#23556;&#31934;&#24230;&#30340;&#22256;&#25200;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#30340;&#20840;&#29699;&#28369;&#22369;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#22914;Unet&#12289;Linknet&#12289;PSP-Net&#12289;PAN&#21644;DeepLab&#65292;&#24182;&#26681;&#25454;&#20854;&#24615;&#33021;&#26500;&#24314;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38480;&#21046;&#12290;&#35813;&#38598;&#25104;&#27169;&#22411;&#22312;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#24471;&#20998;(0.69)&#12290;
&lt;/p&gt;
&lt;p&gt;
With changing climatic conditions, we are already seeing an increase in extreme weather events and their secondary consequences, including landslides. Landslides threaten infrastructure, including roads, railways, buildings, and human life. Hazard-based spatial planning and early warning systems are cost-effective strategies to reduce the risk to society from landslides. However, these both rely on data from previous landslide events, which is often scarce. Many deep learning (DL) models have recently been applied for landside mapping using medium- to high-resolution satellite images as input. However, they often suffer from sensitivity problems, overfitting, and low mapping accuracy. This study addresses some of these limitations by using a diverse global landslide dataset, using different segmentation models, such as Unet, Linknet, PSP-Net, PAN, and DeepLab and based on their performances, building an ensemble model. The ensemble model achieved the highest F1-score (0.69) when combin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#20379;&#20855;&#26377;&#22810;&#26679;&#24615;&#24615;&#36136;&#30340;&#26368;&#20248;&#35299;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05955</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#29992;&#20110;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables. (arXiv:2310.05955v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#20379;&#20855;&#26377;&#22810;&#26679;&#24615;&#24615;&#36136;&#30340;&#26368;&#20248;&#35299;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#36890;&#24120;&#28041;&#21450;&#21040;&#20351;&#29992;&#36153;&#26102;&#30340;&#27169;&#25311;&#20195;&#30721;&#26469;&#39044;&#27979;&#24453;&#35774;&#35745;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#35774;&#35745;&#65292;&#36825;&#20123;&#20195;&#30721;&#32463;&#24120;&#23884;&#20837;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#26469;&#25552;&#20379;&#26368;&#20339;&#35774;&#35745;&#26041;&#26696;&#65292;&#21516;&#26102;&#28385;&#36275;&#35774;&#35745;&#32422;&#26463;&#26465;&#20214;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;&#20855;&#26377;&#22810;&#26679;&#24615;&#24615;&#36136;&#30340;&#26368;&#20248;&#35299;&#38598;&#65292;&#20197;&#35780;&#20272;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#30340;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#36890;&#24120;&#28041;&#21450;&#21040;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#30340;&#35774;&#35745;&#21464;&#37327;&#65292;&#20197;&#32771;&#34385;&#22312;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25216;&#26415;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex engineering design problems, such as those involved in aerospace, civil, or energy engineering, require the use of numerically costly simulation codes in order to predict the behavior and performance of the system to be designed. To perform the design of the systems, these codes are often embedded into an optimization process to provide the best design while satisfying the design constraints. Recently, new approaches, called Quality-Diversity, have been proposed in order to enhance the exploration of the design space and to provide a set of optimal diversified solutions with respect to some feature functions. These functions are interesting to assess trade-offs. Furthermore, complex engineering design problems often involve mixed continuous, discrete, and categorical design variables allowing to take into account technological choices in the optimization problem. In this paper, a new Quality-Diversity methodology based on mixed continuous, discrete and categorical Bayesian opti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#30333;&#30418;&#12289;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#22312;&#21452;&#21521;Raman&#25918;&#22823;&#22120;&#20013;&#23454;&#29616;&#30446;&#26631;&#39057;&#36317;&#25918;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#37117;&#21487;&#20197;&#23454;&#29616;C&#27874;&#27573;&#19979;&#36798;&#21040;1dB&#30340;&#39057;&#36317;&#24179;&#22374;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05954</link><description>&lt;p&gt;
Raman&#25918;&#22823;&#22120;&#30340;&#20248;&#21270;&#65306;&#40657;&#30418;&#12289;&#28784;&#30418;&#21644;&#30333;&#30418;&#27169;&#22411;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Optimization of Raman amplifiers: a comparison between black-, grey- and white-box modeling. (arXiv:2310.05954v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#30333;&#30418;&#12289;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#22312;&#21452;&#21521;Raman&#25918;&#22823;&#22120;&#20013;&#23454;&#29616;&#30446;&#26631;&#39057;&#36317;&#25918;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#37117;&#21487;&#20197;&#23454;&#29616;C&#27874;&#27573;&#19979;&#36798;&#21040;1dB&#30340;&#39057;&#36317;&#24179;&#22374;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#36890;&#20449;&#31995;&#32479;&#19981;&#26029;&#21162;&#21147;&#25552;&#39640;&#21534;&#21520;&#37327;&#30340;&#36807;&#31243;&#20013;&#65292;&#35774;&#35745;&#21644;&#20248;&#21270;&#20809;&#25918;&#22823;&#22120;&#20197;&#26368;&#22823;&#21270;&#31995;&#32479;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#31163;&#32447;&#20248;&#21270;&#20809;&#25918;&#22823;&#22120;&#20381;&#36182;&#20110;&#20174;&#28145;&#20837;&#29289;&#29702;&#23398;&#30340;&#30333;&#30418;&#27169;&#22411;&#21040;&#25968;&#25454;&#39537;&#21160;&#30340;&#19982;&#29289;&#29702;&#26080;&#20851;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#30333;&#30418;&#12289;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#22312;&#21452;&#21521;Raman&#25918;&#22823;&#22120;&#20013;&#23454;&#29616;&#30446;&#26631;&#39057;&#36317;&#25918;&#22823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30740;&#31350;&#30340;&#20219;&#20309;&#19968;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#22312;100&#20844;&#37324;&#33539;&#22260;&#20869;&#23454;&#29616;C&#27874;&#27573;&#19979;&#36798;&#21040;1dB&#30340;&#39057;&#36317;&#24179;&#22374;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#30446;&#26631;&#24212;&#29992;&#22330;&#26223;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12289;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#20248;&#21270;&#36895;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing and optimizing optical amplifiers to maximize system performance is becoming increasingly important as optical communication systems strive to increase throughput. Offline optimization of optical amplifiers relies on models ranging from white-box models deeply rooted in physics to black-box data-driven physics-agnostic models. Here, we compare the capabilities of white-, grey- and black-box models to achieve a target frequency-distance amplification in a bidirectional Raman amplifier. We show that any of the studied methods can achieve down to 1 dB of frequency-distance flatness over the C-band in a 100-km span. Then, we discuss the models' applicability, advantages, and drawbacks based on the target application scenario, in particular in terms of optimization speed and access to training data.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;URL&#36827;&#34892;&#22403;&#22334;&#21644;&#38750;&#22403;&#22334;&#30340;&#20998;&#31867;&#65292;&#21457;&#29616;bagging&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;96.5%&#12290;</title><link>http://arxiv.org/abs/2310.05953</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22403;&#22334;URL&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Spam URLs Using Machine Learning Approaches. (arXiv:2310.05953v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;URL&#36827;&#34892;&#22403;&#22334;&#21644;&#38750;&#22403;&#22334;&#30340;&#20998;&#31867;&#65292;&#21457;&#29616;bagging&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;96.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25552;&#20379;&#20102;&#24555;&#36895;&#21644;&#20813;&#36153;&#30340;&#36890;&#20449;&#24037;&#20855;&#21644;&#24179;&#21488;&#65292;&#20114;&#32852;&#32593;&#27599;&#22825;&#34987;&#25968;&#21313;&#20159;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#27599;&#31186;&#38047;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#22403;&#22334;&#37038;&#20214;&#65292;&#36825;&#19981;&#20165;&#28010;&#36153;&#20102;&#20114;&#32852;&#32593;&#36164;&#28304;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#28010;&#36153;&#20102;&#29992;&#25143;&#30340;&#26102;&#38388;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;URL&#20998;&#31867;&#20026;&#22403;&#22334;&#25110;&#38750;&#22403;&#22334;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;URL&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#27604;&#36739;&#20102;&#20960;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;k&#26368;&#36817;&#37051;&#12289;bagging&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#36923;&#36753;&#22238;&#24402;&#31561;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;bagging&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26368;&#39640;&#65292;&#36798;&#21040;&#20102;96.5%&#12290;&#36825;&#34920;&#26126;&#65292;bagging&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#29992;&#20110;&#23558;URL&#20998;&#31867;&#20026;&#22403;&#22334;&#25110;&#38750;&#22403;&#22334;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet is used by billions of users daily because it offers fast and free communication tools and platforms. Nevertheless, with this significant increase in usage, huge amounts of spam are generated every second, which wastes internet resources and, more importantly, users time. This study investigates using machine learning models to classify URLs as spam or non-spam. We first extract the features from the URL as it has only one feature, and then we compare the performance of several models, including k-nearest neighbors, bagging, random forest, logistic regression, and others. We find that bagging achieves the best accuracy, with an accuracy of 96.5%. This suggests that bagging is a promising approach for classifying URLs as spam or nonspam.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#27169;&#22411;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#30495;&#27491;&#20363;&#29575;&#21644;&#26356;&#20302;&#30340;&#35823;&#25253;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.05952</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20943;&#36731;&#38654;&#35745;&#31639;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Denial of Service Attacks in Fog-Based Wireless Sensor Networks Using Machine Learning Techniques. (arXiv:2310.05952v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#27169;&#22411;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#30495;&#27491;&#20363;&#29575;&#21644;&#26356;&#20302;&#30340;&#35823;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24037;&#19994;&#24212;&#29992;&#65292;&#34987;&#35748;&#20026;&#26159;21&#19990;&#32426;&#26368;&#37325;&#35201;&#21644;&#21019;&#26032;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#29305;&#27530;&#30340;&#29305;&#24615;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#20256;&#24863;&#22120;&#33410;&#28857;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#12290;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#65292;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#26159;&#24120;&#35265;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#36825;&#20123;&#25915;&#20987;&#23545;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#24433;&#21709;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35782;&#21035;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#25915;&#20987;&#65292;&#26412;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#20915;&#31574;&#26641;&#21644;XGBoost&#12290;&#23545;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#27979;&#35797;&#65292;&#20197;&#35782;&#21035;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;XGBoost&#27169;&#22411;&#24212;&#29992;&#20110;&#25972;&#20010;&#25968;&#25454;&#38598;&#26102;&#65292;&#30495;&#27491;&#20363;&#29575;&#65288;98.3%&#65289;&#39640;&#20110;&#20915;&#31574;&#26641;&#26041;&#27861;&#65288;97.3%&#65289;&#65292;&#35823;&#25253;&#29575;&#65288;1.7%&#65289;&#20302;&#20110;&#20915;&#31574;&#26641;&#25216;&#26415;&#65288;2.7%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless sensor networks are considered to be among the most significant and innovative technologies in the 21st century due to their wide range of industrial applications. Sensor nodes in these networks are susceptible to a variety of assaults due to their special qualities and method of deployment. In WSNs, denial of service attacks are common attacks in sensor networks. It is difficult to design a detection and prevention system that would effectively reduce the impact of these attacks on WSNs. In order to identify assaults on WSNs, this study suggests using two machine learning models: decision trees and XGBoost. The WSNs dataset was the subject of extensive tests to identify denial of service attacks. The experimental findings demonstrate that the XGBoost model, when applied to the entire dataset, has a higher true positive rate (98.3%) than the Decision tree approach (97.3%) and a lower false positive rate (1.7%) than the Decision tree technique (2.7%). Like this, with selected d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24178;&#25200;&#31070;&#32463;&#32593;&#32476;&#26469;&#38450;&#24481;ImageNet&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#24212;&#29992;&#39069;&#22806;&#32972;&#26223;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet-152&#26469;&#39640;&#25928;&#23436;&#25104;&#35757;&#32451;&#12290;&#19982;PGD&#25915;&#20987;&#19979;&#30340;&#26368;&#26032;&#25216;&#26415;&#32467;&#26524;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#38450;&#24481;&#25928;&#26524;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.05947</link><description>&lt;p&gt;
&#40065;&#26834;&#39640;&#25928;&#30340;&#24178;&#25200;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38450;&#24481;ImageNet&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust and Efficient Interference Neural Networks for Defending Against Adversarial Attacks in ImageNet. (arXiv:2310.05947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24178;&#25200;&#31070;&#32463;&#32593;&#32476;&#26469;&#38450;&#24481;ImageNet&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#24212;&#29992;&#39069;&#22806;&#32972;&#26223;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet-152&#26469;&#39640;&#25928;&#23436;&#25104;&#35757;&#32451;&#12290;&#19982;PGD&#25915;&#20987;&#19979;&#30340;&#26368;&#26032;&#25216;&#26415;&#32467;&#26524;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#38450;&#24481;&#25928;&#26524;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#22270;&#20687;&#30340;&#23384;&#22312;&#20005;&#37325;&#24433;&#21709;&#20102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20063;&#26159;&#28145;&#24230;&#23398;&#20064;&#24613;&#38656;&#35299;&#20915;&#30340;&#20851;&#38190;&#31185;&#23398;&#38382;&#39064;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#29992;&#22823;&#37327;&#23545;&#25239;&#26679;&#26412;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;ImageNet&#26102;&#65292;&#36825;&#31181;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#19988;&#22312;&#39640;&#24378;&#24230;&#23545;&#25239;&#25915;&#20987;&#19979;&#23578;&#26410;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#39069;&#22806;&#32972;&#26223;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#26500;&#24314;&#24178;&#25200;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet-152&#26469;&#39640;&#25928;&#23436;&#25104;&#35757;&#32451;&#12290;&#19982;PGD&#25915;&#20987;&#19979;&#30340;&#26368;&#26032;&#25216;&#26415;&#32467;&#26524;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#38450;&#24481;&#25928;&#26524;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#25552;&#20379;&#20102;&#23398;&#26415;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of adversarial images has seriously affected the task of image recognition and practical application of deep learning, it is also a key scientific problem that deep learning urgently needs to solve. By far the most effective approach is to train the neural network with a large number of adversarial examples. However, this adversarial training method requires a huge amount of computing resources when applied to ImageNet, and has not yet achieved satisfactory results for high-intensity adversarial attacks. In this paper, we construct an interference neural network by applying additional background images and corresponding labels, and use pre-trained ResNet-152 to efficiently complete the training. Compared with the state-of-the-art results under the PGD attack, it has a better defense effect with much smaller computing resources. This work provides new ideas for academic research and practical applications of effective defense against adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22303;&#35910;&#30149;&#23475;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#21464;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#24179;&#22343;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;95&#65285;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#20063;&#26377;&#31867;&#20284;&#30340;&#34920;&#29616;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;84&#65285;&#12290;</title><link>http://arxiv.org/abs/2310.05943</link><description>&lt;p&gt;
&#20998;&#26512;&#23398;&#20064;&#29305;&#24449;&#21644;&#22303;&#35910;&#30149;&#23475;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Analysis of Learned Features and Framework for Potato Disease Detection. (arXiv:2310.05943v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05943
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22303;&#35910;&#30149;&#23475;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#21464;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#24179;&#22343;&#20998;&#31867;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;95&#65285;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#20063;&#26377;&#31867;&#20284;&#30340;&#34920;&#29616;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;84&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26893;&#29289;&#30149;&#23475;&#26816;&#27979;&#20043;&#31867;&#30340;&#24212;&#29992;&#65292;&#36890;&#24120;&#20250;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30000;&#38388;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#65292;&#20174;&#32780;&#23545;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#29305;&#24449;&#20174;&#21494;&#29255;&#19978;&#30340;&#30149;&#26001;&#25110;&#20581;&#24247;&#21306;&#22495;&#20013;&#23398;&#20064;&#26469;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#38598;&#21464;&#21270;&#12290;&#20854;&#20013;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#26356;&#24555;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RCNN&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#20854;&#23545;&#24212;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#38598;&#19978;&#30340;&#24179;&#22343;&#20998;&#31867;&#20934;&#30830;&#29575;&#32422;&#20026;95&#65285;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#36824;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#30456;&#24403;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;84&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
For applications like plant disease detection, usually, a model is trained on publicly available data and tested on field data. This means that the test data distribution is not the same as the training data distribution, which affects the classifier performance adversely. We handle this dataset shift by ensuring that the features are learned from disease spots in the leaf or healthy regions, as applicable. This is achieved using a faster Region-based convolutional neural network (RCNN) as one of the solutions and an attention-based network as the other. The average classification accuracies of these classifiers are approximately 95% while evaluated on the test set corresponding to their training dataset. These classifiers also performed equivalently, with an average score of 84% on a dataset not seen during the training phase.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#65292;&#35777;&#26126;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#23545;&#25239;&#21508;&#31181;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.05939</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;
&lt;/p&gt;
&lt;p&gt;
Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning. (arXiv:2310.05939v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05939
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#65292;&#35777;&#26126;&#20102;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#23545;&#25239;&#21508;&#31181;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#30340;&#35774;&#35745;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#35282;&#33394;&#20013;&#65292;&#26234;&#33021;&#20195;&#29702;&#22242;&#38431;&#21487;&#33021;&#23637;&#29616;&#20986;&#20445;&#25252;&#32593;&#32476;&#21644;&#21160;&#33021;&#36164;&#20135;&#30340;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#22312;&#27169;&#25311;&#30340;&#28216;&#25103;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#26681;&#25454;&#20854;&#22312;&#20027;&#26426;&#38450;&#24481;&#22330;&#26223;&#20013;&#20849;&#21516;&#20943;&#36731;&#25915;&#20987;&#32773;&#27963;&#21160;&#30340;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#23432;&#26041;&#31995;&#32479;&#20250;&#38754;&#23545;&#33021;&#30772;&#22351;&#32593;&#32476;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#21551;&#21457;&#24335;&#25915;&#20987;&#32773;&#12290;&#23545;&#27604;&#20102;&#22522;&#20110;&#20215;&#20540;&#30340;&#29420;&#31435;&#23398;&#20064;&#21644;&#38598;&#20013;&#22521;&#35757;&#20998;&#25955;&#25191;&#34892;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#32988;&#36807;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#21551;&#21457;&#24335;&#38450;&#24481;&#32773;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#23398;&#20064;&#26377;&#25928;&#30340;&#32593;&#32476;&#38450;&#24481;&#25112;&#26415;&#65292;&#20197;&#25269;&#24481;&#21508;&#31181;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning techniques have opened new possibilities for designing solutions for autonomous cyber defence. Teams of intelligent agents in computer network defence roles may reveal promising avenues to safeguard cyber and kinetic assets. In a simulated game environment, agents are evaluated on their ability to jointly mitigate attacker activity in host-based defence scenarios. Defender systems are evaluated against heuristic attackers with the goals of compromising network confidentiality, integrity, and availability. Value-based Independent Learning and Centralized Training Decentralized Execution (CTDE) cooperative Multi-Agent Reinforcement Learning (MARL) methods are compared revealing that both approaches outperform a simple multi-agent heuristic defender. This work demonstrates the ability of cooperative MARL to learn effective cyber defence tactics against varied threats.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#25506;&#32034;&#20102;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20026;&#35745;&#31639;&#26426;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#22312;&#39118;&#38505;&#35780;&#20272;&#31561;&#26041;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.05935</link><description>&lt;p&gt;
&#28431;&#27934;&#32858;&#31867;&#19982;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#30340;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vulnerability Clustering and other Machine Learning Applications of Semantic Vulnerability Embeddings. (arXiv:2310.05935v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#25506;&#32034;&#20102;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#20026;&#35745;&#31639;&#26426;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#22312;&#39118;&#38505;&#35780;&#20272;&#31561;&#26041;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#23433;&#20840;&#28431;&#27934;&#36890;&#24120;&#20197;&#31616;&#30701;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24418;&#24335;&#21457;&#24067;&#65288;&#20363;&#22914;MITRE&#30340;CVE&#21015;&#34920;&#65289;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#36825;&#20123;&#25551;&#36848;&#20250;&#36827;&#19968;&#27493;&#36890;&#36807;&#24120;&#35265;&#28431;&#27934;&#35780;&#20998;&#31995;&#32479;&#65288;CVSS&#65289;&#31561;&#26631;&#31614;&#36827;&#34892;&#25163;&#21160;&#34917;&#20805;&#12290;&#22312;&#28431;&#27934;AI&#65288;&#20998;&#26512;&#19982;&#24773;&#25253;&#65289;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#20041;&#28431;&#27934;&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#28431;&#27934;&#31354;&#38388;&#30340;&#31616;&#27905;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#23427;&#20204;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20197;&#25903;&#25345;&#35745;&#31639;&#26426;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#22312;&#39118;&#38505;&#35780;&#20272;&#21644;&#20854;&#20182;&#30456;&#20851;&#27963;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#26412;&#25253;&#21578;&#20013;&#31616;&#35201;&#24635;&#32467;&#20102;&#25105;&#20204;&#25506;&#32034;&#30340;&#29305;&#23450;&#24212;&#29992;&#65292;&#21253;&#25324;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#35780;&#20272;&#28431;&#27934;&#31354;&#38388;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber-security vulnerabilities are usually published in form of short natural language descriptions (e.g., in form of MITRE's CVE list) that over time are further manually enriched with labels such as those defined by the Common Vulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and Intelligence) project, we investigated different types of semantic vulnerability embeddings based on natural language processing (NLP) techniques to obtain a concise representation of the vulnerability space. We also evaluated their use as a foundation for machine learning applications that can support cyber-security researchers and analysts in risk assessment and other related activities. The particular applications we explored and briefly summarize in this report are clustering, classification, and visualization, as well as a new logic-based approach to evaluate theories about the vulnerability space.
&lt;/p&gt;</description></item><item><title>&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#35813;&#29702;&#35770;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#20197;&#22312;&#27809;&#26377;&#23545;&#19990;&#30028;&#27169;&#22411;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20855;&#26377;&#20302;&#39118;&#38505;&#30340;&#32479;&#35745;&#20445;&#35777;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2310.05921</link><description>&lt;p&gt;
&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;: &#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions. (arXiv:2310.05921v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05921
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#35813;&#29702;&#35770;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#20197;&#22312;&#27809;&#26377;&#23545;&#19990;&#30028;&#27169;&#22411;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20855;&#26377;&#20302;&#39118;&#38505;&#30340;&#32479;&#35745;&#20445;&#35777;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31526;&#21512;&#20915;&#31574;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19981;&#23436;&#32654;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#23433;&#20840;&#30340;&#33258;&#20027;&#20915;&#31574;&#12290;&#36825;&#31181;&#20915;&#31574;&#30340;&#20363;&#23376;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#20174;&#20381;&#36182;&#20110;&#34892;&#20154;&#39044;&#27979;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#31639;&#27861;&#65292;&#21040;&#26657;&#20934;&#33258;&#21160;&#21270;&#21046;&#36896;&#20197;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#38169;&#35823;&#29575;&#65292;&#20877;&#21040;&#22312;&#36816;&#34892;&#26102;&#36873;&#25321;&#20449;&#20219;&#21517;&#20041;&#31574;&#30053;&#36824;&#26159;&#20999;&#25442;&#21040;&#23433;&#20840;&#22791;&#20221;&#31574;&#30053;&#12290;&#25105;&#20204;&#31639;&#27861;&#20135;&#29983;&#30340;&#20915;&#31574;&#22312;&#32479;&#35745;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#26159;&#23433;&#20840;&#30340;&#65292;&#26080;&#38656;&#23545;&#19990;&#30028;&#27169;&#22411;&#20316;&#20986;&#20219;&#20309;&#20551;&#35774;&#65307;&#35266;&#27979;&#25968;&#25454;&#21487;&#20197;&#19981;&#28385;&#36275;&#29420;&#31435;&#21516;&#20998;&#24067;(I.I.D.)&#30340;&#26465;&#20214;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#23545;&#25239;&#24615;&#30340;&#12290;&#35813;&#29702;&#35770;&#23558;&#31526;&#21512;&#39044;&#27979;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#30452;&#25509;&#26657;&#20934;&#20915;&#31574;&#65292;&#32780;&#19981;&#38656;&#35201;&#26500;&#24314;&#39044;&#27979;&#38598;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22260;&#32469;&#20154;&#31867;&#36827;&#34892;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#12289;&#33258;&#21160;&#32929;&#31080;&#20132;&#26131;&#21644;&#26426;&#22120;&#20154;&#21046;&#36896;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturin
&lt;/p&gt;</description></item><item><title>NEFTune &#26159;&#19968;&#31181;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#26377;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.05914</link><description>&lt;p&gt;
NEFTune: &#22122;&#22768;&#23884;&#20837;&#25913;&#36827;&#25351;&#20196;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NEFTune: Noisy Embeddings Improve Instruction Finetuning. (arXiv:2310.05914v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05914
&lt;/p&gt;
&lt;p&gt;
NEFTune &#26159;&#19968;&#31181;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#37117;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;NEFTune &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#12290;&#22312;&#20351;&#29992; Alpaca &#36827;&#34892;&#26631;&#20934;&#24494;&#35843;&#30340;&#22522;&#30784;&#19978;&#65292;LLaMA-2-7B &#22312; AlpacaEval &#19978;&#30340;&#20934;&#30830;&#29575;&#20174; 29.79% &#25552;&#21319;&#21040;&#20102; 64.69%&#12290;NEFTune &#22312;&#29616;&#20195;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#20063;&#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#20351;&#29992; Evol-Instruct &#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#65292;ShareGPT &#25552;&#21319;&#20102;8%&#65292;OpenPlatypus &#25552;&#21319;&#20102;8%&#12290;&#21363;&#20351;&#26159;&#32463;&#36807; RLHF &#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#22914; LLaMA-2-Chat&#65292;&#20063;&#21487;&#20197;&#36890;&#36807; NEFTune &#30340;&#38468;&#21152;&#35757;&#32451;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.
&lt;/p&gt;</description></item><item><title>M3FPolypSegNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#30340;&#32467;&#32928;&#38236;&#22270;&#20687;&#20013;&#24687;&#32905;&#23450;&#20301;&#30340;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#39057;&#29575;&#32452;&#20214;&#65292;&#20351;&#29992;&#22810;&#39057;&#32534;&#30721;&#22120;&#26144;&#23556;&#21040;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#21487;&#20280;&#32553;&#27880;&#24847;&#21147;&#24378;&#35843;&#24687;&#32905;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.05538</link><description>&lt;p&gt;
M3FPolypSegNet&#65306;&#22810;&#39057;&#29305;&#24449;&#34701;&#21512;&#30340;&#32467;&#32928;&#38236;&#22270;&#20687;&#20013;&#24687;&#32905;&#23450;&#20301;&#30340;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion for Polyp Localization in Colonoscopy Images. (arXiv:2310.05538v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05538
&lt;/p&gt;
&lt;p&gt;
M3FPolypSegNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#30340;&#32467;&#32928;&#38236;&#22270;&#20687;&#20013;&#24687;&#32905;&#23450;&#20301;&#30340;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#39057;&#29575;&#32452;&#20214;&#65292;&#20351;&#29992;&#22810;&#39057;&#32534;&#30721;&#22120;&#26144;&#23556;&#21040;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#21487;&#20280;&#32553;&#27880;&#24847;&#21147;&#24378;&#35843;&#24687;&#32905;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24687;&#32905;&#20998;&#21106;&#23545;&#20110;&#39044;&#38450;&#32467;&#30452;&#32928;&#30284;&#19968;&#31181;&#24120;&#35265;&#30340;&#30284;&#30151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#29992;&#20110;&#33258;&#21160;&#20998;&#21106;&#24687;&#32905;&#65292;&#20174;&#32780;&#20943;&#23569;&#35823;&#35786;&#30340;&#39118;&#38505;&#12290;&#32467;&#32928;&#38236;&#22270;&#20687;&#20013;&#23567;&#30340;&#24687;&#32905;&#23450;&#20301;&#20855;&#26377;&#22797;&#26434;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39068;&#33394;&#12289;&#36974;&#25377;&#21644;&#19981;&#21516;&#24418;&#29366;&#30340;&#24687;&#32905;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#39057;&#29575;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#22810;&#39057;&#29305;&#24449;&#34701;&#21512;&#24687;&#32905;&#20998;&#21106;&#32593;&#32476;&#65288;M3FPolypSegNet&#65289;&#65292;&#29992;&#20110;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20302;&#39057;/&#39640;&#39057;/&#20840;&#39057;&#32452;&#20214;&#20197;&#21033;&#29992;&#27599;&#20010;&#32452;&#20214;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#29420;&#31435;&#30340;&#22810;&#39057;&#32534;&#30721;&#22120;&#23558;&#22810;&#20010;&#36755;&#20837;&#22270;&#20687;&#26144;&#23556;&#21040;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#39057;&#29575;-ASPP&#21487;&#20280;&#32553;&#27880;&#24847;&#27169;&#22359;&#65288;F-ASPP SAM&#65289;&#20013;&#65292;&#24212;&#29992;ASPP&#22312;&#27599;&#20010;&#39057;&#29575;&#32452;&#20214;&#20043;&#38388;&#20197;&#20445;&#30041;&#23610;&#24230;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#24212;&#29992;&#21487;&#20280;&#32553;&#27880;&#24847;&#21147;&#24378;&#35843;&#39640;&#32500;&#29305;&#24449;&#20013;&#30340;&#24687;&#32905;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polyp segmentation is crucial for preventing colorectal cancer a common type of cancer. Deep learning has been used to segment polyps automatically, which reduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images is challenging because of its complex characteristics, such as color, occlusion, and various shapes of polyps. To address this challenge, a novel frequency-based fully convolutional neural network, Multi-Frequency Feature Fusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose the input image into low/high/full-frequency components to use the characteristics of each component. We used three independent multi-frequency encoders to map multiple input images into a high-dimensional feature space. In the Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied between each frequency component to preserve scale information. Subsequently, scalable attention was applied to emphasize polyp regions in a high-dimensional feature 
&lt;/p&gt;</description></item><item><title>ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05537</link><description>&lt;p&gt;
ParFam - &#22522;&#20110;&#36830;&#32493;&#20840;&#23616;&#20248;&#21270;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
ParFam -- Symbolic Regression Based on Continuous Global Optimization. (arXiv:2310.05537v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05537
&lt;/p&gt;
&lt;p&gt;
ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#38382;&#39064;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#27604;&#22914;&#20174;&#32473;&#23450;&#25968;&#25454;&#20013;&#35782;&#21035;&#29289;&#29702;&#23450;&#24459;&#25110;&#25512;&#23548;&#25551;&#36848;&#37329;&#34701;&#24066;&#22330;&#34892;&#20026;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#30446;&#21069;&#23384;&#22312;&#22810;&#31181;&#35299;&#20915;SR&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38750;&#24120;&#22797;&#26434;&#65292;&#38656;&#35201;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;ParFam&#65292;&#23427;&#21033;&#29992;&#36866;&#21512;&#30340;&#31526;&#21495;&#20989;&#25968;&#30340;&#21442;&#25968;&#21270;&#26063;&#23558;&#31163;&#25955;&#30340;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#32622;&#26356;&#21152;&#30452;&#35266;&#12290;&#32467;&#21512;&#24378;&#22823;&#30340;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;SR&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#39640;&#32423;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;&#28155;&#21152;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#25214;&#21040;&#36866;&#21512;&#30340;&#21442;&#25968;&#21270;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05446</link><description>&lt;p&gt;
RetSeg: &#22522;&#20110;&#20445;&#30041;&#26426;&#21046;&#30340;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RetSeg: Retention-based Colorectal Polyps Segmentation Network. (arXiv:2310.05446v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#23558;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#32467;&#30452;&#32928;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20869;&#23384;&#21644;&#24182;&#34892;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30456;&#27604;&#65292;&#22312;&#24687;&#32905;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#32858;&#28966;&#20110;&#29305;&#23450;&#22270;&#20687;&#21306;&#22495;&#65292;ViTs&#22312;&#22788;&#29702;&#35270;&#35273;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#22797;&#26434;&#21307;&#23398;&#22270;&#20687;&#26102;&#23454;&#29616;&#20102;&#24378;&#22823;&#19988;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#21464;&#25442;&#22120;&#20013;&#22266;&#26377;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#36866;&#24212;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23610;&#23544;&#21644;&#20998;&#36776;&#29575;&#65292;&#20026;&#20256;&#32479;&#30340;CNNs&#25152;&#19981;&#20855;&#22791;&#30340;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21464;&#25442;&#22120;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#32780;&#38754;&#20020;&#30528;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#24182;&#34892;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#26102;&#30142;&#30149;&#26816;&#27979;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#31350;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#20445;&#30041;&#26426;&#21046;&#25972;&#21512;&#21040;&#24687;&#32905;&#20998;&#21106;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, intr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#34987;&#24694;&#24847;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38656;&#35201;&#20174;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#30740;&#31350;&#19982;&#25919;&#31574;&#30028;&#37319;&#21462;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05189</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#20107;&#23454;&#24615;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Factuality Challenges in the Era of Large Language Models. (arXiv:2310.05189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05189
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#34987;&#24694;&#24847;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38656;&#35201;&#20174;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#30740;&#31350;&#19982;&#25919;&#31574;&#30028;&#37319;&#21462;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#24494;&#36719;&#30340;Bing Chat&#21644;&#35895;&#27468;&#30340;Bard&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#36825;&#20123;&#38750;&#24120;&#26377;&#29992;&#12289;&#33258;&#28982;&#30340;&#24037;&#20855;&#26631;&#24535;&#30528;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#23384;&#22312;&#29983;&#25104;&#34394;&#20551;&#12289;&#38169;&#35823;&#25110;&#35823;&#23548;&#20869;&#23481;&#30340;&#20542;&#21521;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#34987;&#29992;&#20110;&#24694;&#24847;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#34394;&#20551;&#20294;&#21487;&#20449;&#30340;&#20869;&#23481;&#21644;&#20010;&#20154;&#36164;&#26009;&#12290;&#36825;&#23545;&#20110;&#31038;&#20250;&#26469;&#35828;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#27450;&#39575;&#29992;&#25143;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#20256;&#25773;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#37492;&#20110;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20107;&#23454;&#26680;&#26597;&#21592;&#12289;&#26032;&#38395;&#26426;&#26500;&#21644;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#25919;&#31574;&#30028;&#38656;&#35201;&#30340;&#25216;&#26415;&#21019;&#26032;&#12289;&#30417;&#31649;&#25913;&#38761;&#21644;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20513;&#35758;&#30340;&#31867;&#22411;&#12290;&#36890;&#36807;&#30830;&#23450;&#39118;&#38505;&#12289;&#36843;&#22312;&#30473;&#30571;&#30340;&#23041;&#32961;&#21644;&#19968;&#20123;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to she
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;CloudOps&#39046;&#22495;&#24341;&#20837;&#20102;&#19977;&#20010;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#35266;&#27979;&#28857;&#65292;&#20026;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#22880;&#23450;&#20102;&#23454;&#35777;&#22522;&#30784;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26550;&#26500;&#20316;&#20026;&#24378;&#22823;&#30340;&#38646;-shot&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2310.05063</link><description>&lt;p&gt;
&#22312;CloudOps&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25512;&#21160;&#39044;&#35757;&#32451;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain. (arXiv:2310.05063v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;CloudOps&#39046;&#22495;&#24341;&#20837;&#20102;&#19977;&#20010;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#35266;&#27979;&#28857;&#65292;&#20026;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#22880;&#23450;&#20102;&#23454;&#35777;&#22522;&#30784;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26550;&#26500;&#20316;&#20026;&#24378;&#22823;&#30340;&#38646;-shot&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26102;&#20195;&#65292;&#26102;&#38388;&#24207;&#21015;&#34987;&#25918;&#22312;&#20102;&#21518;&#38754;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#27491;&#22312;&#20139;&#21463;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#26368;&#21463;&#27426;&#36814;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#25968;&#19975;&#20010;&#26102;&#38388;&#27493;&#65292;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20063;&#23545;&#34920;&#36798;&#21147;&#27169;&#22411;&#21644;&#35268;&#27169;&#30340;&#24517;&#35201;&#24615;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26469;&#33258;&#20113;&#25805;&#20316;&#65288;CloudOps&#65289;&#39046;&#22495;&#30340;&#19977;&#20010;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#35266;&#27979;&#28857;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#30740;&#31350;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25193;&#23637;&#24615;&#30340;&#32463;&#39564;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26550;&#26500;&#20026;&#26410;&#26469;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#38646;-shot&#22522;&#32447;&#65292;&#24182;&#20174;&#36827;&#19968;&#27493;&#30340;&#25193;&#23637;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, bot
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Diff-Transfer&#65292;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#26469;&#39640;&#25928;&#20256;&#36755;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#12290;Diff-Transfer&#36890;&#36807;&#22312;&#20219;&#21153;&#31354;&#38388;&#20869;&#21457;&#29616;&#21487;&#34892;&#36335;&#24452;&#65292;&#23558;&#28304;&#20219;&#21153;&#36716;&#21270;&#20026;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20449;&#24687;&#24341;&#23548;&#36866;&#24212;&#24050;&#30693;&#30340;&#21160;&#20316;&#65292;&#25104;&#21151;&#35299;&#20915;&#21478;&#19968;&#20010;&#23376;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Diff-Transfer&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04930</link><description>&lt;p&gt;
Diff-Transfer: &#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation. (arXiv:2310.04930v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04930
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Diff-Transfer&#65292;&#19968;&#31181;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#26469;&#39640;&#25928;&#20256;&#36755;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#12290;Diff-Transfer&#36890;&#36807;&#22312;&#20219;&#21153;&#31354;&#38388;&#20869;&#21457;&#29616;&#21487;&#34892;&#36335;&#24452;&#65292;&#23558;&#28304;&#20219;&#21153;&#36716;&#21270;&#20026;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20449;&#24687;&#24341;&#23548;&#36866;&#24212;&#24050;&#30693;&#30340;&#21160;&#20316;&#65292;&#25104;&#21151;&#35299;&#20915;&#21478;&#19968;&#20010;&#23376;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Diff-Transfer&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25484;&#25569;&#30340;&#25216;&#33021;&#36716;&#31227;&#21040;&#23436;&#25104;&#19968;&#31995;&#21015;&#30456;&#20284;&#20294;&#26032;&#39062;&#20219;&#21153;&#30340;&#33021;&#21147;&#23545;&#20110;&#26234;&#33021;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Diff-Transfer&#65292;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#26469;&#39640;&#25928;&#20256;&#36755;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Diff-Transfer&#22312;&#20219;&#21153;&#31354;&#38388;&#20869;&#21457;&#29616;&#20102;&#19968;&#26465;&#21487;&#34892;&#30340;&#36335;&#24452;&#65292;&#23558;&#28304;&#20219;&#21153;&#24102;&#21040;&#30446;&#26631;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#36335;&#24452;&#30340;&#27599;&#23545;&#30456;&#37051;&#28857;&#19978;&#65292;&#21363;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;Diff-Transfer&#36890;&#36807;&#20174;&#19968;&#20010;&#23376;&#20219;&#21153;&#20013;&#36866;&#24212;&#24050;&#30693;&#30340;&#21160;&#20316;&#26469;&#25104;&#21151;&#35299;&#20915;&#21478;&#19968;&#20010;&#23376;&#20219;&#21153;&#12290;&#36866;&#24212;&#36807;&#31243;&#26159;&#30001;&#21487;&#24494;&#20998;&#29289;&#29702;&#20223;&#30495;&#20135;&#29983;&#30340;&#26799;&#24230;&#20449;&#24687;&#24341;&#23548;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#26377;&#20219;&#21153;&#32423;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;Q&#23398;&#20064;&#26469;&#29983;&#25104;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#19978;&#25191;&#34892;&#20102;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36801;&#31227;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;Diff-Transfer&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability to transfer mastered skills to accomplish a range of similar yet novel tasks is crucial for intelligent robots. In this work, we introduce $\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics simulation to efficiently transfer robotic skills. Specifically, $\textit{Diff-Transfer}$ discovers a feasible path within the task space that brings the source task to the target task. At each pair of adjacent points along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts known actions from one sub-task to tackle the other sub-task successfully. The adaptation is guided by the gradient information from differentiable physics simulations. We propose a novel path-planning method to generate sub-tasks, leveraging $Q$-learning with a task-level state and reward. We implement our framework in simulation experiments and execute four challenging transfer tasks on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$ thr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#36890;&#36807;&#21160;&#24577;&#23459;&#24067;&#21512;&#26684;&#38598;&#21512;&#20197;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20998;&#24067;&#26469;&#20943;&#36731;&#20195;&#29702;&#20154;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#36827;&#32780;&#26368;&#23567;&#21270;&#19982;&#26368;&#20248;&#21512;&#26684;&#38598;&#21512;&#20043;&#38388;&#30340;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2310.04884</link><description>&lt;p&gt;
&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#30340;&#21518;&#24724;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of Repeated Delegated Choice. (arXiv:2310.04884v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#36890;&#36807;&#21160;&#24577;&#23459;&#24067;&#21512;&#26684;&#38598;&#21512;&#20197;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20998;&#24067;&#26469;&#20943;&#36731;&#20195;&#29702;&#20154;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#36827;&#32780;&#26368;&#23567;&#21270;&#19982;&#26368;&#20248;&#21512;&#26684;&#38598;&#21512;&#20043;&#38388;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#37325;&#22797;&#22996;&#25176;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#20811;&#33713;&#22240;&#20271;&#26684;&#21644;&#20811;&#33713;&#22240;&#20271;&#26684;(EC'18)&#22312;&#32447;&#23398;&#20064;&#21464;&#31181;&#30340;&#27169;&#22411;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#22996;&#25176;&#20154;&#19982;&#19968;&#20010;&#25317;&#26377;&#22806;&#29983;&#35299;&#38598;&#30340;&#20195;&#29702;&#20154;&#36827;&#34892;&#37325;&#22797;&#20114;&#21160;&#65292;&#20197;&#23547;&#25214;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#23545;&#22996;&#25176;&#20154;&#21644;&#20195;&#29702;&#20154;&#37117;&#21487;&#20197;&#20135;&#29983;&#19981;&#21516;&#30340;&#25928;&#29992;&#65292;&#20195;&#29702;&#20154;&#21487;&#33021;&#20197;&#33258;&#31169;&#30340;&#26041;&#24335;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26368;&#22823;&#21270;&#33258;&#36523;&#30340;&#25928;&#29992;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#34892;&#20026;&#65292;&#22996;&#25176;&#20154;&#23459;&#24067;&#19968;&#20010;&#21512;&#26684;&#38598;&#21512;&#65292;&#31579;&#25481;&#19968;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22996;&#25176;&#20154;&#20107;&#20808;&#24182;&#19981;&#20102;&#35299;&#35299;&#20915;&#26041;&#26696;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#22996;&#25176;&#20154;&#21160;&#24577;&#22320;&#23459;&#24067;&#21508;&#31181;&#21512;&#26684;&#38598;&#21512;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#24067;&#12290;&#22996;&#25176;&#20154;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#19982;&#20107;&#21518;&#26368;&#20248;&#21512;&#26684;&#38598;&#21512;&#30456;&#27604;&#30340;&#32047;&#31215;&#21518;&#24724;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#38382;&#39064;&#35774;&#32622;&#30340;&#20004;&#20010;&#32500;&#24230;&#65292;&#21363;&#20195;&#29702;&#20154;&#26159;&#26681;&#25454;&#30524;&#21069;&#21033;&#30410;&#34892;&#20107;&#36824;&#26159;&#22312;&#22238;&#21512;&#20043;&#38388;&#21046;&#23450;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study on a repeated delegated choice problem, which is the first to consider an online learning variant of Kleinberg and Kleinberg, EC'18. In this model, a principal interacts repeatedly with an agent who possesses an exogenous set of solutions to search for efficient ones. Each solution can yield varying utility for both the principal and the agent, and the agent may propose a solution to maximize its own utility in a selfish manner. To mitigate this behavior, the principal announces an eligible set which screens out a certain set of solutions. The principal, however, does not have any information on the distribution of solutions in advance. Therefore, the principal dynamically announces various eligible sets to efficiently learn the distribution. The principal's objective is to minimize cumulative regret compared to the optimal eligible set in hindsight. We explore two dimensions of the problem setup, whether the agent behaves myopically or strategizes across the rounds,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04870</link><description>&lt;p&gt;
Lemur&#65306;&#22312;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#23637;&#31034;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#39564;&#35777;&#24037;&#20855;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#35770;&#65292;&#23558;&#20854;&#20316;&#20026;&#25512;&#23548;&#35268;&#21017;&#30340;&#38598;&#21512;&#36827;&#34892;&#35770;&#35777;&#20854;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#25512;&#29702;&#24418;&#25104;&#20026;&#19968;&#20010;&#23436;&#22791;&#30340;&#33258;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#36825;&#22312;&#19968;&#32452;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#24102;&#26469;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#31216;&#20026;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#65292;&#36890;&#36807;&#25913;&#36827;&#22270;&#30340;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#32479;&#35745;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#24230;&#12290;&#35813;&#26426;&#21046;&#22312;&#20272;&#35745;&#22270;&#20869;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#24418;&#27987;&#24230;&#31561;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04859</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#38543;&#26426;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Universal Graph Random Features. (arXiv:2310.04859v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#31216;&#20026;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#65292;&#36890;&#36807;&#25913;&#36827;&#22270;&#30340;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#32479;&#35745;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#24230;&#12290;&#35813;&#26426;&#21046;&#22312;&#20272;&#35745;&#22270;&#20869;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#24418;&#27987;&#24230;&#31561;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26426;&#21046;&#65292;&#31216;&#20026;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#65292;&#20197;&#25913;&#36827;&#22522;&#20110;&#22270;&#30340;&#37319;&#26679;&#12290;&#36890;&#36807;&#22312;&#30456;&#20114;&#20316;&#29992;&#38598;&#21512;&#30340;&#36712;&#36857;&#20043;&#38388;&#24341;&#20837;&#30456;&#20851;&#24615;&#65292;&#20351;&#23427;&#20204;&#30340;&#36793;&#38469;&#36716;&#31227;&#27010;&#29575;&#20445;&#25345;&#19981;&#21464;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#25506;&#32034;&#22270;&#24418;&#65292;&#25552;&#39640;&#32479;&#35745;&#20272;&#35745;&#22120;&#30340;&#38598;&#20013;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#26080;&#20559;&#24615;&#12290;&#35813;&#26426;&#21046;&#21487;&#20197;&#36731;&#26494;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20272;&#35745;&#22270;&#20869;&#26680;&#12289;PageRank&#21521;&#37327;&#21644;&#22270;&#24418;&#27987;&#24230;&#31561;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#40065;&#26834;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#26159;&#31532;&#19968;&#20010;&#22312;&#22270;&#19978;&#30456;&#20851;&#27493;&#34892;&#32773;&#26041;&#21521;&#36827;&#34892;&#20005;&#26684;&#30740;&#31350;&#30340;&#20934;&#33945;&#29305;&#21345;&#32599;&#26041;&#26696;&#65292;&#20026;&#36825;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#20852;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.04821</link><description>&lt;p&gt;
&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value. (arXiv:2310.04821v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#23581;&#35797;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#39044;&#27979;&#24402;&#22240;&#20110;&#20854;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;DNN&#12290;&#20854;&#20013;&#19968;&#20010;&#30740;&#31350;&#20805;&#20998;&#30340;&#24402;&#22240;&#26041;&#27861;&#26159;Integrated Gradients&#65288;IG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36873;&#25321;IG&#30340;&#22522;&#32447;&#26159;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21033;&#29992;&#21333;&#19968;&#22522;&#32447;&#30340;&#20570;&#27861;&#26410;&#33021;&#23454;&#29616;&#36825;&#20010;&#24895;&#26395;&#65292;&#22240;&#27492;&#38656;&#35201;&#22810;&#20010;&#22522;&#32447;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;IG&#19982;&#22885;&#26364;&#8212;&#22799;&#26222;&#21033;&#65288;Aumann-Shapley&#65289;&#20215;&#20540;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#32447;&#30340;&#35774;&#35745;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20986;&#19968;&#32452;&#22522;&#32447;&#19982;Shapley Value&#20013;&#30340;&#32852;&#30431;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#65292;&#31216;&#20026;Shapley Integrated Gradients&#65288;SIG&#65289;&#65292;&#36890;&#36807;&#27604;&#20363;&#25277;&#26679;&#26469;&#25628;&#32034;&#19968;&#32452;&#22522;&#32447;&#65292;&#20197;&#37096;&#20998;&#27169;&#25311;Shapley Value&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;&#22312;GridWorl&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous approaches have attempted to interpret deep neural networks (DNNs) by attributing the prediction of DNN to its input features. One of the well-studied attribution methods is Integrated Gradients (IG). Specifically, the choice of baselines for IG is a critical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios. However, current practice of exploiting a single baseline fails to fulfill this ambition, thus demanding multiple baselines. Fortunately, the inherent connection between IG and Aumann-Shapley Value forms a unique perspective to rethink the design of baselines. Under certain hypothesis, we theoretically analyse that a set of baseline aligns with the coalitions in Shapley Value. Thus, we propose a novel baseline construction method called Shapley Integrated Gradients (SIG) that searches for a set of baselines by proportional sampling to partly simulate the computation path of Shapley Value. Simulations on GridWorl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOCUD&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#34987;&#25439;&#22351;&#30340;&#29992;&#25143;&#34892;&#20026;&#20013;&#23398;&#20064;&#21644;&#21033;&#29992;&#26410;&#30693;&#30340;&#29992;&#25143;&#20851;&#31995;&#20197;&#21152;&#36895;&#23398;&#20064;&#65292;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#35782;&#21035;&#25439;&#22351;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2310.04768</link><description>&lt;p&gt;
&#22312;&#32447;&#25439;&#22351;&#29992;&#25143;&#26816;&#27979;&#21644;&#21518;&#24724;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Corrupted User Detection and Regret Minimization. (arXiv:2310.04768v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOCUD&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#34987;&#25439;&#22351;&#30340;&#29992;&#25143;&#34892;&#20026;&#20013;&#23398;&#20064;&#21644;&#21033;&#29992;&#26410;&#30693;&#30340;&#29992;&#25143;&#20851;&#31995;&#20197;&#21152;&#36895;&#23398;&#20064;&#65292;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#35782;&#21035;&#25439;&#22351;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22312;&#32447;&#32593;&#32476;&#31995;&#32479;&#20013;&#65292;&#22810;&#20010;&#29992;&#25143;&#36890;&#24120;&#25353;&#39034;&#24207;&#36827;&#20837;&#31995;&#32479;&#12290;&#23545;&#20110;&#28857;&#20987;&#27450;&#35784;&#21644;&#34394;&#20551;&#35780;&#35770;&#31561;&#24212;&#29992;&#65292;&#26576;&#20123;&#29992;&#25143;&#21487;&#33021;&#24694;&#24847;&#25191;&#34892;&#25439;&#22351;&#34892;&#20026;&#20197;&#27450;&#39575;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#28508;&#22312;&#30340;&#25439;&#22351;&#29992;&#25143;&#34892;&#20026;&#20013;&#31283;&#20581;&#22320;&#23398;&#20064;&#65292;&#24182;&#22312;&#22312;&#32447;&#26041;&#24335;&#19979;&#20934;&#30830;&#35782;&#21035;&#25439;&#22351;&#29992;&#25143;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#25439;&#22351;&#40065;&#26834;&#30340;&#24378;&#30423;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#20026;&#21333;&#20010;&#29992;&#25143;&#35774;&#35745;&#30340;&#65292;&#19981;&#33021;&#21033;&#29992;&#22810;&#20010;&#29992;&#25143;&#20043;&#38388;&#30340;&#38544;&#24335;&#31038;&#20250;&#20851;&#31995;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20013;&#27809;&#26377;&#32771;&#34385;&#22312;&#22810;&#29992;&#25143;&#24773;&#26223;&#19979;&#22914;&#20309;&#22312;&#32447;&#26816;&#27979;&#25439;&#22351;&#29992;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;LOCUD&#65292;&#20174;&#34987;&#25171;&#26029;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#24182;&#21033;&#29992;&#26410;&#30693;&#30340;&#29992;&#25143;&#20851;&#31995;&#20197;&#21152;&#24555;&#23398;&#20064;&#65292;&#24182;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#35782;&#21035;&#25439;&#22351;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world online web systems, multiple users usually arrive sequentially into the system. For applications like click fraud and fake reviews, some users can maliciously perform corrupted (disrupted) behaviors to trick the system. Therefore, it is crucial to design efficient online learning algorithms to robustly learn from potentially corrupted user behaviors and accurately identify the corrupted users in an online manner. Existing works propose bandit algorithms robust to adversarial corruption. However, these algorithms are designed for a single user, and cannot leverage the implicit social relations among multiple users for more efficient learning. Moreover, none of them consider how to detect corrupted users online in the multiple-user scenario. In this paper, we present an important online learning problem named LOCUD to learn and utilize unknown user relations from disrupted behaviors to speed up learning, and identify the corrupted users in an online setting. To robustly lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffNAS&#30340;&#22522;&#30784;&#27169;&#22411;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26356;&#22909;&#30340;&#32467;&#26500;&#26469;&#21551;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21512;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#20316;&#20026;&#36229;&#32593;&#65292;&#36741;&#20197;&#25628;&#32034;&#20869;&#23384;&#21644;RFID&#20195;&#29702;&#65292;&#20197;&#21450;&#24555;&#36895;&#25910;&#25947;&#35757;&#32451;&#31574;&#30053;&#65292;&#25628;&#32034;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#36798;&#21040;&#20102;2.82&#30340;&#24615;&#33021;&#25552;&#21319;0.37&#12290;</title><link>http://arxiv.org/abs/2310.04750</link><description>&lt;p&gt;
DiffNAS: &#36890;&#36807;&#24341;&#23548;&#26356;&#22909;&#30340;&#32467;&#26500;&#26469;&#21551;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures. (arXiv:2310.04750v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffNAS&#30340;&#22522;&#30784;&#27169;&#22411;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26356;&#22909;&#30340;&#32467;&#26500;&#26469;&#21551;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21512;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#20316;&#20026;&#36229;&#32593;&#65292;&#36741;&#20197;&#25628;&#32034;&#20869;&#23384;&#21644;RFID&#20195;&#29702;&#65292;&#20197;&#21450;&#24555;&#36895;&#25910;&#25947;&#35757;&#32451;&#31574;&#30053;&#65292;&#25628;&#32034;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#36798;&#21040;&#20102;2.82&#30340;&#24615;&#33021;&#25552;&#21319;0.37&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#22312;&#36873;&#25321;&#25193;&#25955;&#36335;&#24452;&#20043;&#21518;&#65292;&#20687;UNet&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#20027;&#35201;&#39044;&#27979;&#38656;&#35201;&#36880;&#27493;&#28040;&#38500;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#19982;&#39044;&#26399;&#39044;&#31639;&#30456;&#19968;&#33268;&#30340;&#27169;&#22411;&#20197;&#20419;&#36827;&#20248;&#33391;&#30340;&#21512;&#25104;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31934;&#24515;&#20998;&#26512;&#20102;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;"DiffNAS"&#30340;&#22522;&#30784;&#27169;&#22411;&#25628;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#20316;&#20026;&#36229;&#32593;&#26469;&#21152;&#36895;&#25628;&#32034;&#65292;&#36741;&#20197;&#25628;&#32034;&#20869;&#23384;&#20197;&#22686;&#24378;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;RFID&#20316;&#20026;&#20195;&#29702;&#65292;&#24555;&#36895;&#23545;GPT-4&#20135;&#29983;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#24555;&#36895;&#25910;&#25947;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22522;&#20110;GPT&#30340;&#24773;&#22659;&#19979;&#21487;&#20197;&#23558;&#25628;&#32034;&#25928;&#29575;&#25552;&#39640;2&#20493;&#65292;&#21516;&#26102;&#21462;&#24471;&#20102;2.82&#30340;&#24615;&#33021;&#65292;&#25913;&#21892;&#20102;0.37&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently exhibited remarkable performance on synthetic data. After a diffusion path is selected, a base model, such as UNet, operates as a denoising autoencoder, primarily predicting noises that need to be eliminated step by step. Consequently, it is crucial to employ a model that aligns with the expected budgets to facilitate superior synthetic performance. In this paper, we meticulously analyze the diffusion model and engineer a base model search approach, denoted "DiffNAS". Specifically, we leverage GPT-4 as a supernet to expedite the search, supplemented with a search memory to enhance the results. Moreover, we employ RFID as a proxy to promptly rank the experimental outcomes produced by GPT-4. We also adopt a rapid-convergence training strategy to boost search efficiency. Rigorous experimentation corroborates that our algorithm can augment the search efficiency by 2 times under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37 improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#32447;&#24615;&#21270;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#24182;&#24212;&#29992;&#20219;&#21153;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04742</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#19982;&#37096;&#20998;&#32447;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Multi-task Model Fusion with Partial Linearization. (arXiv:2310.04742v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#32447;&#24615;&#21270;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#24182;&#24212;&#29992;&#20219;&#21153;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#24182;&#25104;&#20026;&#22522;&#30784;&#32452;&#20214;&#12290;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#22914;&#20219;&#21153;&#31639;&#27861;&#65292;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#30340;&#24494;&#35843;&#26435;&#37325;&#21512;&#24182;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#22810;&#20219;&#21153;&#27169;&#22411;&#34701;&#21512;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22810;&#20219;&#21153;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20687;LoRA&#24494;&#35843;&#36825;&#26679;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#37096;&#20998;&#32447;&#24615;&#21270;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#24182;&#22312;&#32447;&#24615;&#21270;&#30340;&#36866;&#37197;&#22120;&#19978;&#24212;&#29992;&#20219;&#21153;&#31639;&#27861;&#12290;&#36825;&#26679;&#19968;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#27169;&#22411;&#34701;&#21512;&#20248;&#21183;&#26469;&#36827;&#34892;&#32447;&#24615;&#21270;&#24494;&#35843;&#65292;&#21516;&#26102;&#20445;&#25345;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#37096;&#20998;&#32447;&#24615;&#21270;&#25216;&#26415;&#20351;&#22810;&#20010;&#20219;&#21153;&#26356;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#21333;&#20010;&#27169;&#22411;&#20013;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04741</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#24179;&#34913;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#65306;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#36825;&#31181;&#24179;&#34913;&#36827;&#34892;&#20102;&#35299;&#21078;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#39318;&#20808;&#35299;&#20915;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#22256;&#22659;&#21450;&#20854;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#20851;&#31995;&#12290;&#23427;&#23558;&#23398;&#20064;&#35825;&#23548;&#30340;&#28608;&#27963;&#21464;&#21270;&#19982;&#20808;&#21069;&#35835;&#20986;&#33539;&#22260;&#20869;&#30340;&#31283;&#23450;&#24615;&#31243;&#24230;&#21644;&#38646;&#31354;&#38388;&#30340;&#21464;&#21270;&#19982;&#21487;&#22609;&#24615;&#31243;&#24230;&#30456;&#20851;&#32852;&#12290;&#22312;&#22788;&#29702;&#20998;&#35010;CIFAR-110&#20219;&#21153;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#65292;&#35813;&#26694;&#26550;&#38416;&#26126;&#20102;&#24120;&#29992;&#27491;&#21017;&#21270;&#31639;&#27861;&#65288;SI&#12289;EWC&#21644;LwF&#65289;&#20197;&#21450;&#37325;&#25918;&#31639;&#27861;&#65288;GEM&#21644;&#25968;&#25454;&#37325;&#25918;&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plast
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OILCA&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#35782;&#21035;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;"&#23545;&#25239;&#24615;"&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#29615;&#22659;&#21464;&#21270;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04706</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#19982;&#21464;&#20998;&#36870;&#21521;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation Learning with Variational Counterfactual Reasoning. (arXiv:2310.04706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OILCA&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#35782;&#21035;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;"&#23545;&#25239;&#24615;"&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#29615;&#22659;&#21464;&#21270;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#26088;&#22312;&#23398;&#20064;&#19968;&#31181;&#26368;&#20248;&#30340;&#19987;&#23478;&#34892;&#20026;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22312;&#32447;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#65292;&#31163;&#32447;&#25968;&#25454;&#38598;&#26159;&#20174;&#27809;&#26377;&#22870;&#21169;&#30340;&#27425;&#20248;&#34892;&#20026;&#20013;&#25910;&#38598;&#26469;&#30340;&#12290;&#30001;&#20110;&#19987;&#23478;&#25968;&#25454;&#31232;&#32570;&#65292;&#26234;&#33021;&#20307;&#36890;&#24120;&#21482;&#33021;&#31616;&#21333;&#22320;&#35760;&#20303;&#36139;&#20047;&#30340;&#36712;&#36857;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#32570;&#20047;&#23545;&#26032;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#28040;&#38500;&#20250;&#23545;&#26234;&#33021;&#20307;&#36896;&#25104;&#20559;&#24046;&#24182;&#38459;&#30861;&#27867;&#21270;&#30340;&#20266;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OILCA&#30340;&#26694;&#26550;&#65292;&#21363;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#19982;&#23545;&#25239;&#25968;&#25454;&#22686;&#24378;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#35782;&#21035;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;"&#23545;&#25239;&#24615;"&#26679;&#26412;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23545;&#25239;&#24615;&#35782;&#21035;&#21644;&#27867;&#21270;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In offline Imitation Learning (IL), an agent aims to learn an optimal expert behavior policy without additional online environment interactions. However, in many real-world scenarios, such as robotics manipulation, the offline dataset is collected from suboptimal behaviors without rewards. Due to the scarce expert data, the agents usually suffer from simply memorizing poor trajectories and are vulnerable to the variations in the environments, lacking the capability of generalizing to new environments. To effectively remove spurious features that would otherwise bias the agent and hinder generalization, we propose a framework named \underline{O}ffline \underline{I}mitation \underline{L}earning with \underline{C}ounterfactual data \underline{A}ugmentation (OILCA). In particular, we leverage the identifiable variational autoencoder to generate \textit{counterfactual} samples. We theoretically analyze the counterfactual identification and the improvement of generalization. Moreover, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;RUAD&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04693</link><description>&lt;p&gt;
&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#24102;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#30340;&#25552;&#21319;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization. (arXiv:2310.04693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;RUAD&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#22312;&#22312;&#32447;&#33829;&#38144;&#20013;&#23637;&#31034;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#20013;&#23481;&#26131;&#21463;&#21040;&#40065;&#26834;&#24615;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#19978;&#36848;&#29616;&#35937;&#32473;&#20986;&#20102;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#22312;&#32447;&#33829;&#38144;&#20013;&#23384;&#22312;&#29305;&#24449;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#19968;&#20123;&#20851;&#38190;&#29305;&#24449;&#30340;&#25200;&#21160;&#20250;&#20005;&#37325;&#24433;&#21709;&#25552;&#21319;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#23548;&#33268;&#30456;&#21453;&#30340;&#36235;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#24314;&#27169;&#26694;&#26550;&#65288;RUAD&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;RUAD&#36890;&#36807;&#20004;&#20010;&#23450;&#21046;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20943;&#36731;&#25552;&#21319;&#27169;&#22411;&#30340;&#29305;&#24449;&#25935;&#24863;&#24615;&#65292;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#32852;&#21512;&#22810;&#26631;&#31614;&#24314;&#27169;&#30340;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#65292;&#20197;&#20174;&#36755;&#20837;&#29305;&#24449;&#20013;&#35782;&#21035;&#19968;&#20010;&#20851;&#38190;&#23376;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#36719;&#25554;&#20540;&#25805;&#20316;&#30340;&#23545;&#25239;&#29305;&#24449;&#25233;&#21046;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling has shown very promising results in online marketing. However, most existing works are prone to the robustness challenge in some practical applications. In this paper, we first present a possible explanation for the above phenomenon. We verify that there is a feature sensitivity problem in online marketing using different real-world datasets, where the perturbation of some key features will seriously affect the performance of the uplift model and even cause the opposite trend. To solve the above problem, we propose a novel robustness-enhanced uplift modeling framework with adversarial feature desensitization (RUAD). Specifically, our RUAD can more effectively alleviate the feature sensitivity of the uplift model through two customized modules, including a feature selection module with joint multi-label modeling to identify a key subset from the input features and an adversarial feature desensitization module using adversarial training and soft interpolation operations t
&lt;/p&gt;</description></item><item><title>LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04673</link><description>&lt;p&gt;
LauraGPT&#65306;&#20351;&#29992;GPT&#36827;&#34892;&#21548;&#12289;&#20851;&#27880;&#12289;&#29702;&#35299;&#21644;&#20877;&#29983;&#38899;&#39057;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04673
&lt;/p&gt;
&lt;p&gt;
LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#31867;&#20284;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#32570;&#20047;&#20805;&#20998;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#35782;&#21035;&#21644;&#29702;&#35299;&#38899;&#39057;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#35201;&#20040;&#26126;&#26174;&#19981;&#21450;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;SOTA&#65289;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LauraGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;GPT&#27169;&#22411;&#12290;LauraGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#22312;&#20219;&#24847;&#27169;&#24335;&#19979;&#29983;&#25104;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#36827;&#34892;&#19982;&#20869;&#23481;&#12289;&#35821;&#20041;&#12289;&#35821;&#38899;&#23398;&#21644;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20123;&#20540;&#24471;&#27880;&#24847;&#30340;&#20219;&#21153;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#33258;&#21160;&#38899;&#39057;&#25429;&#33719;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#21464;&#23610;&#23544;&#22320;&#22270;&#21644;&#31232;&#30095;&#27979;&#37327;&#20013;&#39044;&#27979;&#38142;&#25509;&#32423;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.04570</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#26041;&#27861;&#29992;&#20110;&#21464;&#23610;&#23544;&#22320;&#22270;&#20013;&#30340;&#38142;&#25509;&#32423;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction from Variable-Sized Maps. (arXiv:2310.04570v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#21464;&#23610;&#23544;&#22320;&#22270;&#21644;&#31232;&#30095;&#27979;&#37327;&#20013;&#39044;&#27979;&#38142;&#25509;&#32423;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21457;&#23556;&#26426;-&#25509;&#25910;&#26426;&#20301;&#32622;&#30340;&#36335;&#24452;&#25439;&#32791;&#23545;&#20110;&#32593;&#32476;&#35268;&#21010;&#21644;&#20999;&#25442;&#31561;&#35768;&#22810;&#29992;&#20363;&#33267;&#20851;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#22522;&#20110;&#22320;&#22270;&#25968;&#25454;&#39044;&#27979;&#26080;&#32447;&#20449;&#36947;&#23646;&#24615;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#19981;&#21516;&#23610;&#23544;&#30340;&#22320;&#22270;&#21644;&#31232;&#30095;&#27979;&#37327;&#20013;&#39044;&#27979;&#38142;&#25509;&#32423;&#23646;&#24615;&#12290;&#22320;&#22270;&#21253;&#21547;&#24314;&#31569;&#29289;&#21644;&#26893;&#34987;&#30340;&#20449;&#24687;&#12290;Transformer&#27169;&#22411;&#20851;&#27880;&#19982;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30456;&#20851;&#30340;&#21306;&#22495;&#65292;&#22240;&#27492;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#21516;&#23610;&#23544;&#30340;&#22320;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#30340;&#21457;&#23556;&#26426;&#21644;&#25509;&#25910;&#26426;&#22352;&#26631;&#65292;&#26080;&#38656;&#31163;&#25955;&#21270;&#22788;&#29702;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#21040;&#20027;&#23548;&#30340;&#36335;&#24452;&#25439;&#32791;&#65292;&#24182;&#22312;&#26032;&#39062;&#22320;&#22270;&#19978;&#36827;&#34892;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating path loss for a transmitter-receiver location is key to many use-cases including network planning and handover. Machine learning has become a popular tool to predict wireless channel properties based on map data. In this work, we present a transformer-based neural network architecture that enables predicting link-level properties from maps of various dimensions and from sparse measurements. The map contains information about buildings and foliage. The transformer model attends to the regions that are relevant for path loss prediction and, therefore, scales efficiently to maps of different size. Further, our approach works with continuous transmitter and receiver coordinates without relying on discretization. In experiments, we show that the proposed model is able to efficiently learn dominant path losses from sparse training data and generalizes well when tested on novel maps.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04373</link><description>&lt;p&gt;
&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#23545;&#25239;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#36866;&#24212;&#20154;&#31867;&#21453;&#39304;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20174;&#20960;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#27169;&#22411;&#20013;&#27966;&#29983;&#20986;&#22870;&#21169;&#65292;&#27599;&#20010;&#27169;&#22411;&#25429;&#25417;&#35821;&#35328;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#32452;&#21512;&#36825;&#20123;&#32452;&#25104;&#30340;&#22870;&#21169;&#27169;&#22411;&#26102;&#65292;&#36866;&#24403;&#22320;&#21152;&#26435;&#21464;&#24471;&#22256;&#38590;&#12290;&#26356;&#21152;&#22256;&#38590;&#30340;&#26159;&#65292;&#30001;&#20110;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#21482;&#26159;&#20154;&#31867;&#35780;&#20215;&#30340;&#20195;&#29702;&#65292;&#36825;&#19968;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#36229;&#36807;&#26576;&#19968;&#28857;&#21518;&#65292;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#19982;&#26356;&#24046;&#30340;&#20154;&#31867;&#35780;&#20215;&#30456;&#20851;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#36825;&#20123;&#28857;&#30340;&#20301;&#32622;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04006</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerating optimization over the space of probability measures. (arXiv:2310.04006v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#30340;&#21152;&#36895;&#26159;&#19968;&#20010;&#38750;&#24120;&#23454;&#29992;&#21644;&#29702;&#35770;&#19978;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20248;&#21270;&#19978;&#65292;&#20294;&#32771;&#34385;&#21040;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#38656;&#35201;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25968;&#20540;&#23454;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.03925</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#23494;&#20999;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#21270;&#27169;&#22411;&#65292;MTL&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#38750;MTL&#27169;&#22411;&#12290;&#23613;&#31649;MTL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;MTL&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#19968;&#32500;&#21367;&#31215;&#30340;TSC&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#65292;TSC&#27169;&#22411;&#30340;&#24615;&#33021;&#23454;&#38469;&#19978;&#20250;&#19979;&#38477;&#12290;&#36890;&#36807;&#23558;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#20989;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#30475;&#20986;&#20302;&#19979;&#30340;&#32467;&#26524;&#26159;&#30001;&#20110;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#33258;&#21160;&#21270;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03748</link><description>&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
Phase Synchrony Component Self-Organization in Brain Computer Interface. (arXiv:2310.03748v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#33258;&#21160;&#21270;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#21516;&#27493;&#20449;&#24687;&#22312;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#30001;&#39044;&#22788;&#29702;&#12289;&#36873;&#25321;&#33041;&#30005;&#37319;&#38598;&#36890;&#36947;&#21644;&#30456;&#20301;&#38145;&#23450;&#20540;&#65288;PLV&#65289;&#35745;&#31639;&#32452;&#25104;&#65292;&#22312;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35813;&#27969;&#31243;&#26159;&#25163;&#21160;&#30340;&#19988;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#20415;&#21033;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#37319;&#29992;&#20102;&#19968;&#33324;&#30340;&#19982;&#25968;&#25454;&#26080;&#20851;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#26469;&#25233;&#21046;&#22122;&#22768;&#65292;&#38459;&#30861;&#20102;&#26356;&#37325;&#35201;&#30340;&#30456;&#20301;&#21516;&#27493;&#29616;&#35937;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#25968;&#25454;&#30456;&#20851;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#33258;&#21160;&#21270;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#31243;&#24207;&#12290;&#22522;&#20110;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#65292;&#30452;&#25509;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Phase synchrony information plays a crucial role in analyzing functional brain connectivity and identifying brain activities. A widely adopted feature extraction pipeline, composed of preprocessing, selection of EEG acquisition channels, and phase locking value (PLV) calculation, has achieved success in motor imagery classification (MI). However, this pipeline is manual and reliant on expert knowledge, limiting its convenience and adaptability to different application scenarios. Moreover, most studies have employed mediocre data-independent spatial filters to suppress noise, impeding the exploration of more significant phase synchronization phenomena. To address the issues, we propose the concept of phase synchrony component self-organization, which enables the adaptive learning of data-dependent spatial filters for automating both the preprocessing and channel selection procedures. Based on this concept, the first deep learning end-to-end network is developed, which directly extracts 
&lt;/p&gt;</description></item><item><title>GPT-MolBERTa&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03030</link><description>&lt;p&gt;
GPT-MolBERTa&#65306;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;GPT&#20998;&#23376;&#29305;&#24449;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03030
&lt;/p&gt;
&lt;p&gt;
GPT-MolBERTa&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Transformer&#26550;&#26500;&#30340;&#20986;&#29616;&#21450;&#20854;&#23545;&#25991;&#26412;&#25968;&#25454;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26032;&#39046;&#22495;&#24050;&#32463;&#24320;&#21551;&#12290;&#23613;&#31649;SMILES&#26159;&#26368;&#24120;&#35265;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20581;&#22766;&#24615;&#12289;&#20016;&#23500;&#20449;&#24687;&#21644;&#35268;&#33539;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25104;&#20026;&#21487;&#25512;&#24191;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-MolBERTa&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#39044;&#27979;&#20854;&#24615;&#36136;&#12290;&#20351;&#29992;ChatGPT&#25910;&#38598;&#20102;326000&#20010;&#20998;&#23376;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25551;&#36848;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;LLM&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32454;&#35843;&#38454;&#27573;&#20351;&#29992;&#20102;BERT&#21644;RoBERTa&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-MolBERTa&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#27880;&#24847;&#21147;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention 
&lt;/p&gt;</description></item><item><title>IBCL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#20219;&#21153;&#26435;&#34913;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30693;&#35782;&#24211;&#24182;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02995</link><description>&lt;p&gt;
IBCL&#65306;&#36830;&#32493;&#23398;&#20064;&#20013;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#29992;&#20110;&#20219;&#21153;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning. (arXiv:2310.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02995
&lt;/p&gt;
&lt;p&gt;
IBCL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#20013;&#20219;&#21153;&#26435;&#34913;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#30693;&#35782;&#24211;&#24182;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#65292;&#23454;&#29616;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36830;&#32493;&#23398;&#20064;&#20855;&#26377;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#38754;&#20020;&#30528;&#19981;&#21516;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#20248;&#21270;&#24403;&#21069;&#20219;&#21153;&#20998;&#24067;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#19968;&#20123;&#20808;&#21069;&#30340;&#20219;&#21153;&#19978;&#29306;&#29298;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#23384;&#22312;&#22810;&#20010;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#35299;&#20915;&#20102;&#19981;&#21516;&#30340;&#20219;&#21153;&#24615;&#33021;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#35752;&#35770;&#20102;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#26435;&#34913;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#19982;&#20559;&#22909;&#25968;&#37327;&#25104;&#27604;&#20363;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#24403;&#23384;&#22312;&#22810;&#20010;&#29978;&#33267;&#26159;&#26080;&#38480;&#22810;&#20010;&#20559;&#22909;&#26102;&#65292;&#36825;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#36127;&#25285;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Imprecise Bayesian Continual Learning (IBCL)&#12290;&#22312;&#26032;&#20219;&#21153;&#20986;&#29616;&#26102;&#65292;IBCL(1)&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#30340;&#20984;&#21253;&#24418;&#24335;&#26356;&#26032;&#30693;&#35782;&#24211;&#65292;(2)&#33719;&#24471;&#20102;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#20219;&#21153;&#26435;&#34913;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some previous tasks. This means that there exist multiple models that are Pareto-optimal at different times, each addressing a distinct task performance trade-off. Researchers have discussed how to train particular models to address specific trade-off preferences. However, existing algorithms require training overheads proportional to the number of preferences -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address task trade-off preferences with zero-shot. That is,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02717</link><description>&lt;p&gt;
&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#22312;&#32447;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#27169;&#22411;&#20559;&#24046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#27599;&#36718;&#20013;&#65292;&#32473;&#23450;&#33218;&#30340;&#29305;&#24449;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#33218;&#26469;&#26368;&#22823;&#21270;&#38271;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31995;&#21015;&#24037;&#20316;&#65292;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#24182;&#22312;&#32463;&#20856;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#27491;&#30830;&#35268;&#23450;&#32447;&#24615;&#29992;&#25143;&#27169;&#22411;&#65292;&#24403;&#36825;&#20010;&#20851;&#38190;&#20551;&#35774;&#19981;&#25104;&#31435;&#26102;&#65292;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22914;&#20309;&#20026;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#35774;&#35745;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#29992;&#25143;&#27169;&#22411;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#27169;&#22411;&#20013;&#30340;&#26399;&#26395;&#22870;&#21169;&#21487;&#33021;&#26377;&#20559;&#24046;&#65292;&#19981;&#26159;&#23436;&#32654;&#30340;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#40065;&#26834;&#30340;&#32858;&#31867;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;RCLUMB&#21644;RSCLUMB&#65288;&#20998;&#21035;&#29992;&#21160;&#24577;&#22270;&#21644;&#38598;&#21512;&#34920;&#31034;&#23398;&#20064;&#21040;&#30340;&#32858;&#31867;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, resp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;MLMC&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#27493;&#39588;&#20013;&#35745;&#31639;&#36807;&#30340;&#26799;&#24230;&#20998;&#37327;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;MLMC&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02402</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Parallel Complexity of Multilevel Monte Carlo in Stocahstic Gradient Descent. (arXiv:2310.02402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;MLMC&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#27493;&#39588;&#20013;&#35745;&#31639;&#36807;&#30340;&#26799;&#24230;&#20998;&#37327;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;MLMC&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29992;&#20110;&#39034;&#24207;&#27169;&#25311;&#65288;&#22914;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65289;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#65292;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#29702;&#35770;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#26420;&#32032;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;MLMC&#22312;&#29616;&#20195;GPU&#31561;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#65292;&#22240;&#20026;&#20854;&#24182;&#34892;&#22797;&#26434;&#24615;&#19982;&#26420;&#32032;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#24403;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24310;&#36831;MLMC&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#27493;&#39588;&#20013;&#35745;&#31639;&#36807;&#30340;&#26799;&#24230;&#20998;&#37327;&#65292;&#22823;&#22823;&#38477;&#20302;MLMC&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#33021;&#22815;&#35777;&#26126;&#38477;&#20302;&#24179;&#22343;&#24182;&#34892;&#22797;&#26434;&#24615;&#65292;&#20294;&#20195;&#20215;&#26159;&#31245;&#24046;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#22312;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23545;&#20914;&#30340;&#31034;&#20363;&#26469;&#35777;&#26126;&#19982;&#26631;&#20934;MLMC&#22312;SGD&#20013;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24182;&#34892;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the stochastic gradient descent (SGD) for sequential simulations such as the neural stochastic differential equations, the Multilevel Monte Carlo (MLMC) method is known to offer better theoretical computational complexity compared to the naive Monte Carlo approach. However, in practice, MLMC scales poorly on massively parallel computing platforms such as modern GPUs, because of its large parallel complexity which is equivalent to that of the naive Monte Carlo method. To cope with this issue, we propose the delayed MLMC gradient estimator that drastically reduces the parallel complexity of MLMC by recycling previously computed gradient components from earlier steps of SGD. The proposed estimator provably reduces the average parallel complexity per iteration at the cost of a slightly worse per-iteration convergence rate. In our numerical experiments, we use an example of deep hedging to demonstrate the superior parallel complexity of our method compared to the standard MLMC in SGD.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31383;&#21475;&#30340;&#27169;&#22411;&#24179;&#22343;&#26041;&#27861;(WIMA)&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#36718;&#27425;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#25429;&#33719;&#22810;&#20010;&#29992;&#25143;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#23450;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#36890;&#20449;&#25110;&#23458;&#25143;&#31471;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.01366</link><description>&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#27169;&#22411;&#24179;&#22343;&#25913;&#21892;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning. (arXiv:2310.01366v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01366
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#27169;&#22411;&#24179;&#22343;&#26041;&#27861;(WIMA)&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#36718;&#27425;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#25429;&#33719;&#22810;&#20010;&#29992;&#25143;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#23450;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#36890;&#20449;&#25110;&#23458;&#25143;&#31471;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#20174;&#20998;&#24067;&#24335;&#29992;&#25143;&#20013;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20445;&#25252;&#20182;&#20204;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#24322;&#26500;&#26102;&#65292;&#23398;&#20064;&#36807;&#31243;&#21464;&#24471;&#22024;&#26434;&#12289;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#20542;&#21521;&#20110;&#26368;&#21518;&#19968;&#27425;&#35266;&#23519;&#21040;&#30340;&#23458;&#25143;&#31471;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#24930;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#27169;&#22411;&#24179;&#22343;&#26041;&#27861;&#65288;WIMA&#65289;&#12290;WIMA&#20351;&#29992;&#22522;&#20110;&#31383;&#21475;&#30340;&#26041;&#27861;&#26469;&#32858;&#21512;&#19981;&#21516;&#36718;&#27425;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#33719;&#22810;&#20010;&#29992;&#25143;&#30340;&#30693;&#35782;&#24182;&#20943;&#23569;&#26368;&#21518;&#20960;&#20010;&#29992;&#25143;&#30340;&#20559;&#24046;&#12290;&#36890;&#36807;&#37319;&#29992;&#31383;&#21475;&#35270;&#22270;&#26469;&#22788;&#29702;&#36718;&#27425;&#65292;WIMA&#21487;&#20197;&#20174;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#23601;&#24212;&#29992;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#36890;&#20449;&#25110;&#23458;&#25143;&#31471;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WIMA&#23545;&#20998;&#24067;&#20559;&#31227;&#21644;&#19981;&#22909;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21576;&#29616;&#20986;&#26356;&#24179;&#28369;&#21644;&#26356;&#31283;&#23450;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;&#27492;&#22806;&#65292;WIMA&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#25193;&#23637;&#21644;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aims to learn a global model from distributed users while protecting their privacy. However, when data are distributed heterogeneously the learning process becomes noisy, unstable, and biased towards the last seen clients' data, slowing down convergence. To address these issues and improve the robustness and generalization capabilities of the global model, we propose WIMA (Window-based Model Averaging). WIMA aggregates global models from different rounds using a window-based approach, effectively capturing knowledge from multiple users and reducing the bias from the last ones. By adopting a windowed view on the rounds, WIMA can be applied from the initial stages of training. Importantly, our method introduces no additional communication or client-side computation overhead. Our experiments demonstrate the robustness of WIMA against distribution shifts and bad client sampling, resulting in smoother and more stable learning trends. Additionally, WIMA can be easily 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#38598;&#25104;&#26041;&#27861;&#30340;&#20117;&#19979;&#29305;&#24615;&#34920;&#24449;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;WGAN-GP&#21644;ES-MDA&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;K&#22330;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20960;&#20010;&#20117;&#19979;&#23454;&#20363;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#26410;&#30693;K&#23383;&#27573;&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.00839</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#20110;&#38598;&#25104;&#26041;&#27861;&#30340;&#20117;&#19979;&#29305;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models. (arXiv:2310.00839v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#38598;&#25104;&#26041;&#27861;&#30340;&#20117;&#19979;&#29305;&#24615;&#34920;&#24449;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;WGAN-GP&#21644;ES-MDA&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;K&#22330;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20960;&#20010;&#20117;&#19979;&#23454;&#20363;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#26410;&#30693;K&#23383;&#27573;&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#22914;&#27700;&#21147;&#20256;&#23548;&#29575;&#65288;K&#65289;&#31561;&#31354;&#38388;&#20998;&#24067;&#23646;&#24615;&#26159;&#20117;&#19979;&#29305;&#24615;&#34920;&#24449;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#31232;&#30095;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#31934;&#24230;&#20302;&#65292;&#36870;&#21521;&#24314;&#27169;&#22312;&#19981;&#36866;&#23450;&#30340;&#39640;&#32500;&#24212;&#29992;&#20013;&#21463;&#38480;&#12290;&#26412;&#25991;&#23558;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#19982;&#26799;&#24230;&#24809;&#32602;&#65288;WGAN-GP&#65289;&#21644;&#22522;&#20110;&#38598;&#25104;&#30340;&#22810;&#20803;&#25968;&#25454;&#21516;&#21270;&#65288;ES-MDA&#65289;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#20117;&#19979;&#29305;&#24615;&#34920;&#24449;&#12290;WGAN-GP&#36890;&#36807;&#35757;&#32451;&#20174;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#29983;&#25104;&#39640;&#32500;K&#22330;&#65292;ES-MDA&#36890;&#36807;&#21516;&#21270;&#21487;&#29992;&#27979;&#37327;&#32467;&#26524;&#26469;&#26356;&#26032;&#28508;&#21464;&#37327;&#12290;&#21033;&#29992;&#20960;&#20010;&#20117;&#19979;&#23454;&#20363;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20197;&#21450;&#26410;&#30693;K&#23383;&#27573;&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating spatially distributed properties such as hydraulic conductivity (K) from available sparse measurements is a great challenge in subsurface characterization. However, the use of inverse modeling is limited for ill-posed, high-dimensional applications due to computational costs and poor prediction accuracy with sparse datasets. In this paper, we combine Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), a deep generative model that can accurately capture complex subsurface structure, and Ensemble Smoother with Multiple Data Assimilation (ES-MDA), an ensemble-based inversion method, for accurate and accelerated subsurface characterization. WGAN-GP is trained to generate high-dimensional K fields from a low-dimensional latent space and ES-MDA then updates the latent variables by assimilating available measurements. Several subsurface examples are used to evaluate the accuracy and efficiency of the proposed method and the main features of the unknown K fie
&lt;/p&gt;</description></item><item><title>SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00533</link><description>&lt;p&gt;
SELF&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00533
&lt;/p&gt;
&lt;p&gt;
SELF&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#26029;&#33258;&#25105;&#36827;&#21270;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#35780;&#20272;&#24037;&#20855;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21331;&#36234;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23398;&#20064;&#21644;&#25512;&#21160;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#8212;&#8212;&#27169;&#22411;&#33258;&#20027;&#36827;&#21270;&#30340;&#36335;&#24452;&#20173;&#28982;&#26410;&#30693;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;"SELF"&#65288;&#24102;&#26377;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#20027;&#36827;&#21270;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLM&#33021;&#22815;&#19981;&#26029;&#22320;&#33258;&#25105;&#36827;&#21270;&#12290;&#27492;&#22806;&#65292;SELF&#21033;&#29992;&#35821;&#35328;&#21453;&#39304;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#20840;&#38754;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#31934;&#30830;&#23450;&#20301;&#21709;&#24212;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#39640;&#33258;&#20027;&#36827;&#21270;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;SELF&#39318;&#20808;&#36827;&#34892;&#20803;&#25216;&#33021;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#33258;&#25105;&#21453;&#39304;&#21644;&#33258;&#25105;&#31934;&#28860;&#12290;&#36825;&#20123;&#20803;&#25216;&#33021;&#26159;&#20851;&#38190;&#65292;&#24341;&#23548;&#27169;&#22411;&#22312;&#33258;&#21046;&#25968;&#25454;&#30340;&#25345;&#32493;&#35757;&#32451;&#21608;&#26399;&#20013;&#36827;&#34892;&#21518;&#32493;&#30340;&#33258;&#25105;&#36827;&#21270;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#20869;&#22312;&#33021;&#21147;&#12290;&#22312;&#32473;&#23450;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;SELF&#20351;&#27169;&#22411;&#20855;&#22791;&#20102;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00369</link><description>&lt;p&gt;
&#25552;&#28860;&#24402;&#32435;&#20559;&#24046;&#65306;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression. (arXiv:2310.00369v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;Vision Transformers (ViTs) &#25552;&#20379;&#20102;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#23454;&#29616;&#32479;&#19968;&#20449;&#24687;&#22788;&#29702;&#30340;&#35825;&#20154;&#21069;&#26223;&#12290;&#20294;&#26159;&#30001;&#20110;ViTs&#32570;&#20047;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#30340;&#24212;&#29992;&#23454;&#38469;&#21487;&#34892;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#36731;&#37327;&#32423;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#12290;&#20197;&#21069;&#30340;&#31995;&#32479;&#20165;&#20381;&#38752;&#22522;&#20110;&#21367;&#31215;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#20542;&#21521;&#30340;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;&#21367;&#31215;&#21644;&#38750;&#32447;&#24615;&#21367;&#31215;&#65289;&#21516;&#26102;&#29992;&#20110;&#25351;&#23548;&#23398;&#29983;Transformer&#12290;&#30001;&#20110;&#36825;&#20123;&#29420;&#29305;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#23384;&#20648;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#28041;&#21450;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;logits&#65292;&#20174;&#26681;&#26412;&#19978;&#23454;&#29616;&#20102;&#38750;&#24402;&#19968;&#21270;&#30340;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalize
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00270</link><description>&lt;p&gt;
SpatialRank: &#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#19982;NDCG&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26102;&#31354;&#25968;&#25454;&#30340;NDCG&#20248;&#21270;&#26469;&#35299;&#20915;&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20107;&#20214;&#25490;&#21517;&#38382;&#39064;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#20107;&#20214;&#65289;&#30340;&#39118;&#38505;&#26368;&#39640;&#30340;&#21069;k&#20010;&#22320;&#28857;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#20844;&#20849;&#23433;&#20840;&#21644;&#22478;&#24066;&#31649;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#28857;&#20043;&#38388;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#31354;&#38388;&#20013;&#22478;&#24066;&#20107;&#20214;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#65292;&#20197;&#21450;&#27491;&#30830;&#23545;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#38468;&#36817;&#22320;&#28857;&#36827;&#34892;&#25490;&#21517;&#30340;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#20027;&#35201;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#25152;&#26377;&#22320;&#28857;&#30340;&#23454;&#38469;&#39118;&#38505;&#24471;&#20998;&#25110;&#20107;&#20214;&#35745;&#25968;&#12290;&#30001;&#20110;&#39044;&#27979;&#38169;&#35823;&#65292;&#30001;&#27492;&#24471;&#21040;&#30340;&#25490;&#21517;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#12290;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#30452;&#25509;&#20248;&#21270;&#35832;&#22914;&#26631;&#20934;&#21270;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;NDCG&#65289;&#20043;&#31867;&#30340;&#25351;&#26631;&#65292;&#20294;&#19981;&#33021;&#22788;&#29702;&#22320;&#28857;&#20043;&#38388;&#23384;&#22312;&#30340;&#26102;&#31354;&#33258;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SpatialRank&#30340;&#26032;&#39062;&#31354;&#38388;&#20107;&#20214;&#25490;&#21517;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named Spati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.00212</link><description>&lt;p&gt;
&#20004;&#20004;&#37051;&#36817;&#31574;&#30053;&#20248;&#21270;: &#21033;&#29992;&#30456;&#23545;&#21453;&#39304;&#36827;&#34892;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#30456;&#23545;&#21453;&#39304;&#26469;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20248;&#21270;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#31639;&#27861;&#35774;&#35745;&#21644;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#26469;&#33719;&#21462;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25509;&#35302;&#21040;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#19981;&#19968;&#33268;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#24341;&#23548;LLMs&#26397;&#30528;&#26377;&#30410;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#30340;&#20027;&#23548;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;Proximal Policy Optimization&#65288;PPO&#65289;&#26159;&#40664;&#35748;&#30340;RL&#20248;&#21270;&#22120;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;PPO&#22312;&#20248;&#21270;&#22522;&#20110;&#27604;&#36739;&#25439;&#22833;&#35757;&#32451;&#30340;&#22870;&#21169;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#26657;&#20934;&#22870;&#21169;&#23610;&#24230;&#65292;PPO&#23545;&#20110;&#21253;&#21547;&#30456;&#21516;&#20559;&#22909;&#20449;&#24687;&#30340;&#31561;&#20215;&#22870;&#21169;&#20989;&#25968;&#19981;&#20855;&#22791;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#36712;&#36857;&#30340;&#20248;&#21270;&#30456;&#27604;&#65292;PPO&#23545;&#20110;&#22522;&#20110;&#20196;&#29260;&#30340;&#26356;&#26032;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#20989;&#25968;&#36924;&#36817;&#21644;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#30456;&#23545;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;Pairwise Proximal Policy Optimization&#65288;PPPO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
&lt;/p&gt;</description></item><item><title>ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.00117</link><description>&lt;p&gt;
ABScribe: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00117
&lt;/p&gt;
&lt;p&gt;
ABScribe&#26159;&#19968;&#31181;&#30028;&#38754;&#65292;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#25506;&#32034;&#22810;&#31181;&#20889;&#20316;&#21464;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#24555;&#36895;&#29983;&#25104;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20197;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#21576;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#20070;&#20889;&#25991;&#26412;&#26469;&#25506;&#32034;&#26367;&#20195;&#24819;&#27861;&#26159;&#20889;&#20316;&#36807;&#31243;&#30340;&#20851;&#38190;&#12290;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#31616;&#21270;&#20889;&#20316;&#21464;&#21270;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30028;&#38754;&#23384;&#22312;&#21516;&#26102;&#32771;&#34385;&#22810;&#31181;&#21464;&#21270;&#30340;&#25361;&#25112;&#65306;&#22312;&#19981;&#35206;&#30422;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26032;&#30340;&#29256;&#26412;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#25353;&#39034;&#24207;&#31896;&#36148;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#25991;&#26723;&#21464;&#24471;&#26434;&#20081;&#65292;&#22686;&#21152;&#24037;&#20316;&#37327;&#65292;&#24182;&#25171;&#26029;&#20316;&#32773;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ABScribe&#65292;&#19968;&#31181;&#25903;&#25345;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#20889;&#20316;&#20219;&#21153;&#20013;&#24555;&#36895;&#19988;&#32467;&#26500;&#21270;&#22320;&#25506;&#32034;&#20889;&#20316;&#21464;&#21270;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;ABScribe&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;LLM&#25552;&#31034;&#24555;&#36895;&#20135;&#29983;&#22810;&#20010;&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#20250;&#33258;&#21160;&#36716;&#25442;&#25104;&#21487;&#37325;&#29992;&#30340;&#25353;&#38062;&#24418;&#24335;&#12290;&#21464;&#20307;&#22312;&#25991;&#26412;&#27573;&#33853;&#20013;&#34987;&#23384;&#20648;&#22312;&#30456;&#37051;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#24037;&#20855;&#26639;&#19978;&#30340;&#40736;&#26631;&#24748;&#20572;&#20132;&#20114;&#36827;&#34892;&#24555;&#36895;&#30340;&#23601;&#22320;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#21517;&#25776;&#20889;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;ABScribe&#33021;&#26174;&#33879;&#20943;&#36731;&#20219;&#21153;&#36127;&#33655;&#65288;d = 1.20, p &lt; 0.001&#65289;&#65292;&#25552;&#39640;&#29992;&#25143;&#30340;&#35748;&#30693;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p &lt; 0.001), enhances user perceptions o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15649</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#35753;LLMs&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#65292;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#65288;TAP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#25351;&#20196;&#21644;&#28436;&#31034;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#22806;&#30340;&#20219;&#21153;&#65288;ATIS&#21644;WSJ&#65289;&#19978;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31532;&#19968;&#27425;&#25195;&#25551;&#31995;&#32479;&#21644;&#37325;&#26032;&#35780;&#20998;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#36890;&#36807;&#20923;&#32467;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#21487;&#20197;&#36798;&#21040;&#19982;&#39046;&#22495;&#35843;&#20248;&#30340;LMs&#37325;&#26032;&#35780;&#20998;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#25552;&#31034;&#25216;&#26415;&#19982;&#24494;&#35843;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20302;&#20110;N-best Oracle&#27700;&#24179;&#30340;&#38169;&#35823;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
&lt;/p&gt;</description></item><item><title>MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15312</link><description>&lt;p&gt;
MAPTree: &#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#20987;&#36133;&#8220;&#26368;&#20248;&#8221;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15312
&lt;/p&gt;
&lt;p&gt;
MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#20173;&#28982;&#26159;&#24403;&#20170;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#24320;&#31665;&#21363;&#29992;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#24402;&#32435;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20915;&#31574;&#26641;&#30340;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#19982;AND/OR&#25628;&#32034;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MAPTree&#30340;AND/OR&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#19990;&#30028;&#22330;&#26223;&#20013;&#23637;&#31034;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;16&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#35201;&#20040;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#35201;&#20040;&#22312;&#24615;&#33021;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#26641;&#12290;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;MAPTree&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#22320;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.15223</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#31532;&#20108;&#27425;&#37325;&#35780;&#20998;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23558;&#39044;&#35757;&#32451;&#38454;&#27573;&#25193;&#23637;&#21644;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#37325;&#35780;&#20998;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;0.08%&#65289;&#26469;&#35757;&#32451;&#37325;&#35780;&#20998;&#30340;BERT&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#36825;&#20123;&#25554;&#20837;&#30340;&#30697;&#38453;&#36890;&#36807;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#21028;&#21035;&#24615;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;Rescore-BERT&#65288;LoRB&#65289;&#20307;&#31995;&#32467;&#26500;&#22312;LibriSpeech&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;5.4&#33267;3.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12301</link><description>&lt;p&gt;
&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26816;&#27979;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#30340;&#26032;&#39062;&#24615;&#65292;&#21516;&#26102;&#23545;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22810;&#20010;&#29615;&#22659;&#30340;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#30830;&#23450;&#19982;&#29615;&#22659;&#26356;&#30456;&#20851;&#32780;&#19981;&#26159;&#20219;&#21153;&#30456;&#20851;&#20869;&#23481;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#22810;&#29615;&#22659;&#35774;&#32622;&#24320;&#22987;&#65292;&#25104;&#21151;&#26681;&#25454;&#20854;&#29615;&#22659;&#20851;&#27880;&#24230;&#23545;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#29615;&#22659;&#20043;&#38388;&#30340;&#29305;&#24449;&#20998;&#24067;&#26041;&#24046;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#24471;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#33293;&#24323;&#24471;&#20998;&#36739;&#39640;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#21435;&#38500;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#27491;&#24577;&#21327;&#26041;&#24046;&#21644;&#23376;&#31181;&#32676;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#30495;&#23454;&#30340;&#36824;&#26159;&#23545;&#20110;&#25105;&#20204;&#20026;&#27492;&#20219;&#21153;&#24341;&#20837;&#30340;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.10916</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#33719;&#24471;&#20160;&#20040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#26159;&#36890;&#36807;&#24494;&#23567;&#25200;&#21160;&#26469;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#65292;&#36215;&#21021;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#24320;&#22987;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36755;&#20837;&#25200;&#21160;&#30340;&#25628;&#32034;&#65292;&#20294;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#21457;&#23637;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#34920;&#24449;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#19968;&#31181;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#12290;&#29305;&#21035;&#26159;&#21069;&#32773;&#30456;&#27604;&#20960;&#20010;&#24378;&#22522;&#20934;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65307;&#27492;&#22806;&#65292;&#23545;&#24433;&#21709;&#20989;&#25968;&#30340;&#26032;&#39062;&#20351;&#29992;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#26681;&#25454;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2309.10400</link><description>&lt;p&gt;
PoSE: &#36890;&#36807;&#20301;&#32622;&#36339;&#36291;&#24335;&#35757;&#32451;&#25552;&#39640;LLMs&#23545;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26377;&#25928;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Positional Skip-wise (PoSE)&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;PoSE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#27169;&#25311;&#38271;&#36755;&#20837;&#65292;&#23558;&#35757;&#32451;&#38271;&#24230;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20998;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38271;&#36755;&#20837;&#24207;&#21015;&#20013;&#36873;&#25321;&#33509;&#24178;&#30701;&#22359;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#36339;&#36291;&#20559;&#32622;&#39033;&#26469;&#20462;&#25913;&#27599;&#20010;&#22359;&#30340;&#20301;&#32622;&#32034;&#24341;&#12290;&#36825;&#20123;&#36339;&#36291;&#20559;&#32622;&#39033;&#20197;&#21450;&#27599;&#20010;&#22359;&#30340;&#38271;&#24230;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#37117;&#20250;&#21464;&#21270;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#25152;&#26377;&#20301;&#32622;&#65292;&#32780;&#26080;&#38656;&#23545;&#23436;&#25972;&#38271;&#24230;&#30340;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#23545;&#23436;&#25972;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;PoSE&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#21033;&#29992;&#36825;&#19968;&#20248;&#21183;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;PoSE&#19982;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#26799;&#24230;&#36924;&#36817;&#23545;&#25239;&#35757;&#32451;(GAAT)&#65292;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#30340;&#37096;&#20998;&#21644;&#26469;&#36817;&#20284;&#23545;&#25239;&#25439;&#22833;&#65292;&#24182;&#36817;&#20284;&#26799;&#24230;&#65292;&#20197;&#38477;&#20302;&#24314;&#31435;&#40065;&#26834;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.09464</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#36924;&#36817;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Reducing Adversarial Training Cost with Gradient Approximation. (arXiv:2309.09464v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#26799;&#24230;&#36924;&#36817;&#23545;&#25239;&#35757;&#32451;(GAAT)&#65292;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#30340;&#37096;&#20998;&#21644;&#26469;&#36817;&#20284;&#23545;&#25239;&#25439;&#22833;&#65292;&#24182;&#36817;&#20284;&#26799;&#24230;&#65292;&#20197;&#38477;&#20302;&#24314;&#31435;&#40065;&#26834;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#32463;&#36807;&#24039;&#22937;&#20294;&#24494;&#23567;&#25200;&#21160;&#30340;&#36755;&#20837;&#38750;&#24120;&#33030;&#24369;&#65292;&#36825;&#34987;&#31216;&#20026;&#23545;&#25239;&#26679;&#26412;(Adversarial Examples, AEs)&#12290;&#22312;&#35768;&#22810;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#26679;&#26412;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#20013;&#65292;&#22522;&#20110;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;(Projected Gradient Descent, PGD)&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#20351;&#24471;&#29983;&#25104;&#36275;&#22815;&#24378;&#28872;&#30340;&#23545;&#25239;&#26679;&#26412;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23545;&#20110;&#20351;&#29992;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#30340;PGD&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26377;&#26102;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#23545;&#25239;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#30340;&#37096;&#20998;&#21644;&#26469;&#36817;&#20284;&#65292;&#24182;&#36817;&#20284;&#23545;&#25239;&#25439;&#22833;&#30340;&#26799;&#24230;&#65292;&#36827;&#32780;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#26799;&#24230;&#36924;&#36817;&#23545;&#25239;&#35757;&#32451;(GAAT)&#65292;&#20197;&#38477;&#20302;&#24314;&#31435;&#40065;&#26834;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have achieved state-of-the-art performances in various domains, while they are vulnerable to the inputs with well-crafted but small perturbations, which are named after adversarial examples (AEs). Among many strategies to improve the model robustness against AEs, Projected Gradient Descent (PGD) based adversarial training is one of the most effective methods. Unfortunately, the prohibitive computational overhead of generating strong enough AEs, due to the maximization of the loss function, sometimes makes the regular PGD adversarial training impractical when using larger and more complicated models. In this paper, we propose that the adversarial loss can be approximated by the partial sum of Taylor series. Furthermore, we approximate the gradient of adversarial loss and propose a new and efficient adversarial training method, adversarial training with gradient approximation (GAAT), to reduce the cost of building up robust models. Additionally, extensive experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#36845;&#20195;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;&#36317;&#31163;&#30340;Fr&#233;chet&#22343;&#20540;&#26500;&#24314;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#19968;&#33268;&#24615;&#20998;&#21106;&#12290;&#19982;STAPLE&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#21644;&#20808;&#39564;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.08066</link><description>&lt;p&gt;
&#22522;&#20110;&#21551;&#21457;&#24335;&#36845;&#20195;&#20248;&#21270;&#30340;&#24418;&#24577;&#23398;&#24863;&#30693;&#19968;&#33268;&#24615;&#35745;&#31639;&#65288;MACCHIatO&#65289;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Morphologically-Aware Consensus Computation via Heuristics-based IterATive Optimization (MACCHIatO). (arXiv:2309.08066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#36845;&#20195;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#36873;&#25321;&#30340;&#36317;&#31163;&#30340;Fr&#233;chet&#22343;&#20540;&#26500;&#24314;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#19968;&#33268;&#24615;&#20998;&#21106;&#12290;&#19982;STAPLE&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#21644;&#20808;&#39564;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#20010;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#25513;&#27169;&#20013;&#25552;&#21462;&#19968;&#33268;&#24615;&#20998;&#21106;&#26159;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#22914;&#35299;&#26512;&#26631;&#27880;&#32773;&#38388;&#30340;&#24046;&#24322;&#24615;&#25110;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#34701;&#21512;&#12290;&#26412;&#25991;&#39318;&#20808;&#35777;&#26126;&#20102;STAPLE&#31639;&#27861;&#30340;&#36755;&#20986;&#21463;&#21040;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#21644;&#20808;&#39564;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#36317;&#31163;&#30340;Fr&#233;chet&#22343;&#20540;&#26500;&#24314;&#20108;&#36827;&#21046;&#25110;&#27010;&#29575;&#19968;&#33268;&#24615;&#20998;&#21106;&#65292;&#20351;&#20854;&#23436;&#20840;&#29420;&#31435;&#20110;&#22270;&#20687;&#32972;&#26223;&#22823;&#23567;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20248;&#21270;&#27492;&#20934;&#21017;&#65292;&#20174;&#32780;&#20351;&#19968;&#20010;&#20307;&#32032;&#30340;&#31867;&#21035;&#23436;&#20840;&#30001;&#23427;&#19982;&#19981;&#21516;&#25513;&#27169;&#30340;&#20307;&#32032;&#32423;&#36317;&#31163;&#12289;&#23427;&#25152;&#23646;&#30340;&#36830;&#36890;&#32452;&#20214;&#21644;&#20998;&#21106;&#23427;&#30340;&#26631;&#27880;&#32773;&#32452;&#20915;&#23450;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#19982;STAPLE&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extraction of consensus segmentations from several binary or probabilistic masks is important to solve various tasks such as the analysis of inter-rater variability or the fusion of several neural network outputs. One of the most widely used methods to obtain such a consensus segmentation is the STAPLE algorithm. In this paper, we first demonstrate that the output of that algorithm is heavily impacted by the background size of images and the choice of the prior. We then propose a new method to construct a binary or a probabilistic consensus segmentation based on the Fr\'{e}chet means of carefully chosen distances which makes it totally independent of the image background size. We provide a heuristic approach to optimize this criterion such that a voxel's class is fully determined by its voxel-wise distance to the different masks, the connected component it belongs to and the group of raters who segmented it. We compared extensively our method on several datasets with the STAPLE met
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00976</link><description>&lt;p&gt;
&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#21487;&#20197;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pure Message Passing Can Estimate Common Neighbor for Link Prediction. (arXiv:2309.00976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#22312;&#38142;&#36335;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#34987;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22914;&#20849;&#21516;&#37051;&#23621;&#65288;CN&#65289;&#25152;&#36229;&#36234;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#19968;&#20010;&#26681;&#26412;&#38480;&#21046;&#65306;&#23613;&#31649;MPNN&#22312;&#33410;&#28857;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32534;&#30721;&#38142;&#36335;&#39044;&#27979;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65288;&#22914;CN&#65289;&#26041;&#38754;&#21017;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#65292;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#30830;&#23454;&#21487;&#20197;&#25429;&#25417;&#21040;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MPNN&#22312;&#36817;&#20284;CN&#21551;&#21457;&#24335;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#28040;&#24687;&#20256;&#36882;&#38142;&#36335;&#39044;&#27979;&#22120;&#65288;MPLP&#65289;&#12290;MPLP&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#25417;&#32467;&#26500;&#29305;&#24449;&#33021;&#22815;&#25913;&#21892;&#38142;&#36335;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture stru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15116</link><description>&lt;p&gt;
&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#29983;&#29289;&#20998;&#23376;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20998;&#23376;&#33021;&#22815;&#27874;&#21160;&#30340;&#21508;&#31181;&#26465;&#20214;&#19979;&#23545;&#19968;&#32452;&#31890;&#23376;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#38750;&#24120;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#22330;&#26223;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;&#28201;&#24230;&#20026;&#27979;&#35797;&#26696;&#20363;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#36830;&#32493;&#30340;&#21160;&#24577;&#26465;&#20214;&#65288;&#22914;&#21387;&#21147;&#21644;&#20307;&#31215;&#65289;&#36827;&#34892;&#26377;&#25928;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#25968;&#25454;&#28151;&#21512;&#25216;&#26415;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20998;&#23376;&#32467;&#26500;&#25968;&#25454;&#21644;&#28201;&#24230;&#25552;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#27604;&#20363;&#30340;&#26041;&#24335;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;2&#65289;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;&#25552;&#39640;&#20102;&#24494;&#35843;&#36807;&#31243;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20026;&#36719;&#25552;&#31034;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.13150</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#30340;&#36801;&#31227;ResNet&#22686;&#24378;&#20083;&#33146;&#30284;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#20998;&#31867;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;ResNet&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#65292;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#65292;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#12290;&#25105;&#20204;&#22312;Breakhis&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20256;&#32479;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#22312;&#24403;&#20195;&#35270;&#35273;&#21464;&#25442;&#22120;&#31561;&#26368;&#26032;&#26041;&#27861;&#19978;&#20063;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;&#22312;&#35832;&#22914;&#31934;&#24230;&#12289;&#20934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;G-means&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#25910;&#25947;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#20123;&#32467;&#26524;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24041;&#22266;&#20102;&#20854;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#65292;&#36890;&#36807;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#19979;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11053</link><description>&lt;p&gt;
&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression. (arXiv:2308.11053v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#65292;&#36890;&#36807;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#19979;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#23545;&#20110;&#20840;&#21452;&#24037;&#36890;&#20449;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#25972;&#19978;&#19981;&#28789;&#27963;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#39057;&#29575;&#21387;&#32553;&#26041;&#38754;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#28388;&#27874;&#22120;&#20195;&#26367;&#25163;&#21160;&#35774;&#35745;&#30340;&#28388;&#27874;&#22120;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#12290;&#22312;&#26102;&#38388;&#21387;&#32553;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#24103;&#36339;&#39044;&#27979;&#20250;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#20294;&#36890;&#36807;&#20855;&#26377;&#23436;&#25972;&#24207;&#21015;&#24314;&#27169;&#30340;&#21518;&#22788;&#29702;&#32593;&#32476;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#26102;&#38388;&#21644;&#39057;&#29575;&#26041;&#27861;&#36827;&#34892;&#21452;&#36335;&#24452;&#21387;&#32553;&#23558;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#25913;&#21464;&#27169;&#22411;&#22823;&#23567;&#65292;&#21387;&#32553;&#27604;&#35206;&#30422;&#33539;&#22260;&#20174;4x&#21040;32x&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#24555;&#36895;FullSubNet&#21644;DeepFilterNet&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#21487;&#20197;&#35775;&#38382;&#28436;&#31034;&#39029;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09858</link><description>&lt;p&gt;
&#24352;&#37327;&#21387;&#32553;&#30340;&#21453;&#21521;&#20256;&#25773;&#20813;&#36153;&#35757;&#32451;&#65288;&#29289;&#29702;&#20449;&#24687;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#21644;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#25913;&#36827;&#20102;&#20248;&#21270;&#21644;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#25193;&#23637;&#20102;&#26694;&#26550;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#35745;&#31639;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#30828;&#20214;&#21644;&#36719;&#20214;&#36164;&#28304;&#26469;&#25903;&#25345;&#33258;&#21160;&#24494;&#20998;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;BP&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#22823;&#22823;&#22686;&#21152;&#20102;&#35774;&#22791;&#19978;&#35757;&#32451;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#19978;&#24066;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;BP&#30340;&#26694;&#26550;&#65292;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#25773;&#23601;&#21487;&#20197;&#35757;&#32451;&#23454;&#38469;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#21387;&#32553;&#30340;&#26041;&#24046;&#32422;&#20943;&#26041;&#27861;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#22823;&#20110;&#20197;&#21069;ZO&#26041;&#27861;&#33021;&#21147;&#30340;&#32593;&#32476;&#23610;&#23544;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26799;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;ZO&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31232;&#30095;&#26684;&#26041;&#27861;&#26469;&#25193;&#23637;&#25105;&#20204;&#30340;BP-free&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06961</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#27531;&#24046;&#65306;&#19968;&#31181;&#35786;&#26029;&#30340;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Structural Residuals: A Learning Approach to Diagnosis. (arXiv:2308.06961v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35786;&#26029;&#20381;&#36182;&#20110;&#26500;&#24314;&#26126;&#30830;&#30340;&#31995;&#32479;&#27169;&#22411;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#36153;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#30340;&#21160;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#23454;&#29616;&#20102;&#22270;&#32467;&#26500;&#23398;&#20064;&#19982;&#27169;&#22411;&#35786;&#26029;&#30340;&#26080;&#32541;&#38598;&#25104;&#65306;(i)&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#12289;(ii)&#24341;&#20837;&#20004;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#12289;(iii)&#36890;&#36807;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
&lt;/p&gt;</description></item><item><title>Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.</title><link>http://arxiv.org/abs/2308.02599</link><description>&lt;p&gt;
&#20998;&#25903;&#28508;&#22312;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Branched Latent Neural Operators. (arXiv:2308.02599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02599
&lt;/p&gt;
&lt;p&gt;
Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#25903;&#28508;&#22312;&#31070;&#32463;&#31639;&#23376;&#65288;BLNOs&#65289;&#26469;&#23398;&#20064;&#32534;&#30721;&#22797;&#26434;&#29289;&#29702;&#36807;&#31243;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#12290;BLNO&#30001;&#19968;&#20010;&#31616;&#21333;&#32039;&#20945;&#30340;&#21069;&#39304;&#37096;&#20998;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#65292;&#35813;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#23558;&#19981;&#21516;&#22266;&#26377;&#35282;&#33394;&#30340;&#36755;&#20837;&#36827;&#34892;&#35299;&#31163;&#65292;&#20363;&#22914;&#23558;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#21464;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#24863;&#20852;&#36259;&#30340;&#36890;&#29992;&#39046;&#22495;&#12290;BLNO&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#36755;&#20986;&#22686;&#24378;&#20102;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#22312;&#21333;&#20010;&#22788;&#29702;&#22120;&#19978;&#20351;&#29992;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#35823;&#24046;&#22312;&#27979;&#35797;&#38454;&#27573;&#37319;&#29992;&#30340;&#31163;&#25955;&#21270;&#26041;&#24335;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#21487;&#27604;&#24615;&#12290;&#27492;&#22806;&#65292;&#37096;&#20998;&#36830;&#25509;&#22312;&#20840;&#36830;&#25509;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#26174;&#33879;&#20943;&#23569;&#20102;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BLNO&#22312;&#28041;&#21450;&#29983;&#29289;&#29289;&#29702;&#32454;&#33410;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed elect
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#32479;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#21457;&#29616;&#20102;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#21644;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01853</link><description>&lt;p&gt;
&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;: Wasserstein&#25200;&#21160;&#19982;&#26497;&#23567;&#26497;&#22823;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#32479;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#21457;&#29616;&#20102;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#21644;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#23558;&#25968;&#25454;&#30340;&#29305;&#24615;&#20174;&#30495;&#23454;&#24773;&#20917;&#20013;&#31995;&#32479;&#22320;&#25913;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#21487;&#33021;&#20250;&#21457;&#29983;&#36731;&#24494;&#25200;&#21160;&#65292;&#32780;&#19981;&#26159;Huber&#27745;&#26579;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#35266;&#27979;&#20540;&#26159;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36229;&#20986;&#29420;&#31435;&#25200;&#21160;&#30340;&#20559;&#31227;&#65292;&#25506;&#32034;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#28857;&#30340;&#25200;&#21160;&#21487;&#20197;&#21327;&#35843;&#36827;&#34892;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#20010;&#37325;&#35201;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#20301;&#32622;&#20272;&#35745;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#22343;&#20540;&#20272;&#35745;&#21644;&#32447;&#24615;&#22238;&#24402;&#30340;&#39044;&#27979;&#35823;&#24046;&#26041;&#24046;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#31934;&#30830;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#12289;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#20998;&#21035;&#26159;&#26368;&#20248;&#30340;&#12290;&#36825;&#36866;&#29992;&#20110;&#29420;&#31435;&#21644;&#32852;&#21512;&#20559;&#31227;&#65292;&#20294;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#21644;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14839</link><description>&lt;p&gt;
&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#20197;&#20854;&#21487;&#36870;&#30340;&#26550;&#26500;&#32780;&#34987;&#25551;&#36848;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21487;&#36870;&#24615;&#35201;&#27714;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#26045;&#21152;&#38480;&#21046;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#26469;&#36798;&#21040;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36716;&#25442;&#26469;&#23454;&#29616;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26367;&#20195;&#30340;&#36716;&#25442;&#26041;&#27861;&#21364;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#20135;&#29983;&#26377;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;&#26680;&#21270;&#27969;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14439</link><description>&lt;p&gt;
&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23398;&#20064;&#20989;&#25968;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31215;&#20998;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#36825;&#31181;&#31215;&#20998;&#36890;&#24120;&#26159;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#26469;&#35745;&#31639;&#30340;&#65292;&#22240;&#20026;&#35299;&#26512;&#35745;&#31639;&#31215;&#20998;&#36807;&#31243;&#22797;&#26434;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#23398;&#20064;&#20989;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968; $f$ &#35299;&#26512;&#31215;&#20998;&#30340;&#26041;&#27861;&#12290;&#36825;&#20801;&#35768;&#31934;&#30830;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#30340;&#31215;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#26469;&#23545;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558; $f$ &#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35768;&#22810;&#24212;&#29992;&#65288;&#20363;&#22914;&#27010;&#29575;&#20998;&#24067;&#12289;&#36317;&#31163;&#24230;&#37327;&#31561;&#65289;&#25152;&#24517;&#38656;&#30340;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20010;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#30340;&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;&#65288;FINN&#65289;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05330</link><description>&lt;p&gt;
&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#20215;&#20540;&#12290; (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36793;&#38469;&#20272;&#20540;&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#19978;&#30340;&#26827;&#23376;&#21644;&#26827;&#30424;&#36827;&#34892;&#35780;&#20215;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#39532;&#12289;&#35937;&#21644;&#20853;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#26827;&#30424;&#19978;&#26827;&#23376;&#30340;&#20215;&#20540;&#65292;&#24182;&#30830;&#23450;&#26827;&#23376;&#22312;&#26827;&#30424;&#19978;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#38543;&#30528;&#22269;&#38469;&#35937;&#26827;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#30340;&#20215;&#20540;&#12290;&#20256;&#32479;&#26041;&#27861;&#23545;&#26827;&#23376;&#36171;&#20104;&#22266;&#23450;&#30340;&#20215;&#20540;$(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26827;&#23376;&#21644;&#26827;&#30424;&#26041;&#38754;&#30340;&#36793;&#38469;&#20272;&#20540;&#26469;&#25913;&#36827;&#36825;&#31181;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#39532;&#21644;&#35937;&#30340;&#20301;&#32622;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#20853;&#30340;&#20215;&#20540;&#30340;&#23453;&#36149;&#35265;&#35299;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23612;&#22982;&#20304;&#32500;&#22855;&#26159;&#20513;&#23548;&#20853;&#30340;&#32467;&#26500;&#21644;&#20215;&#20540;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#38468;&#21152;&#25968;&#25454;&#28304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23646;&#20110;&#20808;&#21069;&#36935;&#21040;&#30340;&#22270;&#20687;&#25152;&#23646;&#31867;&#21035;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#22312;&#33976;&#39311;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#20013;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17560</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33976;&#39311;&#21644;&#37325;&#25773;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning using Diffusion Model for Distillation and Replay. (arXiv:2306.17560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#38468;&#21152;&#25968;&#25454;&#28304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23646;&#20110;&#20808;&#21069;&#36935;&#21040;&#30340;&#22270;&#20687;&#25152;&#23646;&#31867;&#21035;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#22312;&#33976;&#39311;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#20013;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#20197;&#22686;&#37327;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#22810;&#20010;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#37327;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#38468;&#21152;&#25968;&#25454;&#26469;&#24110;&#21161;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#38468;&#21152;&#25968;&#25454;&#28304;&#12290;&#19982;&#20381;&#36182;&#20110;&#22806;&#37096;&#12289;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23646;&#20110;&#20808;&#21069;&#36935;&#21040;&#30340;&#22270;&#20687;&#25152;&#23646;&#31867;&#21035;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#36825;&#20351;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#22312;&#33976;&#39311;&#25439;&#22833;&#20013;&#20351;&#29992;&#36825;&#20123;&#38468;&#21152;&#25968;&#25454;&#26679;&#26412;&#65292;&#36824;&#21487;&#20197;&#22312;&#20998;&#31867;&#25439;&#22833;&#20013;&#36827;&#34892;&#37325;&#25773;&#12290;&#22312;CIFAR100&#12289;ImageNet-Subset&#21644;ImageNet&#31561;&#31454;&#20105;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning aims to learn new classes in an incremental fashion without forgetting the previously learned ones. Several research works have shown how additional data can be used by incremental models to help mitigate catastrophic forgetting. In this work, following the recent breakthrough in text-to-image generative models and their wide distribution, we propose the use of a pretrained Stable Diffusion model as a source of additional data for class-incremental learning. Compared to competitive methods that rely on external, often unlabeled, datasets of real images, our approach can generate synthetic samples belonging to the same classes as the previously encountered images. This allows us to use those additional data samples not only in the distillation loss but also for replay in the classification loss. Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and ImageNet demonstrate how this new approach can be used to further improve the performance of s
&lt;/p&gt;</description></item><item><title>ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16922</link><description>&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#65306;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16922
&lt;/p&gt;
&lt;p&gt;
ELM&#31070;&#32463;&#20803;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#30382;&#23618;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#23427;&#21482;&#38656;&#35201;8K&#20010;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#27169;&#25311;&#22797;&#26434;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#31185;&#23398;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#31616;&#21270;&#30340;&#20010;&#20307;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20381;&#38752;&#38598;&#20307;&#27963;&#21160;&#21644;&#36866;&#24403;&#35843;&#25972;&#30340;&#36830;&#25509;&#26469;&#25191;&#34892;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29983;&#29289;&#30382;&#23618;&#31070;&#32463;&#20803;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#35774;&#22791;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#30740;&#31350;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#30340;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#22797;&#21046;&#35814;&#32454;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#22810;&#20010;&#21442;&#25968;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#24341;&#20837;&#20102;&#34920;&#36798;&#21147;&#24378;&#30340;&#27844;&#28431;&#23384;&#20648;&#22120;&#65288;ELM&#65289;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#35745;&#31639;&#34920;&#36798;&#21147;&#65292;&#21516;&#26102;&#20063;&#38750;&#24120;&#39640;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ELM&#31070;&#32463;&#20803;&#20165;&#38656;&#35201;8,000&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#23601;&#33021;&#20934;&#30830;&#21305;&#37197;&#21069;&#36848;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20934;&#30830;&#30340;&#27169;&#22411;&#38656;&#35201;&#22810;&#20010;&#31867;&#20284;&#20110;&#23384;&#20648;&#22120;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#31361;&#35302;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16788</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#27748;&#65306;&#36890;&#36807;&#27169;&#22411;&#24179;&#22343;&#25913;&#36827;&#20462;&#21098;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21098;&#26525;&#26174;&#33879;&#21387;&#32553;&#65292;&#20174;&#32780;&#24471;&#21040;&#31232;&#30095;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#28014;&#28857;&#36816;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#27748;&#65288;Wortsman&#31561;&#20154;&#65292;2022&#24180;&#65289;&#36890;&#36807;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#25913;&#21892;&#27867;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#22788;&#20110;&#30456;&#21516;&#25439;&#22833;&#21306;&#22495;&#30340;&#27169;&#22411;&#20197;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23545;&#20219;&#24847;&#31232;&#30095;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#20250;&#38477;&#20302;&#25972;&#20307;&#31232;&#30095;&#24230;&#65292;&#21407;&#22240;&#26159;&#19981;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#21333;&#27425;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#20013;&#25506;&#32034;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#65288;&#20363;&#22914;&#25209;&#27425;&#25490;&#24207;&#25110;&#26435;&#37325;&#34928;&#20943;&#65289;&#20135;&#29983;&#30340;&#27169;&#22411;&#36866;&#21512;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#19988;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#24179;&#22343;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#32858;&#21512;&#31639;&#23376;&#65292;GenAgg&#65292;&#23427;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#32858;&#21512;&#22120;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GenAgg&#33021;&#22815;&#34920;&#31034;&#26631;&#20934;&#32858;&#21512;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.13826</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;$f$-&#22343;&#20540;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Generalised $f$-Mean Aggregation for Graph Neural Networks. (arXiv:2306.13826v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#32858;&#21512;&#31639;&#23376;&#65292;GenAgg&#65292;&#23427;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#32858;&#21512;&#22120;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GenAgg&#33021;&#22815;&#34920;&#31034;&#26631;&#20934;&#32858;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26550;&#26500;&#30001;&#20854;&#26356;&#26032;&#21644;&#32858;&#21512;&#27169;&#22359;&#30340;&#23454;&#29616;&#26041;&#24335;&#23450;&#20041;&#12290;&#35768;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26032;&#22411;&#21442;&#25968;&#21270;&#26356;&#26032;&#27169;&#22359;&#30340;&#26041;&#27861;&#19978;&#65292;&#32780;&#32858;&#21512;&#27169;&#22359;&#30456;&#23545;&#36739;&#23569;&#21463;&#21040;&#20851;&#27880;&#12290;&#30001;&#20110;&#32858;&#21512;&#20989;&#25968;&#24456;&#38590;&#21442;&#25968;&#21270;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#36873;&#25321;&#8220;&#26631;&#20934;&#32858;&#21512;&#22120;&#8221;&#65292;&#22914;$\mathrm{mean}$&#12289;$\mathrm{sum}$&#25110;$\mathrm{max}$&#12290;&#23613;&#31649;&#36825;&#31181;&#36873;&#25321;&#36890;&#24120;&#27809;&#26377;&#20219;&#20309;&#29702;&#30001;&#65292;&#20294;&#24050;&#32463;&#34920;&#26126;&#32858;&#21512;&#22120;&#30340;&#36873;&#25321;&#23545;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#26368;&#20339;&#32858;&#21512;&#22120;&#30340;&#36873;&#25321;&#21462;&#20915;&#20110;&#38382;&#39064;&#12290;&#30001;&#20110;&#32858;&#21512;&#26159;&#19968;&#31181;&#26377;&#25439;&#25805;&#20316;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#20449;&#24687;&#20002;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GenAgg&#65292;&#19968;&#31181;&#24191;&#20041;&#32858;&#21512;&#36816;&#31639;&#31526;&#65292;&#23427;&#21442;&#25968;&#21270;&#20102;&#19968;&#20010;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#32858;&#21512;&#22120;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GenAgg&#33021;&#22815;&#34920;&#31034;&#26631;&#20934;&#32858;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a "standard aggregator" such as $\mathrm{mean}$, $\mathrm{sum}$, or $\mathrm{max}$. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#36793;&#30028;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20174;&#26799;&#24230;&#20998;&#26512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#36793;&#30028;&#22914;&#20309;&#24433;&#21709;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11526</link><description>&lt;p&gt;
&#36890;&#36807;&#36793;&#30028;&#30340;&#35282;&#24230;&#29702;&#35299;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Contrastive Learning Through the Lens of Margins. (arXiv:2306.11526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11526
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#36793;&#30028;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20174;&#26799;&#24230;&#20998;&#26512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#36793;&#30028;&#22914;&#20309;&#24433;&#21709;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#21450;&#20854;&#21464;&#20307;&#26159;&#19968;&#31181;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23545;&#27604;&#23398;&#20064;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#24230;&#37327;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#20132;&#21449;&#29109;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36793;&#30028;&#22312;&#22686;&#24378;&#20154;&#33080;&#21644;&#35828;&#35805;&#20154;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23613;&#31649;&#23545;&#27604;&#23398;&#20064;&#21644;&#36793;&#30028;&#20381;&#36182;&#20110;&#30456;&#21516;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#30446;&#26631;&#20989;&#25968;&#65292;&#20294;&#23545;&#27604;&#23398;&#20064;&#24182;&#27809;&#26377;&#31215;&#26497;&#37319;&#29992;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#22522;&#20110;&#20915;&#31574;&#36793;&#30028;&#30340;&#35299;&#37322;&#25165;&#34987;&#29992;&#26469;&#35299;&#37322;&#36793;&#30028;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#36793;&#30028;&#22312;&#26799;&#24230;&#20998;&#26512;&#19978;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#20010;&#26032;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36793;&#30028;&#22914;&#20309;&#24433;&#21709;&#23545;&#27604;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#24182;&#23558;&#20854;&#25928;&#26524;&#20998;&#35299;&#25104;&#26356;&#22522;&#26412;&#30340;&#23618;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, along with its variations, has been a highly effective self-supervised learning method across diverse domains. Contrastive learning measures the distance between representations using cosine similarity and uses cross-entropy for representation learning. Within the same framework of cosine-similarity-based representation learning, margins have played a significant role in enhancing face and speaker recognition tasks. Interestingly, despite the shared reliance on the same similarity metrics and objective functions, contrastive learning has not actively adopted margins. Furthermore, decision-boundary-based explanations are the only ones that have been used to explain the effect of margins in contrastive learning. In this work, we propose a new perspective to understand the role of margins based on gradient analysis. Based on the new perspective, we analyze how margins affect gradients of contrastive learning and separate the effect into more elemental levels. We sepa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#65292;&#23454;&#29616;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;&#24314;&#27169;&#65292;&#20165;&#38656;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#21363;&#21487;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11375</link><description>&lt;p&gt;
&#33258;&#19978;&#32780;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Top-down machine learning of coarse-grained protein force-fields. (arXiv:2306.11375v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11375
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#65292;&#23454;&#29616;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;&#24314;&#27169;&#65292;&#20165;&#38656;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#21363;&#21487;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#34507;&#30333;&#36136;&#31895;&#31890;&#21270;&#34920;&#24449;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#30340;&#25240;&#21472;&#12289;&#21151;&#33021;&#21644;&#22312;&#38271;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#34507;&#30333;&#36136;&#65292;&#24182;&#21033;&#29992;&#24471;&#21040;&#30340;&#36712;&#36857;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#65292;&#28040;&#38500;&#20102;&#20174;&#24191;&#27867;&#27169;&#25311;&#25110;&#20869;&#23384;&#23494;&#38598;&#22411;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#27169;&#25311;&#23548;&#20986;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#24182;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#24182;&#23545;&#35757;&#32451;&#20998;&#24067;&#20869;&#22806;&#30340;&#34507;&#30333;&#36136;&#36827;&#34892;&#25240;&#21472;&#20107;&#20214;&#37319;&#26679;&#65292;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#27169;&#25311;&#34507;&#30333;&#36136;&#30340;&#19982;&#22825;&#28982;&#26500;&#35937;&#30456;&#20284;&#30340;&#26500;&#35937;&#12290;&#30001;&#20110;&#20854;&#29702;&#35770;&#21487;&#36716;&#31227;&#24615;&#21644;&#20165;&#20351;&#29992;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing accurate and efficient coarse-grained representations of proteins is crucial for understanding their folding, function, and interactions over extended timescales. Our methodology involves simulating proteins with molecular dynamics and utilizing the resulting trajectories to train a neural network potential through differentiable trajectory reweighting. Remarkably, this method requires only the native conformation of proteins, eliminating the need for labeled data derived from extensive simulations or memory-intensive end-to-end differentiable simulations. Once trained, the model can be employed to run parallel molecular dynamics simulations and sample folding events for proteins both within and beyond the training distribution, showcasing its extrapolation capabilities. By applying Markov State Models, native-like conformations of the simulated proteins can be predicted from the coarse-grained simulations. Owing to its theoretical transferability and ability to use solely e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#30913;&#20849;&#25391;&#27874;&#35889;&#25216;&#26415;&#20013;&#20195;&#35874;&#29289;&#20449;&#21495;&#23450;&#37327;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09681</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#35823;&#24046;&#22240;&#32032;&#19982;&#24635;&#20307;&#39640;&#20998;&#23376;&#20449;&#21495;&#20272;&#35745;&#30340;&#30913;&#20849;&#25391;&#27874;&#35889;&#23450;&#37327;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations of Imperfection Factors and Overall Macromolecular Signal. (arXiv:2306.09681v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#30913;&#20849;&#25391;&#27874;&#35889;&#25216;&#26415;&#20013;&#20195;&#35874;&#29289;&#20449;&#21495;&#23450;&#37327;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#27874;&#35889;&#65288;MRS&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30340;&#29983;&#29289;&#21307;&#23398;&#26816;&#27979;&#25216;&#26415;&#65292;&#30001;&#20110;&#20195;&#35874;&#29289;&#20449;&#21495;&#30340;&#20005;&#37325;&#37325;&#21472;&#12289;&#38750;&#29702;&#24819;&#37319;&#38598;&#26465;&#20214;&#24341;&#36215;&#30340;&#20449;&#21495;&#22833;&#30495;&#20197;&#21450;&#19982;&#24378;&#32972;&#26223;&#20449;&#21495;&#21253;&#25324;&#39640;&#20998;&#23376;&#20449;&#21495;&#30340;&#24178;&#25200;&#65292;&#20351;&#29992;&#36136;&#23376;MRS&#31934;&#30830;&#37327;&#21270;&#20195;&#35874;&#29289;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#24341;&#20837;&#21040;MRS&#37327;&#21270;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#12289;&#38750;&#29702;&#24819;&#37319;&#38598;&#26465;&#20214;&#30340;&#35823;&#24046;&#22240;&#32032;&#20197;&#21450;&#35774;&#35745;&#20960;&#31181;&#32463;&#39564;&#20808;&#39564;&#22914;&#35832;&#22914;&#20195;&#35874;&#29289;&#21644;&#39640;&#20998;&#23376;&#30340;&#22522;&#38598;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic Resonance Spectroscopy (MRS) is an important non-invasive technique for in vivo biomedical detection. However, it is still challenging to accurately quantify metabolites with proton MRS due to three problems: Serious overlaps of metabolite signals, signal distortions due to non-ideal acquisition conditions and interference with strong background signals including macromolecule signals. The most popular software, LCModel, adopts the non-linear least square to quantify metabolites and addresses these problems by introducing regularization terms, imperfection factors of non-ideal acquisition conditions, and designing several empirical priors such as basissets of both metabolites and macromolecules. However, solving such a large non-linear quantitative problem is complicated. Moreover, when the signal-to-noise ratio of an input MRS signal is low, the solution may have a large deviation. In this work, deep learning is introduced to reduce the complexity of solving this overall quan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#32437;&#21521;&#33016;&#37096;X&#20809;&#21644;&#25253;&#21578;&#26469;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#25253;&#21578;&#38169;&#35823;&#12290;&#36890;&#36807;&#21033;&#29992;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#21253;&#21547;&#22810;&#27169;&#24577;&#32437;&#21521;&#23601;&#35786;&#35760;&#24405;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.08749</link><description>&lt;p&gt;
&#21033;&#29992;&#32437;&#21521;&#33016;&#37096;X&#20809;&#21644;&#25253;&#21578;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports. (arXiv:2306.08749v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#32437;&#21521;&#33016;&#37096;X&#20809;&#21644;&#25253;&#21578;&#26469;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#25253;&#21578;&#38169;&#35823;&#12290;&#36890;&#36807;&#21033;&#29992;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#21253;&#21547;&#22810;&#27169;&#24577;&#32437;&#21521;&#23601;&#35786;&#35760;&#24405;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#36719;&#20214;&#21487;&#20197;&#32553;&#30701;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#23436;&#25104;&#26102;&#38388;&#65292;&#20294;&#25345;&#32493;&#30340;&#27807;&#36890;&#38169;&#35823;&#20250;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#35299;&#37322;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#39044;&#22635;&#20805;&#25918;&#23556;&#23398;&#25253;&#21578;&#22312;&#20943;&#36731;&#25253;&#21578;&#38169;&#35823;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#23613;&#31649;&#25991;&#29486;&#20013;&#23384;&#22312;&#29983;&#25104;&#21307;&#23398;&#25253;&#21578;&#30340;&#21162;&#21147;&#65292;&#20294;&#23545;&#20110;&#21033;&#29992;MIMIC-CXR&#25968;&#25454;&#38598;&#20013;&#30340;&#24739;&#32773;&#23601;&#35786;&#35760;&#24405;&#30340;&#32437;&#21521;&#29305;&#24615;&#32570;&#20047;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21363;&#20197;&#21069;&#30340;&#24739;&#32773;&#23601;&#35786;CXR&#12289;&#24403;&#21069;&#23601;&#35786;CXR&#21644;&#20197;&#21069;&#30340;&#23601;&#35786;&#25253;&#21578;&#65292;&#26469;&#39044;&#20808;&#22635;&#20805;&#24403;&#21069;&#24739;&#32773;&#23601;&#35786;&#25253;&#21578;&#30340;&#8220;&#21457;&#29616;&#8221;&#37096;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;MIMIC-CXR&#25968;&#25454;&#38598;&#20013;&#25910;&#38598;&#20102;26,625&#21517;&#24739;&#32773;&#30340;&#32437;&#21521;&#23601;&#35786;&#20449;&#24687;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Longitudinal-MIMIC&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#21253;&#21547;&#22810;&#27169;&#24577;&#32437;&#21521;&#23601;&#35786;&#35760;&#24405;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the reduction in turn-around times in radiology reports with the use of speech recognition software, persistent communication errors can significantly impact the interpretation of the radiology report. Pre-filling a radiology report holds promise in mitigating reporting errors, and despite efforts in the literature to generate medical reports, there exists a lack of approaches that exploit the longitudinal nature of patient visit records in the MIMIC-CXR dataset. To address this gap, we propose to use longitudinal multi-modal data, i.e., previous patient visit CXR, current visit CXR, and previous visit report, to pre-fill the 'findings' section of a current patient visit report. We first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset and created a new dataset called Longitudinal-MIMIC. With this new dataset, a transformer-based model was trained to capture the information from longitudinal patient visit records containing multi-modal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.07220</link><description>&lt;p&gt;
Strokes2Surface&#65306;&#20174;&#22235;&#32500;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31163;&#32447;&#20960;&#20309;&#37325;&#24314;&#31649;&#36947;Strokes2Surface&#65292;&#23427;&#26159;&#22522;&#20110;4D Sketching Interface&#65292;MR.Sketch&#30340;&#30446;&#26631;&#26159;&#38754;&#21521;&#24314;&#31569;&#35774;&#35745;&#30340;&#12290;&#35813;&#31649;&#36947;&#20174;&#35774;&#35745;&#24072;&#32472;&#21046;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;&#65292;&#22240;&#27492;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#38454;&#27573;&#20043;&#38388;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30340;&#36755;&#20837;&#21253;&#25324;3D&#31508;&#30011;&#30340;&#25240;&#32447;&#39030;&#28857;&#21450;&#20854;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#65288;&#20316;&#20026;&#31532;&#22235;&#20010;&#32500;&#24230;&#65289;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#20960;&#20309;&#21644;&#31508;&#35302;&#30456;&#20851;&#30340;&#35760;&#24405;&#23646;&#24615;&#12290;&#22522;&#20110;&#32032;&#25551;&#21512;&#24182;&#21644;&#22522;&#20110;&#32032;&#25551;&#24314;&#27169;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31649;&#36947;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24182;&#32452;&#21512;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65307;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#20004;&#20010;&#32858;&#31867;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#26681;&#25454;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#35774;&#35745;&#24072;&#36890;&#24120;&#37319;&#29992;&#30340;&#23454;&#36341;&#35266;&#23519;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#19968;&#31508;&#30011;&#26159;&#25551;&#32472;&#36793;&#30028;&#21644;&#36793;&#32536;&#36824;&#26159;&#29992;&#20110;&#22635;&#20805;&#25152;&#38656;&#24314;&#31569;&#29289;&#30340;&#23553;&#38381;&#21306;&#22495;&#21644;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;</title><link>http://arxiv.org/abs/2306.04642</link><description>&lt;p&gt;
DiffusionShield&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#29256;&#26435;&#20445;&#25252;&#25968;&#23383;&#27700;&#21360;&#26041;&#27861;DiffusionShield&#65292;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#20405;&#26435;&#65292;&#31616;&#21333;&#26131;&#23398;&#65292;&#38590;&#20197;&#26816;&#27979;&#65292;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;(GDMs)&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#30340;GDMs&#31038;&#21306;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#24182;&#20419;&#36827;&#20102;GDMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26080;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#36215;&#20102;&#26377;&#20851;&#29256;&#26435;&#20445;&#25252;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#33402;&#26415;&#23478;(&#21253;&#25324;&#30011;&#23478;&#21644;&#25668;&#24433;&#24072;)&#36234;&#26469;&#36234;&#25285;&#24515;GDMs&#21487;&#20197;&#27627;&#19981;&#36153;&#21147;&#22320;&#22797;&#21046;&#20182;&#20204;&#29420;&#29305;&#30340;&#21019;&#24847;&#20316;&#21697;&#32780;&#26080;&#38656;&#25480;&#26435;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;GDMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#27700;&#21360;&#26041;&#26696;&#8212;&#8212;DiffusionShield&#12290;&#36890;&#36807;&#23558;&#25152;&#26377;&#26435;&#20449;&#24687;&#32534;&#30721;&#25104;&#19968;&#20010;&#19981;&#21487;&#23519;&#35273;&#30340;&#27700;&#21360;&#24182;&#23558;&#20854;&#27880;&#20837;&#22270;&#20687;&#20013;&#65292;DiffusionShield&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;GDMs&#20405;&#26435;&#12290;&#23427;&#30340;&#27700;&#21360;&#21487;&#20197;&#34987;GDMs&#36731;&#26494;&#22320;&#23398;&#20064;&#24182;&#37325;&#29616;&#22312;&#23427;&#20204;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#12290;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#27700;&#21360;&#65292;&#21487;&#20197;&#25581;&#38706;&#29256;&#26435;&#20405;&#26435;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03580</link><description>&lt;p&gt;
L-C2ST: &#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; L-C2ST &#30340;&#22522;&#20110;&#26412;&#22320;&#35786;&#26029;&#23454;&#29616;&#27169;&#25311;&#25512;&#26029;&#20013;&#21518;&#39564;&#36817;&#20284;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30446;&#21069;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#38480;&#21046;&#35299;&#20915;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#27169;&#25311;&#25512;&#26029;&#65288;SBI&#65289;&#30340;&#24037;&#20316;&#37117;&#20381;&#36182;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#36817;&#20284;&#22797;&#26434;&#12289;&#39640;&#32500;&#24230;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#36817;&#20284;&#26159;&#21542;&#21487;&#20449;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#22312;&#35266;&#27979;&#31354;&#38388;&#26399;&#26395;&#19979;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19981;&#33021;&#36275;&#22815;&#22320;&#30830;&#23450;&#21738;&#20123;&#35266;&#27979;&#32467;&#26524;&#21487;&#20197;&#20449;&#20219;&#36825;&#20123;&#36817;&#20284;&#25110;&#24212;&#35813;&#25913;&#36827;&#12290;&#25105;&#20204;&#22522;&#20110;&#33879;&#21517;&#30340;&#20998;&#31867;&#22120;&#20004;&#26679;&#26412;&#26816;&#39564; (C2ST)&#65292;&#24341;&#20837; L-C2ST&#65292;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#35266;&#27979;&#19979;&#26412;&#22320;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#22120;&#12290;&#23427;&#25552;&#20379;&#26377;&#29702;&#35770;&#22522;&#30784;&#21644;&#26131;&#20110;&#35299;&#37322;&#30340;&#65292;&#22914;&#22270;&#31034;&#35786;&#26029;&#12290;&#19982; C2ST &#19981;&#21516;&#30340;&#26159;&#65292;L-C2ST &#19981;&#38656;&#35201;&#35775;&#38382;&#30495;&#23454;&#21518;&#39564;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#21518;&#39564;&#20272;&#35745;&#22120;&#65292;L-C2ST &#21487;&#20197;&#19987;&#38376;&#25552;&#20379;&#26356;&#22909;&#30340;&#32479;&#35745;&#21151;&#29575;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22270;&#24418;&#30340;&#26041;&#27861;&#32467;&#21512;&#29305;&#23450;&#20998;&#24067;&#36317;&#31163;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#32467;&#26500;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29305;&#23450;&#27979;&#37327;&#26041;&#27861;&#65292;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20869;&#22312;&#26426;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.00042</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#19982;&#29305;&#23450;&#20998;&#24067;&#36317;&#31163;&#30456;&#32467;&#21512;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based methods coupled with specific distributional distances for adversarial attack detection. (arXiv:2306.00042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22270;&#24418;&#30340;&#26041;&#27861;&#32467;&#21512;&#29305;&#23450;&#20998;&#24067;&#36317;&#31163;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#32467;&#26500;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29305;&#23450;&#27979;&#37327;&#26041;&#27861;&#65292;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20869;&#22312;&#26426;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#34987;&#31934;&#24515;&#25200;&#21160;&#30340;&#36755;&#20837;&#25152;&#27450;&#39575;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#35823;&#20998;&#31867;&#12290;&#36825;&#20123;&#8220;&#23545;&#25239;&#24615;&#8221;&#25915;&#20987;&#24050;&#25104;&#20026;&#24191;&#27867;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#21516;&#26679;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#21644;&#38450;&#24481;&#20063;&#26377;&#22823;&#37327;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#22270;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#20110;&#22270;&#20687;&#65292;&#26080;&#35770;&#26159;&#33391;&#24615;&#30340;&#36824;&#26159;&#23545;&#25239;&#24615;&#30340;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#22914;&#20309;&#24341;&#20837;&#19968;&#20010;&#30456;&#20851;&#30340;&#22270;&#24418;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#22270;&#24418;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29305;&#23450;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#30740;&#31350;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20869;&#22312;&#26426;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These \textit{adversarial} attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an image, benign or adversarial, we study how a neural network's architecture can induce an associated graph. We study this graph and introduce specific measures used to predict and interpret adversarial attacks. We show that graphs-based approaches help to investigate the inner workings of adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;In-Context Learning&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#26469;&#38544;&#24335;&#22320;&#23454;&#29616;ICL&#20272;&#35745;&#37327;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;ICL&#24615;&#33021;&#65292;&#24314;&#31435;&#21518;&#24724;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#36817;&#20284;&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19420</link><description>&lt;p&gt;
In-Context Learning&#23398;&#20064;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#23398;&#20064;&#65311;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#12289;&#21442;&#25968;&#21270;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization. (arXiv:2305.19420v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;In-Context Learning&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#26469;&#38544;&#24335;&#22320;&#23454;&#29616;ICL&#20272;&#35745;&#37327;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;ICL&#24615;&#33021;&#65292;&#24314;&#31435;&#21518;&#24724;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#36817;&#20284;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22238;&#31572;&#20960;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#23545;In-Context Learning&#65288;ICL&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65306;(a)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#26159;&#20160;&#20040;&#31867;&#22411;&#30340;ICL&#20272;&#35745;&#37327;&#65311;(b)&#36866;&#21512;&#35780;&#20272;ICL&#30340;&#24615;&#33021;&#24230;&#37327;&#26159;&#20160;&#20040;&#65292;&#24182;&#19988;&#38169;&#35823;&#29575;&#26159;&#22810;&#23569;&#65311;(c)Transformer&#26550;&#26500;&#22914;&#20309;&#23454;&#29616;ICL&#65311;&#20026;&#20102;&#22238;&#31572;(a)&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#36125;&#21494;&#26031;&#35266;&#28857;&#65292;&#24182;&#35777;&#26126;ICL&#38544;&#21547;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#36817;&#20284;&#21442;&#25968;&#21270;&#12290;(b)&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#20998;&#26512;ICL&#24615;&#33021;&#65292;&#24314;&#31435;&#19968;&#20010;&#21518;&#24724;&#30028;&#38480; $\mathcal{O}(1/T)$&#65292;&#20854;&#20013;$T$&#26159;ICL&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#12290;(c)&#38500;&#20102;&#22312;&#20851;&#27880;&#20013;&#32534;&#30721;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#28041;&#21450;&#26399;&#38388;&#65292;&#23398;&#20064;&#27169;&#22411;&#21644;&#21517;&#20041;&#27169;&#22411;&#20043;&#38388;&#30340;&#24635;&#21464;&#20998;&#36317;&#31163;&#34987;&#19968;&#20010;&#36817;&#20284;&#35823;&#24046;&#21644;&#19968;&#20010;&#27867;&#21270;&#35823;&#24046;&#20043;&#21644;&#25152;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned within language models? (b) What are suitable performance metrics to evaluate ICL accurately and what are the error rates? (c) How does the transformer architecture enable ICL? To answer (a), we take a Bayesian view and demonstrate that ICL implicitly implements the Bayesian model averaging algorithm. This Bayesian model averaging algorithm is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a regret bound $\mathcal{O}(1/T)$, where $T$ is the ICL input sequence length. To address (c), in addition to the encoded Bayesian model averaging algorithm in attention, we show that during pertaining, the total variation distance between the learned model and the nominal model is bounded by a sum of an approximation error and a genera
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19011</link><description>&lt;p&gt;
MiniSUPERB:&#36731;&#37327;&#32423;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MiniSUPERB&#30340;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#27604;SUPERB&#26356;&#20302;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SUPERB&#34987;&#25552;&#20986;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#65292;&#23427;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MiniSUPERB&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20197;&#26126;&#26174;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#24182;&#19988;&#32467;&#26524;&#21487;&#19982;SUPERB&#30456;&#27604;&#12290;&#25105;&#20204;&#31934;&#36873;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#37319;&#26679;&#25968;&#25454;&#38598;&#65292;&#24182;&#31163;&#32447;&#25552;&#21462;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;SUPERB Paper&#21644;SUPERB Challenge&#20998;&#21035;&#36798;&#21040;0.954&#21644;0.982&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20056;-&#32047;&#31215;&#25805;&#20316;&#65288;MACs&#65289;&#26041;&#38754;&#20943;&#23569;&#20102;97&#65285;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#35780;&#20272;SSL&#35821;&#38899;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#26412;&#36523;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
SUPERB was proposed to evaluate the generalizability of self-supervised learning (SSL) speech models across various tasks. However, it incurs high computational costs due to the large datasets and diverse tasks. In this paper, we introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB but lower computational costs significantly. We carefully select representative tasks, sample datasets, and extract model representations offline. Our approach achieves a Spearman's rank correlation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge, respectively. Additionally, we reduce the computational cost by 97% in terms of Multiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech models in few-shot scenarios and observe significant variations in their performance. To our knowledge, this is the first study to examine both the computational cost of the model itself and the cost of evaluating it on a benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18459</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#35268;&#21010;&#22120;&#21644;&#25968;&#25454;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#21517;&#20026;Multi-Task Diffusion Model (MTDiff)&#65292;&#32467;&#21512;&#20102;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22914;&#27492;&#22797;&#26434;&#30340;&#22810;&#20219;&#21153;&#29615;&#22659;&#19979;&#21462;&#24471;&#30456;&#24403;&#19981;&#38169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#20063;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22797;&#26434;&#31574;&#30053;&#25110;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#35774;&#32622;&#65292;&#27809;&#26377;&#32771;&#34385;&#22810;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#21333;&#19968;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#31574;&#30053;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#31163;&#32447;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#21644;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Task Diffusion Model&#65288;MTDiff&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#39592;&#24178;&#21644;&#25552;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31163;&#32447;&#35774;&#32622;&#20013;&#30340;&#29983;&#25104;&#35268;&#21010;&#21644;&#25968;&#25454;&#21512;&#25104;&#12290;MTDiff&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#25191;&#34892;&#38544;&#24335;&#30693;&#35782;&#20849;&#20139;&#20197;&#36827;&#34892;&#34394;&#25311;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14782</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning. (arXiv:2305.14782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32487;&#32493;&#23398;&#20064;&#20063;&#20855;&#26377;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#20248;&#21270;&#24403;&#21069;&#20219;&#21153;&#20998;&#24067;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#29306;&#29298;&#24615;&#33021;&#20197;&#25552;&#39640;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#22810;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#33021;&#22815;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;-&#24615;&#33021;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35752;&#35770;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#20132;&#26131;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#37319;&#26679;&#24320;&#38144;-&#22312;&#23384;&#22312;&#22810;&#20010;&#65292;&#21487;&#33021;&#26159;&#26080;&#38480;&#25968;&#37327;&#30340;&#20559;&#22909;&#26102;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#12290;&#19968;&#26086;&#26377;&#26032;&#20219;&#21153;&#65292;IBCL&#20250;&#65288;1&#65289;&#26356;&#26032;&#19968;&#20010;&#20197;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#23384;&#22312;&#30340;&#30693;&#35782;&#24211;&#65292;&#65288;2&#65289;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#29305;&#23450;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;IBCL&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25968;&#25454;&#23601;&#33021;&#20026;&#19968;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20559;&#22909;&#29983;&#25104;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some tasks to improve on others. This means there exist multiple models that are each optimal at different times, each addressing a distinct task-performance trade-off. Researchers have discussed how to train particular models to address specific preferences on these trade-offs. However, existing algorithms require additional sample overheads -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address preferences with zero-shot. That is, IBCL does not require any additi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14585</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#25968;&#25454;&#24402;&#23646;&#20219;&#21153;&#65292;&#35299;&#37322;&#22411;AI&#30340;&#36827;&#27493;&#20043;&#19968;&#26159;&#36890;&#36807;&#35299;&#37322;&#31034;&#20363;&#31574;&#30053;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#23558;&#20915;&#31574;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23578;&#26410;&#30456;&#20114;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#24418;&#25104;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#30495;&#27491;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#35777;&#26126;&#20102;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65306;(1)&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;(pNTK)&#65292;&#23427;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#20013;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#26356;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#20026;&#26377;&#25928;&#65307;(2)&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#35268;&#33539;&#21270;pNTK&#21019;&#24314;&#30340;&#24402;&#22240;&#27604;&#36825;&#20123;&#26367;&#20195;&#21697;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
&lt;/p&gt;</description></item><item><title>Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.14342</link><description>&lt;p&gt;
Sophia&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14342
&lt;/p&gt;
&lt;p&gt;
Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20248;&#21270;&#31639;&#27861;&#30340;&#24494;&#23567;&#25913;&#36827;&#23558;&#20250;&#22823;&#22823;&#38477;&#20302;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;Adam&#21450;&#20854;&#21464;&#31181;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#20108;&#38454;&#65288;&#22522;&#20110;Hessian&#30340;&#65289;&#20248;&#21270;&#22120;&#24448;&#24448;&#20250;&#24102;&#26469;&#22826;&#22810;&#30340;&#27599;&#27493;&#24320;&#38144;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sophia&#65292;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20272;&#35745;&#30340;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#12290;&#26356;&#26032;&#27493;&#39588;&#26159;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#38500;&#20197;&#20272;&#35745;Hessian&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#65292;&#28982;&#21518;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#12290;&#35009;&#21098;&#25511;&#21046;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26356;&#26032;&#22823;&#23567;&#65292;&#24182;&#25511;&#21046;&#20102;Hessian&#22312;&#36712;&#36857;&#19978;&#30340;&#38750;&#20984;&#24615;&#21644;&#24555;&#36895;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;Sophia&#21482;&#22312;&#27599;&#20960;&#27425;&#36845;&#20195;&#20013;&#20272;&#35745;&#23545;&#35282;Hessian&#65292;&#36825;&#20960;&#20046;&#27809;&#26377;&#24179;&#22343;&#27599;&#27493;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;&#22312;&#20351;&#29992;GPT m&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.14189</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#65306;&#22686;&#21152;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#31034;&#35789;&#35821;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20849;&#20139;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#21644;&#20351;&#29992;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#25552;&#39640;&#30456;&#20284;&#21547;&#20041;&#35789;&#30340;&#23545;&#40784;&#24615;&#21644;BLEU&#20998;&#25968;&#30340;&#19968;&#33268;&#25552;&#21319;&#12290;&#27492;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#26377;&#38480;&#65292;&#24182;&#19988;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#20013;&#65292;&#20351;&#29992;&#20849;&#20139;&#30340;&#35789;&#27719;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#38500;&#20102;&#31616;&#21333;&#30340;&#35774;&#35745;&#22806;&#65292;&#20849;&#20139;&#26631;&#35760;&#22312;&#31215;&#26497;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20551;&#35774;&#20849;&#20139;&#26631;&#35760;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#35789;&#27719;&#30340;&#37325;&#21472;&#36739;&#23567;&#26102;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#19981;&#21516;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#36716;&#31227;&#34987;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35789;&#31561;&#20215;&#31867;&#23450;&#20041;&#20102;&#35789;&#32423;&#20449;&#24687;&#20256;&#36755;&#36335;&#24452;&#65292;&#24182;&#20381;&#36182;&#22270;&#32593;&#32476;&#26469;&#34701;&#21512;&#36328;&#35821;&#35328;&#30340;&#35789;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65306;1) &#20855;&#26377;&#30456;&#20284;&#21547;&#20041;&#30340;&#35789;&#30340;&#23884;&#20837;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26356;&#22909;&#22320;&#23545;&#40784;&#65292;2) &#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39640;&#21644;&#20302;&#36164;&#28304;MNMT&#26041;&#38754;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;BLEU&#25552;&#21319;&#36798;2.3&#20010;&#28857;&#65292;3) &#38656;&#35201;&#23569;&#20110;1.0%&#30340;&#39069;&#22806;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#30340;&#22686;&#21152;&#26377;&#38480;&#65292;&#32780;&#25512;&#29702;&#26102;&#38388;&#19982;&#22522;&#32447;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.11186</link><description>&lt;p&gt;
&#21387;&#32553;&#65292;&#28982;&#21518;&#25552;&#31034;&#65306;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#25913;&#21892;LLM&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#36716;&#31227;&#25552;&#31034;&#26469;&#20248;&#21270;&#21387;&#32553;&#30340;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#31934;&#24230;&#26356;&#39640;&#30340;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25968;&#21313;&#20159;&#30340;&#21442;&#25968;&#65292;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#24102;&#26469;&#26174;&#30528;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#35265;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#65288;&#20363;&#22914;&#21333;&#20010;GPU&#65289;&#26102;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21387;&#32553;&#26469;&#26368;&#23567;&#21270;LLM&#25512;&#29702;&#30340;&#24310;&#36831;&#65292;&#21363;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#27492;&#36807;&#31243;&#24517;&#28982;&#24341;&#21457;&#25928;&#29575;&#21644;&#31934;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#22240;&#20026;&#21387;&#32553;&#30340;LLMs&#36890;&#24120;&#20250;&#32463;&#21382;&#39044;&#27979;&#31934;&#24230;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35270;&#35282;&#65306;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24179;&#34913;&#65292;&#21387;&#32553;&#30340;LLMs&#38656;&#35201;&#19968;&#31181;&#19981;&#21516;&#20110;&#21407;&#22987;&#27169;&#22411;&#30340;&#29420;&#29305;&#36755;&#20837;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#31934;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21387;&#32553;&#30340;LLM&#22312;&#29305;&#23450;&#26597;&#35810;&#26041;&#38754;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#36716;&#31227;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#26377;&#25928;&#25552;&#31034;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#65292;&#24182;&#23454;&#29616;&#20102;4&#20493;&#36895;&#24230;&#30340;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), armed with billions of parameters, exhibit exceptional performance across a wide range of Natural Language Processing (NLP) tasks. However, they present a significant computational challenge during inference, especially when deploying on common hardware such as single GPUs. As such, minimizing the latency of LLM inference by curtailing computational and memory requirements, though achieved through compression, becomes critically important. However, this process inevitably instigates a trade-off between efficiency and accuracy, as compressed LLMs typically experience a reduction in predictive precision. In this research, we introduce an innovative perspective: to optimize this trade-off, compressed LLMs require a unique input format that varies from that of the original models. Our findings indicate that the generation quality in a compressed LLM can be markedly improved for specific queries by selecting prompts with precision. Capitalizing on this insight,
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#24182;&#20351;&#29992;&#19979;&#38480;&#25130;&#26029;&#65292;&#20197;&#21450;&#21363;&#26102;&#20272;&#35745;&#26410;&#30693;&#19979;&#38480;&#65292;&#26469;&#36817;&#20284;&#26368;&#23567;&#21270;&#35813;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.07583</link><description>&lt;p&gt;
MoMo: &#21160;&#37327;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#36890;&#36807;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#24182;&#20351;&#29992;&#19979;&#38480;&#25130;&#26029;&#65292;&#20197;&#21450;&#21363;&#26102;&#20272;&#35745;&#26410;&#30693;&#19979;&#38480;&#65292;&#26469;&#36817;&#20284;&#26368;&#23567;&#21270;&#35813;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#19982;&#20219;&#20309;&#21160;&#37327;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26032;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MoMo&#21644;MoMo-Adam&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#21160;&#37327;&#65288;SGDM&#65289;&#30340;SGD&#21644;Adam&#26041;&#27861;&#19982;&#25105;&#20204;&#30340;&#26032;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;MoMo&#26041;&#27861;&#26159;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#38543;&#26426;&#20248;&#21270;&#26469;&#28608;&#21457;&#30340;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#27599;&#27425;&#36845;&#20195;&#37319;&#26679;&#30340;&#25209;&#37327;&#25439;&#22833;&#21644;&#26799;&#24230;&#30340;&#21160;&#37327;&#20272;&#35745;&#26469;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21033;&#29992;&#20102;&#24050;&#30693;&#25439;&#22833;&#20989;&#25968;&#19979;&#38480;&#30340;&#25130;&#26029;&#26041;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#22810;&#25968;&#25439;&#22833;&#37117;&#34987;&#19979;&#38480;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#36817;&#20284;&#26368;&#23567;&#21270;&#27492;&#27169;&#22411;&#20197;&#35745;&#31639;&#19979;&#19968;&#27493;&#12290;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#19979;&#38480;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#21363;&#26102;&#19979;&#38480;&#20272;&#35745;&#65292;&#36825;&#20123;&#20272;&#35745;&#29992;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MoMo&#26041;&#27861;&#22312;MNIST&#12289;CIFAR10&#12289;CIFAR100&#21644;Imagenet32&#31561;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#20998;&#31867;&#35757;&#32451;&#20013;&#65292;&#30456;&#36739;&#20110;SGDM&#21644;Adam&#65292;&#22312;&#31934;&#24230;&#21644;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new adaptive learning rates that can be used with any momentum method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which are SGD with momentum (SGDM) and Adam together with our new adaptive learning rates. Our MoMo methods are motivated through model-based stochastic optimization, wherein we use momentum estimates of the batch losses and gradients sampled at each iteration to build a model of the loss function. Our model also makes use of any known lower bound of the loss function by using truncation. Indeed most losses are bounded below by zero. We then approximately minimize this model at each iteration to compute the next step. For losses with unknown lower bounds, we develop new on-the-fly estimates of the lower bound that we use in our model. Numerical experiments show that our MoMo methods improve over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR10, CIFAR100, Imagenet32, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedGT&#26694;&#26550;&#65292;&#36890;&#36807;&#32676;&#20307;&#27979;&#35797;&#30340;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24182;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#65292;&#20174;&#32780;&#24179;&#34913;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05506</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24179;&#34913;&#38544;&#31169;&#19982;&#23433;&#20840;&#65306;FedGT&#30340;&#32676;&#20307;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Balancing Privacy and Security in Federated Learning with FedGT: A Group Testing Framework. (arXiv:2305.05506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedGT&#26694;&#26550;&#65292;&#36890;&#36807;&#32676;&#20307;&#27979;&#35797;&#30340;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24182;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#65292;&#20174;&#32780;&#24179;&#34913;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;FedGT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#24182;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#12290;&#21463;&#21040;&#32676;&#20307;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#37325;&#21472;&#30340;&#23458;&#25143;&#32452;&#26469;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#30340;&#23384;&#22312;&#65292;&#24182;&#36890;&#36807;&#35793;&#30721;&#25805;&#20316;&#35782;&#21035;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#34987;&#35782;&#21035;&#30340;&#23458;&#25143;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#21024;&#38500;&#65292;&#24182;&#22312;&#20854;&#20313;&#23458;&#25143;&#20043;&#38388;&#25191;&#34892;&#35757;&#32451;&#12290;FedGT&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20801;&#35768;&#25913;&#36827;&#35782;&#21035;&#33021;&#21147;&#21516;&#26102;&#20173;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26381;&#21153;&#22120;&#23398;&#20064;&#27599;&#20010;&#32452;&#20013;&#23458;&#25143;&#30340;&#32858;&#21512;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedGT&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#65292;&#20855;&#26377;&#20302;&#35823;&#26816;&#21644;&#34394;&#35686;&#27010;&#29575;&#65292;&#20135;&#29983;&#39640;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose FedGT, a novel framework for identifying malicious clients in federated learning with secure aggregation. Inspired by group testing, the framework leverages overlapping groups of clients to detect the presence of malicious clients in the groups and to identify them via a decoding operation. The identified clients are then removed from the training of the model, which is performed over the remaining clients. FedGT strikes a balance between privacy and security, allowing for improved identification capabilities while still preserving data privacy. Specifically, the server learns the aggregated model of the clients in each group. The effectiveness of FedGT is demonstrated through extensive experiments on the MNIST and CIFAR-10 datasets, showing its ability to identify malicious clients with low misdetection and false alarm probabilities, resulting in high model utility.
&lt;/p&gt;</description></item><item><title>Tiny-PPG&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;PPG&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65292;&#20197;&#21450;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.03308</link><description>&lt;p&gt;
Tiny-PPG: &#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices. (arXiv:2305.03308v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03308
&lt;/p&gt;
&lt;p&gt;
Tiny-PPG&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;PPG&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65292;&#20197;&#21450;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#20013;&#36827;&#34892;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;PPG&#20449;&#21495;&#24456;&#23481;&#26131;&#21463;&#21040;&#36816;&#21160;&#20266;&#24433;&#30340;&#27745;&#26579;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Tiny-PPG&#8221;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#19978;&#20934;&#30830;&#23454;&#26102;&#22320;&#20998;&#21106;PPG&#20266;&#24433;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;PPG DaLiA&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#29992;&#25163;&#34920;&#24335;&#35774;&#22791;&#65288;Empatica E4&#65289;&#23545;15&#21517;&#21463;&#35797;&#32773;&#22312;&#21508;&#31181;&#26085;&#24120;&#27963;&#21160;&#20013;&#30340;PPG&#20449;&#21495;&#65292;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#21644;&#24418;&#24577;&#30340;&#22797;&#26434;&#20266;&#24433;&#12290;&#35813;&#27169;&#22411;&#32467;&#26500;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#25439;&#22833;&#20989;&#25968;&#29305;&#21035;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;&#20026;&#20102;&#20248;&#21270;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36328;&#36890;&#36947;&#23398;&#20064;&#21306;&#20998;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;PPG&#20449;&#21495;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Tiny-PPG&#22312;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts in real-world settings, despite their widespread use in Internet-of-Things (IoT) based wearable and smart health devices for cardiovascular health monitoring. This study proposed a lightweight deep neural network, called Tiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge devices. The model was trained and tested on a public dataset, PPG DaLiA, which featured complex artifacts with diverse lengths and morphologies during various daily activities of 15 subjects using a watch-type device (Empatica E4). The model structure, training method and loss function were specifically designed to balance detection accuracy and speed for real-time PPG artifact detection in resource-constrained embedded devices. To optimize the model size and capability in multi-scale feature representation, the model employed deep separable convolution and atrous spatial pyramid pooling modules, respectively. Addition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.01381</link><description>&lt;p&gt;
&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26679;&#26412;&#26377;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#20248;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#24191;&#27867;&#29992;&#20110;&#25351;&#23450;&#31995;&#32479;&#31574;&#30053;&#30340;&#39640;&#32423;&#30446;&#26631;&#65292;&#33258;&#20027;&#31995;&#32479;&#23398;&#20064;&#30456;&#23545;&#20110;&#36825;&#26679;&#30340;&#35268;&#33539;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290; &#20294;&#26159;&#65292;&#20174;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#36731;&#26494;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26356;&#36890;&#29992;&#30340;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#65292;&#24403;&#19982;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26368;&#22823;&#21270;&#32473;&#23450;LTL&#35268;&#33539;&#28385;&#36275;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26377;&#20851;&#36873;&#25321;RL&#20013;&#20851;&#38190;&#21442;&#25968;&#20197;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#20026;&#20102;&#30452;&#25509;&#35780;&#20272;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26469;&#35745;&#31639;LTL&#35268;&#33539;&#30340;&#28385;&#36275;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SAL&#21644;SCoreBO&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11005</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#33258;&#26657;&#27491;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Bayesian Optimization through Bayesian Active Learning. (arXiv:2304.11005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SAL&#21644;SCoreBO&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#24050;&#25104;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#39640;&#26031;&#36807;&#31243;&#30340;&#23436;&#20840;&#21457;&#25381;&#38656;&#35201;&#24039;&#22937;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#32780;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#26377;&#20851;&#20110;&#25214;&#21040;&#27491;&#30830;&#36229;&#21442;&#25968;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36873;&#25321;&#22909;&#30340;&#36229;&#21442;&#25968;&#23545;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26126;&#30830;&#20248;&#20808;&#32771;&#34385;&#27492;&#30446;&#26631;&#30340;&#25910;&#36141;&#20989;&#25968;&#12290;&#32479;&#35745;&#36317;&#31163;&#20027;&#21160;&#23398;&#20064;&#65288;SAL&#65289;&#32771;&#34385;&#21518;&#39564;&#26679;&#26412;&#30340;&#24179;&#22343;&#19981;&#19968;&#33268;&#24615;&#65292;&#30001;&#32479;&#35745;&#36317;&#31163;&#27979;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#27979;&#35797;&#20989;&#25968;&#19978;&#65292;&#23427;&#32988;&#36807;&#20102;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#26657;&#27491;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SCoreBO&#65289;&#65292;&#23427;&#23558;SAL&#25193;&#23637;&#21040;&#21516;&#26102;&#25191;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#36229;&#21442;&#25968;&#23398;&#20064;&#12290;&#30456;&#27604;&#20256;&#32479;BO&#65292;SCoreBO&#20197;&#25913;&#36827;&#30340;&#36895;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#22312;&#26368;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25628;&#32034;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are cemented as the model of choice in Bayesian optimization and active learning. Yet, they are severely dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding the right hyperparameters in the literature. We demonstrate the impact of selecting good hyperparameters for GPs and present two acquisition functions that explicitly prioritize this goal. Statistical distance-based Active Learning (SAL) considers the average disagreement among samples from the posterior, as measured by a statistical distance. It is shown to outperform the state-of-the-art in Bayesian active learning on a number of test functions. We then introduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to perform Bayesian optimization and active hyperparameter learning simultaneously. SCoreBO learns the model hyperparameters at improved rates compared to vanilla BO, while outperforming the latest Bayesian optimization met
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#33041;&#32959;&#30244;MR&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22352;&#26631;&#27880;&#24847;&#21147;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#24615;&#33021;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#21644;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08072</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20004;&#38454;&#27573;&#33041;&#32959;&#30244;MR&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two-stage MR Image Segmentation Method for Brain Tumors based on Attention Mechanism. (arXiv:2304.08072v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08072
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#20004;&#38454;&#27573;&#33041;&#32959;&#30244;MR&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22352;&#26631;&#27880;&#24847;&#21147;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#24615;&#33021;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#21644;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#21487;&#20197;&#25581;&#31034;&#20154;&#20307;&#32452;&#32455;&#30340;&#19981;&#21516;&#27169;&#24335;&#65292;&#23545;&#20110;&#20020;&#24202;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#12289;&#22122;&#22768;&#21644;&#25163;&#21160;&#26631;&#35760;&#30340;&#38480;&#21046;&#65292;&#33719;&#24471;&#22810;&#26679;&#24615;&#21644;&#21487;&#38752;&#30340;&#22810;&#27169;&#24577;MR&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#29983;&#25104;&#21644;&#20998;&#21106;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CycleGAN&#65289;&#30340;&#21327;&#21516;&#31354;&#38388;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CASP-GAN&#65289;&#12290;&#24341;&#20837;&#22352;&#26631;&#27880;&#24847;&#21147;&#65288;CA&#65289;&#27169;&#22359;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#27169;&#22359;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#36825;&#20004;&#20010;&#27169;&#22359;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#25429;&#33719;&#21040;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#20934;&#30830;&#23450;&#20301;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#24182;&#22686;&#24378;&#29983;&#25104;&#22120;&#27169;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#33021;&#22815;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#21644;&#35814;&#32454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal magnetic resonance imaging (MRI) can reveal different patterns of human tissue and is crucial for clinical diagnosis. However, limited by cost, noise and manual labeling, obtaining diverse and reliable multimodal MR images remains a challenge. For the same lesion, different MRI manifestations have great differences in background information, coarse positioning and fine structure. In order to obtain better generation and segmentation performance, a coordination-spatial attention generation adversarial network (CASP-GAN) based on the cycle-consistent generative adversarial network (CycleGAN) is proposed. The performance of the generator is optimized by introducing the Coordinate Attention (CA) module and the Spatial Attention (SA) module. The two modules can make full use of the captured location information, accurately locating the interested region, and enhancing the generator model network structure. The ability to extract the structure information and the detailed informat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#22914;&#20309;&#36890;&#36807;&#21452;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#24335;&#26469;&#25552;&#39640;&#20998;&#21106;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04441</link><description>&lt;p&gt;
&#21452;&#37325;&#19981;&#30830;&#23450;&#24615;&#33258;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-training with dual uncertainty for semi-supervised medical image segmentation. (arXiv:2304.04441v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#22914;&#20309;&#36890;&#36807;&#21452;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#35757;&#32451;&#26041;&#24335;&#26469;&#25552;&#39640;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#30701;&#32570;&#26159;&#19968;&#20010;&#26681;&#26412;&#24615;&#38382;&#39064;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#29305;&#24449;&#20197;&#25552;&#39640;&#20998;&#21106;&#31934;&#24230;&#26159;&#36825;&#19968;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#20256;&#32479;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20026;&#36845;&#20195;&#35757;&#32451;&#29983;&#25104;&#20266;&#26631;&#31614;&#37096;&#20998;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30001;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20135;&#29983;&#30340;&#22122;&#22768;&#30452;&#25509;&#24433;&#21709;&#20102;&#20998;&#21106;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#33258;&#35757;&#32451;&#26694;&#26550;&#20013;&#22686;&#21152;&#20102;&#26679;&#26412;&#23618;&#38754;&#21644;&#20687;&#32032;&#23618;&#38754;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20445;&#23384;&#20102;&#27169;&#22411;&#30340;&#20960;&#20010;&#26102;&#21051;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#22312;&#26410;&#26631;&#35760;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#20316;&#20026;&#35813;&#26679;&#26412;&#30340;&#26679;&#26412;&#23618;&#38754;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36880;&#28176;&#28155;&#21152;&#20174;&#26131;&#21040;&#38590;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#21516;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of semi-supervised medical image segmentation, the shortage of labeled data is the fundamental problem. How to effectively learn image features from unlabeled images to improve segmentation accuracy is the main research direction in this field. Traditional self-training methods can partially solve the problem of insufficient labeled data by generating pseudo labels for iterative training. However, noise generated due to the model's uncertainty during training directly affects the segmentation results. Therefore, we added sample-level and pixel-level uncertainty to stabilize the training process based on the self-training framework. Specifically, we saved several moments of the model during pre-training, and used the difference between their predictions on unlabeled samples as the sample-level uncertainty estimate for that sample. Then, we gradually add unlabeled samples from easy to hard during training. At the same time, we added a decoder with different upsampling method
&lt;/p&gt;</description></item><item><title>StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.03853</link><description>&lt;p&gt;
StepMix: &#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03853
&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#24191;&#20041;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;(&#28508;&#22312;&#21078;&#38754;&#21644;&#28508;&#22312;&#31867;&#20998;&#26512;)&#19982;&#22806;&#37096;&#21464;&#37327;(&#21327;&#21464;&#37327;&#21644;&#36828;&#31243;&#32467;&#26524;)&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;(&#21333;&#27493;&#12289;&#20004;&#27493;&#21644;&#19977;&#27493;&#26041;&#27861;)&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;&#22312;&#35768;&#22810;&#31038;&#20250;&#31185;&#23398;&#30340;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#19981;&#20165;&#26159;&#23558;&#20010;&#20307;&#32858;&#31867;&#25104;&#28508;&#22312;&#31867;&#21035;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#26469;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20998;&#20026;&#19968;&#20010;&#23558;&#28508;&#22312;&#31867;&#21035;&#19982;&#35266;&#23519;&#25351;&#26631;&#30456;&#20851;&#32852;&#30340;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#23558;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#19982;&#28508;&#22312;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;&#27979;&#37327;&#21644;&#32467;&#26500;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25152;&#35859;&#30340;&#19968;&#27493;&#27861;&#20849;&#21516;&#20272;&#35745;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#36880;&#27493;&#26041;&#27861;&#36880;&#27493;&#20272;&#35745;&#65292;&#23545;&#20110;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20272;&#35745;&#28508;&#22312;&#31867;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#38500;&#20102;&#19968;&#27493;&#27861;&#65292;StepMix&#36824;&#23454;&#29616;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#26041;&#20415;&#27169;&#22411;&#30340;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#30340;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#26684;&#24314;&#27169;&#20026;&#19968;&#32452;&#31616;&#21333;&#21327;&#20316;&#30340;&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#22312;&#30456;&#37051;&#32593;&#26684;&#20803;&#32032;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#23454;&#21487;&#20197;&#23398;&#20064;&#21487;&#38752;&#12289;&#21487;&#25193;&#23637;&#30340;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.00818</link><description>&lt;p&gt;
&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#30340;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Swarm Reinforcement Learning For Adaptive Mesh Refinement. (arXiv:2304.00818v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#30340;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32593;&#26684;&#24314;&#27169;&#20026;&#19968;&#32452;&#31616;&#21333;&#21327;&#20316;&#30340;&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#22312;&#30456;&#37051;&#32593;&#26684;&#20803;&#32032;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#23454;&#21487;&#20197;&#23398;&#20064;&#21487;&#38752;&#12289;&#21487;&#25193;&#23637;&#30340;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#20803;&#26041;&#27861;&#26159;&#24037;&#31243;&#23398;&#20013;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#33258;&#36866;&#24212;&#32593;&#26684;&#32454;&#21270;&#65288;AMR&#65289;&#36890;&#36807;&#21160;&#24577;&#32454;&#21270;&#32593;&#26684;&#21306;&#22495;&#65292;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#27169;&#25311;&#31934;&#24230;&#20043;&#38388;&#21462;&#24471;&#26377;&#21033;&#30340;&#24179;&#34913;&#12290;&#20256;&#32479;&#30340;AMR&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#25110;&#26114;&#36149;&#30340;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22797;&#26434;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#23398;&#20064;&#22411;AMR&#26041;&#27861;&#35797;&#22270;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#21482;&#33021;&#24212;&#29992;&#20110;&#31616;&#21333;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;AMR&#34920;&#36848;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#32676;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#32593;&#26684;&#34987;&#24314;&#27169;&#20026;&#19968;&#32452;&#31616;&#21333;&#21327;&#20316;&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#20998;&#35010;&#20026;&#22810;&#20010;&#26032;&#20195;&#29702;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#31354;&#38388;&#22870;&#21169;&#20844;&#24335;&#21270;&#31616;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#22312;&#30456;&#37051;&#32593;&#26684;&#20803;&#32032;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#33258;&#36866;&#24212;&#32676;&#20307;&#32593;&#26684;&#32454;&#21270;&#65288;ASMR&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#23427;&#21487;&#20197;&#23398;&#20064;&#21487;&#38752;&#12289;&#21487;&#25193;&#23637;&#30340;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Finite Element Method, an important technique in engineering, is aided by Adaptive Mesh Refinement (AMR), which dynamically refines mesh regions to allow for a favorable trade-off between computational speed and simulation accuracy. Classical methods for AMR depend on task-specific heuristics or expensive error estimators, hindering their use for complex simulations. Recent learned AMR methods tackle these problems, but so far scale only to simple toy examples. We formulate AMR as a novel Adaptive Swarm Markov Decision Process in which a mesh is modeled as a system of simple collaborating agents that may split into multiple new agents. This framework allows for a spatial reward formulation that simplifies the credit assignment problem, which we combine with Message Passing Networks to propagate information between neighboring mesh elements. We experimentally validate the effectiveness of our approach, Adaptive Swarm Mesh Refinement (ASMR), showing that it learns reliable, scalable,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>RACCER&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#30340;&#19987;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#29305;&#23450;&#23545;&#25239;&#20107;&#23454;&#23646;&#24615;&#65292;&#20445;&#35777;&#26131;&#20110;&#23454;&#29616;&#19988;&#20855;&#26377;&#39640;&#27010;&#29575;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2303.04475</link><description>&lt;p&gt;
RACCER: &#38754;&#21521;&#21487;&#36798;&#21644;&#30830;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#21487;&#36861;&#28335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning. (arXiv:2303.04475v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04475
&lt;/p&gt;
&lt;p&gt;
RACCER&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#30340;&#19987;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#29305;&#23450;&#23545;&#25239;&#20107;&#23454;&#23646;&#24615;&#65292;&#20445;&#35777;&#26131;&#20110;&#23454;&#29616;&#19988;&#20855;&#26377;&#39640;&#27010;&#29575;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#36182;&#20351;&#20854;&#34892;&#20026;&#38590;&#20197;&#29702;&#35299;&#21644;&#20449;&#20219;&#12290;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#20154;&#24615;&#21270;&#30340;&#35299;&#37322;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#25913;&#21464;&#27169;&#22411;&#36755;&#20837;&#20197;&#36798;&#21040;&#39044;&#26399;&#36755;&#20986;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#38543;&#26426;&#24615;&#21644;&#39034;&#24207;&#24615;&#65292;&#21487;&#33021;&#20135;&#29983;&#38590;&#20197;&#33719;&#24471;&#25110;&#26080;&#27861;&#23454;&#29616;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RACCER&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#29983;&#25104;&#23545;&#25239;&#20107;&#23454;&#35299;&#37322;&#30340;&#39318;&#20010;&#19987;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#32452;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#29305;&#23450;&#23545;&#25239;&#20107;&#23454;&#23646;&#24615;&#65292;&#30830;&#20445;&#26131;&#20110;&#23454;&#29616;&#19988;&#20855;&#26377;&#39640;&#27010;&#29575;&#39044;&#26399;&#32467;&#26524;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;&#25105;&#20204;&#20351;&#29992;&#21551;&#21457;&#24335;&#26641;&#25628;&#32034;&#20195;&#29702;&#25191;&#34892;&#36712;&#36857;&#65292;&#20197;&#25214;&#21040;&#26368;&#21512;&#36866;&#30340;&#23545;&#25239;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals that are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly probable desired outcomes. We use a heuristic tree search of the agent's execution trajectories to find the most suitable counterfactuals ba
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20250;&#20197;&#25351;&#25968;&#32423;&#20943;&#24930;&#65292;&#26159;&#39318;&#20010;&#32473;&#20986;&#35813;&#38382;&#39064;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#35777;&#26126;&#19978;&#19979;&#30028;&#65292;&#25105;&#20204;&#31934;&#30830;&#21051;&#30011;&#20986;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25351;&#20986;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#23545;&#20110;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10034</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#20250;&#23548;&#33268;&#26799;&#24230;&#19979;&#38477;&#22312;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#20803;&#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#25351;&#25968;&#32423;&#20943;&#24930;
&lt;/p&gt;
&lt;p&gt;
Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron. (arXiv:2302.10034v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10034
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20250;&#20197;&#25351;&#25968;&#32423;&#20943;&#24930;&#65292;&#26159;&#39318;&#20010;&#32473;&#20986;&#35813;&#38382;&#39064;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#35777;&#26126;&#19978;&#19979;&#30028;&#65292;&#25105;&#20204;&#31934;&#30830;&#21051;&#30011;&#20986;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25351;&#20986;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#23545;&#20110;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#21644;&#26041;&#24418;&#25439;&#22833;&#30340;&#39640;&#26031;&#36755;&#20837;&#19979;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23398;&#29983;&#32593;&#32476;&#20855;&#26377;n&#8805;2&#20010;&#31070;&#32463;&#20803;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#35774;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20197;O(T^-3)&#30340;&#36895;&#24230;&#20840;&#23616;&#25910;&#25947;&#12290;&#36825;&#26159;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#36229;&#36807;&#31934;&#30830;&#21442;&#25968;&#21270;&#35774;&#32622;(n=1)&#30340;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#65292;&#20854;&#20013;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21576;&#29616;&#20986;exp(-&#937;(T))&#30340;&#36895;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26799;&#24230;&#27969;&#31639;&#27861;&#30340;&#19979;&#30028;&#26159;&#937;(T^-3)&#12290;&#36825;&#20004;&#20010;&#19979;&#30028;&#20849;&#21516;&#32473;&#20986;&#20102;&#25910;&#25947;&#36895;&#24230;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#24182;&#39318;&#27425;&#26263;&#31034;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#20250;&#20351;&#25910;&#25947;&#36895;&#24230;&#25351;&#25968;&#32423;&#20943;&#24930;&#12290;&#20026;&#20102;&#35777;&#26126;&#20840;&#23616;&#25910;&#25947;&#65292;&#25105;&#20204;&#38656;&#35201;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#20013;&#23398;&#29983;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#22312;&#31934;&#30830;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of learning a single neuron with ReLU activation under Gaussian input with square loss. We particularly focus on the over-parameterization setting where the student network has $n\ge 2$ neurons. We prove the global convergence of randomly initialized gradient descent with a $O\left(T^{-3}\right)$ rate. This is the first global convergence result for this problem beyond the exact-parameterization setting ($n=1$) in which the gradient descent enjoys an $\exp(-\Omega(T))$ rate. Perhaps surprisingly, we further present an $\Omega\left(T^{-3}\right)$ lower bound for randomly initialized gradient flow in the over-parameterization setting. These two bounds jointly give an exact characterization of the convergence rate and imply, for the first time, that over-parameterization can exponentially slow down the convergence rate. To prove the global convergence, we need to tackle the interactions among student neurons in the gradient descent dynamics, which are not present in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AliasNet&#65292;&#19968;&#20010;&#21035;&#21517;&#20266;&#24433;&#25233;&#21046;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#22312;&#30456;&#32534;&#30721;MRI&#20013;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#23548;&#33268;&#30340;&#21035;&#21517;&#20266;&#24433;&#38382;&#39064;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#31181;&#35299;&#32806;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#20986;&#33394;1D&#19981;&#30456;&#24178;&#29305;&#24615;&#30340;&#20266;&#24433;&#20449;&#21495;&#36827;&#34892;&#26126;&#30830;&#30340;1D&#27491;&#21017;&#21270;&#12290;&#35813;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;1D&#21644;2D&#37325;&#24314;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#31232;&#30095;&#24615;&#27169;&#22411;&#21644;&#22810;&#26041;&#21521;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.08861</link><description>&lt;p&gt;
AliasNet: &#21152;&#36895;&#30456;&#32534;&#30721;MRI&#30340;&#21035;&#21517;&#20266;&#24433;&#25233;&#21046;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AliasNet: Alias Artefact Suppression Network for Accelerated Phase-Encode MRI. (arXiv:2302.08861v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AliasNet&#65292;&#19968;&#20010;&#21035;&#21517;&#20266;&#24433;&#25233;&#21046;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#22312;&#30456;&#32534;&#30721;MRI&#20013;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#23548;&#33268;&#30340;&#21035;&#21517;&#20266;&#24433;&#38382;&#39064;&#12290;&#36890;&#36807;&#24320;&#21457;&#20004;&#31181;&#35299;&#32806;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#20986;&#33394;1D&#19981;&#30456;&#24178;&#29305;&#24615;&#30340;&#20266;&#24433;&#20449;&#21495;&#36827;&#34892;&#26126;&#30830;&#30340;1D&#27491;&#21017;&#21270;&#12290;&#35813;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;1D&#21644;2D&#37325;&#24314;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#31232;&#30095;&#24615;&#27169;&#22411;&#21644;&#22810;&#26041;&#21521;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#37325;&#24314;&#26159;MRI&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#37319;&#38598;&#26102;&#38388;&#24182;&#25552;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#12290;&#30446;&#21069;&#27969;&#34892;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#21387;&#32553;&#24863;&#30693;&#25216;&#26415;&#65288;CS&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#23545;k&#31354;&#38388;&#30340;&#38543;&#26426;&#37319;&#26679;&#26469;&#20135;&#29983;&#19981;&#30456;&#24178;&#30340;&#65288;&#22122;&#22768;&#26679;&#30340;&#65289;&#20266;&#24433;&#12290;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#65292;1D&#31515;&#21345;&#23572;&#30456;&#32534;&#30721;&#30340;&#27424;&#37319;&#26679;&#26041;&#26696;&#22312;2D CS-MRI&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;1D&#27424;&#37319;&#26679;&#38480;&#21046;&#20102;&#27979;&#37327;&#20043;&#38388;&#30340;2D&#19981;&#30456;&#24178;&#24615;&#65292;&#20135;&#29983;&#32467;&#26500;&#21270;&#30340;&#21035;&#21517;&#20266;&#24433;&#65288;&#24189;&#28789;&#65289;&#65292;&#22312;&#20551;&#35774;2D&#31232;&#30095;&#27169;&#22411;&#26102;&#21487;&#33021;&#38590;&#20197;&#21435;&#38500;&#12290;&#37325;&#24314;&#31639;&#27861;&#36890;&#24120;&#38024;&#23545;&#36825;&#20123;&#19982;&#26041;&#21521;&#30456;&#20851;&#30340;&#20266;&#24433;&#37319;&#29992;&#19981;&#32771;&#34385;&#26041;&#21521;&#30340;2D&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#30456;&#32534;&#30721;&#20266;&#24433;&#21487;&#20197;&#20998;&#31163;&#20026;&#36830;&#32493;&#30340;1D&#20449;&#21495;&#65292;&#22240;&#27492;&#24320;&#21457;&#20102;&#20004;&#31181;&#35299;&#32806;&#25216;&#26415;&#65292;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#30340;1D&#27491;&#21017;&#21270;&#24182;&#21033;&#29992;&#20986;&#33394;&#30340;1D&#19981;&#30456;&#24178;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#19968;&#20010;&#32467;&#21512;&#20102;1D+2D&#37325;&#24314;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#31232;&#30095;&#24615;&#27169;&#22411;&#21644;&#22810;&#26041;&#21521;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse reconstruction is an important aspect of MRI, helping to reduce acquisition time and improve spatial-temporal resolution. Popular methods are based mostly on compressed sensing (CS), which relies on the random sampling of k-space to produce incoherent (noise-like) artefacts. Due to hardware constraints, 1D Cartesian phase-encode under-sampling schemes are popular for 2D CS-MRI. However, 1D under-sampling limits 2D incoherence between measurements, yielding structured aliasing artefacts (ghosts) that may be difficult to remove assuming a 2D sparsity model. Reconstruction algorithms typically deploy direction-insensitive 2D regularisation for these direction-associated artefacts. Recognising that phase-encode artefacts can be separated into contiguous 1D signals, we develop two decoupling techniques that enable explicit 1D regularisation and leverage the excellent 1D incoherence characteristics. We also derive a combined 1D + 2D reconstruction technique that takes advantage of spa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.07409</link><description>&lt;p&gt;
&#12298;&#36229;&#36234;&#25209;&#22788;&#29702;&#20108;&#20803;&#20998;&#31867;&#30340;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#12299;
&lt;/p&gt;
&lt;p&gt;
Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Arunachalam&#21644;de Wolf&#65288;2018&#65289;&#35777;&#26126;&#20102;&#22312;&#21487;&#23454;&#29616;&#21644;&#31946;&#28034;&#35774;&#32622;&#19979;&#65292;&#37327;&#23376;&#25209;&#22788;&#29702;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19982;&#30456;&#24212;&#30340;&#32463;&#20856;&#26679;&#26412;&#22797;&#26434;&#24615;&#20855;&#26377;&#30456;&#21516;&#30340;&#24418;&#24335;&#21644;&#25968;&#37327;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26126;&#26174;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#22312;&#32447;&#23398;&#20064;&#32467;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;Dawid&#21644;Tewari&#65288;2022&#65289;&#32463;&#20856;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#23545;&#25163;&#21464;&#20307;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arunachalam and de Wolf (2018) showed that the sample complexity of quantum batch learning of boolean functions, in the realizable and agnostic settings, has the same form and order as the corresponding classical sample complexities. In this paper, we extend this, ostensibly surprising, message to batch multiclass learning, online boolean learning, and online multiclass learning. For our online learning results, we first consider an adaptive adversary variant of the classical model of Dawid and Tewari (2022). Then, we introduce the first (to the best of our knowledge) model of online learning with quantum examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2302.05534</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#35774;&#32622;&#65292;&#36825;&#26159;&#19968;&#20010;&#24182;&#34892;&#20256;&#36755;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#26159;&#23558;&#30693;&#35782;&#20174;&#20302;&#23618;&#65288;&#28304;&#65289;&#20219;&#21153;&#20256;&#36755;&#21040;&#39640;&#23618;&#65288;&#30446;&#26631;&#65289;&#20219;&#21153;&#65292;&#20197;&#20943;&#23569;&#21518;&#32773;&#30340;&#25506;&#32034;&#39118;&#38505;&#65292;&#21516;&#26102;&#24182;&#34892;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#20302;&#23618;&#21644;&#39640;&#23618;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#24577;&#25110;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26368;&#20248;&#20540;&#25903;&#37197;&#8221;&#30340;&#33258;&#28982;&#32780;&#24517;&#35201;&#30340;&#26465;&#20214;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#23545;&#20110;&#39640;&#23618;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#29366;&#24577;&#19978;&#21487;&#20197;&#23454;&#29616;&#24658;&#23450;&#30340;&#36951;&#25022;&#65292;&#36825;&#21462;&#20915;&#20110;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19981;&#30456;&#20284;&#26102;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#36951;&#25022;&#65307;&#32780;&#23545;&#20110;&#20302;&#23618;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#20570;&#20986;&#29306;&#29298;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#20302;&#23618;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#20984;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;ICNN&#19982;Lipschitz&#29305;&#24449;&#26144;&#23556;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#19968;&#20010;&#8220;&#25935;&#24863;&#8221;&#31867;&#30340;&#19981;&#23545;&#31216;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#30830;&#23450;&#24615;&#30340;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2302.01961</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#20984;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#38750;&#23545;&#31216;&#30340;&#21487;&#20449;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Asymmetric Certified Robustness via Feature-Convex Neural Networks. (arXiv:2302.01961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#20984;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;ICNN&#19982;Lipschitz&#29305;&#24449;&#26144;&#23556;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#19968;&#20010;&#8220;&#25935;&#24863;&#8221;&#31867;&#30340;&#19981;&#23545;&#31216;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#30830;&#23450;&#24615;&#30340;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36755;&#20837;&#20984;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ICNNs)&#20316;&#20026;&#20855;&#26377;&#26377;&#21033;&#30340;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#29305;&#24615;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#19982;&#20854;&#20984;&#32467;&#26500;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#20984;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;ICNN&#19982;Lipschitz&#29305;&#24449;&#26144;&#23556;&#32452;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#19968;&#20010;&#8220;&#25935;&#24863;&#8221;&#31867;&#30340;&#19981;&#23545;&#31216;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#24182;&#20026;&#36825;&#20010;&#31867;&#35777;&#26126;&#20102;&#30830;&#23450;&#24615;&#30340;&#12289;&#23553;&#38381;&#24418;&#24335;&#30340;&#21644;&#26131;&#20110;&#35745;&#31639;&#30340;&#20219;&#24847;$\ell_p$&#33539;&#25968;&#30340;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;&#25105;&#20204;&#36890;&#36807;&#34920;&#24449;&#23427;&#20204;&#30340;&#20915;&#31574;&#21306;&#22495;&#20960;&#20309;&#12289;&#23558;ICNN&#22238;&#24402;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25193;&#23637;&#21040;&#20998;&#31867;&#35774;&#23450;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#39640;&#32500;&#24230;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#23436;&#32654;&#22320;&#25311;&#21512;&#29978;&#33267;&#26080;&#32467;&#26500;&#22343;&#21248;&#20998;&#24067;&#25968;&#25454;&#30340;&#27010;&#29575;&#19979;&#30028;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;Malimg&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#21644;MNIST&#23376;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent works have introduced input-convex neural networks (ICNNs) as learning models with advantageous training, inference, and generalization properties linked to their convex structure. In this paper, we propose a novel feature-convex neural network architecture as the composition of an ICNN with a Lipschitz feature map in order to achieve adversarial robustness. We consider the asymmetric binary classification setting with one "sensitive" class, and for this class we prove deterministic, closed-form, and easily-computable certified robust radii for arbitrary $\ell_p$-norms. We theoretically justify the use of these models by characterizing their decision region geometry, extending the universal approximation theorem for ICNN regression to the classification setting, and proving a lower bound on the probability that such models perfectly fit even unstructured uniformly distributed data in sufficiently high dimensions. Experiments on Malimg malware classification and subsets of MNIST,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#36890;&#36807;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#31181;&#23376;&#25968;&#25454;&#20013;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#20445;&#25345;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#26469;&#25351;&#23548;&#24819;&#35937;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.13976</link><description>&lt;p&gt;
&#29992;&#24341;&#23548;&#24819;&#35937;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Expanding Small-Scale Datasets with Guided Imagination. (arXiv:2211.13976v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#36890;&#36807;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#31181;&#23376;&#25968;&#25454;&#20013;&#25193;&#20805;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#28508;&#22312;&#29305;&#24449;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#31867;&#21035;&#20445;&#25345;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#26469;&#25351;&#23548;&#24819;&#35937;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#30340;&#21151;&#25928;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#36890;&#24120;&#36153;&#26102;&#36153;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#39033;&#21517;&#20026;&#25968;&#25454;&#38598;&#25193;&#20805;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#21019;&#24314;&#26032;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#25193;&#20805;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#23548;&#24819;&#35937;&#26694;&#26550;(GIF)&#65292;&#21033;&#29992;DALL-E2&#21644;Stable Diffusion (SD)&#31561;&#23574;&#31471;&#29983;&#25104;&#27169;&#22411;&#30340;&#21147;&#37327;&#65292;&#20174;&#36755;&#20837;&#30340;&#31181;&#23376;&#25968;&#25454;&#20013;&#8220;&#24819;&#35937;&#8221;&#24182;&#21019;&#24314;&#20449;&#24687;&#20016;&#23500;&#30340;&#26032;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GIF&#36890;&#36807;&#22312;&#20808;&#39564;&#27169;&#22411;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#31354;&#38388;&#20013;&#20248;&#21270;&#31181;&#23376;&#25968;&#25454;&#30340;&#28508;&#22312;&#29305;&#24449;&#26469;&#36827;&#34892;&#25968;&#25454;&#30340;&#24819;&#35937;&#65292;&#20174;&#32780;&#21019;&#24314;&#20855;&#26377;&#26032;&#20869;&#23481;&#30340;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#24341;&#23548;&#24819;&#35937;&#26397;&#30528;&#21019;&#24314;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#20449;&#24687;&#20016;&#23500;&#26679;&#26412;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#26631;&#20934;&#65292;&#21363;&#31867;&#21035;&#20445;&#25345;&#20449;&#24687;&#25552;&#21319;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#20419;&#36827;&#12290;&#36825;&#20123;&#26631;&#20934;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of DNNs relies heavily on the quantity and quality of training data. However, collecting and annotating data on a large scale is often expensive and time-consuming. To address this issue, we explore a new task, termed dataset expansion, aimed at expanding a ready-to-use small dataset by automatically creating new labeled samples. To this end, we present a Guided Imagination Framework (GIF) that leverages cutting-edge generative models like DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data from the input seed data. Specifically, GIF conducts data imagination by optimizing the latent features of the seed data in the semantically meaningful space of the prior model, resulting in the creation of photo-realistic images with new content. To guide the imagination towards creating informative samples for model training, we introduce two key criteria, i.e., class-maintained information boosting and sample diversity promotion. These criteria are verified to
&lt;/p&gt;</description></item><item><title>SGD &#21644; AdamW &#26159;&#29992;&#20110;&#35843;&#25972;&#35270;&#35273;&#27169;&#22411;&#30340;&#20004;&#31181;&#24120;&#35265;&#20248;&#21270;&#22120;&#65292;&#24403;&#32454;&#35843;&#26799;&#24230;&#22312;&#23884;&#20837;&#23618;&#20013;&#36739;&#22823;&#26102;&#65292;AdamW &#30340;&#24615;&#33021;&#20248;&#20110; SGD&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20923;&#32467;&#23884;&#20837;&#23618;&#30340;&#31616;&#21333;&#20462;&#27491;&#26041;&#27861;&#65292;&#20351;&#24471; SGD &#34920;&#29616;&#30053;&#22909;&#20110; AdamW &#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2211.09359</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992; SGD &#35843;&#25972;&#35270;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Fine-Tune Vision Models with SGD. (arXiv:2211.09359v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09359
&lt;/p&gt;
&lt;p&gt;
SGD &#21644; AdamW &#26159;&#29992;&#20110;&#35843;&#25972;&#35270;&#35273;&#27169;&#22411;&#30340;&#20004;&#31181;&#24120;&#35265;&#20248;&#21270;&#22120;&#65292;&#24403;&#32454;&#35843;&#26799;&#24230;&#22312;&#23884;&#20837;&#23618;&#20013;&#36739;&#22823;&#26102;&#65292;AdamW &#30340;&#24615;&#33021;&#20248;&#20110; SGD&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20923;&#32467;&#23884;&#20837;&#23618;&#30340;&#31616;&#21333;&#20462;&#27491;&#26041;&#27861;&#65292;&#20351;&#24471; SGD &#34920;&#29616;&#30053;&#22909;&#20110; AdamW &#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SGD &#21644; AdamW &#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#31181;&#26368;&#24120;&#29992;&#30340;&#20248;&#21270;&#22120;&#12290;&#24403;&#36825;&#20004;&#31181;&#26041;&#27861;&#34920;&#29616;&#30456;&#21516;&#26102;&#65292;SGD &#26356;&#21463;&#38738;&#30544;&#65292;&#22240;&#20026;&#23427;&#27604; AdamW &#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#65288;&#20855;&#26377;&#21160;&#37327;&#26102;&#65292;&#27599;&#20010;&#21442;&#25968;&#21344;&#29992; 12 &#23383;&#33410;&#65292;&#32780;&#19981;&#20855;&#26377;&#21160;&#37327;&#26102;&#27599;&#20010;&#21442;&#25968;&#21344;&#29992; 8 &#23383;&#33410;&#65289;&#65292;&#32780; AdamW &#21017;&#38656;&#35201;&#26356;&#22810;&#30340;&#20869;&#23384;&#65288;&#27599;&#20010;&#21442;&#25968;&#21344;&#29992; 16 &#23383;&#33410;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29616;&#20195;&#30340; Vision Transformer &#21644; ConvNeXt &#27169;&#22411;&#19978;&#65292;&#20351;&#29992; AdamW &#36827;&#34892;&#24494;&#35843;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992; SGD&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#24494;&#35843;&#26799;&#24230;&#22312;&#31532;&#19968;&#20010;&#8220;&#23884;&#20837;&#8221;&#23618;&#20013;&#27604;&#20854;&#20313;&#27169;&#22411;&#30340;&#26799;&#24230;&#22823;&#24471;&#22810;&#26102;&#65292;SGD &#21644; AdamW &#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#65306;&#20923;&#32467;&#23884;&#20837;&#23618;&#65288;&#21442;&#25968;&#30340;&#19981;&#21040; 1%&#65289;&#65292;&#36825;&#26679; SGD&#65288;&#20855;&#26377;&#25110;&#19981;&#20855;&#26377;&#21160;&#37327;&#65289;&#30340;&#34920;&#29616;&#30053;&#20248;&#20110; AdamW&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#65288;&#20363;&#22914;&#65292;&#22312; ViT-L &#19978;&#65292;SGD &#20351;&#29992;&#30340; GPU &#20869;&#23384;&#36739;&#23569; 33%&#65289;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#23548;&#33268;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
SGD and AdamW are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we find that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first "embedding" layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1% of the parameters) leads to SGD with or without momentum performing slightly better than AdamW while using less memory (e.g., on ViT-L, SGD uses 33% less GPU memory). Our insights result in state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#27169;&#22411;&#30340;&#26032;&#25915;&#20987;&#65292;&#23637;&#31034;&#20102;&#23545;&#35895;&#27468;GBoard&#24212;&#29992;&#20013;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#36755;&#20837;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#21487;&#20197;&#34987;&#39640;&#20934;&#30830;&#24230;&#22320;&#24674;&#22797;&#65292;&#24341;&#21457;&#20102;&#38544;&#31169;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2210.16947</link><description>&lt;p&gt;
&#20004;&#20010;&#27169;&#22411;&#32988;&#36807;&#19968;&#20010;&#65306;&#32852;&#37030;&#23398;&#20064;&#23545;&#20110;&#35895;&#27468;GBoard&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#24182;&#19981;&#31169;&#23494;
&lt;/p&gt;
&lt;p&gt;
Two Models are Better than One: Federated Learning Is Not Private For Google GBoard Next Word Prediction. (arXiv:2210.16947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#27169;&#22411;&#30340;&#26032;&#25915;&#20987;&#65292;&#23637;&#31034;&#20102;&#23545;&#35895;&#27468;GBoard&#24212;&#29992;&#20013;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#36755;&#20837;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#21487;&#20197;&#34987;&#39640;&#20934;&#30830;&#24230;&#22320;&#24674;&#22797;&#65292;&#24341;&#21457;&#20102;&#38544;&#31169;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#27169;&#22411;&#30340;&#26032;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#35895;&#27468;GBoard&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#35895;&#27468;GBoard&#26159;&#19968;&#27454;&#24191;&#27867;&#20351;&#29992;&#30340;&#31227;&#21160;&#38190;&#30424;&#24212;&#29992;&#65292;&#26089;&#26399;&#37319;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#29983;&#20135;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#24674;&#22797;&#29992;&#25143;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36755;&#20837;&#30340;&#21333;&#35789;&#65292;&#20363;&#22914;&#21457;&#36865;&#30701;&#20449;&#26102;&#65292;&#32780;&#35832;&#22914;&#20351;&#29992;&#23567;&#25209;&#37327;&#21644;&#28155;&#21152;&#26412;&#22320;&#22122;&#38899;&#31561;&#23545;&#31574;&#26080;&#25928;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21487;&#20197;&#39640;&#24230;&#20934;&#30830;&#22320;&#37325;&#24314;&#21333;&#35789;&#39034;&#24207;&#65288;&#20174;&#32780;&#37325;&#24314;&#23454;&#38469;&#36755;&#20837;&#30340;&#21477;&#23376;&#65289;&#12290;&#36825;&#24341;&#21457;&#20102;&#26126;&#26174;&#30340;&#38544;&#31169;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;GBoard&#27491;&#22312;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present new attacks against federated learning when used to train natural language text models. We illustrate the effectiveness of the attacks against the next word prediction model used in Google's GBoard app, a widely used mobile keyboard app that has been an early adopter of federated learning for production use. We demonstrate that the words a user types on their mobile handset, e.g. when sending text messages, can be recovered with high accuracy under a wide range of conditions and that counter-measures such a use of mini-batches and adding local noise are ineffective. We also show that the word order (and so the actual sentences typed) can be reconstructed with high fidelity. This raises obvious privacy concerns, particularly since GBoard is in production use.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;HaarPooling&#25805;&#20316;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;HMPNet&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#36873;&#25321;HaarPooling&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13869</link><description>&lt;p&gt;
&#24102;&#26377;HaarPooling&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32593;&#32476;&#21943;&#27880;&#26631;&#35760;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A jet tagging algorithm of graph network with HaarPooling message passing. (arXiv:2210.13869v4 [hep-ex] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;HaarPooling&#25805;&#20316;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;HMPNet&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#36873;&#25321;HaarPooling&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#22270;&#34920;&#31034;&#30340;&#21943;&#27880;&#20107;&#20214;&#30340;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#20013;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;GNNs&#19982;HaarPooling&#25805;&#20316;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20107;&#20214;&#65292;&#31216;&#20043;&#20026;HaarPooling&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;HMPNet&#65289;&#12290;&#22312;HMPNet&#20013;&#65292;HaarPooling&#19981;&#20165;&#25552;&#21462;&#20102;&#22270;&#30340;&#29305;&#24449;&#65292;&#36824;&#23884;&#20837;&#20102;&#36890;&#36807;k-means&#23545;&#19981;&#21516;&#31890;&#23376;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#24471;&#21040;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#20116;&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#26500;&#24314;&#20102;HaarPooling&#65306;&#32477;&#23545;&#33021;&#37327;$\log E$&#65292;&#27178;&#21521;&#21160;&#37327;$\log p_T$&#65292;&#30456;&#23545;&#22352;&#26631;$(\Delta\eta,\Delta\phi)$&#65292;&#28151;&#21512;&#29305;&#24449;$(\log E, \log p_T)$&#21644;$(\log E, \log p_T, \Delta\eta,\Delta\phi)$&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#36873;&#25321;HaarPooling&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#30340;&#20934;&#30830;&#24615;&#65292;&#23558;$\log P_T$&#30340;&#39069;&#22806;&#20449;&#24687;&#28155;&#21152;&#21040;HMPNet&#20013;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently methods of graph neural networks (GNNs) have been applied to solving the problems in high energy physics (HEP) and have shown its great potential for quark-gluon tagging with graph representation of jet events. In this paper, we introduce an approach of GNNs combined with a HaarPooling operation to analyze the events, called HaarPooling Message Passing neural network (HMPNet). In HMPNet, HaarPooling not only extracts the features of graph, but embeds additional information obtained by clustering of k-means of different particle features. We construct Haarpooling from five different features: absolute energy $\log E$, transverse momentum $\log p_T$, relative coordinates $(\Delta\eta,\Delta\phi)$, the mixed ones $(\log E, \log p_T)$ and $(\log E, \log p_T, \Delta\eta,\Delta\phi)$. The results show that an appropriate selection of information for HaarPooling enhances the accuracy of quark-gluon tagging, as adding extra information of $\log P_T$ to the HMPNet outperforms all the o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;KGRL&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#30693;&#35782;&#31574;&#30053;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#30693;&#35782;&#37325;&#26032;&#25490;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.03729</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#31574;&#30053;&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning. (arXiv:2210.03729v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;KGRL&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#30693;&#35782;&#31574;&#30053;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#30693;&#35782;&#37325;&#26032;&#25490;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064; (RL) &#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#25509;&#36817;&#20154;&#31867;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#20154;&#31867;&#26159;&#20248;&#31168;&#30340;&#35266;&#23519;&#32773;&#65292;&#21487;&#20197;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#22806;&#37096;&#30693;&#35782;&#65288;&#21253;&#25324;&#20182;&#20154;&#22312;&#23581;&#35797;&#20219;&#21153;&#26102;&#30340;&#35266;&#23519;&#65289;&#26469;&#23398;&#20064;&#12290;&#20043;&#21069;&#22312;RL&#20013;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#22806;&#37096;&#30693;&#35782;&#31574;&#30053;&#32467;&#21512;&#21040;&#20195;&#29702;&#20013;&#65292;&#20197;&#24110;&#21161;&#20854;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25191;&#34892;&#20219;&#24847;&#32452;&#21512;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#20173;&#28982;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#36825;&#26159;&#27867;&#21270;&#21644;&#21487;&#36716;&#31227;&#24615;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#24341;&#23548;&#30340;RL(KGRL)&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#22810;&#20010;&#30693;&#35782;&#31574;&#30053;&#24182;&#26088;&#22312;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;RL&#33539;&#24335;&#12290;&#25105;&#20204;&#20026;KGRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28436;&#21592;&#26550;&#26500;&#65292;&#21363;&#30693;&#35782;&#21253;&#23481;&#24615;&#27880;&#24847;&#32593;&#32476;(KIAN)&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#22522;&#20110;&#23884;&#20837;&#30340;&#27880;&#24847;&#21147;&#34892;&#21160;&#39044;&#27979;&#23454;&#29616;&#20102;&#33258;&#30001;&#30340;&#30693;&#35782;&#37325;&#26032;&#25490;&#21015;&#12290;KIAN&#36824;&#35299;&#20915;&#20102;&#29109;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;&#26368;&#22823;&#29109;KGRL&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#20195;&#29702;&#30340;&#39640;&#25928;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents have long sought to approach the efficiency of human learning. Humans are great observers who can learn by aggregating external knowledge from various sources, including observations from others' policies of attempting a task. Prior studies in RL have incorporated external knowledge policies to help agents improve sample efficiency. However, it remains non-trivial to perform arbitrary combinations and replacements of those policies, an essential feature for generalization and transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL paradigm fusing multiple knowledge policies and aiming for human-like efficiency and flexibility. We propose a new actor architecture for KGRL, Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a problem arising in maximum entropy KGRL that hinders an agent from efficiently ex
&lt;/p&gt;</description></item><item><title>Rank-N-Contrast&#26159;&#19968;&#31181;&#23398;&#20064;&#36830;&#32493;&#34920;&#31034;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#22312;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#25490;&#21517;&#36827;&#34892;&#27604;&#36739;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.01189</link><description>&lt;p&gt;
Rank-N-Contrast:&#20026;&#22238;&#24402;&#38382;&#39064;&#23398;&#20064;&#36830;&#32493;&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-N-Contrast: Learning Continuous Representations for Regression. (arXiv:2210.01189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01189
&lt;/p&gt;
&lt;p&gt;
Rank-N-Contrast&#26159;&#19968;&#31181;&#23398;&#20064;&#36830;&#32493;&#34920;&#31034;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#22312;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#25490;&#21517;&#36827;&#34892;&#27604;&#36739;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#65292;&#27809;&#26377;&#26126;&#30830;&#24378;&#35843;&#22238;&#24402;&#24863;&#30693;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#34920;&#29616;&#20986;&#20998;&#25955;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#26679;&#26412;&#39034;&#24207;&#30340;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#24191;&#27867;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rank-N-Contrast&#65288;RNC&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#26679;&#26412;&#22312;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#25490;&#21517;&#36827;&#34892;&#27604;&#36739;&#26469;&#23398;&#20064;&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;RNC&#21487;&#20197;&#20445;&#35777;&#25152;&#23398;&#34920;&#31034;&#30340;&#39034;&#24207;&#19982;&#30446;&#26631;&#39034;&#24207;&#30456;&#31526;&#65292;&#19981;&#20165;&#24615;&#33021;&#26356;&#22909;&#65292;&#32780;&#19988;&#40065;&#26834;&#24615;&#12289;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#37117;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#20351;&#29992;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22238;&#24402;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;RNC&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#20854;&#20869;&#22312;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a regression-aware representation. Consequently, the learned representations exhibit fragmentation and fail to capture the continuous nature of sample orders, inducing suboptimal results across a wide range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a framework that learns continuous representations for regression by contrasting samples against each other based on their rankings in the target space. We demonstrate, theoretically and empirically, that RNC guarantees the desired order of learned representations in accordance with the target orders, enjoying not only better performance but also significantly improved robustness, efficiency, and generalization. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify that RNC achieves state-of-the-art performance, highlighting its intr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#20505;&#36873;&#31243;&#24207;&#21644;&#20505;&#36873;&#35859;&#35789;&#26469;&#39044;&#27979;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#25512;&#26029;&#20986;&#20851;&#20110;&#29983;&#25104;&#20195;&#30721;&#34892;&#20026;&#35299;&#37322;&#30340;&#26377;&#29992;&#35859;&#35789;&#12290;</title><link>http://arxiv.org/abs/2210.00848</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Toward Trustworthy Neural Program Synthesis. (arXiv:2210.00848v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00848
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#20505;&#36873;&#31243;&#24207;&#21644;&#20505;&#36873;&#35859;&#35789;&#26469;&#39044;&#27979;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#25512;&#26029;&#20986;&#20851;&#20110;&#29983;&#25104;&#20195;&#30721;&#34892;&#20026;&#35299;&#37322;&#30340;&#26377;&#29992;&#35859;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32534;&#31243;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#37319;&#26679;&#20505;&#36873;&#31243;&#24207;&#65292;&#21448;&#37319;&#26679;&#20505;&#36873;&#35859;&#35789;&#26469;&#25351;&#23450;&#31243;&#24207;&#30340;&#34892;&#20026;&#12290;&#36825;&#26679;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#33391;&#22909;&#26657;&#20934;&#31243;&#24207;&#27491;&#30830;&#24615;&#27010;&#29575;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#25512;&#26029;&#20986;&#29983;&#25104;&#20195;&#30721;&#34892;&#20026;&#35299;&#37322;&#20013;&#26377;&#29992;&#30340;&#35859;&#35789;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#23454;&#39564;&#20013;&#65292;&#20154;&#20204;&#26356;&#20559;&#22909;&#36825;&#20123;&#35859;&#35789;&#32780;&#19981;&#26159;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an approach to estimate the probability that a program sampled from a large language model is correct. Given a natural language description of a programming problem, our method samples both candidate programs as well as candidate predicates specifying how the program should behave. This allows learning a model that forms a well-calibrated probabilistic prediction of program correctness. Our system also infers which predicates are useful to explain the behavior of the generated code, and humans preferred these in a human study over raw language model outputs. Our method is simple, easy to implement, and maintains state of the art generation accuracy results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20013;&#39046;&#22495;&#29305;&#23450;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#26126;&#26174;&#20248;&#20110;&#20154;&#31867;&#35774;&#35745;&#30340;&#36890;&#29992;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.05598</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#39046;&#22495;&#29305;&#23450;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Learning domain-specific causal discovery from time series. (arXiv:2209.05598v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20013;&#39046;&#22495;&#29305;&#23450;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#26126;&#26174;&#20248;&#20110;&#20154;&#31867;&#35774;&#35745;&#30340;&#36890;&#29992;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#31070;&#32463;&#31185;&#23398;&#12289;&#21307;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22240;&#26524;&#21457;&#29616;&#30340;&#25216;&#26415;&#21253;&#25324;&#38543;&#26426;&#23454;&#39564;&#21644;&#22522;&#20110;&#26684;&#20848;&#26480;&#22240;&#26524;&#24615;&#12289;&#26465;&#20214;&#29420;&#31435;&#24615;&#12289;&#32467;&#26500;&#26041;&#31243;&#20197;&#21450;&#35780;&#20998;&#26041;&#27861;&#31561;&#31639;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#20154;&#31867;&#35774;&#35745;&#32773;&#20570;&#20986;&#24378;&#20551;&#35774;&#26102;&#25165;&#20934;&#30830;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#22240;&#26524;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MOS 6502&#24494;&#22788;&#29702;&#22120;&#12289;NetSim fMRI&#25968;&#25454;&#38598;&#21644;Dream3&#22522;&#22240;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20154;&#31867;&#35774;&#35745;&#30340;&#39046;&#22495;&#36890;&#29992;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#22914;&#20114;&#20449;&#24687;&#12289;VAR-LiNGAM&#21644;&#26684;&#20848;&#26480;&#22240;&#26524;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25913;&#36827;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#36827;&#34892;&#20102;&#32553;&#23567;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Johnson-Lindenstrauss&#23884;&#20837;&#30340;&#35266;&#23519;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#38598;&#21512;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#36924;&#36817;&#39640;&#32500;&#36830;&#32493;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2208.13305</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#24230;&#20013;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#20110;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neural Network Approximation of Continuous Functions in High Dimensions with Applications to Inverse Problems. (arXiv:2208.13305v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#36827;&#34892;&#20102;&#32553;&#23567;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Johnson-Lindenstrauss&#23884;&#20837;&#30340;&#35266;&#23519;&#65292;&#36890;&#36807;&#23558;&#39640;&#32500;&#38598;&#21512;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#36924;&#36817;&#39640;&#32500;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#36870;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#25512;&#21160;&#20102;&#23427;&#20204;&#22312;&#21307;&#23398;&#24433;&#20687;&#21040;&#22320;&#38663;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36870;&#38382;&#39064;&#30340;&#39640;&#32500;&#24230;&#20351;&#24471;&#29616;&#26377;&#29702;&#35770;&#26080;&#27861;&#35299;&#37322;&#22312;&#23454;&#36341;&#20013;&#20026;&#20160;&#20040;&#20351;&#29992;&#30475;&#20284;&#36739;&#23567;&#30340;&#32593;&#32476;&#20063;&#33021;&#24471;&#21040;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#32553;&#23567;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#30028;&#23450;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#39640;&#32500;&#38598;&#21512;&#19978;&#30340;H\"older&#65288;&#25110;&#22343;&#21248;&#65289;&#36830;&#32493;&#20989;&#25968;&#25152;&#38656;&#30340;&#22797;&#26434;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#32473;&#23450;&#19968;&#20010;&#39640;&#32500;&#38598;&#21512;$S\subset\mathbb{R}^D$&#65292;&#22914;&#26524;&#23384;&#22312;&#19968;&#20010;Johnson-Lindenstrauss&#23884;&#20837;$A\in\mathbb{R}^{d\times D}$&#65292;&#20854;&#20013;$d$&#36739;&#23567;&#65292;&#23558;$S$&#23884;&#20837;&#21040;&#19968;&#20010;&#20302;&#32500;&#31435;&#26041;&#20307;$[-M,M]^d$&#20013;&#65292;&#37027;&#20040;&#23545;&#20110;&#20219;&#20309;H\"o
&lt;/p&gt;
&lt;p&gt;
The remarkable successes of neural networks in a huge variety of inverse problems have fueled their adoption in disciplines ranging from medical imaging to seismic analysis over the past decade. However, the high dimensionality of such inverse problems has simultaneously left current theory, which predicts that networks should scale exponentially in the dimension of the problem, unable to explain why the seemingly small networks used in these settings work as well as they do in practice. To reduce this gap between theory and practice, we provide a general method for bounding the complexity required for a neural network to approximate a H\"older (or uniformly) continuous function defined on a high-dimensional set with a low-complexity structure. The approach is based on the observation that the existence of a Johnson-Lindenstrauss embedding $A\in\mathbb{R}^{d\times D}$ of a given high-dimensional set $S\subset\mathbb{R}^D$ into a low dimensional cube $[-M,M]^d$ implies that for any H\"o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#32452;&#20998;&#39044;&#27979;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#25506;&#31350;&#20102;&#22810;&#27425;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#21644;&#23454;&#39564;&#35777;&#23454;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#39118;&#38505;&#26354;&#32447;&#21487;&#20197;&#21576;&#29616;&#20986;&#19977;&#27425;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2208.09897</link><description>&lt;p&gt;
&#22810;&#20010;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#20013;&#30340;&#22810;&#27425;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Multiple Descent in the Multiple Random Feature Model. (arXiv:2208.09897v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#32452;&#20998;&#39044;&#27979;&#27169;&#22411;&#20013;&#36827;&#19968;&#27493;&#25506;&#31350;&#20102;&#22810;&#27425;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#21644;&#23454;&#39564;&#35777;&#23454;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#39118;&#38505;&#26354;&#32447;&#21487;&#20197;&#21576;&#29616;&#20986;&#19977;&#27425;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#23613;&#31649;&#36825;&#19968;&#29616;&#35937;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20294;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#32452;&#20998;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#22810;&#27425;&#19979;&#38477;&#29616;&#35937;&#12290;&#39318;&#20808;&#32771;&#34385;&#19968;&#20010;&#8220;&#21452;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#8221; (DRFM)&#65292;&#23427;&#36830;&#25509;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#38543;&#26426;&#29305;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;DRFM&#22312;&#23725;&#22238;&#24402;&#20013;&#23454;&#29616;&#30340;&#36807;&#24230;&#39118;&#38505;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#39640;&#32500;&#26694;&#26550;&#19979;&#65292;&#24403;&#35757;&#32451;&#26679;&#26412;&#37327;&#12289;&#25968;&#25454;&#32500;&#24230;&#21644;&#38543;&#26426;&#29305;&#24449;&#32500;&#24230;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#36807;&#24230;&#39118;&#38505;&#30340;&#31934;&#30830;&#26497;&#38480;&#12290;&#22522;&#20110;&#36825;&#20010;&#35745;&#31639;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;DRFM&#30340;&#39118;&#38505;&#26354;&#32447;&#21487;&#33021;&#21576;&#29616;&#20986;&#19977;&#27425;&#19979;&#38477;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#8220;&#22810;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#8221; (MRFM)&#65292;&#24182;&#23637;&#31034;&#20102;MRFMs&#38598;&#25104;K&#31181;&#31867;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated a double descent phenomenon in over-parameterized learning. Although this phenomenon has been investigated by recent works, it has not been fully understood in theory. In this paper, we investigate the multiple descent phenomenon in a class of multi-component prediction models. We first consider a ''double random feature model'' (DRFM) concatenating two types of random features, and study the excess risk achieved by the DRFM in ridge regression. We calculate the precise limit of the excess risk under the high dimensional framework where the training sample size, the dimension of data, and the dimension of random features tend to infinity proportionally. Based on the calculation, we further theoretically demonstrate that the risk curves of DRFMs can exhibit triple descent. We then provide a thorough experimental study to verify our theory. At last, we extend our study to the ''multiple random feature model'' (MRFM), and show that MRFMs ensembling $K$ types
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#39044;&#27979;&#32858;&#21512;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#24212;&#29992;&#20110;&#19987;&#23478;&#21629;&#20013;&#21518;&#30340;&#20449;&#21495;&#27719;&#32858;&#12290;&#35770;&#25991;&#20013;&#25552;&#20379;&#20102;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#34920;&#26126;&#35813;&#22797;&#26434;&#24230;&#33267;&#23569;&#20026; $\tilde \Omega(m^{n-2} / \varepsilon)$&#65292;&#20854;&#20013; m &#26159;&#27599;&#20010;&#19987;&#23478;&#20449;&#21495;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2207.13126</link><description>&lt;p&gt;
&#39044;&#27979;&#32858;&#21512;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Forecast Aggregation. (arXiv:2207.13126v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#39044;&#27979;&#32858;&#21512;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#24212;&#29992;&#20110;&#19987;&#23478;&#21629;&#20013;&#21518;&#30340;&#20449;&#21495;&#27719;&#32858;&#12290;&#35770;&#25991;&#20013;&#25552;&#20379;&#20102;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#34920;&#26126;&#35813;&#22797;&#26434;&#24230;&#33267;&#23569;&#20026; $\tilde \Omega(m^{n-2} / \varepsilon)$&#65292;&#20854;&#20013; m &#26159;&#27599;&#20010;&#19987;&#23478;&#20449;&#21495;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#39044;&#27979;&#32858;&#21512;&#27169;&#22411;&#65292;&#26377; n &#20010;&#19987;&#23478;&#26681;&#25454;&#26410;&#30693;&#20108;&#20803;&#20107;&#20214;&#30340;&#31169;&#26377;&#20449;&#21495;&#25253;&#21578;&#20107;&#20214;&#30340;&#21518;&#39564;&#20449;&#24565;&#32473;&#36127;&#36131;&#20154;&#65292;&#38543;&#21518;&#35813;&#36127;&#36131;&#20154;&#23558;&#25253;&#21578;&#32858;&#21512;&#20026;&#23545;&#35813;&#20107;&#20214;&#30340;&#21333;&#20010;&#39044;&#27979;&#12290;&#19987;&#23478;&#30340;&#20449;&#21495;&#21644;&#20107;&#20214;&#30340;&#32467;&#26524;&#26381;&#20174;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23545;&#20110;&#36127;&#36131;&#20154;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#36127;&#36131;&#20154;&#21487;&#20197;&#20174;&#20998;&#24067;&#20013;&#24471;&#21040; i.i.d.&#8220;&#26679;&#26412;&#8221;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#30001;&#19987;&#23478;&#30340;&#25253;&#21578;&#65288;&#19981;&#26159;&#20449;&#21495;&#65289;&#21644;&#20107;&#20214;&#30340;&#23454;&#29616;&#32452;&#25104;&#30340;&#20803;&#32452;&#12290;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#65292;&#36127;&#36131;&#20154;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010; $\varepsilon$-&#26368;&#20248;&#32858;&#21512;&#22120;&#65292;&#20854;&#20013;&#26368;&#20248;&#24615;&#26159;&#20197;&#32858;&#21512;&#39044;&#27979;&#19982;&#20107;&#20214;&#23454;&#29616;&#20043;&#38388;&#30340;&#39044;&#26399;&#24179;&#26041;&#36317;&#31163;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#24847;&#31163;&#25955;&#20998;&#24067;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#33267;&#23569;&#26159; $\tilde \Omega(m^{n-2} / \varepsilon)$&#65292;&#20854;&#20013; m &#26159;&#27599;&#20010;&#19987;&#23478;&#20449;&#21495;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a Bayesian forecast aggregation model where $n$ experts, after observing private signals about an unknown binary event, report their posterior beliefs about the event to a principal, who then aggregates the reports into a single prediction for the event. The signals of the experts and the outcome of the event follow a joint distribution that is unknown to the principal, but the principal has access to i.i.d. "samples" from the distribution, where each sample is a tuple of the experts' reports (not signals) and the realization of the event. Using these samples, the principal aims to find an $\varepsilon$-approximately optimal aggregator, where optimality is measured in terms of the expected squared distance between the aggregated prediction and the realization of the event. We show that the sample complexity of this problem is at least $\tilde \Omega(m^{n-2} / \varepsilon)$ for arbitrary discrete distributions, where $m$ is the size of each expert's signal space. This sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2206.10397</link><description>&lt;p&gt;
&#40065;&#26834;&#39134;&#34892;&#25511;&#21046;&#30340;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21644;&#24212;&#23545;&#24178;&#25200;&#23545;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#36890;&#24120;&#38656;&#35201;&#23545;&#29305;&#23450;&#39134;&#34892;&#22330;&#26223;&#36827;&#34892;&#22823;&#37327;&#35843;&#25972;&#65292;&#25110;&#32773;&#32463;&#36807;&#24191;&#27867;&#30340;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#35757;&#32451;&#65292;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#30001;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;MHE&#20272;&#35745;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#23558;MHE&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#26080;&#32541;&#34701;&#21512;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#36882;&#24402;&#24418;&#24335;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#26799;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#31070;&#32463;&#35843;&#33410;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#33410;&#38543;&#26426;&#26435;&#37325;&#30340;&#27963;&#21160;&#26469;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#23398;&#20064;&#34920;&#31034;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.04297</link><description>&lt;p&gt;
&#23398;&#20064;&#35843;&#33410;&#38543;&#26426;&#26435;&#37325;&#65306;&#21463;&#31070;&#32463;&#35843;&#33410;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Modulate Random Weights: Neuromodulation-inspired Neural Networks For Efficient Continual Learning. (arXiv:2204.04297v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#31070;&#32463;&#35843;&#33410;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#33410;&#38543;&#26426;&#26435;&#37325;&#30340;&#27963;&#21160;&#26469;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#23398;&#20064;&#34920;&#31034;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#26041;&#27861;&#12289;&#22238;&#25918;&#32531;&#20914;&#21306;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#24517;&#39035;&#19981;&#20165;&#32771;&#34385;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#36824;&#35201;&#32771;&#34385;&#35745;&#31639;&#25928;&#29575;&#21644;&#36816;&#34892;&#26102;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#20013;&#30340;&#31070;&#32463;&#35843;&#33410;&#21551;&#21457;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#32463;&#27982;&#39640;&#25928;&#22320;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20026;&#35299;&#37322;&#23398;&#20064;&#34920;&#31034;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#31070;&#32463;&#35843;&#33410;&#26159;&#19968;&#31181;&#29983;&#29289;&#26426;&#21046;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65307;&#23427;&#20197;&#23454;&#26102;&#21160;&#24577;&#25511;&#21046;&#21644;&#24494;&#35843;&#31361;&#35302;&#21160;&#21147;&#23398;&#26469;&#36319;&#36394;&#19981;&#21516;&#34892;&#20026;&#32972;&#26223;&#30340;&#38656;&#27714;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#29615;&#22659;&#19979;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#19968;&#32452;&#21442;&#25968;&#65292;&#36825;&#20123;&#21442;&#25968;&#23545;&#36716;&#25442;&#36755;&#20837;&#30340;&#19981;&#21464;&#30340;&#12289;&#38543;&#26426;&#21270;&#30340;&#26435;&#37325;&#27963;&#21160;&#36827;&#34892;\emph{&#31070;&#32463;&#35843;&#33410;}&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Continual Learning (CL) approaches have focused on addressing catastrophic forgetting by leveraging regularization methods, replay buffers, and task-specific components. However, realistic CL solutions must be shaped not only by metrics of catastrophic forgetting but also by computational efficiency and running time. Here, we introduce a novel neural network architecture inspired by neuromodulation in biological nervous systems to economically and efficiently address catastrophic forgetting and provide new avenues for interpreting learned representations. Neuromodulation is a biological mechanism that has received limited attention in machine learning; it dynamically controls and fine tunes synaptic dynamics in real time to track the demands of different behavioral contexts. Inspired by this, our proposed architecture learns a relatively small set of parameters per task context that \emph{neuromodulates} the activity of unchanging, randomized weights that transform the input. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23545;&#25239;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#23545;&#25968;&#27744;&#21270;&#26041;&#27861;&#23398;&#20064;&#19987;&#23478;&#26435;&#37325;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#26080;&#36951;&#25022;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2202.11219</link><description>&lt;p&gt;
&#26080;&#36951;&#25022;&#23398;&#20064;&#19982;&#26080;&#30028;&#25439;&#22833;&#65306;&#23545;&#25968;&#27744;&#21270;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling. (arXiv:2202.11219v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23545;&#25239;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#23545;&#25968;&#27744;&#21270;&#26041;&#27861;&#23398;&#20064;&#19987;&#23478;&#26435;&#37325;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#26080;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;$T$&#20013;&#65292;$m$&#20010;&#19987;&#23478;&#25253;&#21578;&#20102;&#20851;&#20110;$n$&#20010;&#32467;&#26524;&#30340;&#27010;&#29575;&#20998;&#24067;&#65307;&#25105;&#20204;&#24076;&#26395;&#23398;&#20064;&#19968;&#31181;&#32858;&#21512;&#36825;&#20123;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#26080;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#34987;&#31216;&#20026;&#23545;&#25968;&#27744;&#21270;&#30340;&#22522;&#26412;&#21644;&#23454;&#29992;&#30340;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;log odds &#30340;&#21152;&#26435;&#24179;&#22343;&#65292;&#23427;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#19968;&#31181;&#26368;&#20248;&#30340;&#27744;&#21270;&#26041;&#27861;&#36873;&#25321;&#65292;&#22914;&#26524;&#25105;&#20204;&#24076;&#26395;&#26368;&#23567;&#21270; log &#25439;&#22833;&#65288;&#20316;&#20026;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#23545;&#25239;&#29615;&#22659;&#20013;&#23398;&#20064;&#26368;&#20339;&#21442;&#25968;&#38598;&#65288;&#21363;&#19987;&#23478;&#26435;&#37325;&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#65288;&#24517;&#35201;&#26465;&#20214;&#19979;&#65289;&#65292;&#23545;&#25239;&#36873;&#25321;&#30340;&#32467;&#26524;&#21644;&#39044;&#27979;&#26159;&#19968;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#19987;&#23478;&#25253;&#21578;&#20102;&#32463;&#36807;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;&#26045;&#21152;&#36825;&#20010;&#32422;&#26463;&#26465;&#20214;&#21019;&#24314;&#20102;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#26032;&#39062;&#30340;&#21322;&#23545;&#25239;&#35774;&#32622;&#65292;&#20854;&#20013;&#23545;&#25163;&#20445;&#30041;&#20102;&#22823;&#37327;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#20197;&#19968;&#31181;&#23398;&#20064;&#19987;&#23478;&#26435;&#37325;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;$O(\s
&lt;/p&gt;
&lt;p&gt;
For each of $T$ time steps, $m$ experts report probability distributions over $n$ outcomes; we wish to learn to aggregate these forecasts in a way that attains a no-regret guarantee. We focus on the fundamental and practical aggregation method known as logarithmic pooling -- a weighted average of log odds -- which is in a certain sense the optimal choice of pooling method if one is interested in minimizing log loss (as we take to be our loss function). We consider the problem of learning the best set of parameters (i.e. expert weights) in an online adversarial setting. We assume (by necessity) that the adversarial choices of outcomes and forecasts are consistent, in the sense that experts report calibrated forecasts. Imposing this constraint creates a (to our knowledge) novel semi-adversarial setting in which the adversary retains a large amount of flexibility. In this setting, we present an algorithm based on online mirror descent that learns expert weights in a way that attains $O(\s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24102;&#26399;&#26395;&#32422;&#26463;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#20381;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2202.07868</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24102;&#26399;&#26395;&#32422;&#26463;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Minimax Optimization with Expectation Constraints. (arXiv:2202.07868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24102;&#26399;&#26395;&#32422;&#26463;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#20381;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#38469;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#21253;&#25324;&#33879;&#21517;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#20869;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#26041;&#27861;&#30340;&#20851;&#27880;&#24230;&#26174;&#33879;&#22686;&#21152;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;&#25968;&#25454;&#39537;&#21160;&#32422;&#26463;&#65292;&#22240;&#20026;&#36825;&#20123;&#30828;&#24615;&#32422;&#26463;&#23450;&#20041;&#30340;&#21487;&#34892;&#38598;&#19978;&#30340;&#25237;&#24433;&#35745;&#31639;&#25361;&#25112;&#24040;&#22823;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#38750;&#20809;&#28369;&#20984;&#20985;&#38543;&#26426;&#26497;&#23567;&#26497;&#22823;&#26694;&#26550;&#65292;&#24182;&#23558;&#25968;&#25454;&#39537;&#21160;&#32422;&#26463;&#24418;&#24335;&#21270;&#20026;&#26399;&#26395;&#32422;&#26463;&#12290;&#26497;&#23567;&#26497;&#22823;&#26399;&#26395;&#32422;&#26463;&#38382;&#39064;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#20108;&#20154;&#38646;&#21644;&#21338;&#24328;&#21644;&#25968;&#25454;&#39537;&#21160;&#40065;&#26834;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#39640;&#25928;&#30340;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#26469;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#26399;&#26395;&#32422;&#26463;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;$\mathcal{O}(\frac{1}{\sqrt{N}})$&#30340;&#26368;&#20248;&#36895;&#29575;&#25910;&#25947;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#26469;&#35777;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#38469;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention to data-driven optimization approaches, including the well-known stochastic gradient descent method, has grown significantly over recent decades, but data-driven constraints have rarely been studied, because of the computational challenges of projections onto the feasible set defined by these hard constraints. In this paper, we focus on the non-smooth convex-concave stochastic minimax regime and formulate the data-driven constraints as expectation constraints. The minimax expectation constrained problem subsumes a broad class of real-world applications, including two-player zero-sum game and data-driven robust optimization. We propose a class of efficient primal-dual algorithms to tackle the minimax expectation-constrained problem, and show that our algorithms converge at the optimal rate of $\mathcal{O}(\frac{1}{\sqrt{N}})$. We demonstrate the practical efficiency of our algorithms by conducting numerical experiments on large-scale real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20849;&#35782;&#20998;&#25955;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#20165;&#21462;&#20915;&#20110;&#20840;&#23616;&#30446;&#26631;&#30340;&#24378;&#20984;&#24615;&#65292;&#24182;&#19981;&#35201;&#27714;&#23616;&#37096;&#20989;&#25968;&#26159;&#20984;&#20989;&#25968;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2005.00797</link><description>&lt;p&gt;
&#22810;&#20849;&#35782;&#20998;&#25955;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Multi-consensus Decentralized Accelerated Gradient Descent. (arXiv:2005.00797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20849;&#35782;&#20998;&#25955;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#20165;&#21462;&#20915;&#20110;&#20840;&#23616;&#30446;&#26631;&#30340;&#24378;&#20984;&#24615;&#65292;&#24182;&#19981;&#35201;&#27714;&#23616;&#37096;&#20989;&#25968;&#26159;&#20984;&#20989;&#25968;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#25955;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#12289;&#20256;&#24863;&#22120;&#32593;&#32476;&#21644;&#25511;&#21046;&#29702;&#35770;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#23545;&#20110;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#32473;&#20986;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#20840;&#23616;&#26465;&#20214;&#25968;&#30456;&#20851;&#32780;&#19981;&#26159;&#23616;&#37096;&#26465;&#20214;&#25968;&#30456;&#20851;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#30340;(lower bound)&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#20165;&#21462;&#20915;&#20110;&#20840;&#23616;&#30446;&#26631;&#30340;&#24378;&#20984;&#24615;&#65292;&#24182;&#19981;&#35201;&#27714;&#23616;&#37096;&#20989;&#25968;&#26159;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#20381;&#36182;&#20110;Nesterov &#21152;&#36895;&#12289;&#22810;&#20849;&#35782;&#21644;&#26799;&#24230;&#36861;&#36394;&#31561;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#30340;&#21019;&#26032;&#25972;&#21512;&#12290;&#23454;&#35777;&#30740;&#31350;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the decentralized convex optimization problem, which has a wide range of applications in large-scale machine learning, sensor networks, and control theory. We propose novel algorithms that achieve optimal computation complexity and near optimal communication complexity. Our theoretical results give affirmative answers to the open problem on whether there exists an algorithm that can achieve a communication complexity (nearly) matching the lower bound depending on the global condition number instead of the local one. Furthermore, the linear convergence of our algorithms only depends on the strong convexity of global objective and it does \emph{not} require the local functions to be convex. The design of our methods relies on a novel integration of well-known techniques including Nesterov's acceleration, multi-consensus and gradient-tracking. Empirical studies show the outperformance of our methods for machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25104;&#26412;&#24863;&#30693;&#25506;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#31232;&#30095;&#22870;&#21169;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#25628;&#32034;&#21160;&#24577;&#23376;&#30446;&#26631;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/1910.09143</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21160;&#24577;&#23376;&#30446;&#26631;&#23548;&#21521;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dynamic Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.09143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25104;&#26412;&#24863;&#30693;&#25506;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#31232;&#30095;&#22870;&#21169;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#25628;&#32034;&#21160;&#24577;&#23376;&#30446;&#26631;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22870;&#21169;&#23548;&#33322;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#26114;&#36149;&#21644;&#26377;&#38480;&#30340;&#20132;&#20114;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#38024;&#23545;&#38656;&#35201;&#29616;&#23454;&#19990;&#30028;&#35757;&#32451;&#30340;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#65288;&#24403;&#24265;&#20215;&#27169;&#25311;&#22120;&#19981;&#21487;&#29992;&#26102;&#65289;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#38754;&#20020;&#26410;&#30693;&#29615;&#22659;&#20998;&#24067;&#30340;&#20195;&#29702;&#65292;&#24182;&#19988;&#24517;&#39035;&#20915;&#23450;&#19968;&#31181;&#25506;&#32034;&#31574;&#30053;&#12290;&#22312;&#20174;&#30456;&#21516;&#29615;&#22659;&#20998;&#24067;&#20013;&#36873;&#25321;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#20043;&#21069;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#19968;&#31995;&#21015;&#35757;&#32451;&#29615;&#22659;&#26469;&#25913;&#36827;&#20854;&#31574;&#30053;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20851;&#27880;&#22266;&#23450;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#32780;&#23558;&#25506;&#32034;&#35270;&#20026;&#20803;&#20248;&#21270;&#38382;&#39064;&#30340;&#23569;&#25968;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#25104;&#26412;&#39640;&#25928;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25628;&#32034;&#19968;&#31867;&#22522;&#20110;&#21160;&#24577;&#23376;&#30446;&#26631;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#35843;&#25972;&#22810;&#20010;&#26464;&#26438;--&#23376;&#30446;&#26631;&#30340;&#20301;&#32622;&#65292;&#27599;&#20010;episode&#30340;&#38271;&#24230;&#20197;&#21450;nu&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the nu
&lt;/p&gt;</description></item></channel></rss>