<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24341;&#20837;&#39069;&#22806;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#36890;&#36807;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#38477;&#20302;&#24615;&#33021;&#65292;&#21363;&#20351;&#28155;&#21152;&#30340;&#25968;&#25454;&#20351;&#35757;&#32451;&#20998;&#24067;&#26356;&#25509;&#36817;&#27979;&#35797;&#20998;&#24067;&#12290;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#21307;&#38498;&#29305;&#23450;&#30340;&#22270;&#20687;&#20266;&#20687;&#24341;&#36215;&#30340;&#12290;&#36825;&#19968;&#32467;&#26524;&#25361;&#25112;&#20102;&#24120;&#35265;&#30340;&#35748;&#20026;&#26356;&#22810;&#25968;&#25454;&#33021;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#35266;&#24565;&#12290;</title><link>http://arxiv.org/abs/2308.04431</link><description>&lt;p&gt;
&#24403;&#36234;&#22810;&#36234;&#23569;&#65306;&#24341;&#20837;&#39069;&#22806;&#25968;&#25454;&#38598;&#21487;&#33021;&#36890;&#36807;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#38477;&#20302;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations. (arXiv:2308.04431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04431
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39069;&#22806;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#36890;&#36807;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#38477;&#20302;&#24615;&#33021;&#65292;&#21363;&#20351;&#28155;&#21152;&#30340;&#25968;&#25454;&#20351;&#35757;&#32451;&#20998;&#24067;&#26356;&#25509;&#36817;&#27979;&#35797;&#20998;&#24067;&#12290;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#21307;&#38498;&#29305;&#23450;&#30340;&#22270;&#20687;&#20266;&#20687;&#24341;&#36215;&#30340;&#12290;&#36825;&#19968;&#32467;&#26524;&#25361;&#25112;&#20102;&#24120;&#35265;&#30340;&#35748;&#20026;&#26356;&#22810;&#25968;&#25454;&#33021;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#35266;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#24341;&#20837;&#26356;&#22810;&#25968;&#25454;&#36890;&#24120;&#34987;&#35270;&#20026;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#38752;&#31574;&#30053;&#65307;&#26412;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#19968;&#35266;&#24565;&#65292;&#36890;&#36807;&#35777;&#26126;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24341;&#20837;&#22806;&#37096;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#25439;&#23475;&#25152;&#24471;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#24320;&#28304;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#21644;9&#20010;&#19981;&#21516;&#26631;&#31614;&#30340;&#32452;&#21512;&#36827;&#34892;&#22823;&#35268;&#27169;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;43%&#30340;&#35774;&#32622;&#20013;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#21307;&#38498;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#21307;&#38498;&#19978;&#30340;&#26368;&#24046;&#32452;&#20934;&#30830;&#29575;&#37117;&#27604;&#20351;&#29992;&#20004;&#20010;&#21307;&#38498;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#35201;&#20302;&#12290;&#21363;&#20351;&#28155;&#21152;&#30340;&#21307;&#38498;&#20351;&#35757;&#32451;&#20998;&#24067;&#26356;&#25509;&#36817;&#27979;&#35797;&#20998;&#24067;&#65292;&#36825;&#19968;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#20173;&#20250;&#20986;&#29616;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#19982;&#30142;&#30149;&#21644;&#21307;&#38498;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#20135;&#29983;&#65292;&#36825;&#26159;&#30001;&#20110;&#21307;&#38498;&#29305;&#23450;&#30340;&#22270;&#20687;&#20266;&#20687;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#22810;&#20010;&#25968;&#25454;&#38598;&#26102;&#25152;&#36935;&#21040;&#30340;&#25240;&#34935;&#21462;&#33293;&#65292;&#21363;&#39069;&#22806;&#25968;&#25454;&#30340;&#26126;&#26174;&#22909;&#22788;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#24341;&#20837;&#30340;&#24615;&#33021;&#25439;&#22833;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, incorporating more data is often seen as a reliable strategy for improving model performance; this work challenges that notion by demonstrating that the addition of external datasets in many cases can hurt the resulting model's performance. In a large-scale empirical study across combinations of four different open-source chest x-ray datasets and 9 different labels, we demonstrate that in 43% of settings, a model trained on data from two hospitals has poorer worst group accuracy over both hospitals than a model trained on just a single hospital's data. This surprising result occurs even though the added hospital makes the training distribution more similar to the test distribution. We explain that this phenomenon arises from the spurious correlation that emerges between the disease and hospital, due to hospital-specific image artifacts. We highlight the trade-off one encounters when training on multiple datasets, between the obvious benefit of additional data and i
&lt;/p&gt;</description></item><item><title>SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04430</link><description>&lt;p&gt;
SILO&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#38750;&#21442;&#25968;&#21270;&#25968;&#25454;&#23384;&#20648;&#20013;&#38548;&#31163;&#27861;&#24459;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04430
&lt;/p&gt;
&lt;p&gt;
SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#22312;&#21463;&#29256;&#26435;&#25110;&#21463;&#20854;&#20182;&#38480;&#21046;&#30340;&#25968;&#25454;&#19978;&#30340;&#21512;&#27861;&#24615;&#36827;&#34892;&#28608;&#28872;&#36777;&#35770;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#22312;&#20302;&#39118;&#38505;&#25991;&#26412;&#65288;&#20363;&#22914;&#36807;&#26399;&#29256;&#26435;&#22270;&#20070;&#25110;&#25919;&#24220;&#25991;&#20214;&#65289;&#19978;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#35813;&#25991;&#26412;&#30340;&#35268;&#27169;&#21644;&#39046;&#22495;&#35206;&#30422;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SILO&#65292;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#31181;&#39118;&#38505;-&#24615;&#33021;&#26435;&#34913;&#12290;SILO&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#26500;&#24314;&#65306;&#65288;1&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#26032;&#35821;&#26009;&#24211;&#8220;&#24320;&#25918;&#35768;&#21487;&#35777;&#35821;&#26009;&#24211;&#8221;&#65288;OLC&#65289;&#19978;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;LM&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;228B&#20010;&#20844;&#20849;&#39046;&#22495;&#21644;&#35768;&#21487;&#25991;&#26412;&#12290;&#65288;2&#65289;&#36890;&#36807;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#65288;&#20363;&#22914;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20070;&#25110;&#26032;&#38395;&#30340;&#25968;&#25454;&#65289;&#23545;&#20854;&#36827;&#34892;&#25193;&#20805;&#65292;&#35813;&#25968;&#25454;&#23384;&#20648;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#34987;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#23384;&#20648;&#20801;&#35768;&#20351;&#29992;&#39640;&#39118;&#38505;&#25968;&#25454;&#32780;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#25903;&#25345;&#21477;&#32423;&#25968;&#25454;&#24402;&#23646;&#65292;&#24182;&#20351;&#25968;&#25454;&#29983;&#20135;&#32773;&#21487;&#20197;&#36890;&#36807;&#20174;&#23384;&#20648;&#20013;&#21024;&#38500;&#20869;&#23481;&#26469;&#36873;&#25321;&#36864;&#20986;&#27169;&#22411;&#12290;&#36825;&#20123;&#21151;&#33021;&#21487;&#20197;&#20419;&#36827;&#23545;&#25968;&#25454;&#20351;&#29992;&#35268;&#33539;&#30340;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;
The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04428</link><description>&lt;p&gt;
&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#20803;&#23398;&#20064;&#25805;&#20316;&#31526;&#21040;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Operators to Optimality from Multi-Task Non-IID Data. (arXiv:2308.04428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#22810;&#20219;&#21153;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#24674;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#36817;&#21462;&#24471;&#36827;&#23637;&#30340;&#19968;&#20010;&#24378;&#22823;&#27010;&#24565;&#26159;&#20174;&#24322;&#26500;&#26469;&#28304;&#25110;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#20849;&#21516;&#29305;&#24449;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#23558;&#25152;&#26377;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20849;&#21516;&#30340;&#34920;&#31034;&#20989;&#25968;&#65292;&#26082;&#26377;&#21161;&#20110;&#35745;&#31639;&#25928;&#29575;&#65292;&#21448;&#26377;&#21161;&#20110;&#32479;&#35745;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20943;&#23569;&#35201;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20026;&#20102;&#22312;&#29702;&#35770;&#19978;&#20570;&#20986;&#36825;&#20123;&#20248;&#28857;&#30340;&#26681;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22122;&#22768;&#21521;&#37327;&#27979;&#37327;$y = Mx + w$&#20013;&#22238;&#22797;&#32447;&#24615;&#25805;&#20316;&#31526;$M$&#30340;&#19968;&#33324;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#21327;&#21464;&#37327;$x$&#26082;&#21487;&#20197;&#26159;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#21508;&#21521;&#21516;&#24615;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20250;&#23545;&#34920;&#31034;&#26356;&#26032;&#36896;&#25104;&#20559;&#24046;&#65292;&#36825;&#23548;&#33268;&#22122;&#22768;&#39033;&#30340;&#32553;&#25918;&#19981;&#20877;&#26377;&#21033;&#20110;&#28304;&#20219;&#21153;&#25968;&#37327;&#12290;&#36825;&#21453;&#36807;&#26469;&#20250;&#23548;&#33268;&#34920;&#31034;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21463;&#21040;&#21333;&#20219;&#21153;&#25968;&#25454;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026;&#21435;&#20559;&#24046;&#21644;&#29305;&#24449;&#30333;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias &amp; Feature-Whiten}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#33258;&#21160;&#26816;&#27979;&#21476;&#20195;&#30707;&#30865;&#34920;&#38754;&#30340;&#33258;&#28982;&#30772;&#22351;&#21644;&#20154;&#20026;&#25439;&#22351;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21487;&#20197;&#20840;&#38754;&#26816;&#27979;&#21508;&#31181;&#19981;&#21487;&#39044;&#27979;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.04426</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#21476;&#20195;&#30707;&#30865;&#34920;&#38754;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces. (arXiv:2308.04426v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#33258;&#21160;&#26816;&#27979;&#21476;&#20195;&#30707;&#30865;&#34920;&#38754;&#30340;&#33258;&#28982;&#30772;&#22351;&#21644;&#20154;&#20026;&#25439;&#22351;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21487;&#20197;&#20840;&#38754;&#26816;&#27979;&#21508;&#31181;&#19981;&#21487;&#39044;&#27979;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#26816;&#27979;&#21476;&#20195;&#30707;&#30865;&#34920;&#38754;&#30340;&#33258;&#28982;&#30772;&#22351;&#21644;&#20154;&#20026;&#25439;&#22351;&#26159;&#20854;&#39044;&#38450;&#24615;&#20445;&#25252;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#25991;&#21270;&#36951;&#20135;&#20445;&#25252;&#26041;&#27861;&#26080;&#27861;&#23436;&#32654;&#22320;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22240;&#20026;&#24456;&#38590;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#12289;&#21450;&#26102;&#24615;&#21644;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#23454;&#26102;&#33258;&#21160;&#26816;&#27979;&#21476;&#20195;&#30707;&#30865;&#19978;&#36848;&#32039;&#24613;&#24773;&#20917;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#19981;&#38656;&#35201;&#22823;&#37327;&#24322;&#24120;&#26679;&#26412;&#30340;&#26041;&#24335;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#39044;&#27979;&#24322;&#24120;&#30340;&#20840;&#38754;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#30417;&#27979;&#12289;&#25968;&#25454;&#37319;&#38598;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#26500;&#24314;&#21644;&#21518;&#22788;&#29702;&#31561;&#38454;&#27573;&#12290;&#20197;&#40857;&#38376;&#30707;&#31391;&#30340;&#30707;&#30865;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;AE&#21644;GAN&#26550;&#26500;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#37325;&#26500;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of natural deterioration and man-made damage on the surfaces of ancient stele in the first instance is essential for their preventive conservation. Existing methods for cultural heritage preservation are not able to achieve this goal perfectly due to the difficulty of balancing accuracy, efficiency, timeliness, and cost. This paper presents a deep-learning method to automatically detect above mentioned emergencies on ancient stone stele in real time, employing autoencoder (AE) and generative adversarial network (GAN). The proposed method overcomes the limitations of existing methods by requiring no extensive anomaly samples while enabling comprehensive detection of unpredictable anomalies. the method includes stages of monitoring, data acquisition, pre-processing, model structuring, and post-processing. Taking the Longmen Grottoes' stone steles as a case study, an unsupervised learning model based on AE and GAN architectures is proposed and validated with a reconstru
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;LSTM&#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#32929;&#31080;&#26410;&#26469;&#20215;&#26684;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04419</link><description>&lt;p&gt;
&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#65306;&#22522;&#20110;&#28151;&#21512;LSTM&#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach. (arXiv:2308.04419v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;LSTM&#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#32929;&#31080;&#26410;&#26469;&#20215;&#26684;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#24066;&#26159;&#26368;&#20196;&#20154;&#30528;&#36855;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#19968;&#65292;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21487;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#22312;&#27491;&#30830;&#30340;&#26102;&#38388;&#20570;&#20986;&#26368;&#20339;&#20915;&#31574;&#20174;&#32780;&#33719;&#21033;&#12290;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#37329;&#34701;&#24066;&#22330;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#32929;&#24066;&#21463;&#20004;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#19968;&#26041;&#38754;&#26159;&#22320;&#32536;&#25919;&#27835;&#12289;&#31038;&#20250;&#21644;&#20840;&#29699;&#20107;&#20214;&#65292;&#36825;&#20123;&#20107;&#20214;&#21487;&#33021;&#24433;&#21709;&#20215;&#26684;&#36235;&#21183;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31532;&#20108;&#20010;&#26041;&#38754;&#32431;&#31929;&#20851;&#27880;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#12290;&#26412;&#25991;&#26088;&#22312;&#19987;&#27880;&#20110;&#31532;&#20108;&#20010;&#26041;&#38754;&#65292;&#24314;&#31435;&#19968;&#20010;&#33021;&#22815;&#20197;&#26368;&#23567;&#35823;&#24046;&#39044;&#27979;&#26410;&#26469;&#20215;&#26684;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#22909;&#30340;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#21644;&#39034;&#24207;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046; (LSTM-SSAM) &#30340;&#26032;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#32929;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65306;SBIN&#12289;HDFCBANK &#21644; BANKBARODA&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffCR&#30340;&#24555;&#36895;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;&#20809;&#23398;&#21355;&#26143;&#22270;&#20687;&#20113;&#21435;&#38500;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#21644;&#26465;&#20214;&#24341;&#23548;&#25193;&#25955;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#39068;&#33394;&#34920;&#24449;&#65292;&#30830;&#20445;&#21512;&#25104;&#36755;&#20986;&#19982;&#26465;&#20214;&#36755;&#20837;&#20043;&#38388;&#30340;&#22806;&#35266;&#20449;&#24687;&#20855;&#26377;&#32039;&#23494;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04417</link><description>&lt;p&gt;
DiffCR: &#19968;&#31181;&#24555;&#36895;&#30340;&#20809;&#23398;&#21355;&#26143;&#22270;&#20687;&#20113;&#21435;&#38500;&#30340;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images. (arXiv:2308.04417v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffCR&#30340;&#24555;&#36895;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;&#20809;&#23398;&#21355;&#26143;&#22270;&#20687;&#20113;&#21435;&#38500;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#21644;&#26465;&#20214;&#24341;&#23548;&#25193;&#25955;&#26469;&#25552;&#20379;&#24378;&#22823;&#30340;&#39068;&#33394;&#34920;&#24449;&#65292;&#30830;&#20445;&#21512;&#25104;&#36755;&#20986;&#19982;&#26465;&#20214;&#36755;&#20837;&#20043;&#38388;&#30340;&#22806;&#35266;&#20449;&#24687;&#20855;&#26377;&#32039;&#23494;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#21355;&#26143;&#22270;&#20687;&#26159;&#37325;&#35201;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#28982;&#32780;&#20113;&#35206;&#30422;&#32463;&#24120;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#36136;&#37327;&#65292;&#38459;&#30861;&#22270;&#20687;&#24212;&#29992;&#21644;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#22320;&#20174;&#20809;&#23398;&#21355;&#26143;&#22270;&#20687;&#20013;&#21435;&#38500;&#20113;&#23618;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#34429;&#28982;&#26368;&#36817;&#20113;&#21435;&#38500;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#27425;&#20248;&#12290;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#26679;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#26174;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffCR&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#24102;&#26377;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#30340;&#26465;&#20214;&#24341;&#23548;&#25193;&#25955;&#65292;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#20809;&#23398;&#21355;&#26143;&#22270;&#20687;&#20113;&#21435;&#38500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#26465;&#20214;&#22270;&#20687;&#29305;&#24449;&#25552;&#21462;&#30340;&#35299;&#32806;&#32534;&#30721;&#22120;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#39068;&#33394;&#34920;&#24449;&#65292;&#20197;&#30830;&#20445;&#26465;&#20214;&#36755;&#20837;&#21644;&#21512;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#22806;&#35266;&#20449;&#24687;&#30340;&#32039;&#23494;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical satellite images are a critical data source; however, cloud cover often compromises their quality, hindering image applications and analysis. Consequently, effectively removing clouds from optical satellite images has emerged as a prominent research direction. While recent advancements in cloud removal primarily rely on generative adversarial networks, which may yield suboptimal image quality, diffusion models have demonstrated remarkable success in diverse image-generation tasks, showcasing their potential in addressing this challenge. This paper presents a novel framework called DiffCR, which leverages conditional guided diffusion with deep convolutional networks for high-performance cloud removal for optical satellite imagery. Specifically, we introduce a decoupled encoder for conditional image feature extraction, providing a robust color representation to ensure the close similarity of appearance information between the conditional input and the synthesized output. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04412</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26082;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21448;&#33021;&#20445;&#25345;&#20219;&#21153;&#24050;&#30693;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21464;&#24615;&#21644;&#35745;&#31639;&#25110;&#20869;&#23384;&#36164;&#28304;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;&#24615;&#35774;&#35745;&#26082;&#20855;&#34920;&#36798;&#33021;&#21147;&#21448;&#20855;&#19981;&#21464;&#24615;&#20294;&#20351;&#29992;&#26356;&#23569;&#36164;&#28304;&#30340;&#27169;&#22411;&#12290;&#21463;&#38543;&#26426;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120; (RLCs) &#30340;&#20108;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#21442;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;RLCs &#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#36924;&#36817;&#20219;&#20309;&#65288;&#24179;&#28369;&#65289;&#20989;&#25968;&#65292;&#24182;&#20445;&#25345;&#23545;&#32039;&#33268;&#32676;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#21487;&#39564;&#35777;&#22320;&#27010;&#29575;&#19981;&#21464;&#30340; RLCs&#65292;&#29992;&#20110;&#38598;&#21512;&#12289;&#22270;&#21644;&#29699;&#24418;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23454;&#29616;&#27010;&#29575;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over sets, graphs, and spherical data. We show how these models can achieve probabilistic invari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35828;&#26126;&#24341;&#23548;&#30340;&#22270;&#21453;&#21521;&#38376;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#21453;&#21521;&#38376;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;</title><link>http://arxiv.org/abs/2308.04406</link><description>&lt;p&gt;
XGBD: &#35828;&#26126;&#24341;&#23548;&#30340;&#22270;&#21453;&#21521;&#38376;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
XGBD: Explanation-Guided Graph Backdoor Detection. (arXiv:2308.04406v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35828;&#26126;&#24341;&#23548;&#30340;&#22270;&#21453;&#21521;&#38376;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#21453;&#21521;&#38376;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#38376;&#25915;&#20987;&#23545;&#22270;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#21453;&#21521;&#38376;&#35302;&#21457;&#22120;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23558;&#21453;&#21521;&#38376;&#23884;&#20837;&#21040;&#30446;&#26631;&#27169;&#22411;&#20013;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#23384;&#22312;&#35302;&#21457;&#22120;&#26102;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#23545;&#25239;&#21453;&#21521;&#38376;&#25915;&#20987;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21453;&#21521;&#38376;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#19968;&#31181;&#26032;&#20852;&#30340;&#26816;&#27979;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#22312;&#23558;&#27169;&#22411;&#35757;&#32451;&#20110;&#21253;&#21547;&#21453;&#21521;&#38376;&#21644;&#24178;&#20928;&#26679;&#26412;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#26102;&#65292;&#21453;&#21521;&#38376;&#26679;&#26412;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#26126;&#26174;&#24555;&#20110;&#24178;&#20928;&#26679;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#25439;&#22833;&#20540;&#26368;&#20302;&#30340;&#26679;&#26412;&#26469;&#36731;&#26494;&#26816;&#27979;&#21453;&#21521;&#38376;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#22270;&#25968;&#25454;&#24573;&#30053;&#20102;&#25299;&#25169;&#29305;&#24449;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35828;&#26126;&#24341;&#23548;&#30340;&#21453;&#21521;&#38376;&#26816;&#27979;&#26041;&#27861;&#26469;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#22270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;f
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks pose a significant security risk to graph learning models. Backdoors can be embedded into the target model by inserting backdoor triggers into the training dataset, causing the model to make incorrect predictions when the trigger is present. To counter backdoor attacks, backdoor detection has been proposed. An emerging detection strategy in the vision and NLP domains is based on an intriguing phenomenon: when training models on a mixture of backdoor and clean samples, the loss on backdoor samples drops significantly faster than on clean samples, allowing backdoor samples to be easily detected by selecting samples with the lowest loss values. However, the ignorance of topological feature information on graph data limits its detection effectiveness when applied directly to the graph domain. To this end, we propose an explanation-guided backdoor detection method to take advantage of the topological information. Specifically, we train a helper model on the graph dataset, f
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#23454;&#29616;&#25968;&#25454;&#33719;&#21462;&#21644;&#35745;&#31639;&#30340;&#20998;&#31163;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#31227;&#21160;&#32593;&#32476;&#23588;&#20854;&#26159;6G&#21450;&#20197;&#21518;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04404</link><description>&lt;p&gt;
&#29992;&#32852;&#37030;&#23398;&#20064;&#38761;&#26032;&#26080;&#32447;&#32593;&#32476;&#65306;&#19968;&#39033;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review. (arXiv:2308.04404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04404
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#23454;&#29616;&#25968;&#25454;&#33719;&#21462;&#21644;&#35745;&#31639;&#30340;&#20998;&#31163;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#31227;&#21160;&#32593;&#32476;&#23588;&#20854;&#26159;6G&#21450;&#20197;&#21518;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#25163;&#26426;&#12289;&#24179;&#26495;&#30005;&#33041;&#21644;&#36710;&#36742;&#31561;&#26080;&#32447;&#29992;&#25143;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#19981;&#26029;&#25552;&#39640;&#65292;&#20197;&#21450;&#23545;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#12290;FL&#20351;&#24471;&#25968;&#25454;&#33719;&#21462;&#21644;&#35745;&#31639;&#22312;&#20013;&#22830;&#21333;&#20803;&#20013;&#20998;&#31163;&#65292;&#36825;&#19982;&#22312;&#25968;&#25454;&#20013;&#24515;&#20013;&#36827;&#34892;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#19981;&#21516;&#12290;FL&#36890;&#24120;&#29992;&#20110;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#65292;&#20854;&#20013;&#36890;&#20449;&#36164;&#28304;&#26377;&#38480;&#19988;&#19981;&#21487;&#38752;&#12290;&#24102;&#23485;&#38480;&#21046;&#35201;&#27714;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20165;&#23433;&#25490;&#37096;&#20998;&#29992;&#25143;&#35774;&#22791;&#36827;&#34892;&#26356;&#26032;&#65292;&#24182;&#19988;&#30001;&#20110;&#26080;&#32447;&#20171;&#36136;&#26159;&#20849;&#20139;&#30340;&#65292;&#20256;&#36755;&#26131;&#21463;&#24178;&#25200;&#19988;&#19981;&#20445;&#35777;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#22312;&#26410;&#26469;&#30340;&#31227;&#21160;&#32593;&#32476;&#20013;&#29305;&#21035;&#26159;6G&#21450;&#20197;&#21518;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
These days with the rising computational capabilities of wireless user equipment such as smart phones, tablets, and vehicles, along with growing concerns about sharing private data, a novel machine learning model called federated learning (FL) has emerged. FL enables the separation of data acquisition and computation at the central unit, which is different from centralized learning that occurs in a data center. FL is typically used in a wireless edge network where communication resources are limited and unreliable. Bandwidth constraints necessitate scheduling only a subset of UEs for updates in each iteration, and because the wireless medium is shared, transmissions are susceptible to interference and are not assured. The article discusses the significance of Machine Learning in wireless communication and highlights Federated Learning (FL) as a novel approach that could play a vital role in future mobile networks, particularly 6G and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2308.04396</link><description>&lt;p&gt;
&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining. (arXiv:2308.04396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#21644;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#65292;&#20197;&#25903;&#25345;&#31038;&#20250;&#27969;&#31243;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#25366;&#25496;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#20449;&#24687;&#31995;&#32479;&#30340;&#20107;&#20214;&#26085;&#24535;&#20013;&#21457;&#29616;&#27969;&#31243;&#27169;&#22411;&#12290;&#27969;&#31243;&#25366;&#25496;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#38754;&#21521;&#27969;&#31243;&#30340;&#20225;&#19994;&#31995;&#32479;&#65292;&#20294;&#23545;&#20110;&#38754;&#21521;&#36890;&#20449;&#21644;&#25991;&#26723;&#30340;&#20225;&#19994;&#21327;&#20316;&#31995;&#32479;&#65288;ECS&#65289;&#26469;&#35828;&#19981;&#22826;&#36866;&#29992;&#12290;ECS&#20107;&#20214;&#26085;&#24535;&#38750;&#24120;&#32454;&#31890;&#24230;&#65292;&#23545;&#20854;&#26085;&#24535;&#24212;&#29992;&#27969;&#31243;&#25366;&#25496;&#20250;&#23548;&#33268;&#28151;&#20081;&#30340;&#27169;&#22411;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20107;&#20214;&#25277;&#35937;&#65292;&#21363;&#22312;&#36816;&#34892;&#21457;&#29616;&#31639;&#27861;&#20043;&#21069;&#23558;&#20302;&#32423;&#21035;&#26085;&#24535;&#36716;&#25442;&#20026;&#26356;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26082;&#26377;&#30340;&#20107;&#20214;&#25277;&#35937;&#26041;&#27861;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;ECS&#26085;&#24535;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23450;&#21046;&#30340;ECS&#20107;&#20214;&#25277;&#35937;&#65288;ECSEA&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#35760;&#24405;&#30340;&#23454;&#38469;&#29992;&#25143;&#27963;&#21160;&#65288;&#39640;&#32423;&#21035;&#36319;&#36394;&#65289;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#65288;&#20174;ECS&#20013;&#25552;&#21462;&#65289;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#33258;&#21160;&#23558;&#26410;&#26469;&#30340;&#20302;&#32423;&#21035;&#36319;&#36394;&#36716;&#25442;&#20026;&#25277;&#35937;&#30340;&#39640;&#32423;&#21035;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
One aim of Process Mining (PM) is the discovery of process models from event logs of information systems. PM has been successfully applied to process-oriented enterprise systems but is less suited for communication- and document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are very fine-granular and PM applied to their logs results in spaghetti models. A common solution for this is event abstraction, i.e., converting low-level logs into more abstract high-level logs before running discovery algorithms. ECS logs come with special characteristics that have so far not been fully addressed by existing event abstraction approaches. We aim to close this gap with a tailored ECS event abstraction (ECSEA) approach that trains a model by comparing recorded actual user activities (high-level traces) with the system-generated low-level traces (extracted from the ECS). The model allows us to automatically convert future low-level traces into an abstracted high-level log that can 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#21307;&#23398;&#22270;&#20687;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22823;&#33041;MRI&#20998;&#21106;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#20934;&#30830;&#24615;&#12289;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#23545;&#25239;&#39046;&#22495;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04395</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#21307;&#23398;&#22270;&#20687;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation-Based Unsupervised Domain Adaptation In Medical Imaging. (arXiv:2308.04395v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#21307;&#23398;&#22270;&#20687;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22823;&#33041;MRI&#20998;&#21106;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#20934;&#30830;&#24615;&#12289;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#23545;&#25239;&#39046;&#22495;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#27169;&#22411;&#32463;&#24120;&#22312;&#26032;&#30340;&#25195;&#25551;&#20013;&#38590;&#20197;&#26377;&#25928;&#25512;&#24191;&#65292;&#36825;&#26159;&#30001;&#20110;&#30828;&#20214;&#12289;&#37319;&#38598;&#21442;&#25968;&#12289;&#20154;&#32676;&#21644;&#20266;&#24433;&#24341;&#36215;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MRI&#29305;&#23450;&#22686;&#24378;&#25216;&#26415;&#30340;&#22823;&#33041;MRI&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#24577;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#33021;&#21462;&#24471;&#39640;&#31934;&#24230;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#23545;&#25239;&#39046;&#22495;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based models in medical imaging often struggle to generalize effectively to new scans due to data heterogeneity arising from differences in hardware, acquisition parameters, population, and artifacts. This limitation presents a significant challenge in adopting machine learning models for clinical practice. We propose an unsupervised method for robust domain adaptation in brain MRI segmentation by leveraging MRI-specific augmentation techniques. To evaluate the effectiveness of our method, we conduct extensive experiments across diverse datasets, modalities, and segmentation tasks, comparing against the state-of-the-art methods. The results show that our proposed approach achieves high accuracy, exhibits broad applicability, and showcases remarkable robustness against domain shift in various tasks, surpassing the state-of-the-art performance in the majority of cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#24110;&#21161;&#20154;&#31867;&#20998;&#26512;&#23457;&#26597;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#65292;&#20943;&#23569;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#20915;&#31574;&#30340;&#36136;&#37327;&#21644;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.04375</link><description>&lt;p&gt;
&#29702;&#35299;&#22240;&#26524;&#35299;&#37322;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20020;&#24202;&#20915;&#31574;&#20013;&#20449;&#20219;&#21644;&#20381;&#36182;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making. (arXiv:2308.04375v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#24110;&#21161;&#20154;&#31867;&#20998;&#26512;&#23457;&#26597;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#65292;&#20943;&#23569;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#20915;&#31574;&#30340;&#36136;&#37327;&#21644;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36234;&#26469;&#36234;&#34987;&#32771;&#34385;&#29992;&#20110;&#36741;&#21161;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#22914;&#20581;&#24247;&#65289;&#20013;&#30340;&#20154;&#31867;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#35752;&#35770;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20154;&#31867;&#21487;&#33021;&#20250;&#36807;&#24230;&#20381;&#36182;&#38169;&#35823;&#30340;AI&#27169;&#22411;&#24314;&#35758;&#65292;&#32780;&#38750;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#34917;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26174;&#33879;&#29305;&#24449;&#35299;&#37322;&#21644;&#20551;&#35774;&#24615;&#22240;&#26524;&#35299;&#37322;&#65292;&#20351;&#20154;&#31867;&#33021;&#26356;&#20998;&#26512;&#22320;&#23457;&#26597;AI&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#23545;AI&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#24182;&#25506;&#35752;&#36825;&#20123;&#35299;&#37322;&#23545;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#20449;&#20219;&#21644;&#20381;&#36182;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19971;&#20301;&#27835;&#30103;&#24072;&#21644;&#21313;&#20301;&#38750;&#19987;&#19994;&#20154;&#22763;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#20219;&#21153;&#26159;&#35780;&#20272;&#20013;&#39118;&#21518;&#24184;&#23384;&#32773;&#30340;&#36816;&#21160;&#36136;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#20182;&#20204;&#30340;&#34920;&#29616;&#12289;&#20219;&#21153;&#19978;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#20197;&#21450;&#22312;&#26080;&#35299;&#37322;&#21644;&#26377;&#20004;&#31181;AI&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#23545;AI&#30340;&#20381;&#36182;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20855;&#26377;&#26174;&#33879;&#29305;&#24449;&#21644;&#22240;&#26524;&#35299;&#37322;&#30340;AI&#27169;&#22411;&#24110;&#21161;&#27835;&#30103;&#24072;&#21644;&#38750;&#19987;&#19994;&#20154;&#22763;&#25913;&#21892;&#20102;&#20182;&#20204;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their 
&lt;/p&gt;</description></item><item><title>Pelta&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#21487;&#20449;&#30828;&#20214;&#30340;&#33021;&#21147;&#26469;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25506;&#27979;&#25915;&#20987;&#12290;&#23427;&#36974;&#34109;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#37096;&#20998;&#38142;&#35268;&#21017;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04373</link><description>&lt;p&gt;
Pelta&#65306;&#29992;&#20110;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#36867;&#36991;&#25915;&#20987;&#30340;&#21464;&#21387;&#22120;&#20445;&#25252;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning. (arXiv:2308.04373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04373
&lt;/p&gt;
&lt;p&gt;
Pelta&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#21487;&#20449;&#30828;&#20214;&#30340;&#33021;&#21147;&#26469;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25506;&#27979;&#25915;&#20987;&#12290;&#23427;&#36974;&#34109;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#37096;&#20998;&#38142;&#35268;&#21017;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#20027;&#35201;&#21069;&#25552;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26356;&#26032;&#26159;&#22312;&#26412;&#22320;&#35745;&#31639;&#30340;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#19981;&#20250;&#31163;&#24320;&#35774;&#22791;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#21463; compromise &#30340;&#23458;&#25143;&#31471;&#21487;&#20197;&#36731;&#26131;&#22320;&#22312;&#20854;&#26412;&#22320;&#20869;&#23384;&#20013;&#25506;&#27979;&#27169;&#22411;&#65292;&#23547;&#25214;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24694;&#24847;&#25506;&#27979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Pelta &#25252;&#30462;&#26426;&#21046;&#65292;&#21033;&#29992;&#21487;&#20449;&#30828;&#20214;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659; (TEE) &#30340;&#33021;&#21147;&#65292;Pelta &#36974;&#34109;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#37096;&#20998;&#38142;&#35268;&#21017;&#65292;&#36825;&#36890;&#24120;&#26159;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main premise of federated learning is that machine learning model updates are computed locally, in particular to preserve user data privacy, as those never leave the perimeter of their device. This mechanism supposes the general model, once aggregated, to be broadcast to collaborating and non malicious nodes. However, without proper defenses, compromised clients can easily probe the model inside their local memory in search of adversarial examples. For instance, considering image-based applications, adversarial examples consist of imperceptibly perturbed images (to the human eye) misclassified by the local model, which can be later presented to a victim node's counterpart model to replicate the attack. To mitigate such malicious probing, we introduce Pelta, a novel shielding mechanism leveraging trusted hardware. By harnessing the capabilities of Trusted Execution Environments (TEEs), Pelta masks part of the back-propagation chain rule, otherwise typically exploited by attackers fo
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#36861;&#32034;&#26435;&#65306;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65288;DPM&#65289;&#21644;&#25289;&#26222;&#25289;&#26031;&#36861;&#32034;&#26435;&#65288;LR&#65289;&#12290;&#22312;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;DPM&#21644;LR&#22312;&#20943;&#23569;&#25915;&#20987;&#32773;&#21487;&#20197;&#25512;&#26029;&#30340;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#35823;&#25253;&#29575;&#19979;&#12290;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;LR&#26041;&#27861;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#38544;&#31169;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2308.04341</link><description>&lt;p&gt;
&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#31169;&#23494;&#30340;&#27169;&#22411;&#65306;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#21516;&#26102;&#25552;&#20379;&#36861;&#32034;&#26435;
&lt;/p&gt;
&lt;p&gt;
Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage. (arXiv:2308.04341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#36861;&#32034;&#26435;&#65306;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65288;DPM&#65289;&#21644;&#25289;&#26222;&#25289;&#26031;&#36861;&#32034;&#26435;&#65288;LR&#65289;&#12290;&#22312;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;DPM&#21644;LR&#22312;&#20943;&#23569;&#25915;&#20987;&#32773;&#21487;&#20197;&#25512;&#26029;&#30340;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#35823;&#25253;&#29575;&#19979;&#12290;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;LR&#26041;&#27861;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#38544;&#31169;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#24433;&#21709;&#28145;&#36828;&#30340;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#27169;&#22411;&#20026;&#25910;&#21040;&#36127;&#38754;&#32467;&#26524;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#31639;&#27861;&#36861;&#32034;&#26435;&#12290;&#28982;&#32780;&#65292;&#36861;&#32034;&#26435;&#21487;&#20197;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#26469;&#25259;&#38706;&#31169;&#20154;&#20449;&#24687;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#32531;&#35299;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#36861;&#32034;&#26435;&#65306;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65288;DPM&#65289;&#21644;&#25289;&#26222;&#25289;&#26031;&#36861;&#32034;&#26435;&#65288;LR&#65289;&#12290;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;DPM&#21644;LR&#22312;&#20943;&#23569;&#25915;&#20987;&#32773;&#21487;&#20197;&#25512;&#26029;&#30340;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;FPR&#19979;&#12290;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#36275;&#22815;&#22823;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26032;&#39062;LR&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#21644;&#36861;&#32034;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#38544;&#31169;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are increasingly utilized across impactful domains to predict individual outcomes. As such, many models provide algorithmic recourse to individuals who receive negative outcomes. However, recourse can be leveraged by adversaries to disclose private information. This work presents the first attempt at mitigating such attacks. We present two novel methods to generate differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR). Using logistic regression classifiers and real world and synthetic datasets, we find that DPM and LR perform well in reducing what an adversary can infer, especially at low FPR. When training dataset size is large enough, we find particular success in preventing privacy leakage while maintaining model and recourse accuracy with our novel LR method.
&lt;/p&gt;</description></item><item><title>RLHF-Blender&#26159;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#12289;&#20114;&#21160;&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#20174;&#22810;&#31181;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#31995;&#32479;&#22320;&#25506;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#20197;&#21450;&#20154;&#31867;&#22240;&#32032;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04332</link><description>&lt;p&gt;
RLHF-Blender: &#21487;&#37197;&#32622;&#30340;&#19982;&#20154;&#31867;&#21453;&#39304;&#20132;&#20114;&#23398;&#20064;&#30340;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback. (arXiv:2308.04332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04332
&lt;/p&gt;
&lt;p&gt;
RLHF-Blender&#26159;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#12289;&#20114;&#21160;&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#20174;&#22810;&#31181;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#31995;&#32479;&#22320;&#25506;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#20197;&#21450;&#20154;&#31867;&#22240;&#32032;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20851;&#38190;&#26159;&#36890;&#36807;&#22810;&#31181;&#20154;&#31867;&#21453;&#39304;&#28304;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#32771;&#34385;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#21453;&#39304;&#30340;&#20154;&#31867;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#65292;&#23545;&#20174;&#19981;&#21516;&#31867;&#22411;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RLHF-Blender&#65292;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#12289;&#20114;&#21160;&#30340;&#29992;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30028;&#38754;&#12290;RLHF-Blender&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#23454;&#39564;&#26694;&#26550;&#21644;&#23454;&#29616;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#31995;&#32479;&#22320;&#30740;&#31350;&#20154;&#31867;&#21453;&#39304;&#23545;&#22870;&#21169;&#23398;&#20064;&#30340;&#29305;&#24615;&#21644;&#36136;&#37327;&#12290;&#35813;&#31995;&#32479;&#20419;&#36827;&#20102;&#21508;&#31181;&#21453;&#39304;&#31867;&#22411;&#30340;&#25506;&#32034;&#65292;&#21253;&#25324;&#28436;&#31034;&#12289;&#25490;&#24207;&#12289;&#27604;&#36739;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#32771;&#34385;&#20102;&#20154;&#31867;&#22240;&#32032;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;RLHF-Blender&#25152;&#23454;&#29616;&#30340;&#19968;&#31995;&#21015;&#20855;&#20307;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.04314</link><description>&lt;p&gt;
&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#36172;&#21338;&#26426;&#65306;&#20855;&#26377;&#26368;&#20339;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#36890;&#20449;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs. (arXiv:2308.04314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#32452;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#21512;&#20316;&#29609;&#30456;&#21516;&#30340;&#22810;&#33218;&#36172;&#21338;&#28216;&#25103;&#12290;&#30446;&#26631;&#26159;&#24320;&#21457;&#20855;&#26377;&#26368;&#20339;&#32676;&#20307;&#21644;&#20010;&#20307;&#36951;&#25022;&#20197;&#21450;&#26234;&#33021;&#20307;&#20043;&#38388;&#36890;&#20449;&#25104;&#26412;&#20302;&#30340;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#22312;&#21069;&#26399;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#21644;&#23436;&#20840;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#22312;&#36825;&#20004;&#31181;&#33539;&#24335;&#20013;&#65292;&#20197;&#21069;&#30340;&#31639;&#27861;&#37117;&#33021;&#36798;&#21040;&#26368;&#20339;&#32676;&#20307;&#36951;&#25022;&#12290;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31639;&#27861;&#23454;&#29616;&#20102;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#26368;&#20339;&#20010;&#20307;&#36951;&#25022;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23436;&#20840;&#20998;&#24067;&#24335;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#20339;&#20010;&#20307;&#36951;&#25022;&#65292;&#20294;&#26410;&#33021;&#23454;&#29616;&#24658;&#23450;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36890;&#20449;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#21512;&#20316;&#36172;&#21338;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#26368;&#20248;&#20010;&#20307;&#36951;&#25022;&#21644;&#24658;&#23450;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#27169;&#22411;&#21453;&#28436;&#31363;&#21548;&#25915;&#20987;&#65288;MIEA&#65289;&#65292;&#36890;&#36807;&#31363;&#21548;&#21644;&#27169;&#22411;&#21453;&#28436;&#26469;&#37325;&#26500;&#21407;&#22987;&#28040;&#24687;&#65292;&#30740;&#31350;&#21457;&#29616;MIEA&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#33021;&#22815;&#25104;&#21151;&#22320;&#37325;&#26500;&#20986;&#20855;&#26377;&#33391;&#22909;&#36136;&#37327;&#30340;&#21407;&#22987;&#28040;&#24687;&#12290;&#20026;&#20102;&#23454;&#29616;&#23433;&#20840;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#32622;&#25442;&#21644;&#26367;&#25442;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04304</link><description>&lt;p&gt;
&#22312;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#27169;&#22411;&#21453;&#28436;&#31363;&#21548;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Model Inversion Eavesdropping Attack in Semantic Communication Systems. (arXiv:2308.04304v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#27169;&#22411;&#21453;&#28436;&#31363;&#21548;&#25915;&#20987;&#65288;MIEA&#65289;&#65292;&#36890;&#36807;&#31363;&#21548;&#21644;&#27169;&#22411;&#21453;&#28436;&#26469;&#37325;&#26500;&#21407;&#22987;&#28040;&#24687;&#65292;&#30740;&#31350;&#21457;&#29616;MIEA&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#33021;&#22815;&#25104;&#21151;&#22320;&#37325;&#26500;&#20986;&#20855;&#26377;&#33391;&#22909;&#36136;&#37327;&#30340;&#21407;&#22987;&#28040;&#24687;&#12290;&#20026;&#20102;&#23454;&#29616;&#23433;&#20840;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#32622;&#25442;&#21644;&#26367;&#25442;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#36890;&#20449;&#25928;&#29575;&#30340;&#20248;&#21183;&#65292;&#35821;&#20041;&#36890;&#20449;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#30001;&#20110;&#35821;&#20041;&#36890;&#20449;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#20174;&#21407;&#22987;&#28040;&#24687;&#20013;&#25552;&#21462;&#21547;&#20041;&#65292;&#22240;&#27492;&#23427;&#23481;&#26131;&#21463;&#21040;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#22411;&#21453;&#28436;&#31363;&#21548;&#25915;&#20987;&#65288;MIEA&#65289;&#65292;&#20197;&#25581;&#31034;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20013;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#22312;MIEA&#20013;&#65292;&#25915;&#20987;&#32773;&#39318;&#20808;&#31363;&#21548;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20256;&#36755;&#30340;&#20449;&#21495;&#65292;&#28982;&#21518;&#36827;&#34892;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#20197;&#37325;&#26500;&#21407;&#22987;&#28040;&#24687;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#65292;MIEA&#21487;&#20197;&#25104;&#21151;&#22320;&#37325;&#26500;&#20986;&#20855;&#26377;&#33391;&#22909;&#36136;&#37327;&#30340;&#21407;&#22987;&#28040;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#32622;&#25442;&#21644;&#26367;&#25442;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20197;&#25269;&#24481;MIEA&#65292;&#23454;&#29616;&#23433;&#20840;&#30340;&#35821;&#20041;&#36890;&#20449;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, semantic communication has been a popular research topic for its superiority in communication efficiency. As semantic communication relies on deep learning to extract meaning from raw messages, it is vulnerable to attacks targeting deep learning models. In this paper, we introduce the model inversion eavesdropping attack (MIEA) to reveal the risk of privacy leaks in the semantic communication system. In MIEA, the attacker first eavesdrops the signal being transmitted by the semantic communication system and then performs model inversion attack to reconstruct the raw message, where both the white-box and black-box settings are considered. Evaluation results show that MIEA can successfully reconstruct the raw message with good quality under different channel conditions. We then propose a defense method based on random permutation and substitution to defend against MIEA in order to achieve secure semantic communication. Our experimental results demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;wav2vec 2.0&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#22312;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;ASR&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20004;&#32773;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#23398;&#20064;&#21040;&#30340;&#28388;&#27874;&#22120;&#65292;&#21457;&#29616;ASR&#31995;&#32479;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26159;&#36890;&#36807;&#19968;&#32452;&#24102;&#36890;&#28388;&#27874;&#22120;&#33719;&#24471;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.04286</link><description>&lt;p&gt;
wav2vec 2.0&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of the wav2vec 2.0 Feature Extractor. (arXiv:2308.04286v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;wav2vec 2.0&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#22312;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;ASR&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20004;&#32773;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#23398;&#20064;&#21040;&#30340;&#28388;&#27874;&#22120;&#65292;&#21457;&#29616;ASR&#31995;&#32479;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26159;&#36890;&#36807;&#19968;&#32452;&#24102;&#36890;&#28388;&#27874;&#22120;&#33719;&#24471;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#12290;&#20026;&#20102;&#36991;&#20813;&#23427;&#20204;&#22266;&#26377;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#23454;&#29616;&#20174;&#35821;&#38899;&#21040;&#36716;&#24405;&#25991;&#26412;&#30340;&#26356;&#19968;&#33268;&#30340;&#24314;&#27169;&#65292;&#31070;&#32463;&#21407;&#22987;&#27874;&#24418;&#29305;&#24449;&#25552;&#21462;&#22120;&#65288;FEs&#65289;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#24191;&#21463;&#27426;&#36814;&#30340;wav2vec 2.0&#27169;&#22411;&#20063;&#20351;&#29992;&#20102;&#21367;&#31215;FE&#65292;&#30452;&#25509;&#23545;&#35821;&#38899;&#27874;&#24418;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#24471;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;ASR&#27169;&#22411;&#20013;&#26367;&#20195;&#26631;&#20934;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#19982;&#21478;&#19968;&#31181;&#26367;&#20195;&#24615;&#31070;&#32463;FE&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20004;&#32773;&#37117;&#19982;&#20256;&#32479;&#30340;FEs&#31454;&#20105;&#21147;&#19981;&#30456;&#19978;&#19979;&#65292;&#24182;&#20998;&#26512;&#20102;&#21508;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#23398;&#20064;&#21040;&#30340;&#28388;&#27874;&#22120;&#65292;&#24182;&#26174;&#31034;ASR&#31995;&#32479;&#30340;&#26368;&#37325;&#35201;&#20449;&#24687;&#26159;&#36890;&#36807;&#19968;&#32452;&#24102;&#36890;&#28388;&#27874;&#22120;&#33719;&#24471;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems typically use handcrafted feature extraction pipelines. To avoid their inherent information loss and to achieve more consistent modeling from speech to transcribed text, neural raw waveform feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model, which has recently gained large popularity, uses a convolutional FE which operates directly on the speech waveform. However, it is not yet studied extensively in the literature. In this work, we study its capability to replace the standard feature extraction methods in a connectionist temporal classification (CTC) ASR model and compare it to an alternative neural FE. We show that both are competitive with traditional FEs on the LibriSpeech benchmark and analyze the effect of the individual components. Furthermore, we analyze the learned filters and show that the most important information for the ASR system is obtained by a set of bandpass filters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#20102;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#30340;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#20854;&#21487;&#20197;&#19982;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.04275</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#36827;&#34892;&#19978;&#19979;&#25991;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#20102;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#30340;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#20854;&#21487;&#20197;&#19982;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#35828;&#26126;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25506;&#32034;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32431;&#20928;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; Llama-2&#65292;&#22312;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#20043;&#21069;&#65292;&#24403;&#27169;&#22411;&#34987;&#35201;&#27714;&#25353;&#29031;&#32842;&#22825;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#26816;&#32034;&#21040;&#20102;&#24179;&#22343;9&#20010;&#23545;&#40784;&#28436;&#31034;&#31034;&#20363;&#12290;&#19982;&#30452;&#25509;&#25552;&#31034;&#30456;&#27604;&#65292;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#23548;&#33268;&#20102;&#19982;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#30456;&#27604;&#65292;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#24471;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23218;&#32654;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#24072;&#29983;&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24182;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30693;&#35782;&#21387;&#32553;&#12289;&#30693;&#35782;&#25193;&#23637;&#12289;&#30693;&#35782;&#36866;&#24212;&#21644;&#30693;&#35782;&#22686;&#24378;&#65292;&#36890;&#36807;&#36731;&#37327;&#21270;&#21644;&#36890;&#29992;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#23454;&#29616;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.04268</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#30340;&#24072;&#29983;&#26550;&#26500;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Architecture for Knowledge Distillation: A Survey. (arXiv:2308.04268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#24072;&#29983;&#26550;&#26500;&#65292;&#22312;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24182;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30693;&#35782;&#21387;&#32553;&#12289;&#30693;&#35782;&#25193;&#23637;&#12289;&#30693;&#35782;&#36866;&#24212;&#21644;&#30693;&#35782;&#22686;&#24378;&#65292;&#36890;&#36807;&#36731;&#37327;&#21270;&#21644;&#36890;&#29992;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#23454;&#29616;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#21442;&#25968;&#24222;&#22823;&#65292;&#36825;&#31181;DNN&#24456;&#38590;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#20013;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24072;&#29983;&#26550;&#26500;&#65292;&#20854;&#20013;&#31616;&#21333;&#30340;&#23398;&#29983;&#32593;&#32476;&#21482;&#26377;&#23569;&#37327;&#21442;&#25968;&#65292;&#21364;&#33021;&#36798;&#21040;&#19982;&#25317;&#26377;&#35768;&#22810;&#21442;&#25968;&#30340;&#28145;&#24230;&#24072;&#20613;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#24072;&#29983;&#26550;&#26500;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20010;&#30446;&#26631;&#39046;&#22495;&#65292;&#21253;&#25324;&#30693;&#35782;&#21387;&#32553;&#12289;&#30693;&#35782;&#25193;&#23637;&#12289;&#30693;&#35782;&#36866;&#24212;&#21644;&#30693;&#35782;&#22686;&#24378;&#12290;&#20511;&#21161;&#24072;&#29983;&#26550;&#26500;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#33021;&#22815;&#36890;&#36807;&#36731;&#37327;&#21270;&#21644;&#36890;&#29992;&#21270;&#30340;&#23398;&#29983;&#32593;&#32476;&#23454;&#29616;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#12290;&#19982;&#29616;&#26377;&#30340;&#20027;&#35201;&#20851;&#27880;&#30693;&#35782;&#21387;&#32553;&#30340;&#33976;&#39311;&#35843;&#26597;&#19981;&#21516;&#65292;&#26412;&#35843;&#26597;&#39318;&#27425;&#25506;&#35752;&#20102;&#36328;&#22810;&#20010;&#33976;&#39311;&#30446;&#26631;&#30340;&#24072;&#29983;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This surv
&lt;/p&gt;</description></item><item><title>BarlowRL&#36890;&#36807;&#23558;Barlow Twins&#21644;DER&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20449;&#24687;&#25193;&#25955;&#36991;&#20813;&#20102;&#32500;&#24230;&#25240;&#21472;&#65292;&#20351;&#24471;RL&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04263</link><description>&lt;p&gt;
BarlowRL: Barlow Twins&#29992;&#20110;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning. (arXiv:2308.04263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04263
&lt;/p&gt;
&lt;p&gt;
BarlowRL&#36890;&#36807;&#23558;Barlow Twins&#21644;DER&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20449;&#24687;&#25193;&#25955;&#36991;&#20813;&#20102;&#32500;&#24230;&#25240;&#21472;&#65292;&#20351;&#24471;RL&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BarlowRL&#65292;&#19968;&#31181;&#23558;Barlow Twins&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#19982;DER&#65288;Data-Efficient Rainbow&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;BarlowRL&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;DER&#21644;&#23545;&#27604;&#31639;&#27861;CURL&#12290;BarlowRL&#36890;&#36807;&#30830;&#20445;&#20449;&#24687;&#25193;&#25955;&#21040;&#25972;&#20010;&#31354;&#38388;&#26469;&#36991;&#20813;&#32500;&#24230;&#25240;&#21472;&#12290;&#36825;&#26377;&#21161;&#20110;RL&#31639;&#27861;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26368;&#32456;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;Barlow Twins&#19982;DER&#30340;&#25972;&#21512;&#22686;&#24378;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;RL&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;BarlowRL&#23637;&#31034;&#20102;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31383;&#21475;&#24335;&#30340;transformer&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#25972;&#21512;&#25193;&#24352;&#27880;&#24847;&#26426;&#21046;&#21644;&#21367;&#31215;&#65292;&#29992;&#20110;&#21152;&#36895;MRI&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#36828;&#22788;&#37051;&#22495;&#20687;&#32032;&#20851;&#31995;&#21644;&#23398;&#20064;&#20302;&#32423;&#24179;&#31227;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.04262</link><description>&lt;p&gt;
SDLFormer: &#19968;&#31181;&#31232;&#30095;&#21644;&#23494;&#38598;&#22686;&#24378;&#30340;Transformer&#29992;&#20110;&#21152;&#36895;MR&#22270;&#20687;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction. (arXiv:2308.04262v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31383;&#21475;&#24335;&#30340;transformer&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#25972;&#21512;&#25193;&#24352;&#27880;&#24847;&#26426;&#21046;&#21644;&#21367;&#31215;&#65292;&#29992;&#20110;&#21152;&#36895;MRI&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#36828;&#22788;&#37051;&#22495;&#20687;&#32032;&#20851;&#31995;&#21644;&#23398;&#20064;&#20302;&#32423;&#24179;&#31227;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformer&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#24050;&#25104;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#31354;&#38388;&#22495;&#20013;&#30340;&#38750;&#23616;&#37096;&#21306;&#22495;&#20851;&#31995;&#12290;transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#20351;&#20854;&#33021;&#25429;&#25417;&#22270;&#20687;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#21152;&#36895;MRI&#22270;&#20687;&#37325;&#24314;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#27424;&#37319;&#26679;&#30340;&#24433;&#21709;&#22312;&#22270;&#20687;&#22495;&#20013;&#26159;&#38750;&#23616;&#37096;&#30340;&#12290;&#23613;&#31649;&#31383;&#21475;&#24335;transformer&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#30001;&#20110;&#20381;&#36182;&#20851;&#31995;&#38480;&#20110;&#22270;&#20687;&#31383;&#21475;&#30340;&#33539;&#22260;&#65292;&#22240;&#27492;&#20854;&#24863;&#21463;&#37326;&#21463;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#30340;transformer&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#25972;&#21512;&#20102;&#25193;&#24352;&#27880;&#24847;&#26426;&#21046;&#21644;&#21367;&#31215;&#65292;&#29992;&#20110;&#21152;&#36895;MRI&#22270;&#20687;&#37325;&#24314;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#30001;&#25193;&#24352;&#21644;&#23494;&#38598;&#37051;&#22495;&#27880;&#24847;transformer&#32452;&#25104;&#65292;&#20197;&#22686;&#24378;&#36828;&#22788;&#37051;&#22495;&#20687;&#32032;&#20851;&#31995;&#65292;&#24182;&#22312;transformer&#27169;&#22359;&#20869;&#24341;&#20837;&#28145;&#24230;&#21367;&#31215;&#65292;&#20197;&#23398;&#20064;&#20302;&#32423;&#24179;&#31227;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as viable alternatives to convolutional neural networks owing to their ability to learn non-local region relationships in the spatial domain. The self-attention mechanism of the transformer enables transformers to capture long-range dependencies in the images, which might be desirable for accelerated MRI image reconstruction as the effect of undersampling is non-local in the image domain. Despite its computational efficiency, the window-based transformers suffer from restricted receptive fields as the dependencies are limited to within the scope of the image windows. We propose a window-based transformer network that integrates dilated attention mechanism and convolution for accelerated MRI image reconstruction. The proposed network consists of dilated and dense neighborhood attention transformers to enhance the distant neighborhood pixel relationship and introduce depth-wise convolutions within the transformer module to learn low-level translation invariant f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#26144;&#23556;&#38899;&#39057;&#21644;&#25991;&#23383;&#25551;&#36848;&#26469;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#20854;&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#21644;&#39069;&#22806;&#30340;&#20154;&#24037;&#29983;&#25104;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#26159;&#20851;&#38190;&#32452;&#20214;&#65292;&#21462;&#24471;&#20102;&#22312;DCASE&#25361;&#25112;&#20013;&#30340;&#31532;&#19968;&#21517;&#65292;&#24182;&#22312;ClothoV2&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;5.6&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.04258</link><description>&lt;p&gt;
&#20351;&#29992;PaSST&#21644;&#22823;&#35268;&#27169;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#25512;&#36827;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#38899;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets. (arXiv:2308.04258v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#26144;&#23556;&#38899;&#39057;&#21644;&#25991;&#23383;&#25551;&#36848;&#26469;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#65292;&#20854;&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#21644;&#39069;&#22806;&#30340;&#20154;&#24037;&#29983;&#25104;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#26159;&#20851;&#38190;&#32452;&#20214;&#65292;&#21462;&#24471;&#20102;&#22312;DCASE&#25361;&#25112;&#20013;&#30340;&#31532;&#19968;&#21517;&#65292;&#24182;&#22312;ClothoV2&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;5.6&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#25991;&#26412;&#21644;&#39057;&#35889;&#22270;&#36716;&#25442;&#22120;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#26816;&#32034;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24405;&#38899;&#21644;&#25991;&#23383;&#25551;&#36848;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#38899;&#39057;&#23383;&#24149;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#30340;&#30456;&#20851;&#31034;&#20363;&#38752;&#36817;&#12290;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31995;&#32479;&#30340;&#27599;&#20010;&#32452;&#20214;&#23545;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#22312;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65306;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#29992;&#20110;&#38899;&#39057;&#23884;&#20837;&#21644;&#21033;&#29992;&#39069;&#22806;&#30340;&#20154;&#24037;&#29983;&#25104;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23581;&#35797;&#36890;&#36807;&#28155;&#21152;&#21487;&#29992;&#20851;&#38190;&#23383;&#26469;&#22686;&#21152;ClothoV2&#23383;&#24149;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#36825;&#21482;&#24102;&#26469;&#20102;&#36739;&#23567;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;2023&#24180;DCASE&#25361;&#25112;&#20013;&#21517;&#21015;&#31532;&#19968;&#65292;&#24182;&#19988;&#22312;ClothoV2&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65288;mAP@10&#65289;&#36739;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#21319;&#20102;5.6&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04237</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction. (arXiv:2308.04237v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26381;&#21153;&#22120;&#24076;&#26395;&#26681;&#25454;&#27169;&#22411;&#23545;&#26032;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#12290;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#20849;&#21516;&#30340;&#26080;&#32447;&#20449;&#36947;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#24182;&#19988;&#21487;&#20197;&#35775;&#38382;&#20197;&#21069;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22914;&#26524;&#35774;&#22791;&#26080;&#27861;&#35775;&#38382;&#26032;&#36755;&#20837;&#65292;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#36136;&#37327;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#65292;&#21033;&#29992;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26469;&#25552;&#39640;&#26381;&#21153;&#22120;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#32852;&#21512;CP&#20013;&#65292;&#35774;&#22791;&#21521;&#26381;&#21153;&#22120;&#20256;&#36882;&#20851;&#20110;&#26412;&#22320;&#25968;&#25454;&#19978;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25439;&#22833;&#30340;&#20449;&#24687;&#65292;&#26381;&#21153;&#22120;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#26657;&#20934;&#19968;&#20010;&#20915;&#31574;&#21306;&#38388;&#65292;&#20197;&#20415;&#20854;&#22312;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#21487;&#38752;&#24615;&#27700;&#24179;&#19979;&#20445;&#35777;&#21253;&#21547;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a 
&lt;/p&gt;</description></item><item><title>OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.04226</link><description>&lt;p&gt;
OpinionConv: &#36890;&#36807;&#22522;&#20110;&#30495;&#23454;&#20027;&#35266;&#20307;&#39564;&#30340;&#35266;&#28857;&#23454;&#29616;&#23545;&#35805;&#24335;&#20135;&#21697;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04226
&lt;/p&gt;
&lt;p&gt;
OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#20135;&#21697;&#26102;&#65292;&#20182;&#20154;&#30340;&#35266;&#28857;&#22312;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20135;&#21697;&#30340;&#20027;&#35266;&#20307;&#39564;&#21487;&#20197;&#26159;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#22312;&#38144;&#21806;&#23545;&#35805;&#20013;&#20063;&#26159;&#22914;&#27492;&#65292;&#22312;&#36825;&#31181;&#23545;&#35805;&#20013;&#65292;&#23458;&#25143;&#21644;&#38144;&#21806;&#21161;&#25163;&#20132;&#25442;&#26377;&#20851;&#20135;&#21697;&#30340;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#27492;&#31867;&#23545;&#35805;&#30340;AI&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#32463;&#39564;&#27809;&#26377;&#30495;&#23454;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#20135;&#21697;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#30495;&#23454;&#20027;&#35266;&#21465;&#36848;&#25903;&#25345;&#23545;&#35805;&#24335;AI&#12290;&#36890;&#36807;OpinionConv&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#35266;&#28857;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#30340;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21592;&#20063;&#30830;&#35748;&#20102;&#35266;&#28857;&#23545;&#20110;&#20915;&#31574;&#30340;&#20449;&#24687;&#22522;&#30784;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04220</link><description>&lt;p&gt;
&#23545;GNN&#27169;&#22411;&#22522;&#20110;&#22270;Attention&#30340;&#35299;&#37322;&#30340;&#35821;&#20041;&#35299;&#37322;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models. (arXiv:2308.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#20041;&#20851;&#27880;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#30340;&#25200;&#21160;&#65292;&#24182;&#24314;&#31435;&#39044;&#27979;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22270;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24212;&#29992;&#20110;&#22330;&#26223;&#35299;&#37322;&#31561;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#28789;&#27963;&#30340;&#22270;&#32467;&#26500;&#26469;&#31616;&#27905;&#22320;&#25551;&#36848;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#20013;&#20351;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#32467;&#26500;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#22270;&#29305;&#23450;&#30340;&#26041;&#27861;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22240;&#27492;&#20808;&#21069;&#24050;&#32463;&#20351;&#29992;&#23427;&#20204;&#20026;GNN&#39044;&#27979;&#25552;&#20379;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#37322;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#22270;Attention&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KNN-&#22522;&#20110;LASSO&#30340;&#26041;&#27861;&#65292;&#22312;&#20581;&#24247;&#32467;&#26524;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#20581;&#24247;&#32467;&#26524;&#21644;&#21361;&#38505;&#22240;&#32032;&#20043;&#38388;&#30340;&#24180;&#40836;&#30456;&#20851;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2308.04212</link><description>&lt;p&gt;
&#22522;&#20110;KNN-&#22522;&#20110;LASSO&#30340;&#21306;&#22495;&#20998;&#20301;&#25968;&#21464;&#31995;&#25968;&#27169;&#22411;&#22312;&#20581;&#24247;&#32467;&#26524;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study. (arXiv:2308.04212v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KNN-&#22522;&#20110;LASSO&#30340;&#26041;&#27861;&#65292;&#22312;&#20581;&#24247;&#32467;&#26524;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#20581;&#24247;&#32467;&#26524;&#21644;&#21361;&#38505;&#22240;&#32032;&#20043;&#38388;&#30340;&#24180;&#40836;&#30456;&#20851;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#32467;&#26524;&#65292;&#22914;&#36523;&#20307;&#36136;&#37327;&#25351;&#25968;&#21644;&#32966;&#22266;&#37255;&#27700;&#24179;&#65292;&#24050;&#30693;&#20381;&#36182;&#20110;&#24180;&#40836;&#65292;&#24182;&#34920;&#29616;&#20986;&#19982;&#20854;&#30456;&#20851;&#21361;&#38505;&#22240;&#32032;&#30340;&#21464;&#21270;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;K&#26368;&#36817;&#37051;(Lasso)&#22522;&#20110;&#21306;&#22495;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#21160;&#24577;&#24314;&#27169;&#20581;&#24247;&#32467;&#26524;&#21644;&#21361;&#38505;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#25429;&#25417;&#24180;&#40836;&#30340;&#26102;&#21464;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#32039;&#23494;&#30340;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#21644;&#22312;&#26576;&#20123;&#27491;&#21017;&#26465;&#20214;&#19979;&#26816;&#27979;&#31934;&#30830;&#30340;&#32858;&#31867;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#39640;&#25928;&#35299;&#20915;&#25152;&#24471;&#21040;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;(ADMM)&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#20581;&#24247;&#32467;&#26524;&#21644;&#20854;&#39118;&#38505;&#22240;&#32032;&#20043;&#38388;&#22797;&#26434;&#30340;&#24180;&#40836;&#30456;&#20851;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health outcomes, such as body mass index and cholesterol levels, are known to be dependent on age and exhibit varying effects with their associated risk factors. In this paper, we propose a novel framework for dynamic modeling of the associations between health outcomes and risk factors using varying-coefficients (VC) regional quantile regression via K-nearest neighbors (KNN) fused Lasso, which captures the time-varying effects of age. The proposed method has strong theoretical properties, including a tight estimation error bound and the ability to detect exact clustered patterns under certain regularity conditions. To efficiently solve the resulting optimization problem, we develop an alternating direction method of multipliers (ADMM) algorithm. Our empirical results demonstrate the efficacy of the proposed method in capturing the complex age-dependent associations between health outcomes and their risk factors.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#20998;&#24067;&#24335;&#32447;&#24615;&#22238;&#24402;&#35745;&#31639;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#21644;&#25913;&#36827;&#24322;&#27493;&#31995;&#32479;&#20013;&#30340;&#22359;&#25928;&#24212;&#38887;&#24615;&#65292;&#23558;&#20449;&#24687;&#20445;&#25252;&#19982;&#22238;&#24402;&#38382;&#39064;&#32500;&#24230;&#30340;&#20943;&#23567;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#21644;&#23376;&#37319;&#26679;"&#22359;"&#65292;&#23454;&#29616;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#26032;&#33609;&#22270;&#30340;&#20998;&#24067;&#24335;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23545;&#23376;&#37319;&#26679;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;&#36827;&#34892;&#20102;&#25512;&#24191;&#24182;&#20462;&#25913;&#20197;&#20445;&#35777;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04185</link><description>&lt;p&gt;
&#36845;&#20195;&#24335;&#33609;&#22270;&#29992;&#20110;&#23433;&#20840;&#32534;&#30721;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Iterative Sketching for Secure Coded Regression. (arXiv:2308.04185v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#20998;&#24067;&#24335;&#32447;&#24615;&#22238;&#24402;&#35745;&#31639;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#21644;&#25913;&#36827;&#24322;&#27493;&#31995;&#32479;&#20013;&#30340;&#22359;&#25928;&#24212;&#38887;&#24615;&#65292;&#23558;&#20449;&#24687;&#20445;&#25252;&#19982;&#22238;&#24402;&#38382;&#39064;&#32500;&#24230;&#30340;&#20943;&#23567;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#21644;&#23376;&#37319;&#26679;"&#22359;"&#65292;&#23454;&#29616;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#26032;&#33609;&#22270;&#30340;&#20998;&#24067;&#24335;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23545;&#23376;&#37319;&#26679;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;&#36827;&#34892;&#20102;&#25512;&#24191;&#24182;&#20462;&#25913;&#20197;&#20445;&#35777;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#32447;&#24615;&#22238;&#24402;&#20998;&#24067;&#24335;&#35745;&#31639;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#65292;&#24182;&#25913;&#21892;&#20102;&#24322;&#27493;&#31995;&#32479;&#20013;&#30340;&#22359;&#25928;&#24212;&#38887;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#65292;&#28982;&#21518;&#23545;"&#22359;"&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#20197;&#21516;&#26102;&#20445;&#25252;&#20449;&#24687;&#24182;&#20943;&#23567;&#22238;&#24402;&#38382;&#39064;&#30340;&#32500;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#36716;&#25442;&#23545;&#24212;&#20110;"&#36817;&#20284;&#26799;&#24230;&#32534;&#30721;&#26041;&#26696;"&#20013;&#30340;&#32534;&#30721;&#21152;&#23494;&#65292;&#32780;&#23376;&#37319;&#26679;&#23545;&#24212;&#20110;&#38750;&#25955;&#20081;&#24037;&#20316;&#33410;&#28857;&#30340;&#21709;&#24212;&#65307;&#22312;&#19968;&#20010;&#38598;&#20013;&#24335;&#32534;&#30721;&#35745;&#31639;&#32593;&#32476;&#20013;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;"&#36845;&#20195;&#33609;&#22270;"&#26041;&#27861;&#65292;&#29992;&#20110;$\ell_2$-&#23376;&#31354;&#38388;&#23884;&#20837;&#65292;&#21363;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#19968;&#20010;&#26032;&#30340;&#33609;&#22270;&#12290;&#25105;&#20204;&#36824;&#19987;&#27880;&#20110;"&#23376;&#37319;&#26679;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;"&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23558;&#20854;&#25512;&#24191;&#20026;&#22359;&#37319;&#26679;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#20462;&#25913;&#35813;&#26041;&#27861;&#20197;&#30830;&#20445;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose methods for speeding up linear regression distributively, while ensuring security. We leverage randomized sketching techniques, and improve straggler resilience in asynchronous systems. Specifically, we apply a random orthonormal matrix and then subsample \textit{blocks}, to simultaneously secure the information and reduce the dimension of the regression problem. In our setup, the transformation corresponds to an encoded encryption in an \textit{approximate gradient coding scheme}, and the subsampling corresponds to the responses of the non-straggling workers; in a centralized coded computing network. This results in a distributive \textit{iterative sketching} approach for an $\ell_2$-subspace embedding, \textit{i.e.} a new sketch is considered at each iteration. We also focus on the special case of the \textit{Subsampled Randomized Hadamard Transform}, which we generalize to block sampling; and discuss how it can be modified in order to secure the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#30740;&#31350;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#65288;SUD&#65289;&#20998;&#31867;&#21644;&#26816;&#27979;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;SUD&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#27880;&#37322;&#27169;&#24577;&#23545;SUD&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#39046;&#22495;&#19987;&#23478;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2308.04180</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21516;&#35270;&#35282;&#30740;&#31350;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#20998;&#31867;&#65288;SUD&#65289;&#65306;&#8220;&#25105;&#20204;&#26159;&#21542;&#22312;&#21516;&#19968;&#39029;&#19978;&#65311;&#8221;
&lt;/p&gt;
&lt;p&gt;
Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: "Are we on the same page ?". (arXiv:2308.04180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#30740;&#31350;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#65288;SUD&#65289;&#20998;&#31867;&#21644;&#26816;&#27979;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;SUD&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#27880;&#37322;&#27169;&#24577;&#23545;SUD&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#39046;&#22495;&#19987;&#23478;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#32447;&#25991;&#26412;&#20013;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#65288;SUD&#65289;&#30340;&#29305;&#24449;&#21644;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#19981;&#21516;&#22312;&#32447;&#26469;&#28304;&#30340;&#22823;&#37327;&#25163;&#24037;&#27880;&#37322;&#25991;&#26412;&#65292;&#36825;&#20123;&#25991;&#26412;&#22312;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;SUD&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#20013;&#20351;&#29992;&#12290;&#36825;&#31181;&#20840;&#23616;&#32972;&#26223;&#20351;&#25105;&#20204;&#21487;&#20197;&#27979;&#35797;&#33719;&#21462;&#20851;&#20110;&#30456;&#21516;SUD&#31867;&#21035;&#30340;&#30693;&#35782;&#30340;SUD&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#30693;&#35782;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35752;&#35770;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#20998;&#26512;&#27880;&#37322;&#27169;&#24577;&#21487;&#33021;&#23545;SUD&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#25968;&#25454;&#27934;&#23519;&#65292;&#21487;&#20197;&#25903;&#25345;&#39046;&#22495;&#19987;&#23478;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Socially Unacceptable Discourse (SUD) characterization and detection in online text. We first build and present a novel corpus that contains a large variety of manually annotated texts from different online sources used so far in state-of-the-art Machine learning (ML) SUD detection solutions. This global context allows us to test the generalization ability of SUD classifiers that acquire knowledge around the same SUD categories, but from different contexts. From this perspective, we can analyze how (possibly) different annotation modalities influence SUD learning by discussing open challenges and open research directions. We also provide several data insights which can support domain experts in the annotation task.
&lt;/p&gt;</description></item><item><title>&#21452;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;DI-NN&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#28304;&#23450;&#20301;&#31639;&#27861;&#20013;&#30340;&#38899;&#39057;&#20449;&#21495;&#21644;&#22330;&#26223;&#22768;&#23398;&#23646;&#24615;&#30340;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DI-NN&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#21644;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20855;&#26377;&#26356;&#20302;&#30340;&#23450;&#20301;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.04169</link><description>&lt;p&gt;
&#21452;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22768;&#28304;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Dual input neural networks for positional sound source localization. (arXiv:2308.04169v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04169
&lt;/p&gt;
&lt;p&gt;
&#21452;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;DI-NN&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#28304;&#23450;&#20301;&#31639;&#27861;&#20013;&#30340;&#38899;&#39057;&#20449;&#21495;&#21644;&#22330;&#26223;&#22768;&#23398;&#23646;&#24615;&#30340;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DI-NN&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#21644;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20855;&#26377;&#26356;&#20302;&#30340;&#23450;&#20301;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#20803;&#25968;&#25454;&#21487;&#20197;&#26377;&#21033;&#22320;&#19982;&#39640;&#32500;&#20449;&#21495;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#20135;&#29983;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#22312;&#32463;&#20856;&#30340;&#22768;&#28304;&#23450;&#20301;&#31639;&#27861;&#20013;&#65292;&#23558;&#22810;&#20010;&#20998;&#24067;&#24335;&#40614;&#20811;&#39118;&#25509;&#25910;&#21040;&#30340;&#39640;&#32500;&#22810;&#36890;&#36947;&#38899;&#39057;&#20449;&#21495;&#30340;&#20449;&#24687;&#19982;&#25551;&#36848;&#22330;&#26223;&#22768;&#23398;&#29305;&#24615;&#65288;&#22914;&#40614;&#20811;&#39118;&#22312;&#31354;&#38388;&#20013;&#30340;&#22352;&#26631;&#65289;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20197;&#20272;&#35745;&#22768;&#28304;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21452;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;DI-NNs&#65289;&#20316;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#24314;&#27169;&#36825;&#20004;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#38590;&#24230;&#21644;&#30495;&#23454;&#24615;&#30340;&#22330;&#26223;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;DI-NN&#65292;&#24182;&#23558;&#20854;&#19982;&#21478;&#19968;&#31181;&#26550;&#26500;&#12289;&#32463;&#20856;&#26368;&#23567;&#20108;&#20056;&#65288;LS&#65289;&#26041;&#27861;&#20197;&#21450;&#32463;&#20856;&#21367;&#31215;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;CRNN&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DI-NN&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#65292;&#23450;&#20301;&#35823;&#24046;&#27604;LS&#26041;&#27861;&#20302;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many signal processing applications, metadata may be advantageously used in conjunction with a high dimensional signal to produce a desired output. In the case of classical Sound Source Localization (SSL) algorithms, information from a high dimensional, multichannel audio signals received by many distributed microphones is combined with information describing acoustic properties of the scene, such as the microphones' coordinates in space, to estimate the position of a sound source. We introduce Dual Input Neural Networks (DI-NNs) as a simple and effective way to model these two data types in a neural network. We train and evaluate our proposed DI-NN on scenarios of varying difficulty and realism and compare it against an alternative architecture, a classical Least-Squares (LS) method as well as a classical Convolutional Recurrent Neural Network (CRNN). Our results show that the DI-NN significantly outperforms the baselines, achieving a five times lower localization error than the LS
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#24314;&#35758;&#37319;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.04137</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;&#25581;&#31034;&#20986;&#24778;&#20154;&#30340;&#32570;&#20047;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness. (arXiv:2308.04137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04137
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#24314;&#35758;&#37319;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#32780;&#31283;&#20581;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#24320;&#21457;&#26412;&#36523;&#31283;&#20581;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#22120;&#30340;&#24120;&#35268;&#35780;&#20272;&#21327;&#35758;&#22312;&#32508;&#21512;&#35780;&#20272;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#26377;&#38480;&#31867;&#22411;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#24573;&#35270;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#26631;&#20934;&#27979;&#35797;&#25968;&#25454;&#26080;&#27861;&#35780;&#20272;&#20998;&#31867;&#22120;&#23545;&#20110;&#26410;&#32463;&#35757;&#32451;&#30340;&#31867;&#21035;&#26679;&#26412;&#30340;&#39044;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#21253;&#21547;&#26410;&#30693;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#26080;&#27861;&#35780;&#20272;&#20998;&#31867;&#22120;&#23545;&#20110;&#24050;&#30693;&#31867;&#21035;&#26631;&#31614;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20513;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21487;&#24212;&#29992;&#20110;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#30340;&#21333;&#19968;&#25351;&#26631;&#65292;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#20351;&#29992;&#35748;&#20026;&#26159;&#20840;&#38754;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#20063;&#23384;&#22312;&#32570;&#20047;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates bench-marking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using such a benchmark it is found that current deep neural networks, including those trained with methods that are believed to pro
&lt;/p&gt;</description></item><item><title>OmniDataComposer&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#35843;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#21512;&#24182;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#25968;&#25454;&#26657;&#27491;&#12290;</title><link>http://arxiv.org/abs/2308.04126</link><description>&lt;p&gt;
OmniDataComposer: &#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#30340;&#32479;&#19968;&#25968;&#25454;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation. (arXiv:2308.04126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04126
&lt;/p&gt;
&lt;p&gt;
OmniDataComposer&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#35843;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#21512;&#24182;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#25968;&#25454;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;OmniDataComposer&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21644;&#26080;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#31616;&#21270;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#26680;&#24515;&#30340;&#31361;&#30772;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#21644;&#21512;&#24182;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#21327;&#35843;&#25968;&#25454;&#32467;&#26500;&#65292;&#21253;&#25324;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#35270;&#39057;/&#22270;&#20687;&#23383;&#24149;&#25552;&#21462;&#12289;&#23494;&#38598;&#23383;&#24149;&#25552;&#21462;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#12289;Recognize Anything Model&#65288;RAM&#65289;&#21644;&#29289;&#20307;&#36319;&#36394;&#31561;&#22810;&#31181;&#25805;&#20316;&#30340;&#36827;&#23637;&#12290;OmniDataComposer&#33021;&#22815;&#35782;&#21035;&#36229;&#36807;6400&#31181;&#23545;&#35937;&#31867;&#21035;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#35270;&#35273;&#20449;&#24687;&#30340;&#33539;&#22260;&#12290;&#23427;&#23558;&#36825;&#20123;&#22810;&#26679;&#30340;&#27169;&#24577;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20419;&#36827;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#65292;&#24182;&#20419;&#36827;&#36328;&#27169;&#24577;&#25968;&#25454;&#26657;&#27491;&#12290;&#26368;&#32456;&#36755;&#20986;&#23558;&#27599;&#20010;&#35270;&#39057;&#36755;&#20837;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#39034;&#24207;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text. Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption extraction, Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything Model(RAM), and object tracking. OmniDataComposer is capable of identifying over 6400 categories of objects, substantially broadening the spectrum of visual information. It amalgamates these diverse modalities, promoting reciprocal enhancement among modalities and facilitating cross-modal data correction. \textbf{The final output metamorphoses each video input into an elaborate sequential docum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.04119</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#23450;&#21046;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Constructing Custom Thermodynamics Using Deep Learning. (arXiv:2308.04119v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#26159;&#22522;&#20110;&#20808;&#21069;&#31215;&#32047;&#30340;&#25968;&#25454;&#20197;&#21450;&#24050;&#30693;&#30340;&#29289;&#29702;&#21407;&#29702;&#65288;&#21253;&#25324;&#23545;&#31216;&#24615;&#21644;&#23432;&#24658;&#23450;&#24459;&#65289;&#25552;&#20379;&#30340;&#38480;&#21046;&#65292;&#36827;&#34892;&#33258;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#26679;&#30340;&#33258;&#21160;&#20551;&#35774;&#21019;&#24314;&#21644;&#39564;&#35777;&#21487;&#20197;&#24110;&#21161;&#31185;&#23398;&#23478;&#30740;&#31350;&#22797;&#26434;&#30340;&#29616;&#35937;&#65292;&#20256;&#32479;&#30340;&#29289;&#29702;&#30452;&#35273;&#21487;&#33021;&#26080;&#27861;&#24212;&#23545;&#12290;&#23588;&#20854;&#37325;&#35201;&#30340;&#26159;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20854;&#26102;&#38388;&#28436;&#21464;&#21463;&#21040;&#21464;&#21270;&#30340;&#22806;&#37096;&#21442;&#25968;&#30340;&#24378;&#28872;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#30452;&#25509;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#37027;&#20123;&#24494;&#35266;&#25551;&#36848;&#23436;&#25972;&#19981;&#20999;&#23454;&#38469;&#12289;&#26500;&#24314;&#29702;&#35770;&#23439;&#35266;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#30693;&#35782;&#25110;&#35797;&#38169;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most exciting applications of AI is automated scientific discovery based on previously amassed data, coupled with restrictions provided by the known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Of particular importance are complex dynamic systems where their time evolution is strongly influenced by varying external parameters. In this paper we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes render complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#21152;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#21534;&#21520;&#37327;&#12290;&#35813;&#31574;&#30053;&#32500;&#25252;&#19968;&#20010;&#20010;&#20307;&#38431;&#21015;&#65292;&#24182;&#22312;&#36866;&#24403;&#25968;&#37327;&#30340;&#20010;&#20307;&#34987;&#35780;&#20272;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04102</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24322;&#27493;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Evolution of Deep Neural Network Architectures. (arXiv:2308.04102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#21152;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#21534;&#21520;&#37327;&#12290;&#35813;&#31574;&#30053;&#32500;&#25252;&#19968;&#20010;&#20010;&#20307;&#38431;&#21015;&#65292;&#24182;&#22312;&#36866;&#24403;&#25968;&#37327;&#30340;&#20010;&#20307;&#34987;&#35780;&#20272;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#36827;&#21270;&#31639;&#27861;(EAs)&#21033;&#29992;&#20505;&#36873;&#35299;&#30340;&#24182;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#35780;&#20272;&#26102;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#35768;&#22810;&#24037;&#20316;&#33410;&#28857;(&#21363;&#35745;&#31639;&#23458;&#25143;&#31471;)&#22823;&#37096;&#20998;&#26102;&#38388;&#37117;&#22788;&#20110;&#38386;&#32622;&#29366;&#24577;&#65292;&#31561;&#24453;&#19979;&#19968;&#20195;&#30340;&#21019;&#24314;&#12290;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(ENAS)&#26159;&#19968;&#31867;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;EA&#65292;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;(AES)&#65292;&#28982;&#21518;&#23558;&#20854;&#36866;&#37197;&#21040;ENAS&#19978;&#12290;AES&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#22810;&#36798;$K$&#20010;&#20010;&#20307;&#30340;&#38431;&#21015;&#65292;&#36825;&#20123;&#20010;&#20307;&#24050;&#20934;&#22791;&#22909;&#34987;&#21457;&#36865;&#21040;&#24037;&#20316;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#30001;&#24037;&#20316;&#22120;&#35780;&#20272;&#20102;$M&lt;&lt;K$&#20010;&#20010;&#20307;&#20043;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#12290;&#21512;&#36866;&#30340;$M$&#20540;&#26159;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#30340;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#23637;&#31034;AES&#30340;&#26222;&#36866;&#24615;&#21644;&#33021;&#21147;&#65292;&#39318;&#20808;&#22312;11&#20301;&#22810;&#36335;&#22797;&#29992;&#22120;&#35774;&#35745;(&#19968;&#20010;&#21333;&#20010;&#31181;&#32676;&#21487;&#39564;&#35777;&#30340;&#21457;&#29616;&#20219;&#21153;)&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e.,\ compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of upto $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M&lt;&lt;K$ individuals have been evaluated by the workers. A suitable value for $M$ is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in 11-bit multiplexer design (a single-population verifiable discovery ta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#25193;&#23637;QUARK&#26694;&#26550;&#20197;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#31034;&#20363;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.04082</link><description>&lt;p&gt;
&#20351;&#29992;QUARK&#30340;&#38754;&#21521;&#24212;&#29992;&#30340;&#37327;&#23376;&#29983;&#25104;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK. (arXiv:2308.04082v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04082
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#25193;&#23637;QUARK&#26694;&#26550;&#20197;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#31034;&#20363;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#21487;&#21464;&#24615;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#27169;&#22411;&#20551;&#35774;&#12289;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#25216;&#26415;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;QUARK&#26694;&#26550;&#31616;&#21270;&#21644;&#26631;&#20934;&#21270;&#20102;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#25193;&#23637;QUARK&#30340;&#26041;&#27861;&#65292;&#20197;&#21253;&#25324;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#26356;&#26032;&#21518;&#30340;&#36719;&#20214;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#31034;&#20363;&#24212;&#29992;&#35828;&#26126;&#20102;&#20854;&#28789;&#27963;&#24615;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#30005;&#36335;&#20551;&#35774;&#12289;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#36716;&#25442;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#12290;&#65288;2&#65289;&#25105;&#20204;&#22312;GPU&#21644;&#30495;&#23454;&#30340;&#37327;&#23376;&#30828;&#20214;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#65288;3&#65289;&#25105;&#20204;&#21033;&#29992;&#19968;&#31995;&#21015;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20363;&#22914;&#29983;&#25104;&#25968;&#25454;&#30340;&#26032;&#39062;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking of quantum machine learning (QML) algorithms is challenging due to the complexity and variability of QML systems, e.g., regarding model ansatzes, data sets, training techniques, and hyper-parameters selection. The QUantum computing Application benchmaRK (QUARK) framework simplifies and standardizes benchmarking studies for quantum computing applications. Here, we propose several extensions of QUARK to include the ability to evaluate the training and deployment of quantum generative models. We describe the updated software architecture and illustrate its flexibility through several example applications: (1) We trained different quantum generative models using several circuit ansatzes, data sets, and data transformations. (2) We evaluated our models on GPU and real quantum hardware. (3) We assessed the generalization capabilities of our generative models using a broad set of metrics that capture, e.g., the novelty and validity of the generated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#20449;&#24687;&#30340;&#20195;&#29702;&#26799;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;&#20013;&#30340;&#26597;&#35810;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04077</link><description>&lt;p&gt;
&#20351;&#29992;&#36712;&#36857;&#20449;&#24687;&#30340;&#20195;&#29702;&#26799;&#24230;&#36827;&#34892;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (arXiv:2308.04077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#20449;&#24687;&#30340;&#20195;&#29702;&#26799;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;&#20013;&#30340;&#26597;&#35810;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#20248;&#21270;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#23427;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#65288;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#33021;&#22815;&#20849;&#21516;&#20248;&#21270;&#19968;&#20010;&#20840;&#23616;&#20989;&#25968;&#12290;&#36825;&#20123;&#23458;&#25143;&#31471;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#21482;&#20849;&#20139;&#26412;&#22320;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#32852;&#37030;&#20248;&#21270;&#24212;&#29992;&#20013;&#65292;&#26799;&#24230;&#20449;&#24687;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#24341;&#20986;&#20102;&#32852;&#37030;&#38646;&#38454;&#20248;&#21270;&#65288;ZOO&#65289;&#30340;&#33539;&#24335;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;ZOO&#31639;&#27861;&#23384;&#22312;&#26597;&#35810;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#38480;&#21046;&#65292;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#65306;&#65288;a&#65289;&#23427;&#20204;&#23545;&#26799;&#24230;&#20272;&#35745;&#38656;&#35201;&#22823;&#37327;&#30340;&#20989;&#25968;&#26597;&#35810;&#65307;&#65288;b&#65289;&#23427;&#20204;&#30340;&#23454;&#38469;&#26412;&#22320;&#26356;&#26032;&#19982;&#39044;&#26399;&#20840;&#23616;&#26356;&#26032;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#36712;&#36857;&#20449;&#24687;&#30340;&#26799;&#24230;&#20195;&#29702;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20989;&#25968;&#26597;&#35810;&#21382;&#21490;&#36827;&#34892;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#23545;&#28608;&#27963;&#20989;&#25968;&#38750;&#24120;&#25935;&#24863;&#65292;&#20026;&#20102;&#36991;&#20813;&#20302;&#25928;&#30340;&#25163;&#21160;&#36873;&#25321;&#24182;&#20943;&#36731;&#20248;&#21270;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#23398;&#20064;&#20505;&#36873;&#28608;&#27963;&#20989;&#25968;&#32452;&#21512;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;PINNs&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04073</link><description>&lt;p&gt;
&#20026;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19987;&#38376;&#30340;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Specialized Activation Functions for Physics-informed Neural Networks. (arXiv:2308.04073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#23545;&#28608;&#27963;&#20989;&#25968;&#38750;&#24120;&#25935;&#24863;&#65292;&#20026;&#20102;&#36991;&#20813;&#20302;&#25928;&#30340;&#25163;&#21160;&#36873;&#25321;&#24182;&#20943;&#36731;&#20248;&#21270;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#23398;&#20064;&#20505;&#36873;&#28608;&#27963;&#20989;&#25968;&#32452;&#21512;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;PINNs&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;PINNs&#30340;&#20248;&#21270;&#22256;&#38590;&#19982;&#28608;&#27963;&#20989;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#65292;PINNs&#23545;&#28608;&#27963;&#20989;&#25968;&#38750;&#24120;&#25935;&#24863;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20302;&#25928;&#30340;&#35797;&#38169;&#26041;&#24335;&#36873;&#25321;&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#36991;&#20813;&#20302;&#25928;&#30340;&#25163;&#21160;&#36873;&#25321;&#65292;&#24182;&#20943;&#36731;PINNs&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#22312;&#35299;&#20915;&#19981;&#21516;&#38382;&#39064;&#26102;&#25628;&#32034;&#26368;&#20339;&#20989;&#25968;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;PINNs&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#23398;&#20064;&#20505;&#36873;&#28608;&#27963;&#20989;&#25968;&#32452;&#21512;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;PINNs&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#23545;&#25152;&#23398;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21644;&#22810;&#26679;&#24615;&#26377;&#26356;&#39640;&#35201;&#27714;&#12290;&#36890;&#36807;&#31227;&#38500;&#19981;&#33021;&#28385;&#36275;&#35201;&#27714;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which canno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31614;&#21517;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20840;&#23616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04071</link><description>&lt;p&gt;
&#36335;&#24452;&#31614;&#21517;&#22312;&#27010;&#29575;&#36712;&#36857;&#20248;&#21270;&#20013;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Path Signatures for Diversity in Probabilistic Trajectory Optimisation. (arXiv:2308.04071v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31614;&#21517;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20840;&#23616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25104;&#26412;&#34987;&#26368;&#23567;&#21270;&#65292;&#20197;&#29983;&#25104;&#36712;&#36857;&#20026;&#20989;&#25968;&#12290;&#22312;&#20855;&#26377;&#22810;&#20010;&#38556;&#30861;&#29289;&#21644;&#22797;&#26434;&#20960;&#20309;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#24456;&#38590;&#35299;&#20915;&#65292;&#24182;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35745;&#31639;&#30828;&#20214;&#30340;&#36827;&#27493;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#65292;&#20854;&#20013;&#21516;&#26102;&#24471;&#21040;&#22810;&#20010;&#35299;&#65292;&#27599;&#20010;&#35299;&#20174;&#19981;&#21516;&#30340;&#36215;&#22987;&#28857;&#21021;&#22987;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22914;&#26524;&#27809;&#26377;&#19968;&#20010;&#31574;&#30053;&#38450;&#27490;&#20004;&#20010;&#35299;&#22604;&#38519;&#22312;&#19968;&#36215;&#65292;&#31616;&#21333;&#30340;&#24182;&#34892;&#20248;&#21270;&#20250;&#36973;&#21463;&#27169;&#24335;&#22604;&#38519;&#30340;&#38382;&#39064;&#65292;&#38477;&#20302;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#25214;&#21040;&#20840;&#23616;&#35299;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#31895;&#36335;&#24452;&#29702;&#35770;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#24182;&#34892;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#27169;&#24335;&#22604;&#38519;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20840;&#23616;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Motion planning can be cast as a trajectory optimisation problem where a cost is minimised as a function of the trajectory being generated. In complex environments with several obstacles and complicated geometry, this optimisation problem is usually difficult to solve and prone to local minima. However, recent advancements in computing hardware allow for parallel trajectory optimisation where multiple solutions are obtained simultaneously, each initialised from a different starting point. Unfortunately, without a strategy preventing two solutions to collapse on each other, naive parallel optimisation can suffer from mode collapse diminishing the efficiency of the approach and the likelihood of finding a global solution. In this paper we leverage on recent advances in the theory of rough paths to devise an algorithm for parallel trajectory optimisation that promotes diversity over the range of solutions, therefore avoiding mode collapses and achieving better global properties. Our appro
&lt;/p&gt;</description></item><item><title>ConDistFL&#26694;&#26550;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20805;&#20998;&#35774;&#35745;&#26465;&#20214;&#27010;&#29575;&#34920;&#31034;&#20174;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#25552;&#21462;&#26080;&#26631;&#27880;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#26377;&#38480;&#20840;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22810;&#20010;&#33145;&#37096;CT&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04070</link><description>&lt;p&gt;
ConDistFL&#65306;&#38024;&#23545;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data. (arXiv:2308.04070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04070
&lt;/p&gt;
&lt;p&gt;
ConDistFL&#26694;&#26550;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20805;&#20998;&#35774;&#35745;&#26465;&#20214;&#27010;&#29575;&#34920;&#31034;&#20174;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#25552;&#21462;&#26080;&#26631;&#27880;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#26377;&#38480;&#20840;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22810;&#20010;&#33145;&#37096;CT&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#25551;&#32472;&#22810;&#20010;&#22120;&#23448;&#21644;&#30142;&#30149;&#30340;&#24191;&#20041;&#20998;&#21106;&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26080;&#38656;&#20132;&#25442;&#35757;&#32451;&#25968;&#25454;&#30340;&#21327;&#20316;&#24320;&#21457;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#20840;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#30340;&#33719;&#21462;&#26435;&#38480;&#32473;&#35757;&#32451;&#21487;&#27867;&#21270;&#27169;&#22411;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;ConDistFL&#8221;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;FL&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#22320;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#35774;&#35745;&#30340;&#26465;&#20214;&#27010;&#29575;&#34920;&#31034;&#20174;&#20840;&#23616;&#27169;&#22411;&#30340;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#25552;&#21462;&#26080;&#26631;&#27880;&#22120;&#23448;&#21644;&#32959;&#30244;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;MSD&#21644;KiTS19&#25361;&#25112;&#36187;&#30340;&#22235;&#20010;&#19981;&#21516;&#37096;&#20998;&#26631;&#27880;&#33145;&#37096;CT&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26126;&#26174;&#20248;&#20110;FedAvg&#21644;FedOpt&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#23545;&#22806;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#34920;&#29616;&#26174;&#31034;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing a generalized segmentation model capable of simultaneously delineating multiple organs and diseases is highly desirable. Federated learning (FL) is a key technology enabling the collaborative development of a model without exchanging training data. However, the limited access to fully annotated training data poses a major challenge to training generalizable models. We propose "ConDistFL", a framework to solve this problem by combining FL with knowledge distillation. Local models can extract the knowledge of unlabeled organs and tumors from partially annotated data from the global model with an adequately designed conditional probability representation. We validate our framework on four distinct partially annotated abdominal CT datasets from the MSD and KiTS19 challenges. The experimental results show that the proposed framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the performance on an external test dataset demonstrates superior generalizability c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#30340;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#39033;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04061</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#27491;&#21017;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#25552;&#39640;&#20302;&#26631;&#31614;&#29615;&#22659;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation. (arXiv:2308.04061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#30340;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#39033;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#19982;&#26500;&#24314;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#19979;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#40065;&#26834;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#36825;&#20004;&#20010;&#19978;&#30028;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#39033;&#19982;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;&#23588;&#20854;&#26159;&#19982;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;K-Means&#32858;&#31867;&#26041;&#27861;&#65292;&#21021;&#27493;&#30740;&#31350;&#20102;&#26032;&#35199;&#20848;&#20799;&#31461;&#31119;&#21033;&#31995;&#32479;&#30340;&#39044;&#27979;&#39118;&#38505;&#24314;&#27169;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#24449;&#24182;&#20102;&#35299;&#20102;&#20854;&#23545;&#24403;&#21069;&#39118;&#38505;&#24314;&#27169;&#26694;&#26550;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04060</link><description>&lt;p&gt;
&#25913;&#36827;&#21033;&#29992;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#26032;&#35199;&#20848;&#20799;&#31461;&#31119;&#21033;&#31995;&#32479;&#30340;&#39044;&#27979;&#39118;&#38505;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Toward Improving Predictive Risk Modelling for New Zealand's Child Welfare System Using Clustering Methods. (arXiv:2308.04060v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;K-Means&#32858;&#31867;&#26041;&#27861;&#65292;&#21021;&#27493;&#30740;&#31350;&#20102;&#26032;&#35199;&#20848;&#20799;&#31461;&#31119;&#21033;&#31995;&#32479;&#30340;&#39044;&#27979;&#39118;&#38505;&#24314;&#27169;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#24449;&#24182;&#20102;&#35299;&#20102;&#20854;&#23545;&#24403;&#21069;&#39118;&#38505;&#24314;&#27169;&#26694;&#26550;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#21028;&#26029;&#21644;&#39044;&#27979;&#39118;&#38505;&#27169;&#22411;&#30340;&#32467;&#21512;&#23545;&#31038;&#24037;&#22312;&#21010;&#20998;&#22788;&#20110;&#34384;&#24453;&#39118;&#38505;&#20013;&#30340;&#20799;&#31461;&#24182;&#20915;&#23450;&#20309;&#26102;&#37319;&#21462;&#24178;&#39044;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#25919;&#24220;&#31119;&#21033;&#26426;&#26500;&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#24320;&#22987;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#39044;&#27979;&#39118;&#38505;&#24314;&#27169;&#24037;&#20316;&#12290;&#34429;&#28982;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#19982;&#20799;&#31461;&#34384;&#24453;&#26377;&#20851;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#20294;&#20173;&#23384;&#22312;&#24456;&#22810;&#31354;&#30333;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#39118;&#38505;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#39044;&#27979;&#39118;&#38505;&#27169;&#22411;&#22312;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#20799;&#31461;&#20013;&#26159;&#21542;&#34920;&#29616;&#19981;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;K-Means&#32858;&#31867;&#65292;&#21021;&#27493;&#21457;&#29616;&#20102;&#25105;&#20204;&#22312;&#30830;&#23450;&#36825;&#20123;&#29305;&#24449;&#21450;&#20854;&#23545;&#24403;&#21069;&#39118;&#38505;&#24314;&#27169;&#26694;&#26550;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23545;&#26032;&#35199;&#20848;&#65288;NZ&#65289;&#34987;&#25253;&#21578;&#26377;&#25252;&#29702;&#21644;&#20445;&#25252;&#38382;&#39064;&#30340;&#20799;&#31461;&#30340;&#29616;&#23384;&#12289;&#23578;&#26410;&#30830;&#23450;&#30340;&#32858;&#31867;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of clinical judgement and predictive risk models crucially assist social workers to segregate children at risk of maltreatment and decide when authorities should intervene. Predictive risk modelling to address this matter has been initiated by several governmental welfare authorities worldwide involving administrative data and machine learning algorithms. While previous studies have investigated risk factors relating to child maltreatment, several gaps remain as to understanding how such risk factors interact and whether predictive risk models perform differently for children with different features. By integrating Principal Component Analysis and K-Means clustering, this paper presents initial findings of our work on the identification of such features as well as their potential effect on current risk modelling frameworks. This approach allows examining existent, unidentified yet, clusters of New Zealand (NZ) children reported with care and protection concerns, as well
&lt;/p&gt;</description></item><item><title>&#20116;&#32654;&#20803;&#27169;&#22411;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#32534;&#30721;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#29255;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04052</link><description>&lt;p&gt;
&#20116;&#32654;&#20803;&#27169;&#22411;&#65306;&#20174;&#21477;&#23376;&#23884;&#20837;&#29983;&#25104;&#28216;&#25103;&#22320;&#22270;&#21644;&#31934;&#28789;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings. (arXiv:2308.04052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04052
&lt;/p&gt;
&lt;p&gt;
&#20116;&#32654;&#20803;&#27169;&#22411;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#32534;&#30721;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#29255;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20116;&#32654;&#20803;&#27169;&#22411;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#32534;&#30721;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#29255;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#22312;&#20302;&#32500;&#24230;&#39046;&#22495;&#20013;&#25104;&#21151;&#29983;&#25104;&#20934;&#30830;&#19988;&#32654;&#35266;&#30340;&#20869;&#23481;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#24456;&#23567;&#65292;&#20294;&#29983;&#25104;&#30340;&#22270;&#29255;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22411;&#24212;&#29992;&#20110;&#19977;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#65306;&#20687;&#32032;&#33402;&#26415;&#30340;&#28216;&#25103;&#22320;&#22270;&#12289;&#28216;&#25103;&#35282;&#33394;&#31934;&#28789;&#22270;&#20687;&#21644;&#32553;&#23567;&#30340;&#34920;&#24773;&#31526;&#21495;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;&#26032;&#39062;&#30340;&#25193;&#20805;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#36825;&#20123;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;CLIP VIT-B/32&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#29366;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35774;&#35745;&#31354;&#38388;&#32500;&#24230;&#21644;&#24314;&#27169;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#25928;&#29575;&#21644;&#29983;&#25104;&#26080;&#20960;&#20309;&#24322;&#24120;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.04051</link><description>&lt;p&gt;
&#24418;&#29366;&#20248;&#21270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#35774;&#35745;&#31354;&#38388;&#32500;&#24230;&#38477;&#20302;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization. (arXiv:2308.04051v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#29366;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35774;&#35745;&#31354;&#38388;&#32500;&#24230;&#21644;&#24314;&#27169;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#25928;&#29575;&#21644;&#29983;&#25104;&#26080;&#20960;&#20309;&#24322;&#24120;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24418;&#29366;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20004;&#20010;&#30446;&#26631;&#26159;&#25552;&#39640;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#29983;&#25104;&#27809;&#26377;&#20960;&#20309;&#24322;&#24120;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;&#36890;&#36807;&#20943;&#23569;&#23450;&#20041;&#26032;&#30340;&#20943;&#23569;&#23376;&#31354;&#38388;&#30340;&#21407;&#22987;&#35774;&#35745;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#32447;&#24615;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#24314;&#27169;&#25968;&#25454;&#30340;&#24213;&#23618;&#29983;&#25104;&#36807;&#31243;&#65292;&#22914;&#22240;&#23376;&#20998;&#26512;&#21644;&#27010;&#29575;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24418;&#29366;&#20462;&#25913;&#26041;&#27861;&#26159;&#32447;&#24615;&#30340;&#19988;&#35774;&#35745;&#21464;&#37327;&#22312;&#22343;&#21248;&#38543;&#26426;&#37319;&#26679;&#26102;&#65292;&#25968;&#25454;&#36817;&#20284;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#65292;&#36825;&#26159;&#30001;&#20110;&#30452;&#25509;&#24212;&#29992;&#20102;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;&#21033;&#29992;&#39532;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#35770;&#25991;&#35777;&#26126;&#24322;&#24120;&#35774;&#35745;&#24448;&#24448;&#20855;&#26377;&#36739;&#39640;&#30340;&#35813;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work presents a novel approach to shape optimization, that has the twofold objective to improve the efficiency of global optimization algorithms while promoting the generation of high-quality designs during the optimization process free of geometrical anomalies. This is accomplished by reducing the number of the original design variables defining a new reduced subspace where the geometrical variance is maximized and modeling the underlying generative process of the data via probabilistic linear latent variable models such as Factor Analysis and Probabilistic Principal Component Analysis. We show that the data follows approximately a Gaussian distribution when the shape modification method is linear and the design variables are sampled uniformly at random, due to the direct application of the central limit theorem. The model uncertainty is measured in terms of Mahalanobis distance, and the paper demonstrates that anomalous designs tend to exhibit a high value of this metric. This en
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;TF-IDF&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#24182;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#27604;&#20110;N-Gram&#65292;&#20351;&#29992;TF-IDF&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04037</link><description>&lt;p&gt;
TF-IDF&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#21450;&#20854;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset. (arXiv:2308.04037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;TF-IDF&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#24182;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#27604;&#20110;N-Gram&#65292;&#20351;&#29992;TF-IDF&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#23558;&#25991;&#26412;&#20998;&#31867;&#21040;&#30456;&#20851;&#31867;&#21035;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#31639;&#27861;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#30340;&#26680;&#24515;&#12290;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#26415;&#35821;&#39057;&#29575;-&#36870;&#25991;&#20214;&#39057;&#29575; (TF-IDF) &#21644; NLP &#26159;&#26368;&#24120;&#29992;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#21644;&#20998;&#26512;&#20102;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#20004;&#20010;&#29305;&#24449; N-Gram &#21644; TF-IDF&#65292;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340; IMDB &#30005;&#24433;&#35780;&#35770;&#21644;&#20122;&#39532;&#36874; Alexa &#35780;&#35770;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#21363;&#25903;&#25345;&#21521;&#37327;&#26426; (SVM)&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031; (Multinomial NB)&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#20915;&#31574;&#26641;&#21644; K &#26368;&#36817;&#37051; (KNN)&#12290;&#20174;&#36825;&#20004;&#20010;&#29305;&#24449;&#25552;&#21462;&#20013;&#65292;&#20351;&#29992; TF-IDF &#29305;&#24449;&#30456;&#27604;&#22522;&#20110; N-Gram &#26377;&#20102;&#26174;&#33879;&#30340;&#29305;&#24449;&#25552;&#21462;&#22686;&#21152;&#12290;TF-IDF &#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575; (93.81%)&#12289;&#31934;&#30830;&#29575; (94.20%)&#12289;&#21484;&#22238;&#29575; (93.81%)&#65292;
&lt;/p&gt;
&lt;p&gt;
Text Classification is the process of categorizing text into the relevant categories and its algorithms are at the core of many Natural Language Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP are the most highly used information retrieval methods in text classification. We have investigated and analyzed the feature weighting method for text classification on unstructured data. The proposed model considered two features N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. Then we have used the state-of-the-art classifier to validate the method i.e., Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and k-nearest neighbors (KNN). From those two feature extractions, a significant increase in feature extraction with TF-IDF features rather than based on N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall (93.81%), an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.04028</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04028
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#37327;&#25991;&#26723;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#20197;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#38382;&#31572;&#20381;&#36182;&#20110;&#39640;&#25928;&#30340;&#27573;&#33853;&#26816;&#32034;&#26469;&#36873;&#25321;&#20505;&#36873;&#19978;&#19979;&#25991;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#65292;&#22914;TF-IDF&#25110;BM25&#65292;&#26159;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#22312;&#32593;&#32476;&#19978;&#65292;&#27809;&#26377;&#19968;&#31687;&#25991;&#31456;&#21487;&#20197;&#25552;&#20379;&#25152;&#26377;&#21487;&#33021;&#30340;&#31572;&#26696;&#65292;&#20197;&#22238;&#31572;&#29992;&#25143;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#31264;&#23494;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#23545;&#32500;&#22522;&#30334;&#31185;2018&#24180;12&#26376;20&#26085;&#30340;&#20542;&#38144;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#29992;&#20316;&#22238;&#31572;&#38382;&#39064;&#30340;&#28304;&#25991;&#26723;&#12290;&#38382;&#31572;&#31995;&#32479;&#22312;&#22810;&#20010;&#24320;&#25918;&#39046;&#22495;&#21644;&#26426;&#22120;&#29702;&#35299;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26681;&#25454;&#22810;&#39033;&#35843;&#26597;&#65292;&#26080;&#27861;&#20174;&#32500;&#22522;&#30334;&#31185;&#20934;&#30830;&#22238;&#31572;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33539;&#22260;&#25439;&#22833;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#20998;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04024</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#31867;&#21644;RL&#25506;&#32034;&#30340;&#33539;&#22260;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Scope Loss for Imbalanced Classification and RL Exploration. (arXiv:2308.04024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33539;&#22260;&#25439;&#22833;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#20998;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21644;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#22240;&#27492;&#23558;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#31561;&#21516;&#20110;&#30417;&#30563;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25214;&#20986;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#26041;&#24335;&#19978;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#20998;&#31867;&#30340;&#25439;&#22833;&#20989;&#25968;- &#33539;&#22260;&#25439;&#22833;&#12290;&#33539;&#22260;&#25439;&#22833;&#21487;&#20197;&#35843;&#25972;&#26799;&#24230;&#65292;&#20197;&#38450;&#27490;&#36807;&#24230;&#21033;&#29992;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#24615;&#33021;&#25439;&#22833;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#21644;&#19968;&#20010;&#20559;&#26012;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#33539;&#22260;&#25439;&#22833;&#65292;&#32467;&#26524;&#26174;&#31034;&#33539;&#22260;&#25439;&#22833;&#20248;&#20110;&#20854;&#20182;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26469;&#25913;&#36827;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04018</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#25913;&#36827;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Performance of Semi-Supervised Learning by Adversarial Attacks. (arXiv:2308.04018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#26469;&#25913;&#36827;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26159;&#24314;&#31435;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;&#20551;&#35774;&#19978;&#65292;&#21363;&#35775;&#38382;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#24456;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCAR&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#22914;&#20309;&#25104;&#21151;&#36873;&#25321;&#39640;&#32622;&#20449;&#24230;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19982;&#24403;&#21069;&#39044;&#27979;&#30340;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#65292;&#19977;&#20010;&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#19982;SCAR&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) algorithm is a setup built upon a realistic assumption that access to a large amount of labeled data is tough. In this study, we present a generalized framework, named SCAR, standing for Selecting Clean samples with Adversarial Robustness, for improving the performance of recent SSL algorithms. By adversarially attacking pre-trained models with semi-supervision, our framework shows substantial advances in classifying images. We introduce how adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions. On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65306;&#22914;&#20309;&#65288;&#37325;&#26032;&#65289;&#28909;&#21551;&#21160;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23545;&#25968;&#21313;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#65292;&#23601;&#20250;&#37325;&#26032;&#24320;&#22987;&#36825;&#20010;&#36807;&#31243;&#12290;&#19968;&#31181;&#26356;&#24265;&#20215;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#21363;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#36807;&#21435;&#25968;&#25454;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#38598;&#26102;&#65292;&#38656;&#35201;&#37325;&#26032;&#22686;&#21152;&#23398;&#20064;&#29575;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;Pile&#65288;&#19978;&#28216;&#25968;&#25454;&#65292;300B&#26631;&#35760;&#65289;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;SlimPajama&#65288;&#19979;&#28216;&#25968;&#25454;&#65292;297B&#26631;&#35760;&#65289;&#19978;&#36827;&#34892;&#20102;&#32447;&#24615;&#28909;&#21551;&#21160;&#21644;&#20313;&#24358;&#34928;&#20943;&#30340;&#35843;&#24230;&#12290;&#25105;&#20204;&#22312;Pythia 410M&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#25152;&#26377;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and ev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#32593;&#32476;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#36890;&#36807;&#21033;&#29992;&#32852;&#21512;&#20542;&#21521;&#24471;&#20998;&#30340;&#37325;&#26032;&#21152;&#26435;&#27169;&#24335;&#21644;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#24335;&#36827;&#34892;&#25512;&#23548;&#65292;&#20174;&#32780;&#25903;&#25345;&#32531;&#35299;&#28151;&#28102;&#20559;&#24046;&#21644;&#25351;&#23548;&#23398;&#20064;&#30446;&#26631;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.04011</link><description>&lt;p&gt;
&#20174;&#35266;&#27979;&#32593;&#32476;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization bound for estimating causal effects from observational network data. (arXiv:2308.04011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#32593;&#32476;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#36890;&#36807;&#21033;&#29992;&#32852;&#21512;&#20542;&#21521;&#24471;&#20998;&#30340;&#37325;&#26032;&#21152;&#26435;&#27169;&#24335;&#21644;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#24335;&#36827;&#34892;&#25512;&#23548;&#65292;&#20174;&#32780;&#25903;&#25345;&#32531;&#35299;&#28151;&#28102;&#20559;&#24046;&#21644;&#25351;&#23548;&#23398;&#20064;&#30446;&#26631;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#32593;&#32476;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#30456;&#20851;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#27867;&#21270;&#30028;&#38480;&#30340;&#20998;&#26512;&#65292;&#32780;&#27867;&#21270;&#30028;&#38480;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#25903;&#25345;&#32531;&#35299;&#22797;&#26434;&#30340;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23454;&#38469;&#20013;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#25351;&#23548;&#23398;&#20064;&#30446;&#26631;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#32852;&#21512;&#20542;&#21521;&#24471;&#20998;&#30340;&#37325;&#26032;&#21152;&#26435;&#27169;&#24335;&#21644;&#22522;&#20110;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#24335;&#65292;&#25512;&#23548;&#20986;&#20102;&#29992;&#20110;&#32593;&#32476;&#22330;&#26223;&#20013;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#37325;&#26032;&#21152;&#26435;&#21644;&#34920;&#31034;&#23398;&#20064;&#20004;&#20010;&#35282;&#24230;&#25552;&#20379;&#20102;&#20851;&#20110;&#27867;&#21270;&#30028;&#38480;&#30340;&#29702;&#35299;&#12290;&#22312;&#30028;&#38480;&#20998;&#26512;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#20542;&#21521;&#24471;&#20998;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#21152;&#26435;&#22238;&#24402;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#23454;&#38469;&#32593;&#32476;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#30740;&#31350;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects from observational network data is a significant but challenging problem. Existing works in causal inference for observational network data lack an analysis of the generalization bound, which can theoretically provide support for alleviating the complex confounding bias and practically guide the design of learning objectives in a principled manner. To fill this gap, we derive a generalization bound for causal effect estimation in network scenarios by exploiting 1) the reweighting schema based on joint propensity score and 2) the representation learning schema based on Integral Probability Metric (IPM). We provide two perspectives on the generalization bound in terms of reweighting and representation learning, respectively. Motivated by the analysis of the bound, we propose a weighting regression method based on the joint propensity score augmented with representation learning. Extensive experimental studies on two real-world networks with semi-synthetic data d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03999</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#29702;&#35299;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#32972;&#26223;&#30693;&#35782;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;CNN&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40657;&#30418;&#29305;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Explainable AI&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20934;&#30830;&#35299;&#37322;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#65306;&#20934;&#30830;&#30340;&#35299;&#37322;&#23558;&#20026;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#37096;&#26816;&#27979;&#21040;&#30340;&#36755;&#20837;&#30456;&#20851;&#20869;&#23481;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#29616;&#26377;&#25216;&#26415;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#33410;&#28857;&#30340;&#28608;&#27963;&#21487;&#20197;&#34987;&#20154;&#31867;&#29702;&#35299;&#65292;&#20294;&#26159;&#23545;&#38544;&#34255;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#35299;&#37322;&#36827;&#34892;&#20551;&#35774;&#21644;&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#33258;&#21160;&#21270;&#26041;&#27861;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#20174;&#32500;&#22522;&#30334;&#31185;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#20013;&#31579;&#36873;&#20986;&#30340;&#32422;200&#19975;&#20010;&#31867;&#21035;&#65292;&#20197;&#21450;&#19968;&#20010;&#31216;&#20026;&#27010;&#24565;&#24402;&#32435;&#30340;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#35821;&#20041;Web&#39046;&#22495;&#30340;&#24212;&#29992;&#32780;&#24320;&#21457;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, de-mystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#30340;&#36164;&#28304;&#31649;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.03995</link><description>&lt;p&gt;
&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks. (arXiv:2308.03995v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#30340;&#36164;&#28304;&#31649;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#22825;&#22320;&#19968;&#20307;&#21270;&#32593;&#32476;&#65288;SAGIN&#65289;&#23558;&#21253;&#25324;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#65288;LEO&#65289;&#12289;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#21644;&#22320;&#38754;&#29992;&#25143;&#65288;GU&#65289;&#22312;&#20869;&#30340;&#24322;&#26500;&#35774;&#22791;&#25972;&#21512;&#36215;&#26469;&#65292;&#26377;&#26395;&#25512;&#21160;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;SAGIN&#30340;&#36164;&#28304;&#31649;&#29702;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#32039;&#24613;&#30740;&#31350;&#65292;&#22240;&#20026;&#19981;&#24688;&#24403;&#30340;&#36164;&#28304;&#31649;&#29702;&#20250;&#23548;&#33268;&#25968;&#25454;&#20256;&#36755;&#36136;&#37327;&#24046;&#65292;&#36827;&#32780;&#24433;&#21709;&#26234;&#33021;&#22478;&#24066;&#30340;&#26381;&#21153;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;SAGIN&#31995;&#32479;&#65292;&#21253;&#25324;&#20116;&#31181;&#19981;&#21516;&#30340;&#36890;&#20449;&#38142;&#36335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21512;&#20316;&#24335;&#22810;&#31867;&#22411;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;CMT-MARL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;CMT-MARL&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#25972;&#20307;&#20256;&#36755;&#36895;&#29575;&#21644;&#20256;&#36755;&#25104;&#21151;&#29575;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#26410;&#26469;SAGIN&#23454;&#26045;&#30340;&#28508;&#22312;&#20215;&#20540;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of the proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FNO&#65289;&#29992;&#20110;&#23454;&#26102;&#27169;&#25311;3D&#21160;&#24577;&#22478;&#24066;&#24494;&#27668;&#20505;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;FNO&#22312;&#21152;&#36895;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03985</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#29992;&#20110;&#23454;&#26102;&#27169;&#25311;3D&#21160;&#24577;&#22478;&#24066;&#24494;&#27668;&#20505;
&lt;/p&gt;
&lt;p&gt;
Fourier neural operator for real-time simulation of 3D dynamic urban microclimate. (arXiv:2308.03985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FNO&#65289;&#29992;&#20110;&#23454;&#26102;&#27169;&#25311;3D&#21160;&#24577;&#22478;&#24066;&#24494;&#27668;&#20505;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;FNO&#22312;&#21152;&#36895;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#22478;&#24066;&#21270;&#20984;&#26174;&#20102;&#22478;&#24066;&#24494;&#27668;&#20505;&#23545;&#20154;&#31867;&#33298;&#36866;&#24230;&#12289;&#20581;&#24247;&#21644;&#24314;&#31569;/&#22478;&#24066;&#33021;&#25928;&#30340;&#37325;&#35201;&#24615;&#12290;&#23427;&#20204;&#23545;&#24314;&#31569;&#35774;&#35745;&#21644;&#22478;&#24066;&#35268;&#21010;&#20135;&#29983;&#37325;&#22823;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#29702;&#35299;&#23616;&#37096;&#24494;&#27668;&#20505;&#23545;&#20110;&#22478;&#24066;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#26377;&#25928;&#23454;&#26045;&#24377;&#24615;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#22478;&#24066;&#24494;&#27668;&#20505;&#38656;&#35201;&#22312;&#35745;&#31639;&#22495;&#20869;&#32771;&#34385;&#22797;&#26434;&#30340;&#23460;&#22806;&#21442;&#25968;&#65292;&#28085;&#30422;&#36739;&#38271;&#26102;&#26399;&#65292;&#24182;&#28085;&#30422;&#22478;&#24066;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#25968;&#20540;&#26041;&#27861;&#22914;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#22312;&#35780;&#20272;&#22478;&#24066;&#24494;&#27668;&#20505;&#30340;&#24433;&#21709;&#26102;&#21464;&#24471;&#35745;&#31639;&#22797;&#26434;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20852;&#36215;&#20026;&#21152;&#36895;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#21160;&#21147;&#23398;&#24314;&#27169;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#26368;&#36817;&#65292;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FNO&#65289;&#22312;&#21152;&#36895;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global urbanization has underscored the significance of urban microclimates for human comfort, health, and building/urban energy efficiency. They profoundly influence building design and urban planning as major environmental impacts. Understanding local microclimates is essential for cities to prepare for climate change and effectively implement resilience measures. However, analyzing urban microclimates requires considering a complex array of outdoor parameters within computational domains at the city scale over a longer period than indoors. As a result, numerical methods like Computational Fluid Dynamics (CFD) become computationally expensive when evaluating the impact of urban microclimates. The rise of deep learning techniques has opened new opportunities for accelerating the modeling of complex non-linear interactions and system dynamics. Recently, the Fourier Neural Operator (FNO) has been shown to be very promising in accelerating solving the Partial Differential Equations (PDEs
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36924;&#30495;&#19988;&#35821;&#20041;&#21487;&#25511;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28210;&#26579;&#25152;&#38656;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#12289;&#31934;&#30830;&#25511;&#21046;&#22330;&#26223;&#21644;&#20998;&#24067;&#36716;&#21464;&#20197;&#21450;&#31934;&#32454;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.03977</link><description>&lt;p&gt;
PUG:&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#36924;&#30495;&#19988;&#35821;&#20041;&#21487;&#25511;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning. (arXiv:2308.03977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36924;&#30495;&#19988;&#35821;&#20041;&#21487;&#25511;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28210;&#26579;&#25152;&#38656;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#12289;&#31934;&#30830;&#25511;&#21046;&#22330;&#26223;&#21644;&#20998;&#24067;&#36716;&#21464;&#20197;&#21450;&#31934;&#32454;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#22312;&#35774;&#35745;&#21644;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65306;&#23427;&#20204;&#21487;&#20197;&#28210;&#26579;&#25152;&#38656;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#31934;&#30830;&#25511;&#21046;&#27599;&#20010;&#22330;&#26223;&#24182;&#25552;&#20379;&#31934;&#32454;&#30340;&#30495;&#23454;&#26631;&#31614;&#65288;&#21644;&#26631;&#39064;&#65289;&#65292;&#24182;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#21464;&#65292;&#20197;&#38548;&#31163;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#36827;&#34892;&#21487;&#38752;&#23454;&#39564;&#12290;&#23613;&#31649;&#20855;&#26377;&#22914;&#27492;&#20248;&#21183;&#65292;&#20294;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#30340;&#20351;&#29992;&#20173;&#21463;&#38480;&#21046;&#65292;&#24182;&#19988;&#36890;&#24120;&#30001;&#20110;&#20854;&#32570;&#20047;&#30495;&#23454;&#24615;&#32780;&#34987;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#20173;&#20381;&#36182;&#20110;&#26469;&#33258;&#20114;&#32852;&#32593;&#19978;&#20844;&#24320;&#22270;&#20687;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#29256;&#26435;&#38382;&#39064;&#65292;&#19988;&#23545;&#20110;&#23545;&#35937;&#30340;&#31934;&#30830;&#21576;&#29616;&#26041;&#24335;&#25511;&#21046;&#33021;&#21147;&#36739;&#23567;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#36884;&#24452;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#30340;&#26032;&#19968;&#20195;&#20132;&#20114;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#21487;&#25511;&#24615;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#36824;&#20840;&#23616;&#25628;&#32034;&#26694;&#26550;&#65288;AmorGS&#65289;&#65292;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21152;&#36895;&#23545;&#21021;&#27493;&#36712;&#36857;&#35774;&#35745;&#38382;&#39064;&#30340;&#20840;&#23616;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#36712;&#36857;&#35299;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#24230;&#21644;&#38750;&#20984;&#24615;&#38382;&#39064;&#20013;&#26356;&#39640;&#25928;&#22320;&#23547;&#25214;&#22810;&#20010;&#19981;&#21516;&#29305;&#24449;&#30340;&#35299;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#23454;&#38469;&#38382;&#39064;&#19978;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03960</link><description>&lt;p&gt;
&#39640;&#25674;&#36824;&#20840;&#23616;&#25628;&#32034;&#26041;&#27861;&#22312;&#24102;&#26377;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26377;&#25928;&#21021;&#27493;&#36712;&#36857;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models. (arXiv:2308.03960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#36824;&#20840;&#23616;&#25628;&#32034;&#26694;&#26550;&#65288;AmorGS&#65289;&#65292;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21152;&#36895;&#23545;&#21021;&#27493;&#36712;&#36857;&#35774;&#35745;&#38382;&#39064;&#30340;&#20840;&#23616;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#36712;&#36857;&#35299;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#24230;&#21644;&#38750;&#20984;&#24615;&#38382;&#39064;&#20013;&#26356;&#39640;&#25928;&#22320;&#23547;&#25214;&#22810;&#20010;&#19981;&#21516;&#29305;&#24449;&#30340;&#35299;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#23454;&#38469;&#38382;&#39064;&#19978;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21021;&#27493;&#36712;&#36857;&#35774;&#35745;&#26159;&#19968;&#31181;&#20840;&#23616;&#25628;&#32034;&#38382;&#39064;&#65292;&#26088;&#22312;&#23547;&#25214;&#22810;&#20010;&#22312;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#20013;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#35299;&#12290;&#30001;&#20110;&#20854;&#39640;&#32500;&#24230;&#21644;&#38750;&#20984;&#24615;&#65292;&#20197;&#21450;&#38382;&#39064;&#21442;&#25968;&#30340;&#39057;&#32321;&#35843;&#25972;&#65292;&#20840;&#23616;&#25628;&#32034;&#21464;&#24471;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#35299;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#36824;&#20840;&#23616;&#25628;&#32034;&#65288;AmorGS&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#39044;&#27979;&#19982;&#20808;&#21069;&#35299;&#20915;&#38382;&#39064;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#30340;&#36712;&#36857;&#35299;&#65292;&#20174;&#32780;&#21152;&#36895;&#23545;&#26410;&#35265;&#21442;&#25968;&#20540;&#30340;&#20840;&#23616;&#25628;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;De Jong&#30340;&#31532;&#20116;&#20989;&#25968;&#21644;&#20302;&#25512;&#21147;&#22278;&#24418;&#38480;&#21046;&#24615;&#19977;&#20307;&#38382;&#39064;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preliminary trajectory design is a global search problem that seeks multiple qualitatively different solutions to a trajectory optimization problem. Due to its high dimensionality and non-convexity, and the frequent adjustment of problem parameters, the global search becomes computationally demanding. In this paper, we exploit the clustering structure in the solutions and propose an amortized global search (AmorGS) framework. We use deep generative models to predict trajectory solutions that share similar structures with previously solved problems, which accelerates the global search for unseen parameter values. Our method is evaluated using De Jong's 5th function and a low-thrust circular restricted three-body problem.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#30340;&#31070;&#32463;&#20803;&#21327;&#21464;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#33258;&#27965;&#28608;&#27963;&#65288;SCA&#65289;&#23618;&#65292;&#20854;&#28608;&#27963;&#20855;&#26377;&#22266;&#23450;&#20294;&#21487;&#23398;&#20064;&#30340;&#21327;&#21464;&#24615;&#27169;&#24335;&#65292;&#20197;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03956</link><description>&lt;p&gt;
&#22266;&#23450;&#30340;&#31070;&#32463;&#20803;&#21327;&#21464;&#24615;&#24341;&#21457;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fixed Inter-Neuron Covariability Induces Adversarial Robustness. (arXiv:2308.03956v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03956
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#30340;&#31070;&#32463;&#20803;&#21327;&#21464;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#33258;&#27965;&#28608;&#27963;&#65288;SCA&#65289;&#23618;&#65292;&#20854;&#28608;&#27963;&#20855;&#26377;&#22266;&#23450;&#20294;&#21487;&#23398;&#20064;&#30340;&#21327;&#21464;&#24615;&#27169;&#24335;&#65292;&#20197;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#38519;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#21487;&#38752;&#24615;&#30340;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;DNN&#34987;&#35748;&#20026;&#26159;&#27169;&#25311;&#20154;&#31867;&#24863;&#30693;&#30340;&#65292;&#32780;&#20154;&#31867;&#24863;&#30693;&#23545;&#36825;&#31181;&#25200;&#21160;&#20855;&#26377;&#24456;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#35753;&#20154;&#31867;&#24863;&#30693;&#20855;&#26377;&#40065;&#26834;&#24615;&#20294;&#22312;&#24403;&#21069;DNN&#31867;&#21035;&#20013;&#27809;&#26377;&#34920;&#31034;&#30340;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#29305;&#24449;&#26159;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#32467;&#26500;&#22312;&#36739;&#38271;&#26102;&#38388;&#36328;&#24230;&#19978;&#24448;&#24448;&#26159;&#30456;&#24403;&#31283;&#23450;&#30340;&#65292;&#21363;&#20351;&#23427;&#20250;&#24433;&#21709;&#24615;&#33021;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#20551;&#35774;&#23558;&#36825;&#31181;&#23545;&#28608;&#27963;&#30340;&#32422;&#26463;&#38598;&#25104;&#21040;DNN&#20013;&#20250;&#25552;&#39640;&#20854;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#27965;&#28608;&#27963;&#65288;SCA&#65289;&#23618;&#65292;&#23427;&#30001;&#28608;&#27963;&#24444;&#27492;&#19968;&#33268;&#30340;&#31070;&#32463;&#20803;&#32452;&#25104;&#65292;&#22240;&#20026;&#23427;&#20204;&#31526;&#21512;&#22266;&#23450;&#20294;&#21487;&#20197;&#23398;&#20064;&#30340;&#21327;&#21464;&#24615;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PMU&#27979;&#37327;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#25299;&#25169;&#21464;&#21270;&#12289;&#26679;&#26412;&#26631;&#27880;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#22788;&#29702;&#31561;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#27169;&#22411;&#35780;&#20272;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03953</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;PMU&#27979;&#37327;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning. (arXiv:2308.03953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PMU&#27979;&#37327;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#25299;&#25169;&#21464;&#21270;&#12289;&#26679;&#26412;&#26631;&#27880;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#22788;&#29702;&#31561;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#27169;&#22411;&#35780;&#20272;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#65288;STVSA&#65289;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;STVSA&#26041;&#27861;&#22312;&#36866;&#24212;&#25299;&#25169;&#21464;&#21270;&#12289;&#26679;&#26412;&#26631;&#27880;&#21644;&#22788;&#29702;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PMU&#27979;&#37327;&#30340;STVSA&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;PMUs&#25429;&#33719;&#30340;&#23454;&#26102;&#21160;&#24577;&#20449;&#24687;&#21019;&#24314;&#21021;&#22987;&#25968;&#25454;&#38598;&#12290;&#23427;&#37319;&#29992;&#26102;&#38388;&#38598;&#25104;&#36827;&#34892;&#26679;&#26412;&#26631;&#27880;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20108;&#20056;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;LSGAN&#65289;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#23454;&#29616;&#23545;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#28145;&#24230;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#25925;&#38556;&#20043;&#38388;&#30340;&#36830;&#25509;&#22686;&#24378;&#20102;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;IEEE 39&#33410;&#28857;&#27979;&#35797;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#35780;&#20272;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has emerged as an effective solution for addressing the challenges of short-term voltage stability assessment (STVSA) in power systems. However, existing deep learning-based STVSA approaches face limitations in adapting to topological changes, sample labeling, and handling small datasets. To overcome these challenges, this paper proposes a novel phasor measurement unit (PMU) measurements-based STVSA method by using deep transfer learning. The method leverages the real-time dynamic information captured by PMUs to create an initial dataset. It employs temporal ensembling for sample labeling and utilizes least squares generative adversarial networks (LSGAN) for data augmentation, enabling effective deep learning on small-scale datasets. Additionally, the method enhances adaptability to topological changes by exploring connections between different faults. Experimental results on the IEEE 39-bus test system demonstrate that the proposed method improves model evaluation accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;Transformer&#27169;&#22411;&#23454;&#29616;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#30340;&#21069;&#26223;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#35777;&#26126;&#20102;Transformer&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;FL&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#34920;&#31034;&#30456;&#20284;&#24615;&#26469;&#28145;&#20837;&#20102;&#35299;Transformer&#26377;&#21069;&#26223;&#30340;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2308.03945</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#22686;&#24378;&#22823;&#35268;&#27169;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers. (arXiv:2308.03945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;Transformer&#27169;&#22411;&#23454;&#29616;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#30340;&#21069;&#26223;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#35777;&#26126;&#20102;Transformer&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;FL&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#34920;&#31034;&#30456;&#20284;&#24615;&#26469;&#28145;&#20837;&#20102;&#35299;Transformer&#26377;&#21069;&#26223;&#30340;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#23454;&#29616;&#36328;&#20998;&#24067;&#24335;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;AI&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#26469;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;FL&#30340;&#24191;&#27867;&#37319;&#29992;&#38754;&#20020;&#30528;&#25968;&#25454;&#24322;&#26500;&#21644;&#28041;&#21450;&#21040;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#35268;&#27169;&#24222;&#22823;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;FL&#27169;&#22411;&#22312;&#23454;&#29616;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#26041;&#38754;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#28041;&#21450;&#21040;&#20102;FL&#19982;Transformer&#12289;ResNet&#21644;&#20010;&#24615;&#21270;ResNet-based FL&#26041;&#27861;&#12290;&#36825;&#20123;&#23454;&#39564;&#32771;&#34385;&#20102;&#19981;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#65292;&#20197;&#23637;&#31034;Transformer&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;FL&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#23618;&#21644;FL&#27169;&#22411;&#20043;&#38388;&#30340;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#34920;&#31034;&#30456;&#20284;&#24615;&#26469;&#20998;&#26512;Transformer&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#20854;&#26377;&#21069;&#26223;&#30340;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) addresses data privacy concerns by enabling collaborative training of AI models across distributed data owners. Wide adoption of FL faces the fundamental challenges of data heterogeneity and the large scale of data owners involved. In this paper, we investigate the prospect of Transformer-based FL models for achieving generalization and personalization in this setting. We conduct extensive comparative experiments involving FL with Transformers, ResNet, and personalized ResNet-based FL approaches under various scenarios. These experiments consider varying numbers of data owners to demonstrate Transformers' advantages over deep neural networks in large-scale heterogeneous FL tasks. In addition, we analyze the superior performance of Transformers by comparing the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models to gain insight into the reasons behind their promising capabilities.
&lt;/p&gt;</description></item><item><title>GraPhSyM&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#32593;&#34920;&#20013;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#24310;&#36831;&#21644;&#38754;&#31215;&#25351;&#26631;&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#25351;&#26631;&#21487;&#35265;&#24615;&#32473;&#26089;&#26399;&#30340;EDA&#38454;&#27573;&#65292;&#21487;&#29992;&#20110;&#20840;&#23616;&#21327;&#21516;&#20248;&#21270;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#20248;&#21270;&#26694;&#26550;&#20855;&#26377;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.03944</link><description>&lt;p&gt;
GraPhSyM: &#22270;&#24418;&#29289;&#29702;&#32508;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraPhSyM: Graph Physical Synthesis Model. (arXiv:2308.03944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03944
&lt;/p&gt;
&lt;p&gt;
GraPhSyM&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#32593;&#34920;&#20013;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#24310;&#36831;&#21644;&#38754;&#31215;&#25351;&#26631;&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#25351;&#26631;&#21487;&#35265;&#24615;&#32473;&#26089;&#26399;&#30340;EDA&#38454;&#27573;&#65292;&#21487;&#29992;&#20110;&#20840;&#23616;&#21327;&#21516;&#20248;&#21270;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#20248;&#21270;&#26694;&#26550;&#20855;&#26377;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GraPhSyM&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#32593;&#34920;&#20013;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#24310;&#36831;&#21644;&#38754;&#31215;&#25351;&#26631;&#30340;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GATv2&#65289;&#27169;&#22411;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#27605;&#65292;GraPhSyM&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#35774;&#35745;&#25351;&#26631;&#21487;&#35265;&#24615;&#32473;&#26089;&#26399;&#30340;EDA&#38454;&#27573;&#65292;&#22914;&#36923;&#36753;&#32508;&#21512;&#65292;&#32780;&#26080;&#38656;&#36816;&#34892;&#32531;&#24930;&#30340;&#29289;&#29702;&#21512;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#38454;&#27573;&#30340;&#20840;&#23616;&#21327;&#21516;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;GraPhSym&#25552;&#20379;&#30340;&#24555;&#36895;&#32780;&#31934;&#30830;&#30340;&#21453;&#39304;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#20248;&#21270;&#26694;&#26550;&#33267;&#20851;&#37325;&#35201;&#12290;&#32473;&#23450;&#19968;&#20010;&#34920;&#31034;&#20026;&#22270;&#24418;&#30340;&#30005;&#36335;&#38376;&#32423;&#32593;&#34920;&#65292;GraPhSyM&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#12289;&#36830;&#25509;&#24615;&#21644;&#30005;&#24615;&#29305;&#24449;&#26469;&#39044;&#27979;&#29289;&#29702;&#21512;&#25104;&#36716;&#25442;&#65288;&#22914;&#32531;&#20914;&#22120;&#25554;&#20837;&#21644;&#38376;&#23610;&#23544;&#35843;&#25972;&#65289;&#23545;&#30005;&#36335;&#30340;&#24433;&#21709;&#12290;&#24403;&#22312;&#19968;&#20010;&#21253;&#21547;6000&#20010;&#21069;&#32512;&#21152;&#27861;&#22120;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65288;&#21512;&#25104;&#21040;&#28608;&#36827;&#24310;&#36831;&#30446;&#26631;&#65289;&#65292;GraPhSyM&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#21518;&#21512;&#25104;&#30340;&#24310;&#36831;&#65288;98.3%&#65289;&#21644;&#38754;&#31215;&#65288;96.1%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) model for fast and accurate estimation of post-physical synthesis circuit delay and area metrics from pre-physical synthesis circuit netlists. Once trained, GraPhSyM provides accurate visibility of final design metrics to early EDA stages, such as logic synthesis, without running the slow physical synthesis flow, enabling global co-optimization across stages. Additionally, the swift and precise feedback provided by GraPhSym is instrumental for machine-learning-based EDA optimization frameworks. Given a gate-level netlist of a circuit represented as a graph, GraPhSyM utilizes graph structure, connectivity, and electrical property features to predict the impact of physical synthesis transformations such as buffer insertion and gate sizing. When trained on a dataset of 6000 prefix adder designs synthesized at an aggressive delay target, GraPhSyM can accurately predict the post-synthesis delay (98.3%) and area (96.1%) m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#20811;&#38534;&#25239;&#20307;&#29983;&#20135;&#20013;&#20248;&#21270;&#20999;&#25442;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#27982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#36830;&#32493;&#29983;&#20135;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.03928</link><description>&lt;p&gt;
&#22312;&#21333;&#20811;&#38534;&#25239;&#20307;&#29983;&#20135;&#20013;&#20248;&#21270;&#20999;&#25442;&#25805;&#20316;&#65306;&#32463;&#27982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning. (arXiv:2308.03928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#21333;&#20811;&#38534;&#25239;&#20307;&#29983;&#20135;&#20013;&#20248;&#21270;&#20999;&#25442;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#27982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#36830;&#32493;&#29983;&#20135;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20811;&#38534;&#25239;&#20307;&#65288;mAbs&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#36164;&#20135;&#65292;&#30446;&#21069;&#22788;&#20110;&#29983;&#29289;&#21046;&#33647;&#20135;&#21697;&#24320;&#21457;&#30340;&#21069;&#27839;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#22686;&#38271;&#30340;&#24066;&#22330;&#38656;&#27714;&#21644;mAb&#20020;&#24202;&#27835;&#30103;&#25152;&#38656;&#30340;&#22823;&#37327;&#21058;&#37327;&#20351;&#20854;&#22823;&#35268;&#27169;&#29983;&#20135;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#24037;&#19994;mAb&#29983;&#20135;&#20013;&#30340;&#22823;&#37096;&#20998;&#36807;&#31243;&#20381;&#36182;&#20110;&#25209;&#22788;&#29702;&#25805;&#20316;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#20572;&#26426;&#26102;&#38388;&#12290;&#36716;&#21521;&#23436;&#20840;&#36830;&#32493;&#21644;&#38598;&#25104;&#30340;&#21046;&#36896;&#36807;&#31243;&#21487;&#20197;&#25552;&#39640;&#20135;&#21697;&#20135;&#37327;&#21644;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#23384;&#20648;&#20013;&#38388;&#20135;&#21697;&#25152;&#38656;&#30340;&#39069;&#22806;&#36153;&#29992;&#12290;&#38598;&#25104;&#36830;&#32493;mAb&#29983;&#20135;&#36807;&#31243;&#21487;&#20197;&#20998;&#20026;&#19978;&#28216;&#21644;&#19979;&#28216;&#36807;&#31243;&#12290;&#30830;&#20445;&#38598;&#25104;&#36807;&#31243;&#36830;&#32493;&#24615;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#25429;&#33719;&#26609;&#30340;&#20999;&#25442;&#65292;&#36890;&#24120;&#26159;&#22312;&#19979;&#28216;&#20197;&#28385;&#25209;&#26041;&#24335;&#25805;&#20316;&#30340;&#33394;&#35889;&#26609;&#12290;&#30001;&#20110;&#20999;&#25442;&#25805;&#20316;&#30340;&#31163;&#25955;&#24615;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monoclonal antibodies (mAbs) have emerged as indispensable assets in medicine, and are currently at the forefront of biopharmaceutical product development. However, the growing market demand and the substantial doses required for mAb clinical treatments necessitate significant progress in its large-scale production. Most of the processes for industrial mAb production rely on batch operations, which result in significant downtime. The shift towards a fully continuous and integrated manufacturing process holds the potential to boost product yield and quality, while eliminating the extra expenses associated with storing intermediate products. The integrated continuous mAb production process can be divided into the upstream and downstream processes. One crucial aspect that ensures the continuity of the integrated process is the switching of the capture columns, which are typically chromatography columns operated in a fed-batch manner downstream. Due to the discrete nature of the switching 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#20869;&#37096;&#21464;&#37327;&#30340;&#28145;&#24230;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21482;&#20351;&#29992;&#27979;&#37327;&#30340;&#21147;-&#20301;&#31227;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26448;&#26009;&#30340;&#26412;&#26500;&#23450;&#24459;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#26410;&#35265;&#30340;&#21152;&#36733;&#24773;&#20917;&#19979;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#21464;&#37327;&#65292;&#26080;&#35770;&#26448;&#26009;&#30340;&#24615;&#36136;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2308.03915</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#20869;&#37096;&#21464;&#37327;&#30340;&#28145;&#24230;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21644;&#35299;&#37322;&#38750;&#32447;&#24615;&#26448;&#26009;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables. (arXiv:2308.03915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03915
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#20869;&#37096;&#21464;&#37327;&#30340;&#28145;&#24230;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21482;&#20351;&#29992;&#27979;&#37327;&#30340;&#21147;-&#20301;&#31227;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26448;&#26009;&#30340;&#26412;&#26500;&#23450;&#24459;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#26410;&#35265;&#30340;&#21152;&#36733;&#24773;&#20917;&#19979;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#21464;&#37327;&#65292;&#26080;&#35770;&#26448;&#26009;&#30340;&#24615;&#36136;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#24120;&#24456;&#38590;&#29992;&#32463;&#20856;&#30340;&#29366;&#24577;&#27169;&#22411;&#26469;&#24314;&#27169;&#38750;&#32447;&#24615;&#26448;&#26009;&#30340;&#21407;&#22240;&#26159;&#23427;&#20204;&#20855;&#26377;&#22797;&#26434;&#19988;&#19981;&#20934;&#30830;&#30340;&#29289;&#29702;&#21644;&#25968;&#23398;&#25551;&#36848;&#65292;&#25110;&#32773;&#25105;&#20204;&#26681;&#26412;&#19981;&#30693;&#36947;&#22914;&#20309;&#20174;&#22806;&#37096;&#21644;&#20869;&#37096;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25551;&#36848;&#36825;&#20123;&#26448;&#26009;&#12290;&#22312;&#35768;&#22810;&#23398;&#31185;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35782;&#21035;&#38750;&#24120;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#24320;&#21457;&#30340;&#20855;&#26377;&#20869;&#37096;&#21464;&#37327;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PGNNIV&#65289;&#30340;&#27010;&#24565;&#65292;&#20197;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#20165;&#20351;&#29992;&#27979;&#37327;&#30340;&#21147;-&#20301;&#31227;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26469;&#21457;&#29616;&#26412;&#26500;&#23450;&#24459;&#12290;PGNNIV&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#29289;&#29702;&#29305;&#24615;&#24212;&#29992;&#20110;&#29305;&#23450;&#30340;&#38544;&#34255;&#23618;&#26469;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#33021;&#22815;&#22312;&#19981;&#20351;&#29992;&#20869;&#37096;&#21464;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;PGNNIV&#33021;&#22815;&#22312;&#26410;&#35265;&#36807;&#30340;&#21152;&#36733;&#22330;&#26223;&#19979;&#39044;&#27979;&#20869;&#37096;&#21644;&#22806;&#37096;&#21464;&#37327;&#65292;&#26080;&#35770;&#26448;&#26009;&#30340;&#24615;&#36136;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear materials are often difficult to model with classical state model theory because they have a complex and sometimes inaccurate physical and mathematical description or we simply do not know how to describe such materials in terms of relations between external and internal variables. In many disciplines, Neural Network methods have arisen as powerful tools to identify very complex and non-linear correlations. In this work, we use the very recently developed concept of Physically Guided Neural Networks with Internal Variables (PGNNIV) to discover constitutive laws using a model-free approach and training solely with measured force-displacement data. PGNNIVs make a particular use of the physics of the problem to enforce constraints on specific hidden layers and are able to make predictions without internal variable data. We demonstrate that PGNNIVs are capable of predicting both internal and external variables under unseen load scenarios, regardless of the nature of the material 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#21183;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#65292;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#26399;&#24453;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03908</link><description>&lt;p&gt;
ViLP&#65306;&#20351;&#29992;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#23039;&#21183;&#23884;&#20837;&#36827;&#34892;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#30693;&#35782;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition. (arXiv:2308.03908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#21183;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#65292;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#26399;&#24453;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#22823;&#37327;&#30340;&#20154;&#31867;&#21160;&#20316;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;(MML)&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;2D&#39592;&#39612;&#25110;&#23039;&#21183;&#27169;&#24577;&#32463;&#24120;&#34987;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#35201;&#20040;&#29420;&#31435;&#20351;&#29992;&#65292;&#35201;&#20040;&#19982;&#35270;&#39057;&#20013;&#23384;&#22312;&#30340;&#35270;&#35273;&#20449;&#24687;&#65288;RGB&#27169;&#24577;&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25991;&#26412;&#21644;&#23039;&#21183;&#23646;&#24615;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#23578;&#26410;&#25506;&#32034;&#36807;&#23039;&#21183;&#12289;&#35270;&#35273;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#32452;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;VAR&#30340;&#23039;&#21183;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#20154;&#31867;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;UCF-101&#21644;HMDB-51&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;92.81%&#21644;73.02%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even w
&lt;/p&gt;</description></item><item><title>&#20247;&#21253;&#30417;&#25511;&#31995;&#32479;&#26159;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#38190;&#65292;&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;&#35270;&#35273;&#21644;&#38750;&#22522;&#20110;&#35270;&#35273;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.03907</link><description>&lt;p&gt;
&#20247;&#21253;&#30417;&#25511;&#31995;&#32479;&#30340;&#36827;&#23637;&#65306;&#31995;&#32479;&#26041;&#27861;&#21644;&#33258;&#21160;&#21270;&#31639;&#27861;&#30340;&#32508;&#21512;&#20998;&#26512;&#65306;&#29616;&#29366;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements In Crowd-Monitoring System: A Comprehensive Analysis of Systematic Approaches and Automation Algorithms: State-of-The-Art. (arXiv:2308.03907v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03907
&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#30417;&#25511;&#31995;&#32479;&#26159;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#38190;&#65292;&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;&#35270;&#35273;&#21644;&#38750;&#22522;&#20110;&#35270;&#35273;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21508;&#22269;&#25919;&#24220;&#21644;&#23433;&#20840;&#26426;&#26500;&#23545;&#20844;&#20849;&#23433;&#20840;&#30340;&#26085;&#30410;&#25285;&#24551;&#24050;&#32463;&#24341;&#36215;&#20102;&#20182;&#20204;&#30340;&#27880;&#24847;&#12290;&#36825;&#20123;&#26426;&#26500;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#38656;&#35201;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#20247;&#21253;&#30417;&#25511;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26377;&#25928;&#31649;&#29702;&#20154;&#21592;&#32858;&#38598;&#38656;&#35201;&#37319;&#21462;&#31215;&#26497;&#30340;&#25514;&#26045;&#26469;&#38450;&#27490;&#24847;&#22806;&#20107;&#20214;&#25110;&#22797;&#26434;&#24773;&#20917;&#65292;&#30830;&#20445;&#19968;&#20010;&#23433;&#20840;&#21644;&#21327;&#35843;&#30340;&#29615;&#22659;&#12290;&#30001;&#20110;&#23545;&#20247;&#21253;&#30417;&#25511;&#31995;&#32479;&#21450;&#20854;&#23433;&#20840;&#24433;&#21709;&#32570;&#20047;&#30740;&#31350;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#26085;&#30410;&#22686;&#38271;&#65292;&#25506;&#32034;&#26377;&#25928;&#20445;&#25252;&#20154;&#21592;&#32858;&#38598;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#20247;&#21253;&#30417;&#25511;&#31995;&#32479;&#20381;&#36182;&#20110;&#22522;&#20110;&#35270;&#35273;&#21644;&#38750;&#22522;&#20110;&#35270;&#35273;&#30340;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#23558;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#21151;&#25928;&#21462;&#20915;&#20110;&#20854;&#37096;&#32626;&#30340;&#20855;&#20307;&#29615;&#22659;&#21644;&#26102;&#38388;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing apprehensions surrounding public safety have captured the attention of numerous governments and security agencies across the globe. These entities are increasingly acknowledging the imperative need for reliable and secure crowd-monitoring systems to address these concerns. Effectively managing human gatherings necessitates proactive measures to prevent unforeseen events or complications, ensuring a safe and well-coordinated environment. The scarcity of research focusing on crowd monitoring systems and their security implications has given rise to a burgeoning area of investigation, exploring potential approaches to safeguard human congregations effectively. Crowd monitoring systems depend on a bifurcated approach, encompassing vision-based and non-vision-based technologies. An in-depth analysis of these two methodologies will be conducted in this research. The efficacy of these approaches is contingent upon the specific environment and temporal context in which they are deploye
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#65292;&#35813;&#31995;&#32479;&#26356;&#21152;&#31169;&#23494;&#12289;&#21487;&#38752;&#12289;&#24555;&#36895;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.03905</link><description>&lt;p&gt;
&#22312;&#35774;&#22791;&#19978;&#30340;&#26234;&#33021;&#21161;&#25163;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Intelligent Assistant Language Understanding On Device. (arXiv:2308.03905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#65292;&#35813;&#31995;&#32479;&#26356;&#21152;&#31169;&#23494;&#12289;&#21487;&#38752;&#12289;&#24555;&#36895;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#20010;&#20154;&#25968;&#23383;&#21161;&#25163;&#24212;&#29992;&#20110;&#25163;&#26426;&#21644;&#20854;&#20182;&#20010;&#20154;&#35774;&#22791;&#24050;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;&#19982;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#30456;&#27604;&#65292;&#35813;&#31995;&#32479;&#26356;&#20855;&#31169;&#23494;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#36895;&#24230;&#24555;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#26550;&#26500;&#21644;&#25216;&#26415;&#26041;&#38754;&#20570;&#20986;&#30340;&#20851;&#38190;&#36873;&#25321;&#12290;&#20363;&#22914;&#65292;&#23545;&#35805;&#31995;&#32479;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#26041;&#27861;&#22312;&#37096;&#32626;&#29615;&#22659;&#20013;&#38590;&#20197;&#38271;&#26399;&#32500;&#25252;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#24615;&#19982;&#36890;&#36807;&#26435;&#37325;&#32465;&#23450;&#23454;&#29616;&#30340;&#30495;&#27491;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25351;&#23548;&#23398;&#20064;&#30495;&#27491;&#30340;&#19981;&#21464;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#20173;&#28982;&#21487;&#38752;&#30340;&#19981;&#21464;&#24615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.03904</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#20351;&#29992;&#26435;&#37325;&#32465;&#23450;&#30340;&#30495;&#27491;&#19981;&#21464;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On genuine invariance learning without weight-tying. (arXiv:2308.03904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#24615;&#19982;&#36890;&#36807;&#26435;&#37325;&#32465;&#23450;&#23454;&#29616;&#30340;&#30495;&#27491;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25351;&#23548;&#23398;&#20064;&#30495;&#27491;&#30340;&#19981;&#21464;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#20173;&#28982;&#21487;&#38752;&#30340;&#19981;&#21464;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#24615;&#19982;&#36890;&#36807;&#19981;&#21464;&#26435;&#37325;&#32465;&#23450;&#23454;&#29616;&#30340;&#30495;&#27491;&#19981;&#21464;&#24615;&#30340;&#23646;&#24615;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32676;&#35770;&#30340;&#35270;&#35282;&#65292;&#24182;&#20998;&#26512;&#20102;&#27809;&#26377;&#26435;&#37325;&#32465;&#23450;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#21464;&#24615;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#32593;&#32476;&#23398;&#20250;&#20102;&#22312;&#32676;&#36712;&#36947;&#19978;&#27491;&#30830;&#20998;&#31867;&#26679;&#26412;&#65292;&#36825;&#31181;&#27169;&#22411;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#27809;&#26377;&#36798;&#21040;&#30495;&#27491;&#30340;&#19981;&#21464;&#24615;&#12290;&#30456;&#21453;&#65292;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#36755;&#20837;&#25968;&#25454;&#65292;&#22914;&#26524;&#36755;&#20837;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#65292;&#23427;&#23558;&#21464;&#24471;&#19981;&#21487;&#38752;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#23545;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#24341;&#23548;&#19981;&#21464;&#24615;&#23398;&#20064;&#26397;&#21521;&#30495;&#27491;&#30340;&#19981;&#21464;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#25351;&#26631;&#26469;&#37327;&#21270;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#24615;&#65306;&#65288;i&#65289;&#39044;&#27979;&#20998;&#24067;&#19981;&#21464;&#24615;&#65292;&#65288;ii&#65289;logit&#19981;&#21464;&#24615;&#21644;&#65288;iii&#65289;&#26174;&#33879;&#24615;&#19981;&#21464;&#24615;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#19981;&#21464;&#24615;&#27491;&#35268;&#21270;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate properties and limitations of invariance learned by neural networks from the data compared to the genuine invariance achieved through invariant weight-tying. To do so, we adopt a group theoretical perspective and analyze invariance learning in neural networks without weight-tying constraints. We demonstrate that even when a network learns to correctly classify samples on a group orbit, the underlying decision-making in such a model does not attain genuine invariance. Instead, learned invariance is strongly conditioned on the input data, rendering it unreliable if the input distribution shifts. We next demonstrate how to guide invariance learning toward genuine invariance by regularizing the invariance of a model at the training. To this end, we propose several metrics to quantify learned invariance: (i) predictive distribution invariance, (ii) logit invariance, and (iii) saliency invariance similarity. We show that the invariance learned with the invarianc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03901</link><description>&lt;p&gt;
FLIPS: &#20351;&#29992;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;FLIPS&#36890;&#36807;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#21644;&#26234;&#33021;&#21442;&#19982;&#32773;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;FLIPS&#30456;&#27604;&#38543;&#26426;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLIPS&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31649;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#21442;&#19982;&#32773;&#24322;&#36136;&#24615;&#30340;&#20013;&#38388;&#20214;&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26631;&#31614;&#20998;&#24067;&#32858;&#31867;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#19982;&#32773;&#36873;&#25321;&#20013;&#30340;&#22909;&#22788;&#12290;FLIPS&#26681;&#25454;&#25968;&#25454;&#30340;&#26631;&#31614;&#20998;&#24067;&#39044;&#20808;&#23545;&#21442;&#19982;FL&#35757;&#32451;&#20316;&#19994;&#30340;&#21508;&#26041;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#22312;FL&#35757;&#32451;&#26399;&#38388;&#30830;&#20445;&#27599;&#20010;&#32858;&#31867;&#22312;&#34987;&#36873;&#20013;&#30340;&#21442;&#19982;&#32773;&#20013;&#20844;&#24179;&#22320;&#34920;&#31034;&#12290;FLIPS&#21487;&#20197;&#25903;&#25345;&#26368;&#24120;&#35265;&#30340;FL&#31639;&#27861;&#65292;&#21253;&#25324;FedAvg&#65292;FedProx&#65292;FedDyn&#65292;FedOpt&#21644;FedYogi&#12290;&#20026;&#20102;&#31649;&#29702;&#24179;&#21488;&#30340;&#24322;&#26500;&#24615;&#21644;&#21160;&#24577;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;FLIPS&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#22788;&#29702;&#20998;&#24067;&#24335;&#26234;&#33021;&#31038;&#21306;&#24212;&#29992;&#20013;&#23481;&#37327;&#21464;&#21270;&#30340;&#25302;&#32047;&#31649;&#29702;&#26426;&#21046;&#12290;&#26631;&#31614;&#20998;&#24067;&#12289;&#32858;&#31867;&#21644;&#21442;&#19982;&#32773;&#36873;&#25321;&#30340;&#38544;&#31169;&#36890;&#36807;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;(TEE)&#26469;&#30830;&#20445;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#23558;FLIPS&#19982;&#38543;&#26426;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#32858;&#31867;&#39564;&#35777;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.03894</link><description>&lt;p&gt;
&#19968;&#31181;&#35780;&#20272;&#20869;&#37096;&#32858;&#31867;&#39564;&#35777;&#25351;&#26631;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new approach for evaluating internal cluster validation indices. (arXiv:2308.03894v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#32858;&#31867;&#39564;&#35777;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20998;&#31867;&#26377;&#24456;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#21487;&#20379;&#36873;&#25321;&#12290;&#30001;&#20110;&#27809;&#26377;&#19968;&#20010;&#31639;&#27861;&#21644;&#21442;&#25968;&#35774;&#32622;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#25968;&#25454;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#32858;&#31867;&#39564;&#35777;&#26469;&#36873;&#25321;&#30495;&#27491;&#34920;&#29616;&#26368;&#22909;&#30340;&#31639;&#27861;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#19981;&#20351;&#29992;&#20219;&#20309;&#39069;&#22806;&#65288;&#22806;&#37096;&#65289;&#20449;&#24687;&#30340;&#20869;&#37096;&#39564;&#35777;&#25351;&#26631;&#12290;&#21487;&#20197;&#36890;&#36807;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20855;&#26377;&#24050;&#30693;&#32858;&#31867;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#26469;&#35780;&#20272;&#36825;&#20123;&#20869;&#37096;&#39564;&#35777;&#25351;&#26631;&#12290;&#35780;&#20272;&#26041;&#27861;&#22312;&#22914;&#20309;&#20351;&#29992;&#30495;&#23454;&#20998;&#31867;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast number of different methods are available for unsupervised classification. Since no algorithm and parameter setting performs best in all types of data, there is a need for cluster validation to select the actually best-performing algorithm. Several indices were proposed for this purpose without using any additional (external) information. These internal validation indices can be evaluated by applying them to classifications of datasets with a known cluster structure. Evaluation approaches differ in how they use the information on the ground-truth classification. This paper reviews these approaches, considering their advantages and disadvantages, and then suggests a new approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#19988;&#20844;&#24179;&#30340;&#22823;&#35268;&#27169;&#25945;&#32946;&#25968;&#25454;&#20013;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.03892</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#19988;&#20844;&#24179;&#30340;&#22823;&#35268;&#27169;&#25945;&#32946;&#25968;&#25454;&#20013;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data. (arXiv:2308.03892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#19988;&#20844;&#24179;&#30340;&#22823;&#35268;&#27169;&#25945;&#32946;&#25968;&#25454;&#20013;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23398;&#29983;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#23545;&#20110;&#20351;&#29992;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#21644;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#25163;&#21160;&#35782;&#21035;&#31574;&#30053;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#36827;&#34892;&#21487;&#25193;&#23637;&#19988;&#20844;&#24179;&#30340;&#31574;&#30053;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;MVec&#30340;&#23884;&#20837;&#65292;&#36890;&#36807;&#23398;&#29983;&#25484;&#25569;&#31243;&#24230;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#23545;&#23398;&#29983;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding a student's problem-solving strategy can have a significant impact on effective math learning using Intelligent Tutoring Systems (ITSs) and Adaptive Instructional Systems (AISs). For instance, the ITS/AIS can better personalize itself to correct specific misconceptions that are indicated by incorrect strategies, specific problems can be designed to improve strategies and frustration can be minimized by adapting to a student's natural way of thinking rather than trying to fit a standard strategy for all. While it may be possible for human experts to identify strategies manually in classroom settings with sufficient student interaction, it is not possible to scale this up to big data. Therefore, we leverage advances in Machine Learning and AI methods to perform scalable strategy prediction that is also fair to students at all skill levels. Specifically, we develop an embedding called MVec where we learn a representation based on the mastery of students. We then cluster thes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#36941;&#21382;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#32463;&#39564;&#27861;&#21017;&#65292;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03888</link><description>&lt;p&gt;
&#20174;&#36941;&#21382;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks from the perspective of ergodic theory. (arXiv:2308.03888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#36941;&#21382;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#32463;&#39564;&#27861;&#21017;&#65292;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#26469;&#35828;&#65292;&#20173;&#28982;&#26356;&#22810;&#22320;&#26159;&#19968;&#38376;&#33402;&#26415;&#32780;&#19981;&#26159;&#31934;&#30830;&#30340;&#31185;&#23398;&#12290;&#36890;&#36807;&#23558;&#36941;&#21382;&#29702;&#35770;&#32771;&#34385;&#24341;&#20837;&#21040;&#23558;&#32593;&#32476;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#26102;&#38388;&#28436;&#21270;&#30340;&#35266;&#28857;&#19978;&#65292;&#20854;&#20013;&#27599;&#19968;&#23618;&#23545;&#24212;&#20110;&#19968;&#20010;&#26102;&#38388;&#23454;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#32463;&#39564;&#27861;&#21017;&#21487;&#20197;&#24402;&#23646;&#20026;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#30340;&#31070;&#31192;&#24615;&#24471;&#20197;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of deep neural networks remains somewhat of an art rather than precise science. By tentatively adopting ergodic theory considerations on top of viewing the network as the time evolution of a dynamical system, with each layer corresponding to a temporal instance, we show that some rules of thumb, which might otherwise appear mysterious, can be attributed heuristics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;</title><link>http://arxiv.org/abs/2308.03887</link><description>&lt;p&gt;
&#29992;&#19968;&#31181;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach. (arXiv:2308.03887v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35270;&#39057;&#26174;&#24494;&#38236;&#35760;&#24405;&#20934;&#30830;&#36319;&#36394;&#27963;&#32454;&#32990;&#20173;&#28982;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#26041;&#27861;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#20960;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#24212;&#29992;&#23581;&#35797;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#25972;&#21512;&#21040;&#35813;&#20219;&#21153;&#20013;&#65292;&#20294;&#22823;&#37096;&#20998;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#23884;&#20837;&#20854;&#26550;&#26500;&#25110;&#20854;&#20182;&#21069;&#25552;&#26465;&#20214;&#20013;&#30340;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24191;&#20041;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#32454;&#32990;&#21487;&#20197;&#26681;&#25454;&#20854;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#30340;&#20551;&#35774;&#65292;&#32780;&#38750;&#20165;&#38480;&#20110;&#36830;&#32493;&#24103;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#39069;&#22806;&#20248;&#28857;&#26159;&#32454;&#32990;&#30340;&#36816;&#21160;&#27169;&#24335;&#21487;&#20197;&#23436;&#20840;&#30001;&#39044;&#27979;&#22120;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#24182;&#19988;&#20855;&#26377;&#22788;&#29702;&#22823;&#37327;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#35270;&#39057;&#24103;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated thro
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#34920;&#32852;&#21512;&#25628;&#32034;&#21019;&#24314;&#32467;&#26500;&#21270;&#25968;&#25454;&#22522;&#20934;</title><link>http://arxiv.org/abs/2308.03883</link><description>&lt;p&gt;
&#34920;&#32852;&#21512;&#25628;&#32034;&#29983;&#25104;&#24615;&#22522;&#20934;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
Generative Benchmark Creation for Table Union Search. (arXiv:2308.03883v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03883
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#34920;&#32852;&#21512;&#25628;&#32034;&#21019;&#24314;&#32467;&#26500;&#21270;&#25968;&#25454;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#20256;&#32479;&#19978;&#20381;&#38752;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#29983;&#25104;&#32467;&#26500;&#21270;&#22522;&#20934;&#65292;&#22914;TPC&#22871;&#20214;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#37325;&#35201;&#21442;&#25968;&#22914;&#25968;&#25454;&#22823;&#23567;&#21644;&#20998;&#24067;&#12290;&#36825;&#20123;&#22522;&#20934;&#23545;&#20110;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;&#30340;&#25104;&#21151;&#21644;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#31649;&#29702;&#38382;&#39064;&#23646;&#20110;&#35821;&#20041;&#24615;&#36136;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;&#25214;&#21040;&#21487;&#20197;&#32852;&#21512;&#30340;&#34920;&#12290;&#34429;&#28982;&#20219;&#20309;&#20855;&#26377;&#30456;&#21516;&#22522;&#25968;&#30340;&#20004;&#20010;&#34920;&#37117;&#21487;&#20197;&#32852;&#21512;&#65292;&#34920;&#32852;&#21512;&#25628;&#32034;&#26159;&#25214;&#21040;&#20854;&#32852;&#21512;&#22312;&#35821;&#20041;&#19978;&#36830;&#36143;&#30340;&#34920;&#30340;&#38382;&#39064;&#12290;&#35821;&#20041;&#38382;&#39064;&#26080;&#27861;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30446;&#21069;&#21019;&#24314;&#30340;&#22522;&#20934;&#30340;&#26041;&#27861;&#28041;&#21450;&#23454;&#38469;&#25968;&#25454;&#30340;&#25163;&#21160;&#31574;&#21010;&#21644;&#26631;&#35760;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#31283;&#20581;&#19988;&#19981;&#21487;&#25193;&#23637;&#65292;&#32780;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#19981;&#28165;&#26970;&#25152;&#21019;&#24314;&#30340;&#22522;&#20934;&#30340;&#31283;&#20581;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#34920;&#32852;&#21512;&#25628;&#32034;&#21019;&#24314;&#32467;&#26500;&#21270;&#25968;&#25454;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.03882</link><description>&lt;p&gt;
&#36890;&#36807;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#22686;&#24378;&#21033;&#29992;&#24191;&#20041;&#21270;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#36827;&#34892;&#20445;&#23432;&#20215;&#20540;&#35780;&#20272;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#26080;&#27169;&#22411;&#26041;&#27861;&#20250;&#23545;&#25152;&#26377;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#32780;&#26377;&#27169;&#22411;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#36890;&#36807;&#27169;&#22411;&#23637;&#24320;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#36827;&#34892;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25214;&#21040;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#26102;&#23384;&#22312;&#22256;&#38590;&#65306;(a)&#30001;&#20110;&#32423;&#32852;&#27169;&#22411;&#35823;&#24046;&#65292;&#27169;&#22411;&#30340;&#23637;&#24320;&#33539;&#22260;&#38750;&#24120;&#30701;&#65292;(b)&#27169;&#22411;&#23637;&#24320;&#20165;&#20197;&#31163;&#32447;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#20026;&#36215;&#28857;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#31532;&#20108;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26410;&#35265;&#36807;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#20801;&#35768;&#23398;&#24471;&#30340;&#27169;&#22411;&#21644;&#20215;&#20540;&#20272;&#35745;&#22312;&#26410;&#35265;&#29366;&#24577;&#20013;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#36827;&#34892;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#26469;&#25214;&#21040;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#65292;&#28982;&#21518;&#36890;&#36807;&#36807;&#28388;&#20855;&#26377;&#36807;&#39640;&#30340;&#21551;&#21457;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;&#39640;&#35823;&#24046;&#65289;&#25110;&#36807;&#20302;&#30340;&#65288;&#36807;&#20110;&#30456;&#20284;&#65289;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#37322;&#26041;&#27861;ASTxplainer&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21487;&#38752;&#22320;&#23558;&#27169;&#22411;&#30340;&#39044;&#27979;&#26144;&#23556;&#21040;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#19978;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03873</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#35299;&#37322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20013;&#20351;&#29992;&#35821;&#27861;&#32467;&#26500;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Explaining Large Language Models for Code Using Syntactic Structures. (arXiv:2308.03873v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#37322;&#26041;&#27861;ASTxplainer&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21487;&#38752;&#22320;&#23558;&#27169;&#22411;&#30340;&#39044;&#27979;&#26144;&#23556;&#21040;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#19978;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#39640;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#35757;&#32451;&#20110;&#28023;&#37327;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#27169;&#22411;&#27491;&#22312;&#24555;&#36895;&#24212;&#29992;&#20110;&#21830;&#19994;&#21270;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#32773;&#24037;&#20855;&#65292;&#22914;GitHub CoPilot&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#20854;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#34913;&#37327;&#21644;&#35299;&#37322;&#23427;&#20204;&#22312;&#31243;&#24207;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#35780;&#20272;&#21644;&#35299;&#37322;&#20195;&#30721;&#30340;LLM&#26041;&#27861;&#26159;&#23494;&#19981;&#21487;&#20998;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24517;&#39035;&#23558;&#20854;&#21487;&#38752;&#22320;&#26144;&#23556;&#21040;&#32454;&#31890;&#24230;&#12289;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#19978;&#12290;&#19968;&#26086;&#23454;&#29616;&#20102;&#36825;&#31181;&#26144;&#23556;&#65292;&#23601;&#21487;&#20197;&#24320;&#23637;&#26032;&#30340;&#35814;&#32454;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;&#35299;&#37322;&#33021;&#21147;&#25216;&#26415;&#21644;&#35780;&#20272;&#22522;&#20934;&#37117;&#38598;&#20013;&#22312;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#25110;&#21333;&#20010;&#20219;&#21153;&#24615;&#33021;&#19978;&#65292;&#32780;&#38750;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;ASTxplainer&#65292;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#30340;LLM&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.  To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#21644;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#31561;&#20215;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#23558;&#26597;&#35810;&#26144;&#23556;&#20026;&#25628;&#32034;&#24847;&#22270;&#21521;&#37327;&#34920;&#31034;&#12289;&#35782;&#21035;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#31561;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03869</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#30340;&#35821;&#20041;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Equivalence of e-Commerce Queries. (arXiv:2308.03869v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#21644;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#31561;&#20215;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#23558;&#26597;&#35810;&#26144;&#23556;&#20026;&#25628;&#32034;&#24847;&#22270;&#21521;&#37327;&#34920;&#31034;&#12289;&#35782;&#21035;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#31561;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#21464;&#21270;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#25628;&#32034;&#24847;&#22270;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#34920;&#23618;&#24046;&#24322;&#30340;&#19981;&#21516;&#26597;&#35810;&#26469;&#34920;&#36798;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#21033;&#29992;&#26597;&#35810;&#31561;&#20215;&#24615;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#25628;&#32034;&#24847;&#22270;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#35782;&#21035;&#34920;&#36798;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#65292;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;&#34920;&#38754;&#30456;&#20284;&#24615;&#28041;&#21450;&#22522;&#20110;&#35789;&#30340;&#21464;&#24418;&#12289;&#35789;&#24207;&#12289;&#22797;&#21512;&#21644;&#22122;&#22768;&#35789;&#26469;&#35268;&#33539;&#21270;&#26597;&#35810;&#12290;&#34892;&#20026;&#30456;&#20284;&#24615;&#21033;&#29992;&#21382;&#21490;&#25628;&#32034;&#34892;&#20026;&#29983;&#25104;&#26597;&#35810;&#24847;&#22270;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#31163;&#32447;&#36807;&#31243;&#29992;&#20110;&#35757;&#32451;&#21477;&#23376;&#30456;&#20284;&#24615;&#27169;&#22411;&#65292;&#32780;&#22312;&#32447;&#26368;&#36817;&#37051;&#26041;&#27861;&#25903;&#25345;&#23545;&#26410;&#35265;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#22768;&#26126;&#24335;&#20247;&#21253;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#26126;&#24335;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#36136;&#37327;&#20248;&#21270;&#21644;&#25104;&#26412;&#25511;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03854</link><description>&lt;p&gt;
&#36890;&#36807;&#22768;&#26126;&#24335;&#20247;&#21253;&#37325;&#26032;&#23457;&#35270;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Revisiting Prompt Engineering via Declarative Crowdsourcing. (arXiv:2308.03854v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#22768;&#26126;&#24335;&#20247;&#21253;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#26126;&#24335;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#36136;&#37327;&#20248;&#21270;&#21644;&#25104;&#26412;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#26497;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#23481;&#26131;&#33030;&#24369;&#21644;&#38169;&#35823;&#12290;&#36817;&#24180;&#26469;&#28044;&#29616;&#20102;&#20197;&#25152;&#35859;&#30340;&#25552;&#31034;&#24037;&#31243;&#20026;&#20013;&#24515;&#30340;&#24037;&#20855;&#21253;&#21644;&#25216;&#26415;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21521;LLM&#25552;&#20986;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;LLM&#39537;&#21160;&#30340;&#25968;&#25454;&#22788;&#29702;&#24037;&#20316;&#27969;&#20013;&#65292;&#20248;&#21270;&#36136;&#37327;&#24182;&#20445;&#25345;&#25104;&#26412;&#26377;&#38480;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#25163;&#21160;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22768;&#26126;&#24335;&#25552;&#31034;&#24037;&#31243;&#30340;&#24895;&#26223;&#12290;&#25105;&#20204;&#23558;LLM&#35270;&#20026;&#20247;&#21253;&#24037;&#20154;&#65292;&#24182;&#20511;&#37492;&#20102;&#22768;&#26126;&#24335;&#20247;&#21253;&#25991;&#29486;&#20013;&#30340;&#24605;&#24819;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#30830;&#20445;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#25506;&#32034;&#28151;&#21512;LLM&#38750;LLM&#26041;&#27861;&#65292;&#20197;&#20351;&#25552;&#31034;&#24037;&#31243;&#36807;&#31243;&#26356;&#21152;&#21407;&#21017;&#24615;&#12290;&#23545;&#25490;&#24207;&#12289;&#23454;&#20307;&#35299;&#26512;&#21644;&#25554;&#34917;&#30340;&#21021;&#27493;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;JinaAI&#26500;&#24314;&#38899;&#20048;&#34892;&#19994;&#30340;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#21305;&#37197;&#27468;&#26354;&#27468;&#35789;&#21644;&#25552;&#20379;&#20934;&#30830;&#30340;&#25628;&#32034;&#32467;&#26524;&#26469;&#35299;&#20915;&#29616;&#26377;&#25628;&#32034;&#24341;&#25806;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03842</link><description>&lt;p&gt;
&#20351;&#29992;JinaAI&#26500;&#24314;&#38899;&#20048;&#34892;&#19994;&#30340;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Search Engine and Recommendation System for the Music Industry built with JinaAI. (arXiv:2308.03842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;JinaAI&#26500;&#24314;&#38899;&#20048;&#34892;&#19994;&#30340;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#21305;&#37197;&#27468;&#26354;&#27468;&#35789;&#21644;&#25552;&#20379;&#20934;&#30830;&#30340;&#25628;&#32034;&#32467;&#26524;&#26469;&#35299;&#20915;&#29616;&#26377;&#25628;&#32034;&#24341;&#25806;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#38899;&#20048;&#34892;&#19994;&#20013;&#25628;&#32034;&#24341;&#25806;&#21644;&#22522;&#20110;&#25512;&#33616;&#30340;&#31995;&#32479;&#30340;&#21457;&#23637;&#26159;&#19968;&#20010;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#36777;&#35770;&#20043;&#19968;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25628;&#32034;&#24341;&#25806;&#39046;&#22495;&#23384;&#22312;&#30528;&#20005;&#37325;&#30340;&#25233;&#37057;&#65292;&#36825;&#26159;&#30001;&#20110;&#35832;&#22914;&#36895;&#24230;&#12289;&#20934;&#30830;&#24615;&#20197;&#21450;&#26597;&#35810;&#25968;&#25454;&#30340;&#26684;&#24335;&#31561;&#20196;&#20154;&#25285;&#24551;&#30340;&#22240;&#32032;&#25152;&#23548;&#33268;&#30340;&#12290;&#20154;&#20204;&#32463;&#24120;&#22312;&#20165;&#26681;&#25454;&#26631;&#39064;&#25628;&#32034;&#27468;&#26354;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21333;&#20010;&#26597;&#35810;&#36755;&#20837;&#23436;&#25104;&#25628;&#32034;&#20998;&#26512;&#65292;&#24182;&#19982;&#25968;&#25454;&#24211;&#20013;&#30340;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#21069;&#27839;&#25216;&#26415;&#24037;&#20855;&#20197;&#24320;&#21457;&#29992;&#25143;&#21451;&#22909;&#30340;&#25628;&#32034;&#24341;&#25806;&#33267;&#20851;&#37325;&#35201;&#12290;Jina AI&#26159;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#25628;&#32034;&#24341;&#25806;&#30340;MLOps&#26694;&#26550;&#65292;&#23427;&#34987;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#33719;&#24471;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;Jina AI&#26377;&#25928;&#22320;&#24110;&#21161;&#32500;&#25252;&#21644;&#25552;&#21319;&#25628;&#32034;&#24341;&#25806;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#24615;&#33021;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;JinaAI&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#38899;&#20048;&#34892;&#19994;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most intriguing debates regarding a novel task is the development of search engines and recommendation-based systems in the music industry. Studies have shown a drastic depression in the search engine fields, due to concerning factors such as speed, accuracy and the format of data given for querying. Often people face difficulty in searching for a song solely based on the title, hence a solution is proposed to complete a search analysis through a single query input and is matched with the lyrics of the songs present in the database. Hence it is essential to incorporate cutting-edge technology tools for developing a user-friendly search engine. Jina AI is an MLOps framework for building neural search engines that are utilized, in order for the user to obtain accurate results. Jina AI effectively helps to maintain and enhance the quality of performance for the search engine for the query given. An effective search engine and a recommendation system for the music industry, buil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#25552;&#31034;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#30740;&#31350;&#12290;&#36890;&#36807;&#27979;&#37327;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#30340;&#21807;&#19968;&#29305;&#24449;&#21644;&#20027;&#35201;&#25915;&#20987;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#36234;&#29425;&#25552;&#31034;&#36234;&#26469;&#36234;&#22810;&#22320;&#20174;&#20844;&#20849;&#24179;&#21488;&#36716;&#31227;&#21040;&#31169;&#20154;&#24179;&#21488;&#65292;&#32473;LLM&#20379;&#24212;&#21830;&#22312;&#20027;&#21160;&#26816;&#27979;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.03825</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#30340;&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. (arXiv:2308.03825v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#25552;&#31034;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#30740;&#31350;&#12290;&#36890;&#36807;&#27979;&#37327;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#30340;&#21807;&#19968;&#29305;&#24449;&#21644;&#20027;&#35201;&#25915;&#20987;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#36234;&#29425;&#25552;&#31034;&#36234;&#26469;&#36234;&#22810;&#22320;&#20174;&#20844;&#20849;&#24179;&#21488;&#36716;&#31227;&#21040;&#31169;&#20154;&#24179;&#21488;&#65292;&#32473;LLM&#20379;&#24212;&#21830;&#22312;&#20027;&#21160;&#26816;&#27979;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28389;&#29992;&#24050;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;LLM&#20379;&#24212;&#21830;&#30340;&#37325;&#35270;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20123;&#21162;&#21147;&#24050;&#32463;&#34987;&#20570;&#20986;&#26469;&#65292;&#20351;LLM&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21363;&#36234;&#29425;&#25552;&#31034;&#65292;&#24050;&#32463;&#20986;&#29616;&#24182;&#19981;&#26029;&#28436;&#21464;&#20197;&#32469;&#36807;&#20445;&#38556;&#24182;&#24341;&#21457;LLM&#20013;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#37326;&#22806;&#36234;&#29425;&#25552;&#31034;&#36827;&#34892;&#20102;&#27979;&#37327;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;6,387&#20010;&#22312;&#20845;&#20010;&#26376;&#20869;&#20174;&#22235;&#20010;&#24179;&#21488;&#19978;&#33719;&#24471;&#30340;&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#22522;&#20110;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36234;&#29425;&#25552;&#31034;&#30340;&#29420;&#29305;&#29305;&#24449;&#21450;&#20854;&#20027;&#35201;&#25915;&#20987;&#31574;&#30053;&#65292;&#22914;&#25552;&#31034;&#27880;&#20837;&#21644;&#26435;&#38480;&#25552;&#21319;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#36234;&#29425;&#25552;&#31034;&#36234;&#26469;&#36234;&#22810;&#22320;&#20174;&#20844;&#20849;&#24179;&#21488;&#36716;&#31227;&#21040;&#31169;&#20154;&#24179;&#21488;&#65292;&#32473;LLM&#20379;&#24212;&#21830;&#22312;&#20027;&#21160;&#26816;&#27979;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#36234;&#29425;&#25552;&#31034;&#21487;&#33021;&#36896;&#25104;&#30340;&#21361;&#23475;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;
&lt;/p&gt;
&lt;p&gt;
The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39044;&#31639;&#30340;&#20998;&#24067;&#40065;&#26834;&#20998;&#31867;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35757;&#32451;&#38598;&#21644;&#25511;&#21046;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;240&#19975;&#20010;&#22270;&#20687;&#26679;&#26412;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#19982;&#22312;4&#20159;&#20010;&#22270;&#20687;&#26679;&#26412;&#19978;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03821</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39044;&#31639;&#30340;&#20998;&#24067;&#40065;&#26834;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Classification on a Data Budget. (arXiv:2308.03821v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39044;&#31639;&#30340;&#20998;&#24067;&#40065;&#26834;&#20998;&#31867;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35757;&#32451;&#38598;&#21644;&#25511;&#21046;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;240&#19975;&#20010;&#22270;&#20687;&#26679;&#26412;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#19982;&#22312;4&#20159;&#20010;&#22270;&#20687;&#26679;&#26412;&#19978;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#38656;&#35201;&#22312;&#20998;&#24067;&#36716;&#21464;&#19979;&#20855;&#26377;&#21487;&#39044;&#27979;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#20687;CLIP&#36825;&#26679;&#30340;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#30456;&#23218;&#32654;&#30340;&#33258;&#28982;&#20998;&#24067;&#40065;&#26834;&#24615;&#65292;&#20294;&#21487;&#33021;&#38656;&#35201;&#25968;&#20159;&#20010;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#33021;&#21542;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#39046;&#22495;&#35757;&#32451;&#40065;&#26834;&#30340;&#23398;&#20064;&#32773;&#65311;&#20026;&#20102;&#20005;&#26684;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JANuS&#65288;&#32852;&#21512;&#27880;&#37322;&#21644;&#21517;&#31216;&#38598;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#26631;&#31614;&#21644;&#30456;&#24212;&#26631;&#39064;&#30340;&#22235;&#20010;&#26032;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#25511;&#21046;&#30340;&#35843;&#26597;&#30740;&#31350;&#20102;&#24433;&#21709;&#22270;&#20687;&#20998;&#31867;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#65292;&#24182;&#23558;&#36825;&#20123;&#32467;&#26524;&#19982;&#22823;&#35268;&#27169;&#20803;&#20998;&#26512;&#30340;&#21457;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;240&#19975;&#20010;&#22270;&#20687;&#26679;&#26412;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;&#26631;&#20934;ResNet-50&#21487;&#20197;&#36798;&#21040;&#19982;&#22312;4&#20159;&#20010;&#26679;&#26412;&#19978;&#35757;&#32451;&#30340;CLIP ResNet-50&#30456;&#23218;&#32654;&#30340;&#40065;&#26834;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23637;&#31034;&#65288;&#25509;&#36817;&#65289;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on
&lt;/p&gt;</description></item><item><title>XFlow&#26159;&#19968;&#31181;&#22312;&#22270;&#19978;&#35780;&#20272;&#27969;&#34892;&#20026;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27969;&#21160;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#31639;&#27861;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03819</link><description>&lt;p&gt;
XFlow: &#22312;&#22270;&#19978;&#35780;&#20272;&#27969;&#34892;&#20026;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XFlow: Benchmarking Flow Behaviors over Graphs. (arXiv:2308.03819v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03819
&lt;/p&gt;
&lt;p&gt;
XFlow&#26159;&#19968;&#31181;&#22312;&#22270;&#19978;&#35780;&#20272;&#27969;&#34892;&#20026;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27969;&#21160;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#31639;&#27861;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#25193;&#25955;&#29616;&#35937;&#26159;&#26222;&#36941;&#32780;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#35875;&#35328;&#20256;&#25773;&#12289;&#31867;&#20284;&#27969;&#24863;&#30149;&#27602;&#30340;&#20256;&#25773;&#12289;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#31561;&#12290;&#29702;&#35299;&#27969;&#21160;&#34892;&#20026;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#31181;&#23376;&#30340;&#20998;&#24067;&#12289;&#20256;&#25773;&#27169;&#22411;&#21644;&#22270;&#30340;&#25299;&#25169;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#32593;&#32476;&#30740;&#31350;&#28085;&#30422;&#20102;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#31561;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#30340;&#29305;&#24615;&#20351;&#24471;&#32593;&#32476;&#30740;&#31350;&#20855;&#26377;&#39640;&#24230;&#19987;&#19994;&#21270;&#21644;&#20998;&#21106;&#65292;&#30001;&#27492;&#24102;&#26469;&#30340;&#21512;&#20316;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#24179;&#21488;&#26469;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#30340;&#31639;&#27861;&#12290;&#24403;&#21069;&#30740;&#31350;&#22312;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#31574;&#21010;&#22909;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#26469;&#30740;&#31350;&#27969;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The occurrence of diffusion on a graph is a prevalent and significant phenomenon, as evidenced by the spread of rumors, influenza-like viruses, smart grid failures, and similar events. Comprehending the behaviors of flow is a formidable task, due to the intricate interplay between the distribution of seeds that initiate flow propagation, the propagation model, and the topology of the graph. The study of networks encompasses a diverse range of academic disciplines, including mathematics, physics, social science, and computer science. This interdisciplinary nature of network research is characterized by a high degree of specialization and compartmentalization, and the cooperation facilitated by them is inadequate. From a machine learning standpoint, there is a deficiency in a cohesive platform for assessing algorithms across various domains. One of the primary obstacles to current research in this field is the absence of a comprehensive curated benchmark suite to study the flow behaviors
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#31232;&#30095;&#32534;&#30721;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#65292;&#20026;&#29616;&#26377;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03818</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32534;&#30721;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#21450;&#22312;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A sparse coding approach to inverse problems with application to microwave tomography imaging. (arXiv:2308.03818v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#31232;&#30095;&#32534;&#30721;&#30340;&#36870;&#38382;&#39064;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#65292;&#20026;&#29616;&#26377;&#31639;&#27861;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#20250;&#36935;&#21040;&#19968;&#20123;&#19981;&#36866;&#23450;&#30340;&#36870;&#38382;&#39064;&#22270;&#20687;&#65292;&#36825;&#20123;&#39046;&#22495;&#21253;&#25324;&#21307;&#23398;&#35786;&#26029;&#21644;&#22825;&#25991;&#30740;&#31350;&#31561;&#12290;&#20026;&#20102;&#20174;&#19981;&#23436;&#25972;&#21644;&#22833;&#30495;&#30340;&#25968;&#25454;&#20013;&#37325;&#24314;&#22270;&#20687;&#65292;&#38656;&#35201;&#21019;&#24314;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#29983;&#25104;&#36825;&#20123;&#27979;&#37327;&#30340;&#29289;&#29702;&#26426;&#21046;&#21644;&#25152;&#20998;&#26512;&#22270;&#20687;&#30340;&#26412;&#36136;&#29305;&#24615;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22270;&#20687;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#23427;&#26159;&#21463;&#21754;&#20083;&#21160;&#29289;&#35270;&#35273;&#31995;&#32479;&#21551;&#21457;&#30340;&#19968;&#31181;&#23454;&#38469;&#12289;&#32039;&#20945;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#22270;&#20687;&#38598;&#19978;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#20915;&#19981;&#36866;&#23450;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31232;&#30095;&#32534;&#30721;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#35299;&#20915;&#24494;&#27874;&#26029;&#23618;&#25104;&#20687;&#20013;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36866;&#23450;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#25913;&#36827;&#29616;&#26377;&#30340;&#31639;&#27861;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse imaging problems that are ill-posed can be encountered across multiple domains of science and technology, ranging from medical diagnosis to astronomical studies. To reconstruct images from incomplete and distorted data, it is necessary to create algorithms that can take into account both, the physical mechanisms responsible for generating these measurements and the intrinsic characteristics of the images being analyzed. In this work, the sparse representation of images is reviewed, which is a realistic, compact and effective generative model for natural images inspired by the visual system of mammals. It enables us to address ill-posed linear inverse problems by training the model on a vast collection of images. Moreover, we extend the application of sparse coding to solve the non-linear and ill-posed problem in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.03813</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#30340;&#20302;&#20998;&#36776;&#29575;&#28857;&#20113;&#23436;&#25104;&#21464;&#25442;&#22120;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers. (arXiv:2308.03813v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#37117;&#26377;&#25104;&#21315;&#19978;&#19975;&#30340;&#20154;&#36973;&#21463;&#21508;&#31181;&#31867;&#22411;&#30340;&#39045;&#39592;&#20260;&#23475;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#26893;&#20837;&#29289;&#65292;&#25163;&#24037;&#35774;&#35745;&#26114;&#36149;&#19988;&#36153;&#26102;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19987;&#29992;&#31995;&#32479;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#38750;&#24120;&#26377;&#24517;&#35201;&#12290;&#33258;&#21160;&#39045;&#39592;&#32570;&#25439;&#37325;&#24314;&#30340;&#38382;&#39064;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#24418;&#29366;&#23436;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#19987;&#29992;&#28145;&#24230;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30446;&#21069;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20307;&#31215;&#34920;&#31034;&#27861;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#36866;&#24212;&#39640;&#20998;&#36776;&#29575;&#20307;&#31215;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#20998;&#36776;&#29575;&#19979;&#37325;&#24314;&#39045;&#32570;&#25439;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed meth
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25512;&#24191;&#21040;&#38750;&#32039;&#33268;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#19968;&#33268;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#20195;&#25968;&#32467;&#26500;&#30340;&#24847;&#22806;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.03812</link><description>&lt;p&gt;
&#38750;&#32039;&#33268;&#32479;&#19968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Noncompact uniform universal approximation. (arXiv:2308.03812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03812
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25512;&#24191;&#21040;&#38750;&#32039;&#33268;&#36755;&#20837;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#26377;&#30028;&#28608;&#27963;&#20989;&#25968;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#19968;&#33268;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#20195;&#25968;&#32467;&#26500;&#30340;&#24847;&#22806;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25512;&#24191;&#21040;&#22312;&#65288;&#38750;&#32039;&#33268;&#65289;&#36755;&#20837;&#31354;&#38388; \(\mathbb R^n\) &#19978;&#30340;&#19968;&#33268;&#25910;&#25947;&#12290;&#25152;&#26377;&#22312;&#26080;&#31351;&#36828;&#22788;&#20026;&#38646;&#30340;&#36830;&#32493;&#20989;&#25968;&#37117;&#21487;&#20197;&#29992;&#20855;&#26377;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#65292;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;&#28176;&#36817;&#32447;&#24615;&#34892;&#20026;&#30340;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968; \(\varphi\neq0\)&#12290;&#24403; \(\varphi\) &#36824;&#34987;&#38480;&#21046;&#22312;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#20934;&#30830;&#30830;&#23450;&#20102;&#21738;&#20123;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#65292;&#24471;&#21040;&#20102;&#20197;&#19979;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;&#35753; \(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\) &#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377; \(l\) &#20010;&#38544;&#34255;&#23618;&#21644; \(n\) &#20010;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19968;&#33268;&#36924;&#36817;&#30340;&#20989;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#12290;&#23545;&#20110;&#25152;&#26377;&#30340; \(n\) &#21644;&#25152;&#26377;&#30340; \(l\geq2\)&#65292;\(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\) &#22312;&#36880;&#28857;&#20056;&#31215;&#19979;&#26159;&#19968;&#20010;&#20195;&#25968;&#12290;&#22914;&#26524; \(\varphi\) &#30340;&#24038;&#26497;&#38480;&#19981;&#31561;&#20110;&#20854;&#21491;&#26497;&#38480;&#65288;&#20363;&#22914;&#65292;&#24403; \(\varphi\) &#26159;sigmoid&#20989;&#25968;&#26102;&#65289;&#65292;&#20195;&#25968; \(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\)&#65288;\(l\geq2\)&#65289;&#20250;&#20135;&#29983;&#19968;&#20123;&#24847;&#22806;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. When $\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\geq2$, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\varphi$ differs from its right limit (for instance, when $\varphi$ is sigmoidal) the algebra $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq2$) 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#24179;&#22343;&#30340;&#21333;&#24490;&#29615;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#22120;&#65288;SOBOW&#65289;&#65292;&#23427;&#33021;&#22312;&#22788;&#29702;&#20989;&#25968;&#21464;&#21270;&#21644;&#30495;&#23454;&#36229;&#26799;&#24230;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.03811</link><description>&lt;p&gt;
&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#21644;&#26102;&#21464;&#30446;&#26631;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Bilevel Optimization with Time-Varying Objective Functions. (arXiv:2308.03811v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#24179;&#22343;&#30340;&#21333;&#24490;&#29615;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#22120;&#65288;SOBOW&#65289;&#65292;&#23427;&#33021;&#22312;&#22788;&#29702;&#20989;&#25968;&#21464;&#21270;&#21644;&#30495;&#23454;&#36229;&#26799;&#24230;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#24378;&#22823;&#24037;&#20855;&#65292;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#32771;&#34385;&#30340;&#26159;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#38745;&#24577;&#20989;&#25968;&#65292;&#22312;&#26032;&#20852;&#30340;&#22312;&#32447;&#24212;&#29992;&#20013;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#21644;&#26102;&#21464;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#65288;OBO&#65289;&#65292;&#20854;&#20013;&#20989;&#25968;&#21487;&#20197;&#26102;&#21464;&#65292;&#24182;&#19988;&#20195;&#29702;&#19981;&#26029;&#26681;&#25454;&#22312;&#32447;&#27969;&#25968;&#25454;&#26356;&#26032;&#20915;&#31574;&#12290;&#20026;&#20102;&#22788;&#29702;OBO&#20013;&#30340;&#20989;&#25968;&#21464;&#21270;&#21644;&#30495;&#23454;&#36229;&#26799;&#24230;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#24179;&#22343;&#30340;&#21333;&#24490;&#29615;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#22120;&#65288;SOBOW&#65289;&#65292;&#23427;&#26681;&#25454;&#20869;&#23618;&#31383;&#21475;&#24179;&#22343;&#30340;&#26368;&#36817;&#36229;&#26799;&#24230;&#20272;&#35745;&#20540;&#26469;&#26356;&#26032;&#22806;&#23618;&#30340;&#20915;&#31574;&#12290;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#65292;SOBOW&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#20808;&#21069;&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#22788;&#29702;&#21333;&#24490;&#29615;&#26356;&#26032;&#21644;&#20989;&#25968;&#21464;&#21270;&#24102;&#26469;&#30340;&#29420;&#29305;&#25216;&#26415;&#22256;&#38590;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#22686;&#21152;&#31232;&#30095;&#24615;&#20197;&#21450;&#32467;&#26500;&#21644;&#20840;&#23616;&#20449;&#24687;&#26469;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for 
&lt;/p&gt;</description></item><item><title>AdaER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#35760;&#24518;&#22238;&#24518;&#31574;&#30053;&#65292;&#36873;&#25321;&#24615;&#22320;&#37325;&#25773;&#26368;&#20914;&#31361;&#30340;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2308.03810</link><description>&lt;p&gt;
AdaER: &#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning. (arXiv:2308.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03810
&lt;/p&gt;
&lt;p&gt;
AdaER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#35760;&#24518;&#22238;&#24518;&#31574;&#30053;&#65292;&#36873;&#25321;&#24615;&#22320;&#37325;&#25773;&#26368;&#20914;&#31361;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#23398;&#20064;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#32773;&#20250;&#20197;&#39034;&#24207;&#26041;&#24335;&#25345;&#32493;&#33719;&#21462;&#26032;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#35757;&#32451;&#25968;&#25454;&#30340;&#38750;&#31283;&#24577;&#24615;&#32473;&#36825;&#20010;&#36807;&#31243;&#24102;&#26469;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25351;&#30340;&#26159;&#22312;&#24341;&#20837;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#32463;&#39564;&#37325;&#25773;(ER)&#65292;&#24050;&#34987;&#25552;&#20986;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#65292;&#23588;&#20854;&#22312;&#22686;&#37327;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#32780;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#32463;&#39564;&#37325;&#25773;(AdaER)&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;AdaER&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#35760;&#24518;&#37325;&#25773;&#21644;&#35760;&#24518;&#26356;&#26032;&#12290;&#22312;&#35760;&#24518;&#37325;&#25773;&#38454;&#27573;&#65292;AdaER&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#35760;&#24518;&#22238;&#24518;&#31574;&#30053;(C-CMR)&#65292;&#36873;&#25321;&#24615;&#22320;&#37325;&#25773;&#26368;&#20914;&#31361;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual lifelong learning is an machine learning framework inspired by human learning, where learners are trained to continuously acquire new knowledge in a sequential manner. However, the non-stationary nature of streaming training data poses a significant challenge known as catastrophic forgetting, which refers to the rapid forgetting of previously learned knowledge when new tasks are introduced. While some approaches, such as experience replay (ER), have been proposed to mitigate this issue, their performance remains limited, particularly in the class-incremental scenario which is considered natural and highly challenging. In this paper, we present a novel algorithm, called adaptive-experience replay (AdaER), to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflictin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#20108;Nesterov&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22270;&#20687;&#37325;&#24314;&#26102;&#20943;&#36731;&#20266;&#24433;&#65292;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2308.03807</link><description>&lt;p&gt;
Nest-DGIL: Nesterov&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#29992;&#20110;CS&#22270;&#20687;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction. (arXiv:2308.03807v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#20108;Nesterov&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22270;&#20687;&#37325;&#24314;&#26102;&#20943;&#36731;&#20266;&#24433;&#65292;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#26159;&#35299;&#20915;&#22270;&#20687;&#21453;&#38382;&#39064;&#30340;&#24120;&#29992;&#31574;&#30053;&#20043;&#19968;&#65292;&#26131;&#20110;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#22270;&#20687;&#37325;&#26500;&#20013;&#36890;&#24120;&#20250;&#20135;&#29983;&#20005;&#37325;&#30340;&#20266;&#24433;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#25913;&#36827;&#26041;&#27861;&#26159;&#36890;&#36807;&#24494;&#35843;&#27491;&#21017;&#21270;&#21442;&#25968;&#26469;&#20943;&#36731;&#36825;&#20123;&#20266;&#24433;&#65292;&#20294;&#30001;&#20110;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#36275;&#22815;&#25110;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#20108;Nesterov&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#32593;&#32476;&#19981;&#20165;&#20855;&#26377;&#23545;&#39640;/&#20302;&#39057;&#22270;&#20687;&#29305;&#24449;&#30340;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#22312;&#21021;&#27493;&#32447;&#24615;&#37325;&#24314;&#20013;&#20174;&#29702;&#35770;&#19978;&#20445;&#35777;&#20960;&#20309;&#32441;&#29702;&#32454;&#33410;&#30340;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36991;&#20813;&#20013;&#38388;&#37325;&#24314;&#32467;&#26524;&#36229;&#20986;&#20960;&#20309;&#20998;&#35299;&#22495;&#30340;&#39118;&#38505;&#65292;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#37325;&#24314;&#26694;&#26550;&#34987;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;:&#22270;&#20687;&#21021;&#27493;&#32447;&#24615;&#37325;&#24314;&#21644;&#28145;&#24230;&#22686;&#37327;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proximal gradient-based optimization is one of the most common strategies for solving image inverse problems as well as easy to implement. However, these techniques often generate heavy artifacts in image reconstruction. One of the most popular refinement methods is to fine-tune the regularization parameter to alleviate such artifacts, but it may not always be sufficient or applicable due to increased computational costs. In this work, we propose a deep geometric incremental learning framework based on second Nesterov proximal gradient optimization. The proposed end-to-end network not only has the powerful learning ability for high/low frequency image features,but also can theoretically guarantee that geometric texture details will be reconstructed from preliminary linear reconstruction.Furthermore, it can avoid the risk of intermediate reconstruction results falling outside the geometric decomposition domains and achieve fast convergence. Our reconstruction framework is decomposed int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22810;&#20010;&#34920;&#31034;&#31354;&#38388;&#24182;&#26681;&#25454;&#22810;&#20010;&#26041;&#38754;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03805</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#20998;&#26512;&#30340;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables. (arXiv:2308.03805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24369;&#30417;&#30563;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22810;&#20010;&#34920;&#31034;&#31354;&#38388;&#24182;&#26681;&#25454;&#22810;&#20010;&#26041;&#38754;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#29615;&#22659;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#12289;&#20154;&#21592;&#35782;&#21035;&#25110;&#20581;&#24247;&#30417;&#27979;&#31561;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#27963;&#21160;&#21644;&#20256;&#24863;&#22120;&#27969;&#20998;&#26512;&#26041;&#38754;&#30340;&#22823;&#37096;&#20998;&#20808;&#21069;&#24037;&#20316;&#37117;&#20391;&#37325;&#20110;&#25968;&#25454;&#30340;&#26576;&#19968;&#26041;&#38754;&#65292;&#20363;&#22914;&#20165;&#35782;&#21035;&#27963;&#21160;&#31867;&#22411;&#25110;&#20165;&#35782;&#21035;&#25191;&#34892;&#27963;&#21160;&#30340;&#20154;&#21592;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#22810;&#36755;&#20986;&#23402;&#29983;&#32593;&#32476;&#65292;&#23398;&#20064;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22810;&#20010;&#34920;&#31034;&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#34920;&#31034;&#31354;&#38388;&#20391;&#37325;&#20110;&#25968;&#25454;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#25968;&#25454;&#26679;&#26412;&#30340;&#34920;&#31034;&#21521;&#37327;&#22312;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#20351;&#24471;&#22312;&#35813;&#26041;&#38754;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#24847;&#20041;&#30340;&#25968;&#25454;&#24444;&#27492;&#38752;&#36817;&#12290;&#22240;&#27492;&#65292;&#27491;&#22914;&#19968;&#31995;&#21015;&#23454;&#39564;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#22810;&#20010;&#26041;&#38754;&#20026;&#25968;&#25454;&#32858;&#31867;&#25552;&#20379;&#24230;&#37327;&#25351;&#26631;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and eve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#30740;&#31350;&#23545;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>http://arxiv.org/abs/2308.03800</link><description>&lt;p&gt;
&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#25991;&#26412;&#25968;&#25454;&#25366;&#25496;&#65306;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#30740;&#31350;&#23545;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;&#20197;&#19979;&#31616;&#31216;NLP&#65289;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#12290;&#39318;&#20808;&#65292;&#25105;&#25628;&#32034;&#20102;&#28207;&#20132;&#25152;&#26032;&#38395;&#30340;&#30417;&#31649;&#20844;&#21578;&#21644;&#25191;&#27861;&#20844;&#21578;&#65292;&#20197;&#23450;&#20041;&#27450;&#35784;&#20844;&#21496;&#24182;&#25552;&#21462;&#20854;MD&#65286;A&#25253;&#21578;&#65292;&#28982;&#21518;&#25972;&#29702;&#20102;&#25253;&#21578;&#20013;&#30340;&#21477;&#23376;&#65292;&#24182;&#26631;&#35760;&#20102;&#25253;&#21578;&#26102;&#38388;&#12290;&#25105;&#30340;&#26041;&#27861;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#20855;&#26377;&#23884;&#20837;&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#22522;&#26412;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#38480;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#22810;&#26679;&#21270;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#25105;&#26088;&#22312;&#20840;&#38754;&#27604;&#36739;&#23427;&#20204;&#22312;&#26816;&#27979;&#37329;&#34701;&#27450;&#35784;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#30340;&#32467;&#26524;&#23545;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#36825;&#39033;&#24037;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#65292;NLP&#21644;&#37329;&#34701;&#20132;&#21449;&#30740;&#31350;&#30340;&#19981;&#26029;&#22686;&#38271;&#36129;&#29486;&#20102;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;
In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&amp;A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuabl
&lt;/p&gt;</description></item><item><title>ReCLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#31354;&#38388;&#21644;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#21644;&#23884;&#20837;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03793</link><description>&lt;p&gt;
ReCLIP: &#20351;&#29992;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#20248;&#21270;&#23545;&#27604;&#24615;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation. (arXiv:2308.03793v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03793
&lt;/p&gt;
&lt;p&gt;
ReCLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#31354;&#38388;&#21644;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#21644;&#23884;&#20837;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#22312;&#27809;&#26377;&#30475;&#21040;&#20219;&#20309;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ImageNet&#19978;&#23454;&#29616;&#20102;76.3&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#36825;&#20026;&#35768;&#22810;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#23558;CLIP&#24212;&#29992;&#20110;&#19979;&#28216;&#30446;&#26631;&#39046;&#22495;&#26102;&#65292;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#24046;&#24322;&#20197;&#21450;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#19981;&#20934;&#30830;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24456;&#22823;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReCLIP&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#28304;&#25968;&#25454;&#25110;&#30446;&#26631;&#26631;&#27880;&#25968;&#25454;&#12290;ReCLIP&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#25237;&#24433;&#31354;&#38388;&#26469;&#20943;&#36731;&#19981;&#23545;&#40784;&#30340;&#35270;&#35273;-&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#23398;&#20064;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#20266;&#26631;&#31614;&#37096;&#32626;&#20132;&#21449;&#27169;&#24577;&#33258;&#35757;&#32451;&#65292;&#20197;&#36845;&#20195;&#22320;&#26356;&#26032;&#35270;&#35273;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20248;&#21270;&#26631;&#31614;&#65292;&#24182;&#20943;&#23569;&#39046;&#22495;&#24046;&#36317;&#21644;&#23884;&#20837;&#19981;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;ReCLIP&#33021;&#22815;&#20943;&#23569;&#24179;&#22343;...
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21333;&#19968;&#30340;&#23545;&#25239;&#25200;&#21160;P&#65292;&#22312;&#22810;&#20010;&#22270;&#20687;&#19978;&#23454;&#29616;&#23558;&#20854;&#21407;&#22987;&#31867;&#21035;&#25913;&#21464;&#20026;&#19981;&#21516;&#30446;&#26631;&#31867;&#21035;&#30340;&#22810;&#37325;&#25915;&#20987;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#23384;&#22312;&#22823;&#37327;&#39640;&#31867;&#21035;&#32622;&#20449;&#24230;&#30340;&#21306;&#22495;&#65292;&#23545;&#20840;&#38754;&#30340;&#38450;&#24481;&#31574;&#30053;&#26500;&#25104;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.03792</link><description>&lt;p&gt;
&#22810;&#37325;&#25915;&#20987;: &#22810;&#24352;&#22270;&#20687;+&#30456;&#21516;&#30340;&#23545;&#25239;&#25915;&#20987; -&gt; &#22810;&#20010;&#30446;&#26631;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels. (arXiv:2308.03792v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21333;&#19968;&#30340;&#23545;&#25239;&#25200;&#21160;P&#65292;&#22312;&#22810;&#20010;&#22270;&#20687;&#19978;&#23454;&#29616;&#23558;&#20854;&#21407;&#22987;&#31867;&#21035;&#25913;&#21464;&#20026;&#19981;&#21516;&#30446;&#26631;&#31867;&#21035;&#30340;&#22810;&#37325;&#25915;&#20987;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#23384;&#22312;&#22823;&#37327;&#39640;&#31867;&#21035;&#32622;&#20449;&#24230;&#30340;&#21306;&#22495;&#65292;&#23545;&#20840;&#38754;&#30340;&#38450;&#24481;&#31574;&#30053;&#26500;&#25104;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#25239;&#25200;&#21160;P&#21487;&#20197;&#23558;n&#20010;&#22270;&#20687;X1&#65292;X2&#65292;...&#65292;Xn&#20174;&#21407;&#22987;&#30340;&#26410;&#21463;&#25200;&#21160;&#30340;&#31867;&#21035;c1&#65292;c2&#65292;...&#65292;cn&#25913;&#21464;&#20026;&#26399;&#26395;&#30340;&#65288;&#19981;&#19968;&#23450;&#30456;&#21516;&#30340;&#65289;&#31867;&#21035;c^*_1&#65292;c^*_2&#65292;...&#65292;c^*_n&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;"&#22810;&#37325;&#25915;&#20987;"&#12290;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#65292;&#22914;&#22270;&#20687;&#20998;&#36776;&#29575;&#31561;&#65292;&#25105;&#20204;&#20272;&#35745;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#26576;&#20010;&#22270;&#20687;&#21608;&#22260;&#23384;&#22312;&#39640;&#31867;&#21035;&#32622;&#20449;&#24230;&#30340;&#21306;&#22495;&#25968;&#37327;&#22823;&#32422;&#20026;10^&#65288;O&#65288;100&#65289;&#65289;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#30340;&#38450;&#24481;&#31574;&#30053;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#31435;&#21363;&#20135;&#29983;&#30340;&#24433;&#21709;&#65306;&#26681;&#25454;&#24378;&#24230;&#25913;&#21464;&#32467;&#26524;&#31867;&#21035;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#21450;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#23637;&#31034;&#20687;&#32032;&#31354;&#38388;&#20013;&#31867;&#21035;&#20915;&#31574;&#36793;&#30028;&#30340;&#20887;&#20313;&#21644;&#20016;&#23500;&#24615;&#65292;&#25105;&#20204;&#23547;&#25214;&#20102;&#20854;&#22312;&#20108;&#32500;&#24179;&#38754;&#19978;&#36861;&#36394;&#22270;&#20687;&#21644;&#25340;&#20889;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision boundaries in the pixel space, we look for its two-dimensional sections that trace images and spel
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#20215;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT Base&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;11%&#30340;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.03782</link><description>&lt;p&gt;
Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#35770;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction. (arXiv:2308.03782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03782
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#20215;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT Base&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;11%&#30340;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#20998;&#26512;&#24739;&#32773;&#33647;&#29289;&#35780;&#35770;&#24182;&#20934;&#30830;&#20998;&#31867;&#28385;&#24847;&#31243;&#24230;&#20026;&#31215;&#26497;&#12289;&#20013;&#24615;&#25110;&#28040;&#26497;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#35265;&#35299;&#65292;&#36825;&#26159;&#27835;&#30103;&#25928;&#26524;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#22810;&#20010;&#20998;&#31867;&#27169;&#22411;&#65292;&#21253;&#25324;BERT base&#27169;&#22411;&#12289;Bio+Clinical BERT&#20197;&#21450;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;CNN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT base&#27169;&#22411;&#65292;&#22914;&#34920;2&#25152;&#31034;&#65292;&#23427;&#22312;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#19978;&#25552;&#21319;&#20102;11%&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#20197;&#25506;&#32034;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#27169;&#22411;&#30340;&#29305;&#23450;&#20248;&#21183;&#12290;Bio+Clinical BERT&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#21307;&#23398;&#34892;&#35805;&#26041;&#38754;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;CNN&#21017;&#23637;&#29616;&#20986;&#35782;&#21035;&#20851;&#38190;&#35789;&#21644;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to develop natural language processing (NLP) models that can analyze patients' drug reviews and accurately classify their satisfaction levels as positive, neutral, or negative. Such models would reduce the workload of healthcare professionals and provide greater insight into patients' quality of life, which is a critical indicator of treatment effectiveness. To achieve this, we implemented and evaluated several classification models, including a BERT base model, Bio+Clinical BERT, and a simpler CNN. Results indicate that the medical domain-specific Bio+Clinical BERT model significantly outperformed the general domain base BERT model, achieving macro f1 and recall score improvement of 11%, as shown in Table 2. Future research could explore how to capitalize on the specific strengths of each model. Bio+Clinical BERT excels in overall performance, particularly with medical jargon, while the simpler CNN demonstrates the ability to identify crucial words and a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#24615;&#27010;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;&#65292;&#24182;&#20197;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#20026;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.03773</link><description>&lt;p&gt;
&#23646;&#24615;&#27010;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Goodness-of-Fit of Attributed Probabilistic Graph Generative Models. (arXiv:2308.03773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#24615;&#27010;&#29575;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#25311;&#21512;&#20248;&#24230;&#65292;&#24182;&#20197;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#20026;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#26159;&#33021;&#22815;&#36827;&#34892;&#34920;&#31034;&#21644;&#25277;&#26679;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#21019;&#24314;&#20102;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#23646;&#24615;&#30340;&#22270;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20010;&#38543;&#26426;&#23646;&#24615;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30830;&#23450;&#20854;&#25311;&#21512;&#20248;&#24230;&#30340;&#19968;&#33324;&#26465;&#20214;&#24182;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#38543;&#26426;&#20108;&#36827;&#21046;&#32593;&#32476;&#30340;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#20026;&#22522;&#20934;&#26469;&#23450;&#20041;&#25311;&#21512;&#20248;&#24230;&#12290;&#23545;&#20110;&#36825;&#20010;&#32479;&#35745;&#37327;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#23646;&#24615;&#22270;&#32467;&#26500;&#36136;&#37327;&#30340;&#36807;&#31243;&#65292;&#30830;&#20445;&#22343;&#26041;&#20114;&#20449;&#24687;&#31995;&#25968;&#30340;&#20559;&#24046;&#26368;&#23567;&#65292;&#24182;&#19988;&#39640;&#27010;&#29575;&#19979;&#20445;&#25345;&#24658;&#23450;&#25110;&#38543;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20934;&#21017;&#24212;&#29992;&#20110;&#39564;&#35777;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#27969;&#34892;&#30340;&#22270;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic generative models of graphs are important tools that enable representation and sampling. Many recent works have created probabilistic models of graphs that are capable of representing not only entity interactions but also their attributes. However, given a generative model of random attributed graph(s), the general conditions that establish goodness of fit are not clear a-priori. In this paper, we define goodness of fit in terms of the mean square contingency coefficient for random binary networks. For this statistic, we outline a procedure for assessing the quality of the structure of a learned attributed graph by ensuring that the discrepancy of the mean square contingency coefficient (constant, or random) is minimal with high probability. We apply these criteria to verify the representation capability of a probabilistic generative model for various popular types of graph models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#32534;&#30721;&#20307;&#21644;&#25552;&#20379;&#22810;&#23610;&#24230;&#20960;&#20309;&#20449;&#24687;&#65292;&#21516;&#26102;&#36827;&#34892;&#28145;&#24230;&#39044;&#27979;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#65292;&#20197;&#23454;&#29616;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#20934;&#30830;&#24314;&#27169;&#21644;&#35270;&#35282;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.03772</link><description>&lt;p&gt;
&#20351;&#29992;&#20266;&#28145;&#24230;&#21644;&#34701;&#21512;&#25216;&#26415;&#25913;&#36827;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Improved Neural Radiance Fields Using Pseudo-depth and Fusion. (arXiv:2308.03772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#32534;&#30721;&#20307;&#21644;&#25552;&#20379;&#22810;&#23610;&#24230;&#20960;&#20309;&#20449;&#24687;&#65292;&#21516;&#26102;&#36827;&#34892;&#28145;&#24230;&#39044;&#27979;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#65292;&#20197;&#23454;&#29616;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#20934;&#30830;&#24314;&#27169;&#21644;&#35270;&#35282;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#25216;&#26415;&#38382;&#19990;&#20197;&#26469;&#65292;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#36752;&#23556;&#22330;&#37325;&#24314;&#36890;&#24120;&#36890;&#36807;&#20174;&#38468;&#36817;&#30340;&#28304;&#22270;&#20687;&#20013;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#20307;&#20316;&#20026;&#39069;&#22806;&#30340;&#36755;&#20837;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#32534;&#30721;&#30495;&#23454;&#22330;&#26223;&#20013;&#21508;&#31181;&#35268;&#27169;&#30340;&#29289;&#20307;/&#32467;&#26500;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26500;&#24314;&#22810;&#23610;&#24230;&#32534;&#30721;&#20307;&#24182;&#20026;NeRF&#27169;&#22411;&#25552;&#20379;&#22810;&#23610;&#24230;&#20960;&#20309;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#20026;&#20351;&#26500;&#24314;&#30340;&#32534;&#30721;&#20307;&#23613;&#21487;&#33021;&#25509;&#36817;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#34920;&#38754;&#21644;&#28210;&#26579;&#28145;&#24230;&#26356;&#20934;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#36827;&#34892;&#28145;&#24230;&#39044;&#27979;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#12290;&#39044;&#27979;&#30340;&#28145;&#24230;&#22270;&#23558;&#29992;&#20110;&#30417;&#30563;&#28210;&#26579;&#28145;&#24230;&#12289;&#32553;&#23567;&#28145;&#24230;&#33539;&#22260;&#24182;&#24341;&#23548;&#28857;&#37319;&#26679;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#36974;&#25377;&#12289;&#20809;&#29031;&#31561;&#21407;&#22240;&#65292;&#28857;&#20307;&#29305;&#24449;&#20013;&#21253;&#21547;&#30340;&#20960;&#20309;&#20449;&#24687;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the advent of Neural Radiance Fields, novel view synthesis has received tremendous attention. The existing approach for the generalization of radiance field reconstruction primarily constructs an encoding volume from nearby source images as additional inputs. However, these approaches cannot efficiently encode the geometric information of real scenes with various scale objects/structures. In this work, we propose constructing multi-scale encoding volumes and providing multi-scale geometry information to NeRF models. To make the constructed volumes as close as possible to the surfaces of objects in the scene and the rendered depth more accurate, we propose to perform depth prediction and radiance field reconstruction simultaneously. The predicted depth map will be used to supervise the rendered depth, narrow the depth range, and guide points sampling. Finally, the geometric information contained in point volume features may be inaccurate due to occlusion, lighting, etc. To this en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#27169;&#25311;&#21644;&#20998;&#26512;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#21487;&#35843;&#33410;&#36763;&#24418;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#39044;&#27979;&#21160;&#21147;&#23398;&#26102;&#20445;&#30041;&#20102;&#21704;&#23494;&#39039;&#26041;&#31243;&#21644;&#30456;&#31354;&#38388;&#30340;&#36763;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03763</link><description>&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#24314;&#27169;&#21644;&#20998;&#26512;&#21160;&#21147;&#31995;&#32479;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of Machine Learning to Modelling and Analysing Dynamical Systems. (arXiv:2308.03763v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#27169;&#25311;&#21644;&#20998;&#26512;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#21487;&#35843;&#33410;&#36763;&#24418;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#39044;&#27979;&#21160;&#21147;&#23398;&#26102;&#20445;&#30041;&#20102;&#21704;&#23494;&#39039;&#26041;&#31243;&#21644;&#30456;&#31354;&#38388;&#30340;&#36763;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#26512;&#20855;&#26377;&#36816;&#21160;&#31532;&#19968;&#31215;&#20998;&#30340;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#21160;&#21147;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#32467;&#21512;&#21040;&#21487;&#35843;&#33410;&#30340;&#36763;&#24418;&#29366;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#25972;&#20010;&#21442;&#25968;&#31354;&#38388;&#30340;&#21160;&#21147;&#23398;&#26102;&#20445;&#30041;&#20102;&#21704;&#23494;&#39039;&#26041;&#31243;&#21644;&#30456;&#31354;&#38388;&#30340;&#36763;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26550;&#26500;&#22312;&#39044;&#27979;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#22810;&#20010;&#21442;&#25968;&#30340;&#21183;&#33021;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#38750;&#32447;&#24615;&#30340;Henon-Heiles&#21183;&#33021;&#22312;&#28151;&#27788;&#12289;&#20934;&#21608;&#26399;&#21644;&#21608;&#26399;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#33021;&#21147;&#26469;&#39044;&#27979;&#20165;&#20973;&#37096;&#20998;&#20449;&#24687;&#21363;&#21487;&#33719;&#21462;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of Physics Informed Neural Networks to analyse nonlinear Hamiltonian Dynamical Systems with a first integral of motion. In this work, we propose an architecture which combines existing Hamiltonian Neural Network structures into Adaptable Symplectic Recurrent Neural Networks which preserve Hamilton's equations as well as the symplectic structure of phase space while predicting dynamics for the entire parameter space. This architecture is found to significantly outperform previously proposed neural networks when predicting Hamiltonian dynamics especially in potentials which contain multiple parameters. We demonstrate its robustness using the nonlinear Henon-Heiles potential under chaotic, quasiperiodic and periodic conditions.  The second problem we tackle is whether we can use the high dimensional nonlinear capabilities of neural networks to predict the dynamics of a Hamiltonian system given only partial information of the same. Hence we attempt to take advantage of L
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#35813;&#31639;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#19982;&#31169;&#23494;&#30340;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.03735</link><description>&lt;p&gt;
&#38543;&#26426;&#31639;&#27861;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Randomized algorithms for precise measurement of differentially-private, personalized recommendations. (arXiv:2308.03735v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#35813;&#31639;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#19982;&#31169;&#23494;&#30340;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#26159;&#24403;&#20170;&#20114;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#24110;&#21161;&#33402;&#26415;&#23478;&#21644;&#21019;&#20316;&#32773;&#21560;&#24341;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#65292;&#21516;&#26102;&#20063;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#26032;&#30340;&#26377;&#36259;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#20154;&#25968;&#25454;&#21644;&#25968;&#25454;&#38544;&#31169;&#30340;&#21382;&#21490;&#19978;&#31895;&#24515;&#23545;&#24453;&#65292;&#35768;&#22810;&#29992;&#25143;&#23545;&#20010;&#24615;&#21270;&#25512;&#33616;&#24179;&#21488;&#25345;&#24576;&#30097;&#24577;&#24230;&#12290;&#29616;&#22312;&#65292;&#20381;&#36182;&#20110;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20225;&#19994;&#27491;&#36827;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#20363;&#65292;&#38656;&#35201;&#23545;&#20182;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25512;&#33616;&#31639;&#27861;&#65292;&#26082;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#27979;&#37327;&#65292;&#21448;&#21487;&#20197;&#20445;&#25252;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#20197;&#24191;&#21578;&#20026;&#20363;&#24212;&#29992;&#65292;&#24182;&#36827;&#34892;&#31163;&#32447;&#23454;&#39564;&#65292;&#37327;&#21270;&#25552;&#20986;&#30340;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#23545;&#29992;&#25143;&#20307;&#39564;&#12289;&#24191;&#21578;&#21830;&#20215;&#20540;&#21644;&#24179;&#21488;&#25910;&#20837;&#31561;&#20851;&#38190;&#25351;&#26631;&#30340;&#24433;&#21709;&#65292;&#19982;&#65288;&#31169;&#23494;&#30340;&#65289;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#30340;&#26497;&#31471;&#24773;&#20917;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommendations form an important part of today's internet ecosystem, helping artists and creators to reach interested users, and helping users to discover new and engaging content. However, many users today are skeptical of platforms that personalize recommendations, in part due to historically careless treatment of personal data and data privacy. Now, businesses that rely on personalized recommendations are entering a new paradigm, where many of their systems must be overhauled to be privacy-first. In this article, we propose an algorithm for personalized recommendations that facilitates both precise and differentially-private measurement. We consider advertising as an example application, and conduct offline experiments to quantify how the proposed privacy-preserving algorithm affects key metrics related to user experience, advertiser value, and platform revenue compared to the extremes of both (private) non-personalized and non-private, personalized implementations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#20998;&#24067;&#24335;&#22270;&#20687;&#20256;&#36755;&#65292;&#36890;&#36807;&#20840;&#23616;&#32858;&#21512;&#25913;&#36827;&#20102;&#35821;&#20041;&#25552;&#21462;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03713</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22270;&#20687;&#35821;&#20041;&#26080;&#32447;&#20256;&#36755;&#30340;&#36890;&#20449;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission. (arXiv:2308.03713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#20998;&#24067;&#24335;&#22270;&#20687;&#20256;&#36755;&#65292;&#36890;&#36807;&#20840;&#23616;&#32858;&#21512;&#25913;&#36827;&#20102;&#35821;&#20041;&#25552;&#21462;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33410;&#28857;&#36890;&#20449;&#22312;&#29289;&#32852;&#32593;&#22330;&#26223;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#25968;&#25454;&#27969;&#37327;&#21644;&#20219;&#21153;&#25193;&#23637;&#30340;&#19981;&#28789;&#27963;&#24615;&#65292;&#20419;&#20351;&#20102;&#23545;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#20256;&#36755;&#26694;&#26550;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#26412;&#25991;&#38024;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#22810;&#20219;&#21153;&#20998;&#24067;&#24335;&#22270;&#20687;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;FLSC&#65289;&#26694;&#26550;&#12290;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20840;&#23616;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#27599;&#20010;&#29992;&#25143;&#29420;&#31435;&#30340;&#35821;&#20041;&#36890;&#20449;&#38142;&#36335;&#35774;&#35745;&#65292;&#21516;&#26102;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35821;&#20041;&#25552;&#21462;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;FLSC&#20013;&#30340;&#27599;&#20010;&#38142;&#36335;&#30001;&#22522;&#20110;&#23618;&#27425;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;HVT&#65289;&#30340;&#25552;&#21462;&#22120;&#21644;&#26681;&#25454;&#29305;&#23450;&#20869;&#23481;&#36827;&#34892;&#31895;&#21040;&#32454;&#30340;&#35821;&#20041;&#25552;&#21462;&#21644;&#24847;&#20041;&#36716;&#25442;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#32763;&#35793;&#22120;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-node communication, which refers to the interaction among multiple devices, has attracted lots of attention in many Internet-of-Things (IoT) scenarios. However, its huge amounts of data flows and inflexibility for task extension have triggered the urgent requirement of communication-efficient distributed data transmission frameworks. In this paper, inspired by the great superiorities on bandwidth reduction and task adaptation of semantic communications, we propose a federated learning-based semantic communication (FLSC) framework for multi-task distributed image transmission with IoT devices. Federated learning enables the design of independent semantic communication link of each user while further improves the semantic extraction and task performance through global aggregation. Each link in FLSC is composed of a hierarchical vision transformer (HVT)-based extractor and a task-adaptive translator for coarse-to-fine semantic extraction and meaning translation according to specific
&lt;/p&gt;</description></item><item><title>MedMine&#36890;&#36807;&#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.03629</link><description>&lt;p&gt;
MedMine: &#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MedMine: Examining Pre-trained Language Models on Medication Mining. (arXiv:2308.03629v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03629
&lt;/p&gt;
&lt;p&gt;
MedMine&#36890;&#36807;&#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#36827;&#34892;&#33647;&#29289;&#25366;&#25496;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#23545;&#21307;&#30103;&#24212;&#29992;&#30340;&#30495;&#23454;&#24433;&#21709;&#20197;&#21450;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20840;&#33258;&#21160;&#25552;&#21462;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#38556;&#30861;&#65292;&#20197;&#20415;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#20020;&#24202;&#23454;&#36341;&#20013;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#38556;&#30861;&#21253;&#25324;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#65292;&#21253;&#25324;&#22522;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;Med7&#21644;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa&#30340;&#26041;&#24335;&#65292;&#26816;&#39564;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#25361;&#25112;&#36187;&#30340;&#21382;&#21490;&#33647;&#29289;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23427;&#20204;&#30340;&#20248;&#21155;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20123;&#24494;&#35843;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#20197;&#20415;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#22914;&#20309;&#32467;&#21512;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25110;&#32773;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or impr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02632</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27169;&#25311;FMCW&#38647;&#36798;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#22522;&#20110;&#23556;&#32447;&#36861;&#36394;&#65292;&#36890;&#24120;&#35745;&#31639;&#23494;&#38598;&#19988;&#19981;&#33021;&#32771;&#34385;&#32972;&#26223;&#22122;&#22768;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;FMCW&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#21512;&#25104;&#30340;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;&#35813;&#26041;&#27861;&#29983;&#25104;&#20102;16&#20010;&#21516;&#26102;&#30340;&#33033;&#20914;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#24320;&#21457;&#29992;&#20110;&#22788;&#29702;&#38647;&#36798;&#25968;&#25454;&#65288;&#28388;&#27874;&#21644;&#32858;&#31867;&#65289;&#30340;&#31639;&#27861;&#12290;&#36825;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#19981;&#21487;&#22797;&#29616;&#30340;&#19981;&#23384;&#22312;&#25110;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#25705;&#25176;&#36710;&#30340;&#38647;&#36798;&#27979;&#37327;&#25968;&#25454;&#23545;GAN&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#29983;&#25104;&#25705;&#25176;&#36710;&#30452;&#32447;&#34892;&#39542;&#30340;&#21512;&#25104;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#26102;&#65292;&#20351;&#29992;&#20102;&#25705;&#25176;&#36710;&#30340;&#36317;&#31163;&#21644;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2308.02013</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02013
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#24335;&#65292;&#20801;&#35768;&#36793;&#32536;&#35774;&#22791;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#20687;Alexa&#21644;Siri&#36825;&#26679;&#30340;&#36793;&#32536;&#35774;&#22791;&#26159;&#28508;&#22312;&#30340;&#38750;&#26631;&#35760;&#38899;&#39057;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;FL&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36981;&#23432;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#26465;&#20214;&#65292;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;Libri-Light&#20013;&#30340;&#35828;&#35805;&#32773;&#21644;&#31456;&#33410;&#20449;&#24687;&#65292;&#27169;&#25311;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35828;&#35805;&#32773;&#38548;&#31163;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;FedSGD&#22312;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#19979;&#36827;&#34892;LSTM&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#20013;&#39044;&#35757;&#32451;&#30340;ASR&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#19982;&#20013;&#24515;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#26377;12-15%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32852;&#37030;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#65292;&#27861;&#35821;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;20%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25512;&#23548;&#20986;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31934;&#30830;&#26680;&#23545;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00824</link><description>&lt;p&gt;
&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Exact Kernel Equivalence for Finite Classification Models. (arXiv:2308.00824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25512;&#23548;&#20986;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#26377;&#38480;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#26680;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31934;&#30830;&#26680;&#23545;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25512;&#23548;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20219;&#24847;&#26377;&#38480;&#22823;&#23567;&#21442;&#25968;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31934;&#30830;&#34920;&#31034;&#19982;&#33879;&#21517;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#30456;&#23545;&#20110;NTK&#21644;&#20854;&#20182;&#38750;&#31934;&#30830;&#36335;&#24452;&#26680;&#20844;&#24335;&#30340;&#36817;&#20284;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#36924;&#30495;&#32593;&#32476;&#30340;&#26680;&#21040;&#26426;&#22120;&#31934;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#31934;&#30830;&#26680;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22914;&#20309;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#30340;&#27867;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ProtoFL&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#27491;&#35268;&#27969;&#30340;&#26412;&#22320;&#21333;&#31867;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#20808;&#21069;&#26041;&#27861;&#20855;&#26377;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12450</link><description>&lt;p&gt;
ProtoFL: &#36890;&#36807;&#21407;&#22411;&#33976;&#39311;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProtoFL: Unsupervised Federated Learning via Prototypical Distillation. (arXiv:2307.12450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ProtoFL&#65292;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#27491;&#35268;&#27969;&#30340;&#26412;&#22320;&#21333;&#31867;&#20998;&#31867;&#22120;&#20197;&#25552;&#39640;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#20808;&#21069;&#26041;&#27861;&#20855;&#26377;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#25552;&#39640;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#26377;&#28508;&#21147;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36523;&#20221;&#39564;&#35777;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#36890;&#20449;&#36718;&#27425;&#12289;&#31232;&#32570;&#30340;&#34920;&#31034;&#21644;&#21487;&#25193;&#23637;&#24615;&#32473;&#20854;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20854;&#21457;&#25381;&#20840;&#37096;&#28508;&#21147;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"ProtoFL"&#65292;&#22522;&#20110;&#21407;&#22411;&#34920;&#31034;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#20840;&#23616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27491;&#35268;&#27969;&#30340;&#26412;&#22320;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#25506;&#35752;&#20102;&#20351;&#29992;FL&#26469;&#25552;&#39640;&#21333;&#31867;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;MNIST&#12289;CIFAR-10&#12289;CIFAR-100&#12289;ImageNet-30&#21644;Keystroke-Dynamics&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#26041;&#27861;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33016;&#36879;&#35786;&#26029;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#20116;&#31181;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#21644;&#19968;&#31181;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;SHAP&#21644;Attri-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#36825;&#31181;&#31867;&#22411;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.12344</link><description>&lt;p&gt;
&#38169;&#35823;&#30340;&#21407;&#22240;&#32780;&#27491;&#30830;&#30340;&#65306;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#26816;&#27979;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?. (arXiv:2307.12344v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33016;&#36879;&#35786;&#26029;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#20116;&#31181;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#21644;&#19968;&#31181;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;SHAP&#21644;Attri-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#36825;&#31181;&#31867;&#22411;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24456;&#23481;&#26131;&#23398;&#20064;&#21040;&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#37027;&#20040;&#36825;&#31181;&#23545;&#28151;&#28102;&#20449;&#24687;&#30340;&#20381;&#36182;&#20250;&#24456;&#38590;&#36890;&#36807;&#24615;&#33021;&#25351;&#26631;&#26469;&#26816;&#27979;&#20986;&#26469;&#12290;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#20107;&#21518;&#35299;&#37322;&#25110;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#25215;&#35834;&#21487;&#20197;&#35782;&#21035;&#20986;&#38169;&#35823;&#30340;&#27169;&#22411;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#20570;&#21040;&#36825;&#19968;&#28857;&#23384;&#22312;&#30528;&#19968;&#20123;&#28151;&#21512;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#35299;&#37322;&#25216;&#26415;&#27491;&#30830;&#35782;&#21035;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#12290;&#20351;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#31181;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#21644;&#19968;&#31181;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#23545;&#20110;&#22312;&#33016;&#36879;&#35786;&#26029;&#20219;&#21153;&#20013;&#26816;&#27979;&#19977;&#31181;&#20154;&#20026;&#28155;&#21152;&#30340;&#28151;&#28102;&#22240;&#23376;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;SHAP&#20197;&#21450;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;Attri-Net&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11661</link><description>&lt;p&gt;
&#29992;GPT-4&#22686;&#24378;CLIP&#65306;&#21033;&#29992;&#35270;&#35273;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;VLMs&#36890;&#36807;&#35774;&#35745;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25552;&#31034;&#26469;0-shot&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25552;&#31034;&#24037;&#31243;&#21033;&#29992;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#20808;&#36827;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24037;&#20855;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#34987;&#25805;&#20316;&#20197;&#25552;&#20379;&#20219;&#20309;&#32467;&#26500;&#21270;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#65288;&#22914;EuroSAT&#65288;~7&#65285;&#65289;&#12289;DTD&#65288;~7&#65285;&#65289;&#12289;SUN397&#65288;~4.6&#65285;&#65289;&#21644;CUB&#65288;~3.3&#65285;&#65289;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23569;&#37327;&#26679;&#26412;&#36866;&#37197;&#22120;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#30340;s
&lt;/p&gt;
&lt;p&gt;
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#26469;&#35843;&#20248;SecureBoost&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#26368;&#20339;&#24179;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.10579</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#23545;SecureBoost&#36229;&#21442;&#25968;&#36827;&#34892;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning. (arXiv:2307.10579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#26469;&#35843;&#20248;SecureBoost&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#26368;&#20339;&#24179;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SecureBoost&#26159;&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#20445;&#25252;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#38544;&#31169;&#30340;&#26641;&#25552;&#21319;&#31639;&#27861;&#12290;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#12289;&#25928;&#26524;&#21644;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#65292;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;SecureBoost&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#39640;&#21644;&#26631;&#31614;&#27844;&#28431;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;SecureBoost&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#20180;&#32454;&#36873;&#25321;SecureBoost&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#32463;&#39564;&#24615;&#22320;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#35201;&#20040;&#21551;&#21457;&#24335;&#22320;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#36828;&#26410;&#36798;&#21040;&#26368;&#20248;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#22810;&#30446;&#26631;SecureBoost (CMOSB) &#31639;&#27861;&#65292;&#20197;&#23547;&#25214;&#27599;&#20010;&#35299;&#37117;&#26159;&#22312;&#25928;&#29992;&#25439;&#22833;&#12289;&#35757;&#32451;&#25104;&#26412;&#21644;&#38544;&#31169;&#27844;&#28431;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#30340;Pareto&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#30446;&#26631;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#38544;&#31169;&#27844;&#28431;&#26159;&#29992;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#25935;&#24863;&#30340;&#31181;&#26063;&#21644;&#26063;&#35028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#31181;&#26063;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;BiLSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#22312;&#22806;&#26679;&#26412;&#65288;OOS&#65289;F1&#24471;&#20998;&#19978;&#27604;&#25991;&#29486;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;36.8%&#12290;&#27492;&#22806;&#65292;&#36824;&#26500;&#24314;&#20102;&#32654;&#22269;&#26368;&#20840;&#38754;&#30340;&#22995;&#27663;&#21644;&#21517;&#23383;&#20998;&#24067;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20844;&#27491;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#24110;&#21161;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#32773;&#12290;</title><link>http://arxiv.org/abs/2307.08496</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;&#31181;&#26063;&#39044;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Trust Race Prediction?. (arXiv:2307.08496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#25935;&#24863;&#30340;&#31181;&#26063;&#21644;&#26063;&#35028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#31181;&#26063;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;BiLSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#22312;&#22806;&#26679;&#26412;&#65288;OOS&#65289;F1&#24471;&#20998;&#19978;&#27604;&#25991;&#29486;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;36.8%&#12290;&#27492;&#22806;&#65292;&#36824;&#26500;&#24314;&#20102;&#32654;&#22269;&#26368;&#20840;&#38754;&#30340;&#22995;&#27663;&#21644;&#21517;&#23383;&#20998;&#24067;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20844;&#27491;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#24110;&#21161;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#25935;&#24863;&#30340;&#31181;&#26063;&#21644;&#26063;&#35028;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20844;&#21496;&#37117;&#20511;&#21161;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20351;&#29992;&#26469;&#33258;&#32654;&#22269;50&#20010;&#24030;&#30340;&#36873;&#27665;&#27880;&#20876;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;BiLSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#20854;&#22312;&#22806;&#26679;&#26412;&#65288;OOS&#65289;F1&#24471;&#20998;&#19978;&#27604;&#25991;&#29486;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;36.8%&#12290;&#27492;&#22806;&#65292;&#25105;&#26500;&#24314;&#20102;&#32654;&#22269;&#26368;&#20840;&#38754;&#30340;&#22995;&#27663;&#21644;&#21517;&#23383;&#20998;&#24067;&#25968;&#25454;&#24211;&#65292;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#25913;&#36827;&#22995;&#27663;&#22320;&#29702;&#32534;&#30721;&#65288;BISG&#65289;&#21644;&#36125;&#21494;&#26031;&#25913;&#36827;&#21517;&#23383;&#22995;&#27663;&#22320;&#29702;&#32534;&#30721;&#65288;BIFSG&#65289;&#30340;&#35206;&#30422;&#21644;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20844;&#27491;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#24110;&#21161;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies. In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature. Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.07873</link><description>&lt;p&gt;
&#25506;&#32034;&#20174;&#26367;&#20195;&#35757;&#32451;&#20013;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;DNNs&#30340;&#23545;&#25239;&#26679;&#26412;(AEs)&#24050;&#32463;&#34920;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65306;&#25104;&#21151;&#27450;&#39575;&#30333;&#30418;&#23376;&#26367;&#20195;&#27169;&#22411;&#30340;AEs&#20063;&#21487;&#20197;&#27450;&#39575;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#12290;&#34429;&#28982;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#24230;&#21487;&#36716;&#31227;AE&#30340;&#25351;&#23548;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#32570;&#20047;&#35299;&#37322;&#29978;&#33267;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#22312;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#26041;&#38754;&#12290;&#20174;&#30528;&#21517;&#30340;&#23567;&#20581;&#22766;&#24615;&#29616;&#35937;&#24320;&#22987;&#65292;&#36890;&#36807;&#20197;&#36731;&#24494;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23427;&#20204;&#30340;&#20849;&#21516;&#25928;&#26524;&#19978;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#19982;&#21487;&#36716;&#31227;&#24615;&#30340;&#21333;&#29420;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.06713</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#26657;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#20808;&#39564;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#27491;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22823;&#37327;&#26080;&#30417;&#30563;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#26657;&#20934;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#26041;&#27861;&#36827;&#34892;&#36866;&#24212;&#20197;&#25191;&#34892;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#23558;LLM&#35270;&#20026;&#40657;&#30418;&#65292;&#22312;&#27169;&#22411;&#23631;&#38556;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#26657;&#20934;&#27169;&#22411;&#21518;&#39564;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#25552;&#31034;&#35757;&#32451;&#26679;&#26412;&#21644;&#26080;&#36866;&#24212;&#25968;&#25454;&#19979;&#30340;&#26657;&#20934;&#26041;&#27861;&#20013;&#20248;&#20110;&#26410;&#36866;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#20860;&#23481;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03571</link><description>&lt;p&gt;
&#24179;&#28369;&#36793;&#32536;&#65306;&#21033;&#29992;Hadamard&#36229;&#21442;&#25968;&#21270;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#24179;&#28369;&#20248;&#21270;&#20013;&#30340;&#19968;&#33324;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#20860;&#23481;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#65288;&#32467;&#26500;&#21270;&#65289;&#31232;&#30095;&#27491;&#21017;&#21270;&#38382;&#39064;&#20013;&#30340;$\ell_q$&#21644;$\ell_{p,q}$&#27491;&#21017;&#21270;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#36825;&#20123;&#38750;&#24179;&#28369;&#19988;&#21487;&#33021;&#38750;&#20984;&#30340;&#38382;&#39064;&#30340;&#20248;&#21270;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#38376;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#19968;&#33324;&#26694;&#26550;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21152;&#36895;&#21464;&#20307;&#65289;&#20860;&#23481;&#65292;&#26080;&#38656;&#20219;&#20309;&#20462;&#25913;&#12290;&#36825;&#26159;&#36890;&#36807;&#24179;&#28369;&#20248;&#21270;&#36716;&#31227;&#23454;&#29616;&#30340;&#65292;&#20854;&#20013;&#36873;&#23450;&#27169;&#22411;&#21442;&#25968;&#30340;&#36229;&#21442;&#25968;&#21270;&#20351;&#29992;Hadamard&#20056;&#31215;&#21644;&#24809;&#32602;&#30340;&#25913;&#21464;&#12290;&#22312;&#36229;&#21442;&#25968;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#29992;&#26367;&#20195;&#21442;&#25968;&#36827;&#34892;&#24179;&#28369;&#21644;&#20984;&#24615;&#30340;$\ell_2$&#27491;&#21017;&#21270;&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#21442;&#25968;&#21270;&#20013;&#24341;&#20837;&#38750;&#24179;&#28369;&#21644;&#38750;&#20984;&#24615;&#30340;$\ell_q$&#25110;$\ell_{p,q}$&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#36824;&#33021;&#24471;&#21040;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#36825;&#22312;&#38750;&#20984;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25214;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#21046;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21019;&#24314;&#21463;&#20856;&#22411;&#23545;&#35937;&#24433;&#21709;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26631;&#20934;&#27169;&#22411;&#65292;&#35299;&#20915;&#24402;&#22240;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09345</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Evaluating Data Attribution for Text-to-Image Models. (arXiv:2306.09345v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#21046;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21019;&#24314;&#21463;&#20856;&#22411;&#23545;&#35937;&#24433;&#21709;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26631;&#20934;&#27169;&#22411;&#65292;&#35299;&#20915;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#8220;&#26032;&#39062;&#8221;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20123;&#22270;&#20687;&#24517;&#28982;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#21453;&#26144;&#12290;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#25968;&#25454;&#24402;&#22240;&#30340;&#38382;&#39064;&#8212;&#8212;&#22312;&#35757;&#32451;&#38598;&#20013;&#21738;&#20123;&#22270;&#20687;&#23545;&#20110;&#29983;&#25104;&#30340;&#22270;&#20687;&#22806;&#35266;&#26368;&#36127;&#36131;&#8212;&#8212;&#26159;&#19968;&#20010;&#22256;&#38590;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#35813;&#38382;&#39064;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#25105;&#20204;&#36890;&#36807;&#8220;&#23450;&#21046;&#8221;&#26041;&#27861;&#26469;&#35780;&#20272;&#24402;&#22240;&#65292;&#35813;&#26041;&#27861;&#23558;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#35843;&#25972;&#21040;&#32473;&#23450;&#30340;&#20856;&#22411;&#23545;&#35937;&#25110;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#36825;&#26679;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#26377;&#25928;&#22320;&#21019;&#24314;&#21463;&#20856;&#22411;&#23545;&#35937;&#24433;&#21709;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#21508;&#31181;&#25968;&#25454;&#24402;&#22240;&#31639;&#27861;&#21644;&#19981;&#21516;&#21487;&#33021;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#35753;DINO&#12289;CLIP&#21644;ViT&#31561;&#26631;&#20934;&#27169;&#22411;&#38024;&#23545;&#24402;&#22240;&#38382;&#39064;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large text-to-image models are able to synthesize "novel" images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar
&lt;/p&gt;</description></item><item><title>GCformer&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#20840;&#23616;&#21367;&#31215;&#21644;&#23616;&#37096;Transformer&#20998;&#25903;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#32467;&#26500;&#21270;&#21367;&#31215;&#26680;&#65292;GCformer&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08325</link><description>&lt;p&gt;
GCformer:&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#21644;&#21487;&#25193;&#23637;&#30340;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GCformer: An Efficient Framework for Accurate and Scalable Long-Term Multivariate Time Series Forecasting. (arXiv:2306.08325v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08325
&lt;/p&gt;
&lt;p&gt;
GCformer&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#20840;&#23616;&#21367;&#31215;&#21644;&#23616;&#37096;Transformer&#20998;&#25903;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#32467;&#26500;&#21270;&#21367;&#31215;&#26680;&#65292;GCformer&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#19981;&#22815;&#20934;&#30830;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#26410;&#33021;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38271;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23610;&#23544;&#22823;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GCformer&#65292;&#23427;&#23558;&#29992;&#20110;&#22788;&#29702;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#32467;&#26500;&#21270;&#20840;&#23616;&#21367;&#31215;&#20998;&#25903;&#19982;&#29992;&#20110;&#25429;&#25417;&#30701;&#26399;&#36817;&#26399;&#20449;&#21495;&#30340;&#23616;&#37096;Transformer&#20998;&#25903;&#30456;&#32467;&#21512;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#23616;&#21367;&#31215;&#26680;&#30340;&#36830;&#36143;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#20840;&#23616;&#20998;&#25903;&#20013;&#36873;&#25321;&#30340;&#32467;&#26500;&#21270;&#21367;&#31215;&#26680;&#32463;&#36807;&#29305;&#27530;&#35774;&#35745;&#65292;&#20855;&#26377;&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#32780;&#22024;&#26434;&#30340;&#36755;&#20837;&#20449;&#21495;&#12290;&#23545;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;GCformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have emerged as promising tools for time series forecasting.  However, these model cannot make accurate prediction for long input time series. On the one hand, they failed to capture global dependencies within time series data. On the other hand, the long input sequence usually leads to large model size and high time complexity.  To address these limitations, we present GCformer, which combines a structured global convolutional branch for processing long input sequences with a local Transformer-based branch for capturing short, recent signals. A cohesive framework for a global convolution kernel has been introduced, utilizing three distinct parameterization methods. The selected structured convolutional kernel in the global branch has been specifically crafted with sublinear complexity, thereby allowing for the efficient and effective processing of lengthy and noisy input signals. Empirical studies on six benchmark datasets demonstrate that GCformer outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;,&#24182;&#34920;&#26126;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2305.19259</link><description>&lt;p&gt;
Shuffle SGD&#24635;&#26159;&#27604;SGD&#26356;&#22909;&#65306;&#23545;&#20855;&#26377;&#20219;&#24847;&#25968;&#25454;&#39034;&#24207;&#30340;SGD&#36827;&#34892;&#25913;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders. (arXiv:2305.19259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;,&#24182;&#34920;&#26126;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#38543;&#26426;&#37325;&#25490;&#65288;RR&#65289;&#21644;&#21333;&#27425;&#27927;&#29260;&#65288;SS&#65289;&#26159;&#36890;&#36807;&#24490;&#29615;&#36941;&#21382;&#35757;&#32451;&#25968;&#25454;&#30340;&#38543;&#26426;&#25110;&#21333;&#20010;&#25490;&#21015;&#30340;&#24120;&#35265;&#36873;&#25321;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#29616;&#26377;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#30340;&#35757;&#32451;&#22330;&#26223;&#20013;&#65292;&#24403;&#26102;&#20195;&#30340;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#38598;&#22823;&#23567;&#26102;&#65292;RR&#21487;&#33021;&#34920;&#29616;&#19981;&#22914;SGD&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#38543;&#26426;/&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#30340;&#22909;&#22788;&#65292;&#24182;&#20026;&#20854;&#38750;&#20984;&#25910;&#25947;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) algorithms are widely used in optimizing neural networks, with Random Reshuffling (RR) and Single Shuffle (SS) being popular choices for cycling through random or single permutations of the training data. However, the convergence properties of these algorithms in the non-convex case are not fully understood. Existing results suggest that, in realistic training scenarios where the number of epochs is smaller than the training set size, RR may perform worse than SGD.  In this paper, we analyze a general SGD algorithm that allows for arbitrary data orderings and show improved convergence rates for non-convex functions. Specifically, our analysis reveals that SGD with random and single shuffling is always faster or at least as good as classical SGD with replacement, regardless of the number of iterations. Overall, our study highlights the benefits of using SGD with random/single shuffling and provides new insights into its convergence properties for non-co
&lt;/p&gt;</description></item><item><title>UMD&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#65292;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#26032;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#26469;&#27979;&#37327;&#21644;&#36873;&#25321;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.18651</link><description>&lt;p&gt;
UMD: &#26080;&#30417;&#30563;&#27169;&#22411;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
UMD: Unsupervised Model Detection for X2X Backdoor Attacks. (arXiv:2305.18651v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18651
&lt;/p&gt;
&lt;p&gt;
UMD&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#65292;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#26032;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#26469;&#27979;&#37327;&#21644;&#36873;&#25321;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#65288;&#29305;&#27931;&#20234;&#65289;&#25915;&#20987;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#24120;&#35265;&#23041;&#32961;&#65292;&#20854;&#20013;&#23884;&#20837;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#31867;&#21035;&#30340;&#26679;&#26412;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#20026;&#23545;&#25239;&#30446;&#26631;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#20998;&#31867;&#22120;&#26159;&#21542;&#36973;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#38024;&#23545;&#21333;&#19968;&#23545;&#25239;&#30446;&#26631;&#30340;&#25915;&#20987;&#35774;&#35745;&#30340;&#65288;&#20363;&#22914;&#65292;&#20840;&#23545;&#19968;&#25915;&#20987;&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#20219;&#24847;&#25968;&#37327;&#30340;&#28304;&#31867;&#21035;&#30340;&#26356;&#26222;&#36941;&#30340;X2X&#25915;&#20987;&#65292;&#27599;&#20010;&#28304;&#31867;&#21035;&#37117;&#19982;&#20219;&#24847;&#30446;&#26631;&#31867;&#21035;&#37197;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UMD&#65292;&#31532;&#19968;&#20010;&#36890;&#36807;&#32852;&#21512;&#25512;&#26029;&#23545;&#25239;&#65288;&#28304;&#65292;&#30446;&#26631;&#65289;&#31867;&#21035;&#23545;&#26469;&#26377;&#25928;&#26816;&#27979;X2X&#21518;&#38376;&#25915;&#20987;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36716;&#31227;&#24615;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#37327;&#24230;&#21644;&#36873;&#25321;&#19968;&#32452;&#28508;&#22312;&#30340;&#21518;&#38376;&#31867;&#21035;&#23545;&#30340;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#36873;&#25321;&#30340;&#31867;&#21035;&#23545;&#26159;&#22522;&#20110;&#32852;&#21512;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.13452</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#24314;&#27169;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36523;&#20307;&#20869;&#22312;&#21160;&#26426;&#36827;&#34892;&#20102;&#37327;&#21270;&#24314;&#27169;&#65292;&#21457;&#29616;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#23545;&#29289;&#29702;&#24773;&#22659;&#30340;&#36259;&#21619;&#21453;&#24212;&#65292;&#36824;&#21457;&#29616;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#39044;&#27979;&#20154;&#31867;&#21453;&#24212;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#21644;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#26377;&#39537;&#21160;&#21147;&#30340;&#20114;&#21160;&#24615;&#20195;&#29702;&#65292;&#20182;&#20204;&#36861;&#27714;&#26377;&#36259;&#30340;&#29289;&#29702;&#21160;&#21147;&#23398;&#24773;&#22659;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24418;&#24335;&#21270;&#30340;&#29289;&#29702;&#20869;&#22312;&#21160;&#26426;&#24418;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#22810;&#31181;&#29289;&#29702;&#24773;&#22659;&#30340;&#35780;&#20998;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#20381;&#36182;&#20110;&#31616;&#21333;&#22330;&#26223;&#29305;&#24449;&#30340;&#27169;&#22411;&#21040;&#20381;&#36182;&#20110;&#21069;&#21521;&#29289;&#29702;&#39044;&#27979;&#30340;&#27169;&#22411;&#30340;&#21508;&#31181;&#20869;&#22312;&#21160;&#26426;&#20551;&#35774;&#26469;&#24314;&#27169;&#20154;&#31867;&#30340;&#36259;&#21619;&#21453;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#31867;&#21453;&#24212;&#30340;&#21333;&#19968;&#26368;&#20339;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#29289;&#29702;&#39044;&#27979;&#25439;&#22833;&#25512;&#23548;&#20986;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#22330;&#26223;&#29305;&#24449;&#27169;&#22411;&#19981;&#33021;&#22312;&#25152;&#26377;&#24773;&#22659;&#20013;&#25512;&#24191;&#20182;&#20204;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#23558;&#23545;&#25239;&#27169;&#22411;&#19982;&#22330;&#26223;&#20013;&#30896;&#25758;&#25968;&#37327;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#23545;&#20154;&#31867;&#21453;&#24212;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#26126;&#20154;&#31867;&#20542;&#21521;&#20110;&#36861;&#27714;&#39640;&#20449;&#24687;&#22686;&#30410;&#21644;&#36523;&#20307;&#27963;&#21160;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#32593;&#32476;&#29983;&#25104;&#31283;&#20581;&#30340;&#35821;&#20041;&#20998;&#21106;&#25552;&#35758;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12522</link><description>&lt;p&gt;
P-NOC:&#23545;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#36827;&#34892;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
P-NOC: Adversarial CAM Generation for Weakly Supervised Semantic Segmentation. (arXiv:2305.12522v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#32593;&#32476;&#29983;&#25104;&#31283;&#20581;&#30340;&#35821;&#20041;&#20998;&#21106;&#25552;&#35758;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#23545;&#22823;&#37327;&#26377;&#30417;&#30563;&#20998;&#21106;&#27880;&#37322;&#38598;&#30340;&#20381;&#36182;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#20808;&#36827;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#28608;&#21457;&#20998;&#21106;&#20808;&#39564;&#20013;&#26377;&#29992;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#39044;&#27979;&#23436;&#25972;&#24615;&#21644;&#23545;&#35821;&#20041;&#36793;&#30028;&#30340;&#24544;&#23454;&#24230;&#65289;&#30340;&#21457;&#23637;&#65292;&#32780;&#19981;&#32771;&#34385;&#27880;&#37322;&#20449;&#24687;&#30340;&#32570;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#20114;&#34917;&#30340;WSSS&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#25830;&#38500;&#31574;&#30053;&#65292;&#21253;&#25324;&#36880;&#28176;&#25913;&#36827;&#30340;&#20004;&#20010;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#20135;&#29983;&#31283;&#20581;&#30340;&#35821;&#20041;&#20998;&#21106;&#25552;&#35758;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20934;&#30340;&#25928;&#26524;&#65292;&#22312;Pascal VOC 2012&#21644;MS COCO 2014&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the necessity for large amounts of supervised segmentation annotation sets, multiple Weakly Supervised Semantic Segmentation (WSSS) strategies have been devised. These will often rely on advanced data and model regularization strategies to instigate the development of useful properties (e.g., prediction completeness and fidelity to semantic boundaries) in segmentation priors, notwithstanding the lack of annotated information. In this work, we first create a strong baseline by analyzing complementary WSSS techniques and regularizing strategies, considering their strengths and limitations. We then propose a new Class-specific Adversarial Erasing strategy, comprising two adversarial CAM generating networks being gradually refined to produce robust semantic segmentation proposals. Empirical results suggest that our approach induces substantial improvement in the effectiveness of the baseline, resulting in a noticeable improvement over both Pascal VOC 2012 and MS COCO 2014 datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC&#30340;&#26032;&#38271;&#23614;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01160</link><description>&lt;p&gt;
&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#23454;&#29616;&#38271;&#23614;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC&#30340;&#26032;&#38271;&#23614;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#31181;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#38271;&#23614;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;&#36923;&#36753;&#26031;&#33922;&#35843;&#25972;&#25216;&#26415;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#32452;&#21512;&#26159;&#20020;&#26102;&#30340;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#29702;&#35770;&#32972;&#26223;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#32972;&#26223;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#38271;&#23614;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#23427;&#20204;&#35797;&#22270;&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#36755;&#20837;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#30001;&#20110;&#19981;&#32771;&#34385;&#30495;&#23454;&#26631;&#31614;&#30340;&#26368;&#22823;&#21270;&#65292;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#35299;&#37322;&#20026;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#38598;&#25104;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#36923;&#36753;&#26031;&#33922;&#35843;&#25972;&#25216;&#26415;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#28508;&#22312;&#31867;&#21035;&#65288;LC&#65289;&#26041;&#27861;&#65292;&#23427;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#30495;&#23454;&#26631;&#31614;&#30340;&#20998;&#24067;&#65292;&#24182;&#32852;&#21512;&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#23545;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22312;&#20869;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The goal of this paper is to provide the background and further improve the performance. First, we show that the fundamental reason contrastive learning methods struggle with long-tailed tasks is that they try to maximize the mutual information maximization between latent features and input data. As ground-truth labels are not considered in the maximization, they are not able to address imbalances between class labels. Rather, we interpret the long-tailed recognition task as a mutual information maximization between latent features and ground-truth labels. This approach integrates contrastive learning a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#36817;&#36793;&#32536;&#26696;&#20363;&#30340;&#38754;&#37096;&#39564;&#35777;&#38382;&#39064;&#65292;&#21457;&#29616;&#32467;&#21512;&#20154;&#26426;&#20915;&#31574;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08134</link><description>&lt;p&gt;
&#35299;&#20915;&#38754;&#37096;&#39564;&#35777;&#36793;&#32536;&#26696;&#20363;&#65306;&#28145;&#24230;&#20998;&#26512;&#21644;&#20154;&#26426;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#36817;&#36793;&#32536;&#26696;&#20363;&#30340;&#38754;&#37096;&#39564;&#35777;&#38382;&#39064;&#65292;&#21457;&#29616;&#32467;&#21512;&#20154;&#26426;&#20915;&#31574;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#26426;&#22120;&#26080;&#27861;&#27491;&#30830;&#20998;&#31867;&#30340;&#36793;&#32536;&#26696;&#20363;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#21644;&#20154;&#25805;&#20316;&#21592;&#22312;&#38754;&#37096;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#25928;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#36793;&#32536;&#26696;&#20363;&#65292;&#20197;&#21457;&#29616;&#24120;&#35265;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#24615;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#36873;&#23450;&#20219;&#21153;&#20013;&#30340;60&#20010;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#26426;&#22120;&#21644;&#20154;&#31867;&#20915;&#31574;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, face recognition systems surpass human performance on several datasets. However, there are still edge cases that the machine can't correctly classify. This paper investigates the effect of a combination of machine and human operators in the face verification task. First, we look closer at the edge cases for several state-of-the-art models to discover common datasets' challenging settings. Then, we conduct a study with 60 participants on these selected tasks with humans and provide an extensive analysis. Finally, we demonstrate that combining machine and human decisions can further improve the performance of state-of-the-art face verification systems on various benchmark datasets. Code and data are publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>PMAA&#26159;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#39640;&#24615;&#33021;&#21355;&#26143;&#36965;&#24863;&#20113;&#21435;&#38500;&#26550;&#26500;&#12290;&#20854;&#20013;&#65292;&#29420;&#29305;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#23616;&#37096;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#21516;&#26102;&#34920;&#31034;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#29305;&#24449;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.16565</link><description>&lt;p&gt;
PMAA&#65306;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#24335;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#21355;&#26143;&#36965;&#24863;&#20113;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery. (arXiv:2303.16565v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16565
&lt;/p&gt;
&lt;p&gt;
PMAA&#26159;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#39640;&#24615;&#33021;&#21355;&#26143;&#36965;&#24863;&#20113;&#21435;&#38500;&#26550;&#26500;&#12290;&#20854;&#20013;&#65292;&#29420;&#29305;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#23616;&#37096;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#21516;&#26102;&#34920;&#31034;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#29305;&#24449;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#36965;&#24863;&#22270;&#20687;&#20998;&#26512;&#22312;&#36828;&#31243;&#24863;&#30693;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#30001;&#20113;&#23618;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28176;&#36827;&#24335;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#8221;&#65288;PMAA&#65289;&#30340;&#39640;&#24615;&#33021;&#20113;&#21435;&#38500;&#26550;&#26500;&#65292;&#21516;&#26102;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#20854;&#20027;&#35201;&#21253;&#25324;&#20113;&#26816;&#27979;&#21644;&#20113;&#21435;&#38500;&#27169;&#22359;&#12290;&#20113;&#26816;&#27979;&#36890;&#36807;&#20113;&#25513;&#27169;&#21152;&#24378;&#20113;&#21306;&#22495;&#20197;&#20419;&#36827;&#20113;&#21435;&#38500;&#12290;&#20113;&#21435;&#38500;&#27169;&#22359;&#20027;&#35201;&#21253;&#25324;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;MAM&#65289;&#21644;&#23616;&#37096;&#20132;&#20114;&#27169;&#22359;&#65288;LIM&#65289;&#12290;PMAA&#36890;&#36807;MAM&#24314;&#31435;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#65292;&#24182;&#36890;&#36807;LIM&#35843;&#33410;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#21516;&#19968;&#32423;&#21035;&#19978;&#21516;&#26102;&#21576;&#29616;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;&#20511;&#21161;&#19981;&#21516;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#30340;&#24110;&#21161;&#19979;&#65292;PMAA&#22312;&#23450;&#37327;&#21644;&#35270;&#35273;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite imagery analysis plays a vital role in remote sensing, but the information loss caused by cloud cover seriously hinders its application. This study presents a high-performance cloud removal architecture called Progressive Multi-scale Attention Autoencoder (PMAA), which simultaneously leverages global and local information. It mainly consists of a cloud detection backbone and a cloud removal module. The cloud detection backbone uses cloud masks to reinforce cloudy areas to prompt the cloud removal module. The cloud removal module mainly comprises a novel Multi-scale Attention Module (MAM) and a Local Interaction Module (LIM). PMAA establishes the long-range dependency of multi-scale features using MAM and modulates the reconstruction of the fine-grained details using LIM, allowing for the simultaneous representation of fine- and coarse-grained features at the same level. With the help of diverse and multi-scale feature representation, PMAA outperforms the previous state-of-the
&lt;/p&gt;</description></item><item><title>GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.16459</link><description>&lt;p&gt;
GNNBuilder&#65306;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#29983;&#25104;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization. (arXiv:2303.16459v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16459
&lt;/p&gt;
&lt;p&gt;
GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#24456;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21152;&#36895;&#22120;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#29992;&#25143;&#30340;&#30828;&#20214;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38024;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#23454;&#38469;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNBuilder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#12290;&#23427;&#20855;&#26377;&#22235;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;GNNBuilder&#21487;&#20197;&#33258;&#21160;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#29983;&#25104;GNN&#21152;&#36895;&#22120;&#65307;&#65288;2&#65289;GNNBuilder&#37319;&#29992;&#26631;&#20934;&#30340;PyTorch&#32534;&#31243;&#25509;&#21475;&#65292;&#20026;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#38646;&#24320;&#38144;&#65307;&#65288;3&#65289;GNNBuilder&#25903;&#25345;&#31471;&#21040;&#31471;&#30340;&#20195;&#30721;&#29983;&#25104;&#12289;&#27169;&#25311;&#12289;&#21152;&#36895;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#37096;&#32626;&#65292;&#23454;&#29616;&#20102;GNN&#21152;&#36895;&#22120;&#35774;&#35745;&#30340;&#19968;&#38190;&#24335;&#25805;&#20316;&#65307;&#65288;4&#65289;GNNBuilder&#37197;&#22791;&#20102;&#20854;&#25152;&#29983;&#25104;&#30340;&#21152;&#36895;&#22120;&#30340;&#20934;&#30830;&#24615;&#33021;&#27169;&#22411;&#65292;&#20351;&#24471;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65288;DSE&#65289;&#24555;&#36895;&#32780;&#28789;&#27963;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21152;&#36895;&#22120;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#21152;&#36895;&#22120;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#65292;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#20102;&#22810;&#36798;12.95&#20493;&#12290;&#20854;&#27425;&#65292;&#23545;&#19981;&#35268;&#21017;&#22270;&#22788;&#29702;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GNNBuilder&#30340;&#20986;&#33394;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#21333;&#20010;GPU&#30340;&#26381;&#21153;&#22120;&#19978;&#65292;&#25105;&#20204;&#23545;400&#20010;GNN&#27169;&#22411;&#36827;&#34892;&#20102;30&#20998;&#38047;&#30340;DSE&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;GNNBuilder&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are plenty of graph neural network (GNN) accelerators being proposed. However, they highly rely on users' hardware expertise and are usually optimized for one specific GNN model, making them challenging for practical use . Therefore, in this work, we propose GNNBuilder, the first automated, generic, end-to-end GNN accelerator generation framework. It features four advantages: (1) GNNBuilder can automatically generate GNN accelerators for a wide range of GNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorch programming interface, introducing zero overhead for algorithm developers; (3) GNNBuilder supports end-to-end code generation, simulation, accelerator optimization, and hardware deployment, realizing a push-button fashion for GNN accelerator design; (4) GNNBuilder is equipped with accurate performance models of its generated accelerator, enabling fast and flexible design space exploration (DSE). In the experiments, first, we show that our accelerator pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Attri-Net&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#23427;&#33021;&#25552;&#20379;&#36879;&#26126;&#12289;&#21487;&#20449;&#36182;&#21644;&#20154;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#31867;&#21035;&#29305;&#23450;&#30340;&#24402;&#22240;&#22270;&#65292;&#22522;&#20110;&#23545;&#25239;&#20107;&#23454;&#26469;&#30830;&#23450;&#22270;&#20687;&#20013;&#23545;&#24212;&#20110;&#29305;&#23450;&#21307;&#30103;&#21457;&#29616;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23545;&#24402;&#22240;&#22270;&#36827;&#34892;&#20998;&#31867;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.00500</link><description>&lt;p&gt;
&#20197;&#31867;&#21035;&#29305;&#23450;&#30340;&#23545;&#25239;&#20107;&#23454;&#20026;&#22522;&#30784;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals. (arXiv:2303.00500v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00500
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Attri-Net&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#23427;&#33021;&#25552;&#20379;&#36879;&#26126;&#12289;&#21487;&#20449;&#36182;&#21644;&#20154;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#31867;&#21035;&#29305;&#23450;&#30340;&#24402;&#22240;&#22270;&#65292;&#22522;&#20110;&#23545;&#25239;&#20107;&#23454;&#26469;&#30830;&#23450;&#22270;&#20687;&#20013;&#23545;&#24212;&#20110;&#29305;&#23450;&#21307;&#30103;&#21457;&#29616;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23545;&#24402;&#22240;&#22270;&#36827;&#34892;&#20998;&#31867;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#39640;&#24615;&#33021;&#30340;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#19981;&#20250;&#25552;&#20379;&#20854;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#21512;&#20316;&#19981;&#21487;&#20449;&#20219;&#21644;&#27425;&#20248;&#12290;&#30446;&#21069;&#30340;&#21518;&#22788;&#29702;&#35299;&#37322;&#25216;&#26415;&#22312;&#22810;&#26631;&#31614;&#24773;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#20854;&#20013;&#21516;&#19968;&#22270;&#20687;&#20013;&#21487;&#33021;&#21516;&#26102;&#20986;&#29616;&#22810;&#20010;&#21307;&#30103;&#21457;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Attri-Net&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;Attri-Net&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#65292;&#25552;&#20379;&#36879;&#26126;&#12289;&#21487;&#20449;&#36182;&#21644;&#20154;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#22522;&#20110;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#31867;&#21035;&#29305;&#23450;&#30340;&#24402;&#22240;&#22270;&#65292;&#20197;&#30830;&#23450;&#21738;&#20123;&#22270;&#20687;&#21306;&#22495;&#23545;&#24212;&#20110;&#29305;&#23450;&#30340;&#21307;&#30103;&#21457;&#29616;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23545;&#24402;&#22240;&#22270;&#36827;&#34892;&#20998;&#31867;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is essential for machine learning algorithms in high-stakes application fields such as medical image analysis. However, high-performing black-box neural networks do not provide explanations for their predictions, which can lead to mistrust and suboptimal human-ML collaboration. Post-hoc explanation techniques, which are widely used in practice, have been shown to suffer from severe conceptual problems. Furthermore, as we show in this paper, current explanation techniques do not perform adequately in the multi-label scenario, in which multiple medical findings may co-occur in a single image. We propose Attri-Net, an inherently interpretable model for multi-label classification. Attri-Net is a powerful classifier that provides transparent, trustworthy, and human-understandable explanations. The model first generates class-specific attribution maps based on counterfactuals to identify which image regions correspond to certain medical findings. Then a simple logistic regre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.00286</link><description>&lt;p&gt;
&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#65292;&#23545;&#24453;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#26377;&#24046;&#24322;&#24615;&#65306;&#21033;&#29992;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#20016;&#23500;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#29992;&#20110;&#19982;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#12290;&#23427;&#20204;&#20351;&#29992;&#32771;&#34385;&#20102;&#19968;&#25209;&#24471;&#20998;&#19977;&#20803;&#32452;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20256;&#32479;&#26041;&#27861;&#35748;&#20026;&#19977;&#20803;&#32452;&#30340;&#26631;&#31614;&#35201;&#20040;&#20026;&#30495;&#65292;&#35201;&#20040;&#20026;&#20551;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#36127;&#26679;&#26412;&#24212;&#35813;&#34987;&#24179;&#31561;&#23545;&#24453;&#12290;&#19982;&#36825;&#19968;&#26368;&#36817;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#39046;&#22495;&#21644;&#33539;&#22260;&#32422;&#26463;&#22312;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#36127;&#26679;&#26412;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25439;&#22833;&#20989;&#25968;&#24212;&#35813;&#23558;&#23427;&#20204;&#19982;&#35821;&#20041;&#19978;&#26080;&#25928;&#30340;&#36127;&#26679;&#26412;&#21306;&#21035;&#23545;&#24453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#38142;&#25509;&#39044;&#27979;&#30340;&#19977;&#20010;&#20027;&#35201;&#25439;&#22833;&#20989;&#25968;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#24191;&#27867;&#21644;&#21463;&#25511;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#20844;&#20849;&#22522;&#20934;KG&#19978;&#31995;&#32479;&#22320;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;SBAG&#65289;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#27880;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#26469;&#25915;&#20987;GCNs&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.14353</link><description>&lt;p&gt;
&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A semantic backdoor attack against Graph Convolutional Networks. (arXiv:2302.14353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;SBAG&#65289;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#27880;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#26469;&#25915;&#20987;GCNs&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#35299;&#20915;&#21508;&#31181;&#22270;&#32467;&#26500;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#65289;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GCNs&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#26032;&#22411;&#23041;&#32961;&#65292;&#31216;&#20026;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#27880;&#20837;GCNs&#20013;&#65292;&#20351;&#24471;&#25915;&#20987;&#27169;&#22411;&#22312;&#33391;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26159;&#22914;&#26524;&#25915;&#20987;&#32773;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#28608;&#27963;&#20102;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#23558;&#34987;&#24694;&#24847;&#22320;&#20462;&#25913;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#26631;&#31614;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GCNs&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65288;SBAG&#65289;&#26469;&#25581;&#31034;GCNs&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;SBAG&#20351;&#29992;&#26679;&#26412;&#20013;&#30340;&#26576;&#31181;&#33410;&#28857;&#20316;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#27880;&#20837;&#21040;GCNs&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) have been very effective in addressing the issue of various graph-structured related tasks, such as node classification and graph classification. However, recent research has shown that GCNs are vulnerable to a new type of threat called the backdoor attack, where the adversary can inject hidden backdoor into the GCNs so that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed to the attacker-specified target label if the hidden backdoor is activated by the attacker-defined trigger. In this paper, we investigate whether such semantic backdoor attacks are possible for GCNs and propose a Semantic Backdoor Attack against GCNs(SBAG) under the context of graph classification to reveal the existence of this security vulnerability in GCNs. The SBAG uses a certain type of node in the samples as a backdoor trigger and injects hidden backdoor into GCNs models through poisoning training data. The backdoor will b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#35768;&#22810;&#20986;&#29616;&#22312;&#25991;&#29486;&#20013;&#30340;&#22240;&#26524;&#21059;&#20992;&#65292;&#24182;&#29305;&#21035;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#19981;&#22826;&#21463;&#27426;&#36814;&#30340;&#22240;&#26524;&#21059;&#20992;&#8212;&#8212;&#21442;&#25968;&#26368;&#23567;&#24615;&#12290;&#36923;&#36753;&#32467;&#26524;&#25581;&#31034;&#20102;&#36873;&#25321;&#21512;&#29702;&#24471;&#20998;&#26631;&#20934;&#26102;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2302.10331</link><description>&lt;p&gt;
&#22240;&#26524;&#21059;&#20992;
&lt;/p&gt;
&lt;p&gt;
Causal Razors. (arXiv:2302.10331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#35768;&#22810;&#20986;&#29616;&#22312;&#25991;&#29486;&#20013;&#30340;&#22240;&#26524;&#21059;&#20992;&#65292;&#24182;&#29305;&#21035;&#30740;&#31350;&#20102;&#22312;&#22810;&#39033;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#19981;&#22826;&#21463;&#27426;&#36814;&#30340;&#22240;&#26524;&#21059;&#20992;&#8212;&#8212;&#21442;&#25968;&#26368;&#23567;&#24615;&#12290;&#36923;&#36753;&#32467;&#26524;&#25581;&#31034;&#20102;&#36873;&#25321;&#21512;&#29702;&#24471;&#20998;&#26631;&#20934;&#26102;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#65292;&#24517;&#39035;&#23545;&#30495;&#23454;&#22240;&#26524;&#26426;&#21046;&#22914;&#20309;&#19982;&#24213;&#23618;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30456;&#23545;&#24212;&#20570;&#20986;&#20551;&#35774;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#20551;&#35774;&#31216;&#20026;&#22240;&#26524;&#21059;&#20992;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#35768;&#22810;&#20986;&#29616;&#22312;&#25991;&#29486;&#20013;&#30340;&#22240;&#26524;&#21059;&#20992;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#36923;&#36753;&#27604;&#36739;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;&#22312;&#22810;&#39033;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#19981;&#22826;&#21463;&#27426;&#36814;&#30340;&#22240;&#26524;&#21059;&#20992;&#8212;&#8212;&#21442;&#25968;&#26368;&#23567;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#22240;&#26524;&#21059;&#20992;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#36923;&#36753;&#32467;&#26524;&#22312;&#20026;&#22522;&#20110;&#20998;&#25968;&#30340;&#22240;&#26524;&#25628;&#32034;&#31639;&#27861;&#36873;&#25321;&#21512;&#29702;&#24471;&#20998;&#26631;&#20934;&#26102;&#25552;&#20986;&#20102;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
When performing causal discovery, assumptions have to be made on how the true causal mechanism corresponds to the underlying joint probability distribution. These assumptions are labeled as causal razors in this work. We review numerous causal razors that appeared in the literature, and offer a comprehensive logical comparison of them. In particular, we scrutinize an unpopular causal razor, namely parameter minimality, in multinomial causal models and its logical relations with other well-studied causal razors. Our logical result poses a dilemma in selecting a reasonable scoring criterion for score-based casual search algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GAN&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65292;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#33719;&#24471;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#37325;&#26032;&#32553;&#25918;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#21521;&#37327;&#22330;&#36827;&#34892;&#31890;&#23376;&#27969;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.01075</link><description>&lt;p&gt;
MonoFlow: &#20174;Wasserstein&#26799;&#24230;&#27969;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;Divergence GANs
&lt;/p&gt;
&lt;p&gt;
MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows. (arXiv:2302.01075v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GAN&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65292;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#33719;&#24471;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#37325;&#26032;&#32553;&#25918;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#21521;&#37327;&#22330;&#36827;&#34892;&#31890;&#23376;&#27969;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#36890;&#36807;&#21028;&#21035;&#22120;&#26469;&#20272;&#35745;&#31163;&#25955;&#24230;&#65292;&#29983;&#25104;&#22120;&#23398;&#20064;&#26368;&#23567;&#21270;&#36825;&#20010;&#31163;&#25955;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#35768;&#22810;GANs&#21464;&#20307;&#37117;&#26159;&#25353;&#29031;&#36825;&#20010;&#33539;&#20363;&#24320;&#21457;&#30340;&#65292;&#20294;&#24403;&#21069;GANs&#30340;&#29702;&#35770;&#29702;&#35299;&#21644;&#23454;&#38469;&#31639;&#27861;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#23637;&#31034;&#20102;&#26679;&#26412;&#31354;&#38388;&#20869;&#31890;&#23376;&#28436;&#21270;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26469;&#33719;&#24471;GANs&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#31639;&#27861;&#21551;&#31034;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;MonoFlow&#65306;&#31890;&#23376;&#28436;&#21270;&#36890;&#36807;&#23494;&#24230;&#27604;&#20363;&#30340;&#21333;&#35843;&#36882;&#22686;&#26144;&#23556;&#36827;&#34892;&#37325;&#26032;&#32553;&#25918;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36807;&#31243;&#65292;&#39318;&#20808;&#36890;&#36807;&#35757;&#32451;&#37492;&#21035;&#22120;&#33719;&#24471;MonoFlow&#30340;&#21521;&#37327;&#22330;&#65292;&#28982;&#21518;&#29983;&#25104;&#22120;&#23398;&#20064;&#30001;&#30456;&#24212;&#21521;&#37327;&#22330;&#25152;&#23450;&#20041;&#30340;&#31890;&#23376;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional understanding of adversarial training in generative adversarial networks (GANs) is that the discriminator is trained to estimate a divergence, and the generator learns to minimize this divergence. We argue that despite the fact that many variants of GANs were developed following this paradigm, the current theoretical understanding of GANs and their practical algorithms are inconsistent. In this paper, we leverage Wasserstein gradient flows which characterize the evolution of particles in the sample space, to gain theoretical insights and algorithmic inspiration of GANs. We introduce a unified generative modeling framework - MonoFlow: the particle evolution is rescaled via a monotonically increasing mapping of the log density ratio. Under our framework, adversarial training can be viewed as a procedure first obtaining MonoFlow's vector field via training the discriminator and the generator learns to draw the particle flow defined by the corresponding vector field. We al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#26435;&#37325;&#39044;&#27979;&#25216;&#26415;&#24341;&#20837;AdamW&#20248;&#21270;&#22120;&#65292;&#21152;&#36895;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36807;&#31243;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#21319;&#20102;AdamW&#30340;&#25910;&#25947;&#24615;&#24182;&#22312;&#35757;&#32451;DNN&#27169;&#22411;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00195</link><description>&lt;p&gt;
&#26435;&#37325;&#39044;&#27979;&#25552;&#21319;AdamW&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weight Prediction Boosts the Convergence of AdamW. (arXiv:2302.00195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#26435;&#37325;&#39044;&#27979;&#25216;&#26415;&#24341;&#20837;AdamW&#20248;&#21270;&#22120;&#65292;&#21152;&#36895;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#36807;&#31243;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#21319;&#20102;AdamW&#30340;&#25910;&#25947;&#24615;&#24182;&#22312;&#35757;&#32451;DNN&#27169;&#22411;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26435;&#37325;&#39044;&#27979;&#24341;&#20837;AdamW&#20248;&#21270;&#22120;&#65292;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#25910;&#25947;&#36807;&#31243;&#12290;&#22312;&#27599;&#20010;&#23567;&#25209;&#37327;&#35757;&#32451;&#20043;&#21069;&#65292;&#25105;&#20204;&#26681;&#25454;AdamW&#30340;&#26356;&#26032;&#35268;&#21017;&#39044;&#27979;&#26410;&#26469;&#30340;&#26435;&#37325;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#26410;&#26469;&#26435;&#37325;&#24212;&#29992;&#20110;&#21069;&#21521;&#20256;&#25773;&#21644;&#21453;&#21521;&#20256;&#25773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;AdamW&#20248;&#21270;&#22120;&#22987;&#32456;&#21033;&#29992;&#19982;&#26410;&#26469;&#26435;&#37325;&#30456;&#20851;&#30340;&#26799;&#24230;&#32780;&#19981;&#26159;&#24403;&#21069;&#26435;&#37325;&#26469;&#26356;&#26032;DNN&#21442;&#25968;&#65292;&#20351;&#24471;AdamW&#20248;&#21270;&#22120;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#31616;&#21333;&#30452;&#35266;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#20294;&#22312;&#21152;&#36895;DNN&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#25552;&#21319;AdamW&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#35757;&#32451;DNN&#27169;&#22411;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce weight prediction into the AdamW optimizer to boost its convergence when training the deep neural network (DNN) models. In particular, ahead of each mini-batch training, we predict the future weights according to the update rule of AdamW and then apply the predicted future weights to do both forward pass and backward propagation. In this way, the AdamW optimizer always utilizes the gradients w.r.t. the future weights instead of current weights to update the DNN parameters, making the AdamW optimizer achieve better convergence. Our proposal is simple and straightforward to implement but effective in boosting the convergence of DNN training. We performed extensive experimental evaluations on image classification and language modeling tasks to verify the effectiveness of our proposal. The experimental results validate that our proposal can boost the convergence of AdamW and achieve better accuracy than AdamW when training the DNN models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#21644;&#30452;&#35266;&#30340;&#26041;&#27861;&#29983;&#25104;&#23436;&#20840;&#26631;&#27880;&#30340;&#26174;&#24494;&#38236;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#23545;&#20154;&#24037;&#26631;&#27880;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2301.10227</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#23436;&#20840;&#26631;&#27880;&#26174;&#24494;&#38236;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets. (arXiv:2301.10227v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26080;&#30417;&#30563;&#21644;&#30452;&#35266;&#30340;&#26041;&#27861;&#29983;&#25104;&#23436;&#20840;&#26631;&#27880;&#30340;&#26174;&#24494;&#38236;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#23545;&#20154;&#24037;&#26631;&#27880;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#22312;&#36924;&#30495;&#22270;&#20687;&#25968;&#25454;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in computer vision have led to significant progress in the generation of realistic image data, with denoising diffusion probabilistic models proving to be a particularly effective method. In this study, we demonstrate that diffusion models can effectively generate fully-annotated microscopy image data sets through an unsupervised and intuitive approach, using rough sketches of desired structures as the starting point. The proposed pipeline helps to reduce the reliance on manual annotations when training deep learning-based segmentation approaches and enables the segmentation of diverse datasets without the need for human annotations. This approach holds great promise in streamlining the data generation process and enabling a more efficient and scalable training of segmentation models, as we show in the example of different practical experiments involving various organisms and cell types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#24182;&#32467;&#21512;dropout&#25216;&#26415;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#21017;&#26159;&#19968;&#31181;&#39640;&#25928;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.00790</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#24207;&#34920;&#26684;&#25968;&#25454;&#30340;&#21160;&#24577;&#29305;&#24449;&#24037;&#31243;&#21644;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#22312;&#21046;&#24230;&#21464;&#21270;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Feature Engineering and model selection methods for temporal tabular datasets with regime changes. (arXiv:2301.00790v2 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#24182;&#32467;&#21512;dropout&#25216;&#26415;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#21017;&#26159;&#19968;&#31181;&#39640;&#25928;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20005;&#37325;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#22312;&#21046;&#24230;&#21464;&#21270;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#21046;&#24230;&#21464;&#21270;&#19979;&#23545;&#26102;&#24207;&#38754;&#26495;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#36827;&#34892;&#25490;&#21517;&#12290;&#31649;&#36947;&#35780;&#20272;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#21644;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#31616;&#21333;&#29305;&#24449;&#24037;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;dropout&#30340;GBDT&#27169;&#22411;&#20855;&#26377;&#39640;&#24615;&#33021;&#12289;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#30456;&#23545;&#22797;&#26434;&#24230;&#36739;&#20302;&#12289;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#27979;&#21518;&#22788;&#29702;&#20013;&#29992;&#20110;&#22686;&#24378;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#29305;&#24449;&#20013;&#21644;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#25928;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of deep learning algorithms to temporal panel datasets is difficult due to heavy non-stationarities which can lead to over-fitted models that under-perform under regime changes. In this work we propose a new machine learning pipeline for ranking predictions on temporal panel datasets which is robust under regime changes of data. Different machine-learning models, including Gradient Boosting Decision Trees (GBDTs) and Neural Networks with and without simple feature engineering are evaluated in the pipeline with different settings. We find that GBDT models with dropout display high performance, robustness and generalisability with relatively low complexity and reduced computational cost. We then show that online learning techniques can be used in post-prediction processing to enhance the results. In particular, dynamic feature neutralisation, an efficient procedure that requires no retraining of models and can be applied post-prediction to any machine learning model, impr
&lt;/p&gt;</description></item><item><title>Genie&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#24320;&#21457;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#36866;&#21512;&#38646;&#26679;&#26412;&#37327;&#21270;&#30340;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2212.04780</link><description>&lt;p&gt;
Genie: &#23637;&#31034;&#25105;&#37327;&#21270;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Genie: Show Me the Data for Quantization. (arXiv:2212.04780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04780
&lt;/p&gt;
&lt;p&gt;
Genie&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#24320;&#21457;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#36866;&#21512;&#38646;&#26679;&#26412;&#37327;&#21270;&#30340;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#65288;&#21253;&#25324;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#65289;&#26080;&#27861;&#35775;&#38382;&#26102;&#65292;&#38646;&#26679;&#26412;&#37327;&#21270;&#26159;&#24320;&#21457;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;FP32&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#30340;&#23398;&#20064;&#21442;&#25968;&#65288;$\mu$&#21644;$\sigma$&#65289;&#65292;&#38646;&#26679;&#26412;&#37327;&#21270;&#26041;&#26696;&#19987;&#27880;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#38543;&#21518;&#65292;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20256;&#36882;&#32473;&#37327;&#21270;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#65292;&#20351;&#24471;&#37327;&#21270;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#38646;&#26679;&#26412;&#37327;&#21270;&#20027;&#35201;&#22312;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#30340;&#19978;&#19979;&#25991;&#20013;&#35752;&#35770;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#21644;&#38271;&#26399;&#30340;&#20248;&#21270;&#65292;&#23601;&#20687;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#37327;&#21270;&#65292;&#21487;&#20197;&#22312;&#20960;&#23567;&#26102;&#20869;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37327;&#21270;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Genie&#8221;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36866;&#21512;&#38646;&#26679;&#26412;&#37327;&#21270;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot quantization is a promising approach for developing lightweight deep neural networks when data is inaccessible owing to various reasons, including cost and issues related to privacy. By exploiting the learned parameters ($\mu$ and $\sigma$) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowledge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be optimized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the context of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantization scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Furthermore, we propose a framework called \genie~that generates data suited for q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#35760;&#24518;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;SMRLS&#65289;&#30340;&#23454;&#26102;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#32463;&#20856;&#36951;&#24536;&#26426;&#21046;&#36716;&#21270;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#26679;&#26412;&#30340;&#37319;&#38598;&#26102;&#38388;&#21644;&#26102;&#31354;&#20998;&#24067;&#26469;&#35780;&#20272;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.07909</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35760;&#24518;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65306;&#23558;&#36951;&#24536;&#36716;&#21270;&#20026;&#24452;&#21521;&#22522;&#31070;&#32463;&#32593;&#32476;&#23454;&#26102;&#23398;&#20064;&#20013;&#30340;&#35760;&#24518; (arXiv:2211.07909v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Selective Memory Recursive Least Squares: Recast Forgetting into Memory in RBF Neural Network Based Real-Time Learning. (arXiv:2211.07909v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#35760;&#24518;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;SMRLS&#65289;&#30340;&#23454;&#26102;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#32463;&#20856;&#36951;&#24536;&#26426;&#21046;&#36716;&#21270;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#26679;&#26412;&#30340;&#37319;&#38598;&#26102;&#38388;&#21644;&#26102;&#31354;&#20998;&#24067;&#26469;&#35780;&#20272;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;RBFNN&#65289;&#30340;&#23454;&#26102;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#24191;&#27867;&#20351;&#29992;&#36951;&#24536;&#26426;&#21046;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#23545;&#26032;&#25968;&#25454;&#20445;&#25345;&#25935;&#24863;&#24615;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36951;&#24536;&#26426;&#21046;&#65292;&#19968;&#20123;&#26377;&#29992;&#30340;&#30693;&#35782;&#20250;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#24456;&#20037;&#20197;&#21069;&#32780;&#34987;&#36951;&#24536;&#65292;&#36825;&#34987;&#31216;&#20026;&#34987;&#21160;&#30693;&#35782;&#36951;&#24536;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#35760;&#24518;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;SMRLS&#65289;&#65292;&#20854;&#20013;&#23558;&#32463;&#20856;&#36951;&#24536;&#26426;&#21046;&#36716;&#21270;&#20026;&#19968;&#31181;&#35760;&#24518;&#26426;&#21046;&#12290;&#19982;&#36951;&#24536;&#26426;&#21046;&#19981;&#21516;&#65292;&#35813;&#35760;&#24518;&#26426;&#21046;&#20027;&#35201;&#36890;&#36807;&#26679;&#26412;&#30340;&#37319;&#38598;&#26102;&#38388;&#20197;&#21450;&#26102;&#31354;&#20998;&#24067;&#35780;&#20272;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;SMRLS&#65292;RBFNN&#30340;&#36755;&#20837;&#31354;&#38388;&#34987;&#22343;&#21248;&#21010;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20998;&#21306;&#65292;&#24182;&#20351;&#29992;&#32508;&#21512;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#32508;&#21512;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radial basis function neural network (RBFNN) based real-time learning tasks, forgetting mechanisms are widely used such that the neural network can keep its sensitivity to new data. However, with forgetting mechanisms, some useful knowledge will get lost simply because they are learned a long time ago, which we refer to as the passive knowledge forgetting phenomenon. To address this problem, this paper proposes a real-time training method named selective memory recursive least squares (SMRLS) in which the classical forgetting mechanisms are recast into a memory mechanism. Different from the forgetting mechanism, which mainly evaluates the importance of samples according to the time when samples are collected, the memory mechanism evaluates the importance of samples through both temporal and spatial distribution of samples. With SMRLS, the input space of the RBFNN is evenly divided into a finite number of partitions and a synthesized objective function is developed using synthesized 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;&#12290;&#36890;&#36807;&#32416;&#27491;&#20256;&#32479;&#25490;&#24207;&#26694;&#26550;&#20013;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25151;&#28304;&#21305;&#37197;&#65292;&#24182;&#21152;&#20837;&#20102;&#32771;&#34385;&#25151;&#28304;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2210.07774</link><description>&lt;p&gt;
&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning To Rank Diversely At Airbnb. (arXiv:2210.07774v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Airbnb&#19978;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#26041;&#27861;&#12290;&#36890;&#36807;&#32416;&#27491;&#20256;&#32479;&#25490;&#24207;&#26694;&#26550;&#20013;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25490;&#24207;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25151;&#28304;&#21305;&#37197;&#65292;&#24182;&#21152;&#20837;&#20102;&#32771;&#34385;&#25151;&#28304;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Airbnb&#26159;&#19968;&#20010;&#21452;&#36793;&#24066;&#22330;&#65292;&#23558;&#20986;&#31199;&#25151;&#28304;&#30340;&#25151;&#19996;&#19982;&#26469;&#33258;&#20840;&#29699;&#30340;&#28508;&#22312;&#23458;&#20154;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#24212;&#29992;&#21040;&#21305;&#37197;&#23458;&#20154;&#19982;&#25151;&#19996;&#30340;&#36807;&#31243;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#25490;&#24207;&#25913;&#36827;&#26159;&#36890;&#36807;&#19968;&#31181;&#26680;&#24515;&#31574;&#30053;&#39537;&#21160;&#30340;&#65306;&#25353;&#29031;&#20854;&#39044;&#35745;&#30340;&#39044;&#35746;&#27010;&#29575;&#23545;&#25151;&#28304;&#36827;&#34892;&#25490;&#24207;&#65292;&#28982;&#21518;&#36845;&#20195;&#22320;&#25913;&#36827;&#36825;&#20123;&#39044;&#35746;&#27010;&#29575;&#20272;&#35745;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31574;&#30053;&#26263;&#21547;&#30340;&#19968;&#20010;&#20551;&#35774;&#26159;&#65292;&#25628;&#32034;&#32467;&#26524;&#20013;&#30340;&#27599;&#20010;&#25151;&#28304;&#30340;&#39044;&#35746;&#27010;&#29575;&#21487;&#20197;&#29420;&#31435;&#30830;&#23450;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20010;&#20551;&#35774;&#30340;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32416;&#27491;&#36825;&#20010;&#20551;&#35774;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#21450;&#22522;&#20110;&#35813;&#29702;&#35770;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#36890;&#36807;&#26174;&#24335;&#32771;&#34385;&#25151;&#28304;&#20043;&#38388;&#30340;&#21487;&#33021;&#30456;&#20284;&#24615;&#24182;&#20943;&#23567;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Airbnb is a two-sided marketplace, bringing together hosts who own listings for rent, with prospective guests from around the globe. Applying neural network-based learning to rank techniques has led to significant improvements in matching guests with hosts. These improvements in ranking were driven by a core strategy: order the listings by their estimated booking probabilities, then iterate on techniques to make these booking probability estimates more and more accurate. Embedded implicitly in this strategy was an assumption that the booking probability of a listing could be determined independently of other listings in search results. In this paper we discuss how this assumption, pervasive throughout the commonly-used learning to rank frameworks, is false. We provide a theoretical foundation correcting this assumption, followed by efficient neural network architectures based on the theory. Explicitly accounting for possible similarities between listings, and reducing them to diversify
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#26032;&#20219;&#21153;DVS-GC&#24182;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#29616;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#23545;&#20107;&#20214;&#39034;&#24207;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2209.14915</link><description>&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#65306;&#29702;&#35299;&#20854;&#20248;&#21183;&#30340;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks for event-based action recognition: A new task to understand their advantage. (arXiv:2209.14915v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#26032;&#20219;&#21153;DVS-GC&#24182;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#29616;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#23545;&#20107;&#20214;&#39034;&#24207;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#29420;&#29305;&#30340;&#26102;&#38388;&#21160;&#24577;&#29305;&#24615;&#65292;&#20294;&#26159;&#36825;&#31181;&#35745;&#31639;&#30340;&#24615;&#36136;&#21644;&#20248;&#21183;&#20173;&#28982;&#19981;&#22826;&#34987;&#20102;&#35299;&#12290;&#20026;&#20102;&#25552;&#20379;&#31572;&#26696;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#23454;&#29616;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#65292;&#26080;&#38656;&#36882;&#24402;&#31361;&#35302;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20256;&#32479;&#31070;&#32463;&#20803;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#20219;&#21153;DVS-Gesture-Chain&#65288;DVS-GC&#65289;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#29616;&#23454;&#20013;&#30340;&#20107;&#20214;&#39537;&#21160;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;DVS Gesture&#22522;&#20934;&#21487;&#20197;&#34987;&#19981;&#36827;&#34892;&#26102;&#38388;&#29305;&#24449;&#25552;&#21462;&#30340;&#32593;&#32476;&#35299;&#20915;&#65292;&#32780;&#26032;&#30340;DVS-GC&#21017;&#38656;&#35201;&#23545;&#20107;&#20214;&#30340;&#39034;&#24207;&#36827;&#34892;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNN) are characterised by their unique temporal dynamics, but the properties and advantages of such computations are still not well understood. In order to provide answers, in this work we demonstrate how Spiking neurons can enable temporal feature extraction in feed-forward neural networks without the need for recurrent synapses, showing how their bio-inspired computing principles can be successfully exploited beyond energy efficiency gains and evidencing their differences with respect to conventional neurons. This is demonstrated by proposing a new task, DVS-Gesture-Chain (DVS-GC), which allows, for the first time, to evaluate the perception of temporal dependencies in a real event-based action recognition dataset. Our study proves how the widely used DVS Gesture benchmark could be solved by networks without temporal feature extraction, unlike the new DVS-GC which demands an understanding of the ordering of the events. Furthermore, this setup allowed us to un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#26680;&#24515;&#38598;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23567;&#23376;&#38598;&#26469;&#38477;&#20302;&#31283;&#20581;&#35757;&#32451;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.05785</link><description>&lt;p&gt;
&#38024;&#23545;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Adversarial Coreset Selection for Efficient Robust Training. (arXiv:2209.05785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#26680;&#24515;&#38598;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23567;&#23376;&#38598;&#26469;&#38477;&#20302;&#31283;&#20581;&#35757;&#32451;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#33030;&#24369;&#24615;&#65306;&#21521;&#20854;&#36755;&#20837;&#21152;&#20837;&#24494;&#23567;&#20294;&#31934;&#24515;&#21046;&#20316;&#30340;&#25200;&#21160;&#21487;&#20197;&#25913;&#21464;&#20854;&#36755;&#20986;&#12290;&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#25269;&#24481;&#27492;&#31867;&#25915;&#20987;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#24930;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20026;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#24515;&#38598;&#36873;&#25321;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#26159;&#20943;&#23569;&#31283;&#20581;&#35757;&#32451;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;&#26377;&#21407;&#21017;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#23545;&#25239;&#24615;&#26680;&#24515;&#38598;&#36873;&#25321;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#19978;&#30028;&#19982;&#26680;&#24515;&#38598;&#33021;&#22815;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#26799;&#24230;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26799;&#24230;&#36817;&#20284;&#35823;&#24046;&#20316;&#20026;&#23545;&#25239;&#24615;&#26680;&#24515;&#38598;&#36873;&#25321;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches to training robust models against such attacks. Unfortunately, this method is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration. By leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a principled approach to reducing the time complexity of robust training. To this end, we first provide convergence guarantees for adversarial coreset selection. In particular, we show that the convergence bound is directly related to how well our coresets can approximate the gradient computed over the entire training data. Motivated by our theoretical analysis, we propose using this gradient approximation error as our adversarial coreset selection obj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2208.00085</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#34588;&#34562;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26159;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#35299;&#20915;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#30417;&#27979;&#34588;&#34562;&#32676;&#20307;&#24182;&#26816;&#26597;&#20854;&#20581;&#24247;&#29366;&#20917;&#65292;&#20174;&#32780;&#22312;&#24773;&#20917;&#21464;&#24471;&#20005;&#37325;&#20043;&#21069;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#21361;&#38505;&#29366;&#24577;&#65292;&#25110;&#32773;&#26356;&#22909;&#22320;&#35745;&#21010;&#23450;&#26399;&#34588;&#34562;&#32676;&#20307;&#26816;&#26597;&#65292;&#20174;&#32780;&#33410;&#30465;&#37325;&#35201;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29992;&#20110;&#34588;&#34562;&#30417;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#20026;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#38754;&#21521;&#20861;&#21307;&#23398;&#21644;&#34588;&#34562;&#23398;&#19987;&#19994;&#20154;&#21592;&#21644;&#19987;&#23478;&#65292;&#26088;&#22312;&#21521;&#20182;&#20204;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#27599;&#20010;&#24212;&#29992;&#31867;&#21035;&#37117;&#20197;&#31616;&#35201;&#30340;&#29702;&#35770;&#20171;&#32461;&#21644;&#19982;&#20854;&#22522;&#26412;&#26041;&#27861;&#30456;&#20851;&#30340;&#21160;&#26426;&#24320;&#31687;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#33021;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;...
&lt;/p&gt;
&lt;p&gt;
Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;Bellman&#31639;&#23376;&#21644;&#31574;&#30053;&#35780;&#20272;&#31639;&#23376;&#25512;&#24191;&#20026;&#20540;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#21387;&#32553;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#25552;&#21319;&#20026;&#20316;&#29992;&#20110;&#20540;&#20989;&#25968;&#38598;&#21512;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;&#12290;&#21033;&#29992;&#38598;&#21512;&#35770;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23558;&#32463;&#20856;&#40065;&#26834;MDP&#25991;&#29486;&#20013;&#30340;&#30697;&#24418;&#26465;&#20214;&#25512;&#24191;&#20026;&#36866;&#29992;&#20110;&#26356;&#22810;&#21442;&#25968;&#19981;&#30830;&#23450;MDP&#21644;&#21160;&#24577;&#35268;&#21010;&#20013;&#30340;&#21387;&#32553;&#31639;&#23376;&#30340;&#21253;&#21547;&#26465;&#20214;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;&#20855;&#26377;&#21387;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.07271</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
Set-based value operators for non-stationary Markovian environments. (arXiv:2207.07271v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;Bellman&#31639;&#23376;&#21644;&#31574;&#30053;&#35780;&#20272;&#31639;&#23376;&#25512;&#24191;&#20026;&#20540;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#21387;&#32553;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#25552;&#21319;&#20026;&#20316;&#29992;&#20110;&#20540;&#20989;&#25968;&#38598;&#21512;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;&#12290;&#21033;&#29992;&#38598;&#21512;&#35770;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23558;&#32463;&#20856;&#40065;&#26834;MDP&#25991;&#29486;&#20013;&#30340;&#30697;&#24418;&#26465;&#20214;&#25512;&#24191;&#20026;&#36866;&#29992;&#20110;&#26356;&#22810;&#21442;&#25968;&#19981;&#30830;&#23450;MDP&#21644;&#21160;&#24577;&#35268;&#21010;&#20013;&#30340;&#21387;&#32553;&#31639;&#23376;&#30340;&#21253;&#21547;&#26465;&#20214;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;&#20855;&#26377;&#21387;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#21442;&#25968;&#30340;&#26377;&#38480;&#29366;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#30340;&#19981;&#21160;&#28857;&#29702;&#35770;&#37325;&#26032;&#23457;&#35270;&#20102;&#40065;&#26834;MDP&#30340;&#32467;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;Bellman&#31639;&#23376;&#21644;&#31574;&#30053;&#35780;&#20272;&#31639;&#23376;&#25512;&#24191;&#20026;&#20540;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#21387;&#32553;&#31639;&#23376;&#65292;&#24182;&#31216;&#20043;&#20026;&#8220;&#20540;&#36816;&#31639;&#31526;&#8221;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20540;&#36816;&#31639;&#31526;&#25552;&#21319;&#20026;&#20316;&#29992;&#20110;&#20540;&#20989;&#25968;&#38598;&#21512;&#30340;&#8220;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;&#8221;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32039;&#33268;&#20540;&#20989;&#25968;&#38598;&#21512;&#31354;&#38388;&#20013;&#65292;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;&#26159;&#8220;&#21387;&#32553;&#8221;&#36816;&#31639;&#31526;&#12290;&#20511;&#21161;&#38598;&#21512;&#35770;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23558;&#32463;&#20856;&#40065;&#26834;MDP&#25991;&#29486;&#20013;&#30340;&#30697;&#24418;&#26465;&#20214;&#25512;&#24191;&#20026;&#36866;&#29992;&#20110;&#26356;&#22810;&#21442;&#25968;&#19981;&#30830;&#23450;MDP&#21644;&#21160;&#24577;&#35268;&#21010;&#20013;&#30340;&#21387;&#32553;&#31639;&#23376;&#30340;&#21253;&#21547;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#30456;&#23545;&#36739;&#24369;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30697;&#24418;&#26465;&#20214;&#21644;&#21253;&#21547;&#26465;&#20214;&#37117;&#36275;&#20197;&#30830;&#20445;&#22522;&#20110;&#38598;&#21512;&#30340;&#20540;&#36816;&#31639;&#31526;&#30340;&#22855;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyzes finite state Markov Decision Processes (MDPs) with uncertain parameters in compact sets and re-examines results from robust MDP via set-based fixed point theory. To this end, we generalize the Bellman and policy evaluation operators to contracting operators on the value function space and denote them as \emph{value operators}. We lift these value operators to act on \emph{sets} of value functions and denote them as \emph{set-based value operators}. We prove that the set-based value operators are \emph{contractions} in the space of compact value function sets. Leveraging insights from set theory, we generalize the rectangularity condition in classic robust MDP literature to a containment condition for all value operators, which is weaker and can be applied to a larger set of parameter-uncertain MDPs and contracting operators in dynamic programming. We prove that both the rectangularity condition and the containment condition sufficiently ensure that the set-based val
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26159;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#65292;AEAIL&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;&#20110;&#22122;&#22768;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.11004</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Auto-Encoding Adversarial Imitation Learning. (arXiv:2206.11004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11004
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26159;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#65292;AEAIL&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;&#20110;&#22122;&#22768;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#20854;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#25581;&#31034;&#20102;&#22312;&#27809;&#26377;&#26469;&#33258;&#29615;&#22659;&#30340;&#22870;&#21169;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#33719;&#21462;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#32534;&#30721;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AEAIL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#20174;&#31034;&#33539;&#20013;&#25512;&#23548;&#20986;&#19987;&#23478;&#31574;&#30053;&#65292;AEAIL&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36825;&#27604;&#20043;&#21069;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22810;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;&#30446;&#26631;&#20989;&#25968;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;AEAIL&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#31034;&#33539;&#19987;&#23478;&#20855;&#26377;&#22122;&#22768;&#26102;&#65292;AEAIL&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) provides a powerful framework for decision-making, but its application in practice often requires a carefully designed reward function. Adversarial Imitation Learning (AIL) sheds light on automatic policy acquisition without access to the reward signal from the environment. In this work, we propose Auto-Encoding Adversarial Imitation Learning (AEAIL), a robust and scalable AIL framework. To induce expert policies from demonstrations, AEAIL utilizes the reconstruction error of an auto-encoder as a reward signal, which provides more information for optimizing policies than the prior discriminator-based ones. Subsequently, we use the derived objective functions to train the auto-encoder and the agent policy. Experiments show that our AEAIL performs superior compared to state-of-the-art methods on both state and image based environments. More importantly, AEAIL shows much better robustness when the expert demonstrations are noisy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#30340;&#32593;&#32476;&#32676;&#32452;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#25104;&#25945;&#24072;&#32452;&#21644;&#23398;&#29983;&#32452;&#65292;&#23398;&#29983;&#32452;&#23398;&#20064;&#25945;&#24072;&#30340;&#30693;&#35782;&#65292;&#20351;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#31574;&#30053;&#23558;&#23398;&#29983;&#32452;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#32593;&#32476;&#26187;&#21319;&#20026;&#25945;&#24072;&#32452;&#65292;&#36890;&#36807;&#35757;&#32451;&#25945;&#24072;&#32452;&#20197;&#25913;&#36827;&#20854;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.01186</link><description>&lt;p&gt;
ORC: &#21033;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#30340;&#32593;&#32476;&#32676;&#32452;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
ORC: Network Group-based Knowledge Distillation using Online Role Change. (arXiv:2206.01186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#30340;&#32593;&#32476;&#32676;&#32452;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#25104;&#25945;&#24072;&#32452;&#21644;&#23398;&#29983;&#32452;&#65292;&#23398;&#29983;&#32452;&#23398;&#20064;&#25945;&#24072;&#30340;&#30693;&#35782;&#65292;&#20351;&#29992;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#31574;&#30053;&#23558;&#23398;&#29983;&#32452;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#32593;&#32476;&#26187;&#21319;&#20026;&#25945;&#24072;&#32452;&#65292;&#36890;&#36807;&#35757;&#32451;&#25945;&#24072;&#32452;&#20197;&#25913;&#36827;&#20854;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#30001;&#20110;&#21333;&#19968;&#30340;&#20840;&#33021;&#25945;&#24072;&#32593;&#32476;&#26080;&#27861;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#65292;&#26368;&#36817;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#20010;&#25945;&#24072;&#30340;&#30693;&#35782;&#33976;&#39311;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#23427;&#20204;&#30340;&#25913;&#36827;&#25928;&#26524;&#24182;&#19981;&#22914;&#39044;&#26399;&#65292;&#22240;&#20026;&#19968;&#20123;&#19981;&#25104;&#29087;&#30340;&#25945;&#24072;&#21487;&#33021;&#20250;&#23558;&#38169;&#35823;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23398;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#24182;&#21033;&#29992;&#22810;&#20010;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#26412;&#25991;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#25104;&#25945;&#24072;&#32452;&#21644;&#23398;&#29983;&#32452;&#12290;&#23398;&#29983;&#32452;&#30001;&#38656;&#35201;&#23398;&#20064;&#25945;&#24072;&#30693;&#35782;&#30340;&#19981;&#25104;&#29087;&#32593;&#32476;&#32452;&#25104;&#65292;&#32780;&#25945;&#24072;&#32452;&#21017;&#30001;&#33021;&#22815;&#25104;&#21151;&#25945;&#25480;&#30340;&#36873;&#20013;&#32593;&#32476;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#35282;&#33394;&#21464;&#25442;&#31574;&#30053;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23558;&#23398;&#29983;&#32452;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#32593;&#32476;&#26187;&#21319;&#20026;&#25945;&#24072;&#32452;&#12290;&#36890;&#36807;&#20351;&#29992;&#23398;&#29983;&#32452;&#30340;&#38169;&#35823;&#26679;&#26412;&#35757;&#32451;&#25945;&#24072;&#32452;&#20197;&#25913;&#36827;&#20854;&#30693;&#35782;&#65292;&#25105;&#20204;&#36716;&#31227;&#21327;&#20316;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In knowledge distillation, since a single, omnipotent teacher network cannot solve all problems, multiple teacher-based knowledge distillations have been studied recently. However, sometimes their improvements are not as good as expected because some immature teachers may transfer the false knowledge to the student. In this paper, to overcome this limitation and take the efficacy of the multiple networks, we divide the multiple networks into teacher and student groups, respectively. That is, the student group is a set of immature networks that require learning the teacher's knowledge, while the teacher group consists of the selected networks that are capable of teaching successfully. We propose our online role change strategy where the top-ranked networks in the student group are able to promote to the teacher group at every iteration. After training the teacher group using the error samples of the student group to refine the teacher group's knowledge, we transfer the collaborative kno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#26041;&#27861;&#65292;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#65292;&#36890;&#36807;&#32467;&#21512;&#19977;&#32500;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#26041;&#27861;&#21644;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;SAR&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2204.01248</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Differentiable Rendering for Synthetic Aperture Radar Imagery. (arXiv:2204.01248v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#26041;&#27861;&#65292;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#65292;&#36890;&#36807;&#32467;&#21512;&#19977;&#32500;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#26041;&#27861;&#21644;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;SAR&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#21487;&#24494;&#20998;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#25104;&#20687;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31561;&#19968;&#38454;&#26041;&#27861;&#26469;&#26126;&#30830;&#22320;&#24314;&#27169;&#20960;&#20309;&#20808;&#39564;&#21644;&#32422;&#26463;&#65292;&#20197;&#20248;&#21270;&#27969;&#27700;&#32447;&#12290;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#20854;&#20013;&#21487;&#20197;&#35757;&#32451;&#20986;&#26356;&#31283;&#20581;&#19988;&#25968;&#25454;&#38656;&#27714;&#36739;&#23567;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#33021;&#22815;&#35299;&#20915;&#30149;&#24577;&#30340;&#36870;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#21487;&#24494;&#20998;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#30005;&#20809;&#20256;&#24863;&#22120;&#25104;&#20687;&#65292;&#29305;&#21035;&#26159;&#20256;&#32479;&#30340;RGB&#25104;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#26041;&#27861;&#19982;&#31070;&#32463;&#28210;&#26579;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;SAR&#25968;&#25454;&#65292;&#20197;&#26377;&#38480;&#30340;SAR&#25104;&#20687;&#26469;&#23454;&#29616;&#20102;&#23545;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#30340;&#36870;&#22270;&#24418;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is rising interest in differentiable rendering, which allows explicitly modeling geometric priors and constraints in optimization pipelines using first-order methods such as backpropagation. Incorporating such domain knowledge can lead to deep neural networks that are trained more robustly and with limited data, as well as the capability to solve ill-posed inverse problems. Existing efforts in differentiable rendering have focused on imagery from electro-optical sensors, particularly conventional RGB-imagery. In this work, we propose an approach for differentiable rendering of Synthetic Aperture Radar (SAR) imagery, which combines methods from 3D computer graphics with neural rendering. We demonstrate the approach on the inverse graphics problem of 3D Object Reconstruction from limited SAR imagery using high-fidelity simulated SAR data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20174;&#20013;&#31561;&#35268;&#27169;&#22270;&#36801;&#31227;&#21040;&#22823;&#35268;&#27169;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22270;&#26680;&#23494;&#24230;&#28388;&#27874;&#22120;&#21644;&#22270;&#26680;&#23494;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;WNNs&#65289;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#36924;&#36817;&#26041;&#27861;&#20197;&#38480;&#23450;&#36801;&#31227;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2112.04629</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20256;&#36882;&#24615;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferability Properties of Graph Neural Networks. (arXiv:2112.04629v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.04629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20174;&#20013;&#31561;&#35268;&#27169;&#22270;&#36801;&#31227;&#21040;&#22823;&#35268;&#27169;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22270;&#26680;&#23494;&#24230;&#28388;&#27874;&#22120;&#21644;&#22270;&#26680;&#23494;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;WNNs&#65289;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#36924;&#36817;&#26041;&#27861;&#20197;&#38480;&#23450;&#36801;&#31227;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#22270;&#21367;&#31215;&#21644;&#36880;&#28857;&#38750;&#32447;&#24615;&#23618;&#32452;&#25104;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#19981;&#21464;&#24615;&#21644;&#31283;&#23450;&#24615;&#29305;&#24615;&#65292;GNNs&#22312;&#23398;&#20064;&#22522;&#20110;&#20013;&#31561;&#35268;&#27169;&#22270;&#30340;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#23398;&#20064;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20013;&#31561;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;GNNs&#24182;&#23558;&#23427;&#20204;&#36801;&#31227;&#21040;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;&#22270;&#26680;&#23494;&#24230;&#30340;&#22270;&#38480;&#21046;&#26469;&#23450;&#20041;&#22270;&#28388;&#27874;&#22120;&#21644;GNNs&#30340;&#26497;&#38480;&#23545;&#35937; - &#22270;&#26680;&#23494;&#24230;&#28388;&#27874;&#22120;&#21644;&#22270;&#26680;&#23494;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;WNNs&#65289;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#35299;&#37322;&#20026;&#22270;&#28388;&#27874;&#22120;&#21644;GNNs&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#26680;&#23494;&#24230;&#28388;&#27874;&#22120;&#21644;WNNs&#21487;&#20197;&#36890;&#36807;&#20174;&#23427;&#20204;&#19978;&#36827;&#34892;&#37319;&#26679;&#30340;&#22270;&#28388;&#27874;&#22120;&#21644;GNNs&#22312;&#21152;&#26435;&#21644;&#38543;&#26426;&#22270;&#19978;&#36827;&#34892;&#36924;&#36817;&#12290;&#30001;&#20110;&#36825;&#20123;&#36924;&#36817;&#30340;&#35823;&#24046;&#21487;&#20197;&#24471;&#21040;&#19978;&#30028;&#65292;&#36890;&#36807;&#19977;&#35282;&#19981;&#31561;&#24335;&#30340;&#35770;&#35777;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#30028;&#23450;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#20256;&#36882;&#22270;&#28388;&#27874;&#22120;&#25110;GNN&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are composed of layers consisting of graph convolutions and pointwise nonlinearities. Due to their invariance and stability properties, GNNs are provably successful at learning representations from data supported on moderate-scale graphs. However, they are difficult to learn on large-scale graphs. In this paper, we study the problem of training GNNs on graphs of moderate size and transferring them to large-scale graphs. We use graph limits called graphons to define limit objects for graph filters and GNNs -- graphon filters and graphon neural networks (WNNs) -- which we interpret as generative models for graph filters and GNNs. We then show that graphon filters and WNNs can be approximated by graph filters and GNNs sampled from them on weighted and stochastic graphs. Because the error of these approximations can be upper bounded, by a triangle inequality argument we can further bound the error of transferring a graph filter or a GNN across graphs. Our resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2111.10275</link><description>&lt;p&gt;
&#24102;&#26377;&#26680;&#30340;&#22797;&#21512;&#36866;&#21512;&#24615;&#26816;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Composite Goodness-of-fit Tests with Kernels. (arXiv:2111.10275v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#21487;&#33021;&#20250;&#23545;&#27010;&#29575;&#27169;&#22411;&#30340;&#23454;&#29616;&#36896;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#24320;&#21457;&#20986;&#19968;&#20123;&#30452;&#25509;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#40065;&#26834;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26356;&#20026;&#22797;&#26434;&#30340;&#26041;&#27861;&#26159;&#21542;&#38656;&#35201;&#21462;&#20915;&#20110;&#27169;&#22411;&#26159;&#21542;&#30495;&#30340;&#38169;&#35823;&#65292;&#30446;&#21069;&#32570;&#20047;&#36890;&#29992;&#30340;&#26041;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#26159;&#21542;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26469;&#33258;&#26576;&#20123;&#21442;&#25968;&#27169;&#22411;&#26063;&#20013;&#30340;&#20219;&#20309;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#21033;&#29992;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#26680;Stein&#24046;&#24322;&#30340;&#26368;&#23567;&#36317;&#31163;&#20272;&#35745;&#22120;&#12290;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#24403;&#21442;&#25968;&#27169;&#22411;&#30340;&#23494;&#24230;&#24050;&#30693;&#38500;&#26631;&#20934;&#21270;&#24120;&#25968;&#22806;&#65292;&#25110;&#32773;&#22914;&#26524;&#27169;&#22411;&#37319;&#29992;&#27169;&#25311;&#22120;&#24418;&#24335;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31435;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#24322;&#24120;&#26816;&#27979;&#24212;&#29992;&#26696;&#20363;&#28436;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the param
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24052;&#25343;&#36203;&#31354;&#38388;&#20013;&#32447;&#24615;&#20989;&#25968;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;&#25972;&#20010;&#29702;&#35770;&#65292;&#21253;&#25324;&#34920;&#31034;&#23450;&#29702;&#12289;&#20266;&#36924;&#36817;&#23450;&#29702;&#21644;&#25910;&#25947;&#23450;&#29702;&#12290;&#27491;&#21017;&#21270;&#23398;&#20064;&#36890;&#36807;&#26368;&#23567;&#21270;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26469;&#36924;&#36817;&#26410;&#30693;&#25110;&#26410;&#26126;&#30830;&#30340;&#21407;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2109.03159</link><description>&lt;p&gt;
&#22312;&#24052;&#25343;&#36203;&#31354;&#38388;&#20013;&#23545;&#32447;&#24615;&#20989;&#25968;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Regularized Learning for Linear-functional Data in Banach Spaces. (arXiv:2109.03159v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24052;&#25343;&#36203;&#31354;&#38388;&#20013;&#32447;&#24615;&#20989;&#25968;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;&#25972;&#20010;&#29702;&#35770;&#65292;&#21253;&#25324;&#34920;&#31034;&#23450;&#29702;&#12289;&#20266;&#36924;&#36817;&#23450;&#29702;&#21644;&#25910;&#25947;&#23450;&#29702;&#12290;&#27491;&#21017;&#21270;&#23398;&#20064;&#36890;&#36807;&#26368;&#23567;&#21270;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26469;&#36924;&#36817;&#26410;&#30693;&#25110;&#26410;&#26126;&#30830;&#30340;&#21407;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24052;&#25343;&#36203;&#31354;&#38388;&#20013;&#32447;&#24615;&#20989;&#25968;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;&#25972;&#20010;&#29702;&#35770;&#65292;&#21253;&#25324;&#34920;&#31034;&#23450;&#29702;&#12289;&#20266;&#36924;&#36817;&#23450;&#29702;&#21644;&#25910;&#25947;&#23450;&#29702;&#12290;&#36755;&#20837;&#30340;&#35757;&#32451;&#25968;&#25454;&#30001;&#24052;&#25343;&#36203;&#31354;&#38388;&#30340;&#21069;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#20989;&#25968;&#32452;&#25104;&#65292;&#20197;&#34920;&#31034;&#22810;&#27169;&#22411;&#25968;&#25454;&#21644;&#22810;&#23610;&#24230;&#27169;&#22411;&#30340;&#31163;&#25955;&#23616;&#37096;&#20449;&#24687;&#12290;&#35757;&#32451;&#25968;&#25454;&#21644;&#22810;&#25439;&#22833;&#20989;&#25968;&#34987;&#29992;&#26469;&#35745;&#31639;&#32463;&#39564;&#39118;&#38505;&#20197;&#36924;&#36817;&#26399;&#26395;&#39118;&#38505;&#65292;&#32780;&#27491;&#21017;&#21270;&#23398;&#20064;&#21017;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#24052;&#25343;&#36203;&#31354;&#38388;&#19978;&#30340;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26469;&#23454;&#29616;&#30340;&#12290;&#21363;&#20351;&#21407;&#38382;&#39064;&#26410;&#30693;&#25110;&#26410;&#26126;&#30830;&#65292;&#27491;&#21017;&#21270;&#23398;&#20064;&#20063;&#21487;&#20197;&#20840;&#23616;&#36924;&#36817;&#21407;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#12290;&#22312;&#25910;&#25947;&#23450;&#29702;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24052;&#25343;&#36203;&#31354;&#38388;&#30340;&#24369;*&#25299;&#25169;&#35777;&#26126;&#20102;&#36817;&#20284;&#35299;&#25910;&#25947;&#20110;&#31934;&#30830;&#35299;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#23398;&#20064;&#23450;&#29702;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we study the whole theory of regularized learning for linear-functional data in Banach spaces including representer theorems, pseudo-approximation theorems, and convergence theorems. The input training data are composed of linear functionals in the predual space of the Banach space to represent the discrete local information of multimodel data and multiscale models. The training data and the multi-loss functions are used to compute the empirical risks to approximate the expected risks, and the regularized learning is to minimize the regularized empirical risks over the Banach spaces. The exact solutions of the original problems are approximated globally by the regularized learning even if the original problems are unknown or unformulated. In the convergence theorems, we show the convergence of the approximate solutions to the exact solutions by the weak* topology of the Banach space. Moreover, the theorems of the regularized learning are applied to solve many problems 
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20294;&#20854;&#36125;&#21494;&#26031;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#22312;&#26576;&#20123;&#37325;&#35201;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#29992;&#19988;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;&#65292;&#30456;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#20934;&#30830;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#20559;&#24046;&#26377;&#20248;&#38597;&#30340;&#36864;&#21270;&#12290;</title><link>http://arxiv.org/abs/2105.02796</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#23454;&#29992;&#19988;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Practical and Rigorous Uncertainty Bounds for Gaussian Process Regression. (arXiv:2105.02796v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.02796
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20294;&#20854;&#36125;&#21494;&#26031;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#22312;&#26576;&#20123;&#37325;&#35201;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#29992;&#19988;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;&#65292;&#30456;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#20934;&#30830;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#20559;&#24046;&#26377;&#20248;&#38597;&#30340;&#36864;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#20854;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20272;&#35745;&#26159;&#36125;&#21494;&#26031;&#24615;&#36136;&#30340;&#65292;&#23545;&#20110;&#19968;&#20123;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#22914;&#20855;&#26377;&#23433;&#20840;&#20445;&#35777;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#65292;&#38656;&#35201;&#39057;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;&#12290;&#23613;&#31649;&#23545;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;&#36825;&#26679;&#20005;&#26684;&#30340;&#30028;&#38480;&#26377;&#21487;&#29992;&#65292;&#20294;&#23427;&#20204;&#36807;&#20110;&#20445;&#23432;&#65292;&#22312;&#24212;&#29992;&#20013;&#27809;&#26377;&#29992;&#22788;&#12290;&#36825;&#36890;&#24120;&#23548;&#33268;&#23454;&#36341;&#32773;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26367;&#20195;&#36825;&#20123;&#30028;&#38480;&#65292;&#20174;&#32780;&#30772;&#22351;&#20102;&#25152;&#26377;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;&#65292;&#26082;&#20005;&#26684;&#21448;&#23454;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#30028;&#38480;&#21487;&#20197;&#26126;&#30830;&#22320;&#35780;&#20272;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#32467;&#26524;&#35201;&#20445;&#23432;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26576;&#20123;&#27169;&#22411;&#20559;&#24046;&#21482;&#20250;&#24341;&#36215;&#20248;&#38597;&#30340;&#36864;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20248;&#21183;&#20197;&#21450;&#25105;&#20204;&#32467;&#26524;&#22312;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Regression is a popular nonparametric regression method based on Bayesian principles that provides uncertainty estimates for its predictions. However, these estimates are of a Bayesian nature, whereas for some important applications, like learning-based control with safety guarantees, frequentist uncertainty bounds are required. Although such rigorous bounds are available for Gaussian Processes, they are too conservative to be useful in applications. This often leads practitioners to replacing these bounds by heuristics, thus breaking all theoretical guarantees. To address this problem, we introduce new uncertainty bounds that are rigorous, yet practically useful at the same time. In particular, the bounds can be explicitly evaluated and are much less conservative than state of the art results. Furthermore, we show that certain model misspecifications lead to only graceful degradation. We demonstrate these advantages and the usefulness of our results for learning-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#31934;&#30830;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#21457;&#29616;&#22686;&#24378;&#20256;&#28909;&#30340;&#23618;&#27969;&#27969;&#36947;&#22721;&#20462;&#39280;&#12290;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#20934;&#30830;&#35780;&#20272;&#22721;&#38754;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22721;&#38754;&#32467;&#26500;&#31579;&#36873;&#12290;</title><link>http://arxiv.org/abs/2101.08130</link><description>&lt;p&gt;
&#29992;&#20110;&#24555;&#36895;&#21457;&#29616;&#22686;&#24378;&#20256;&#28909;&#30340;&#23618;&#27969;&#27969;&#36947;&#22721;&#20462;&#39280;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning for rapid discovery of laminar flow channel wall modifications that enhance heat transfer. (arXiv:2101.08130v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.08130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#31934;&#30830;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#21457;&#29616;&#22686;&#24378;&#20256;&#28909;&#30340;&#23618;&#27969;&#27969;&#36947;&#22721;&#20462;&#39280;&#12290;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#20934;&#30830;&#35780;&#20272;&#22721;&#38754;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22721;&#38754;&#32467;&#26500;&#31579;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#20307;&#30340;&#25968;&#20540;&#27169;&#25311;&#22312;&#24314;&#27169;&#35768;&#22810;&#29289;&#29702;&#29616;&#35937;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20419;&#36827;&#20102;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#26377;&#21161;&#20110;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#23454;&#36341;&#65292;&#24182;&#25193;&#22823;&#20102;&#25105;&#20204;&#23545;&#21508;&#31181;&#33258;&#28982;&#21644;&#24037;&#31243;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;&#22312;&#31616;&#21333;&#24179;&#26495;&#27969;&#36947;&#20013;&#30340;&#27969;&#20307;&#20256;&#28909;&#35745;&#31639;&#23545;&#20110;&#21508;&#31181;&#27169;&#25311;&#26041;&#27861;&#26469;&#35828;&#26159;&#30456;&#23545;&#23481;&#26131;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#27969;&#36947;&#20960;&#20309;&#24418;&#29366;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#25968;&#20540;&#27169;&#25311;&#23601;&#25104;&#20026;&#20248;&#21270;&#22721;&#38754;&#20960;&#20309;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#25968;&#20540;&#27169;&#25311;&#20219;&#24847;&#24179;&#26495;&#21644;&#38750;&#24179;&#26495;&#27969;&#36947;&#20197;&#21450;&#39044;&#27979;&#38459;&#21147;&#31995;&#25968;&#21644;&#26031;&#22374;&#39039;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#24615;&#36136;&#65292;&#19988;&#25152;&#38656;&#26102;&#38388;&#20165;&#26159;&#25968;&#20540;&#27169;&#25311;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123; CNN &#27169;&#22411;&#29992;&#20110;&#34394;&#25311;&#39640;&#36890;&#37327;&#31579;&#36873;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#22823;&#37327;&#21487;&#33021;&#30340;&#38543;&#26426;&#29983;&#25104;&#30340;&#22721;&#38754;&#32467;&#26500;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#25216;&#26415;...
&lt;/p&gt;
&lt;p&gt;
Numerical simulation of fluids plays an essential role in modeling many physical phenomena, which enables technological advancements, contributes to sustainable practices, and expands our understanding of various natural and engineered systems. The calculation of heat transfer in fluid flow in simple flat channels is a relatively easy task for various simulation methods. However, once the channel geometry becomes more complex, numerical simulations become a bottleneck in optimizing wall geometries. We present a combination of accurate numerical simulations of arbitrary, flat, and non-flat channels and machine learning models predicting drag coefficient and Stanton number. We show that convolutional neural networks (CNN) can accurately predict the target properties at a fraction of the time of numerical simulations. We use the CNN models in a virtual high-throughput screening approach to explore a large number of possible, randomly generated wall architectures. Data Augmentation was app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#30340;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#20505;&#36873;&#29238;&#33410;&#28857;&#38598;&#21512;&#30340;&#30830;&#23450;&#21644;&#20998;&#35299;&#65292;&#20197;&#21450;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#33021;&#22815;&#22312;&#27604;&#29305;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#35299;&#20915;&#22522;&#20110;&#35780;&#20998;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2006.06926</link><description>&lt;p&gt;
&#29992;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Bayesian Networks with Annealing Machine. (arXiv:2006.06926v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#30340;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#20505;&#36873;&#29238;&#33410;&#28857;&#38598;&#21512;&#30340;&#30830;&#23450;&#21644;&#20998;&#35299;&#65292;&#20197;&#21450;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#33021;&#22815;&#22312;&#27604;&#29305;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#35299;&#20915;&#22522;&#20110;&#35780;&#20998;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#26377;&#28508;&#21147;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#30340;&#27604;&#29305;&#23481;&#37327;&#30446;&#21069;&#26377;&#38480;&#12290;&#20026;&#20102;&#21033;&#29992;&#27169;&#25311;&#36864;&#28779;&#25216;&#26415;&#65292;&#38656;&#35201;&#23558;&#22522;&#20110;&#35780;&#20998;&#30340;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#27604;&#29305;&#23481;&#37327;&#20869;&#30340;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#20803;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36716;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#20505;&#36873;&#29238;&#33410;&#28857;&#38598;&#21512;&#30340;&#30830;&#23450;&#21644;&#20854;&#20998;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#20197;&#25214;&#21040;&#26368;&#23567;&#21270;&#25152;&#38656;&#27604;&#29305;&#25968;&#30340;&#20998;&#35299;&#12290;&#22312;&#21253;&#21547;&#21464;&#37327;&#20174;75&#21040;223&#30340;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#27604;&#29305;&#25968;&#27604;&#22235;&#20195;&#23500;&#22763;&#36890;&#25968;&#23383;&#36864;&#28779;&#22120;&#65288;&#19968;&#31181;&#37319;&#29992;&#21322;&#23548;&#20307;&#25216;&#26415;&#24320;&#21457;&#30340;&#20840;&#32806;&#21512;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#65289;&#30340;100K&#27604;&#29305;&#23481;&#37327;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have reported that annealing machines are capable of solving combinatorial optimization problems with high accuracy. Annealing machines can potentially be applied to score-based Bayesian network structure learning. However, the bit capacity of an annealing machine is currently limited. To utilize the annealing technology, converting score-based learning problems into quadratic unconstrained binary optimizations within the bit capacity is necessary. In this paper, we propose an efficient conversion method with the advanced identification of candidate parent sets and their decomposition. We also provide an integer programming problem to find the decomposition that minimizes the number of required bits. Experimental results on $7$ benchmark datasets with variables from $75$ to $223$ show that our approach requires less bits than the $100$K bit capacity of the fourth-generation Fujitsu Digital Annealer, a fully coupled annealing machine developed with semiconductor technolog
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21028;&#21035;&#22120;&#20248;&#21270;&#36807;&#31243;&#22914;&#20309;&#22686;&#21152;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;Wasserstein&#36317;&#31163;&#30340;&#23545;&#20598;&#20195;&#20215;&#20989;&#25968;&#30340;&#19979;&#38480;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#22909;&#30340;&#21028;&#21035;&#22120;&#33021;&#22815;&#36817;&#20284;&#26368;&#20248;&#36755;&#36816;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#21035;&#22120;&#26368;&#20248;&#36755;&#36816;&#65288;DOT&#65289;&#26041;&#26696;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/1910.06832</link><description>&lt;p&gt;
&#21028;&#21035;&#22120;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Discriminator optimal transport. (arXiv:1910.06832v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.06832
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21028;&#21035;&#22120;&#20248;&#21270;&#36807;&#31243;&#22914;&#20309;&#22686;&#21152;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;Wasserstein&#36317;&#31163;&#30340;&#23545;&#20598;&#20195;&#20215;&#20989;&#25968;&#30340;&#19979;&#38480;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#22909;&#30340;&#21028;&#21035;&#22120;&#33021;&#22815;&#36817;&#20284;&#26368;&#20248;&#36755;&#36816;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#21035;&#22120;&#26368;&#20248;&#36755;&#36816;&#65288;DOT&#65289;&#26041;&#26696;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21028;&#21035;&#22120;&#20248;&#21270;&#36807;&#31243;&#22686;&#21152;&#20102;&#30446;&#26631;&#20998;&#24067;$p$&#21644;&#29983;&#25104;&#22120;&#20998;&#24067;$p_G$&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#23545;&#20598;&#20195;&#20215;&#20989;&#25968;&#30340;&#19979;&#38480;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#22909;&#30340;&#21028;&#21035;&#22120;&#21487;&#20197;&#36817;&#20284;&#20174;$p_G$&#21040;$p$&#30340;&#26368;&#20248;&#36755;&#36816;&#12290;&#22522;&#20110;&#19968;&#20123;&#23454;&#39564;&#21644;&#19968;&#28857;&#36755;&#36816;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#21035;&#22120;&#26368;&#20248;&#36755;&#36816;&#65288;DOT&#65289;&#26041;&#26696;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;CIFAR-10&#12289;STL-10&#21644;&#19968;&#20010;&#20197;ImageNet&#20026;&#26465;&#20214;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;GAN&#35745;&#31639;&#30340;&#20869;&#28085;&#20998;&#25968;&#21644;FID&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within a broad class of generative adversarial networks, we show that discriminator optimization process increases a lower bound of the dual cost function for the Wasserstein distance between the target distribution $p$ and the generator distribution $p_G$. It implies that the trained discriminator can approximate optimal transport (OT) from $p_G$ to $p$.Based on some experiments and a bit of OT theory, we propose a discriminator optimal transport (DOT) scheme to improve generated images. We show that it improves inception score and FID calculated by un-conditional GAN trained by CIFAR-10, STL-10 and a public pre-trained model of conditional GAN by ImageNet.
&lt;/p&gt;</description></item></channel></rss>