<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;CDGraph&#29992;&#20110;&#21512;&#25104;&#31038;&#20132;&#22270;&#65292;&#36890;&#36807;&#25429;&#25417;&#21452;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21644;&#20445;&#25345;&#33410;&#28857;&#36830;&#36890;&#24615;&#26469;&#23454;&#29616;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31038;&#20132;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2311.01729</link><description>&lt;p&gt;
CDGraph&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21452;&#26465;&#20214;&#31038;&#20132;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model. (arXiv:2311.01729v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;CDGraph&#29992;&#20110;&#21512;&#25104;&#31038;&#20132;&#22270;&#65292;&#36890;&#36807;&#25429;&#25417;&#21452;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21644;&#20445;&#25345;&#33410;&#28857;&#36830;&#36890;&#24615;&#26469;&#23454;&#29616;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31038;&#20132;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#30340;&#31038;&#20132;&#22270;&#22240;&#25968;&#25454;&#31232;&#32570;&#21644;&#29992;&#25143;&#38544;&#31169;&#30340;&#25285;&#24551;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#38656;&#27714;&#12290;&#29983;&#25104;&#31038;&#20132;&#32593;&#32476;&#30340;&#20851;&#38190;&#24615;&#33021;&#26631;&#20934;&#20043;&#19968;&#26159;&#23545;&#29305;&#23450;&#26465;&#20214;&#30340;&#24544;&#23454;&#24230;&#65292;&#20363;&#22914;&#20855;&#26377;&#29305;&#23450;&#20250;&#21592;&#36164;&#26684;&#21644;&#36130;&#21153;&#29366;&#20917;&#30340;&#29992;&#25143;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#26465;&#20214;&#31038;&#20132;&#22270;&#30340;&#21512;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;CDGraph&#65292;&#23427;&#22522;&#20110;&#20004;&#20010;&#25351;&#23450;&#26465;&#20214;&#26469;&#35757;&#32451;&#21644;&#21512;&#25104;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CDGraph&#30340;&#21435;&#22122;&#36807;&#31243;&#20013;&#30340;&#20849;&#21516;&#28436;&#21270;&#20381;&#36182;&#24615;&#65292;&#20197;&#25429;&#25417;&#21452;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36827;&#19968;&#27493;&#32467;&#21512;&#31038;&#20250;&#21516;&#36136;&#24615;&#21644;&#31038;&#20250;&#20256;&#26579;&#21147;&#20197;&#20445;&#25345;&#33410;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#25351;&#23450;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The social graphs synthesized by the generative models are increasingly in demand due to data scarcity and concerns over user privacy. One of the key performance criteria for generating social networks is the fidelity to specified conditionals, such as users with certain membership and financial status. While recent diffusion models have shown remarkable performance in generating images, their effectiveness in synthesizing graphs has not yet been explored in the context of conditional social graphs. In this paper, we propose the first kind of conditional diffusion model for social networks, CDGraph, which trains and synthesizes graphs based on two specified conditions. We propose the co-evolution dependency in the denoising process of CDGraph to capture the mutual dependencies between the dual conditions and further incorporate social homophily and social contagion to preserve the connectivity between nodes while satisfying the specified conditions. Moreover, we introduce a novel class
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.01378</link><description>&lt;p&gt;
Vision-Language Foundation Models&#20316;&#20026;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#23427;&#20204;&#29702;&#35299;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#35270;&#35273;&#35821;&#35328;&#25805;&#20316;&#26694;&#26550;&#65292;&#21517;&#20026;RoboFlamingo&#65292;&#23427;&#24314;&#31435;&#22312;&#24320;&#28304;&#30340;VLMs&#65292;OpenFlamingo&#20043;&#19978;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;RoboFlamingo&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VLMs&#36827;&#34892;&#21333;&#27493;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#20351;&#29992;&#26174;&#24335;&#31574;&#30053;&#22836;&#27169;&#25311;&#39034;&#24207;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#21482;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#20998;&#35299;&#20026;RoboFlamingo&#25552;&#20379;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#36827;&#34892;&#24320;&#29615;&#25511;&#21046;&#21644;&#37096;&#32626;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#22312;&#27979;&#35797;&#22522;&#20934;&#19978;&#22823;&#24133;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RoboFlamingo&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
&lt;/p&gt;</description></item><item><title>TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.01301</link><description>&lt;p&gt;
TRIALSCOPE&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01301
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#30340;&#24555;&#36895;&#25968;&#23383;&#21270;&#20026;&#20248;&#21270;&#21307;&#30103;&#26381;&#21153;&#21644;&#21152;&#36895;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#23384;&#22312;&#65292;&#22914;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#31508;&#35760;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRIALSCOPE&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#32676;&#32423;&#35266;&#23519;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;TRIALSCOPE&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#35268;&#27169;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#24120;&#35265;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#21033;&#29992;&#20020;&#24202;&#35797;&#39564;&#35268;&#33539;&#20316;&#20026;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;TRIALSCOPE&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#38190;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#29983;&#25104;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#30284;&#30151;&#24739;&#32773;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20316;&#29289;&#30149;&#23475;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#32511;&#33394;&#33394;&#24230;&#22352;&#26631;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#20132;&#20114;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#26368;&#20339;&#30340;&#20316;&#29289;&#31181;&#26893;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2311.00429</link><description>&lt;p&gt;
&#20351;&#29992;&#32511;&#33394;&#33394;&#24230;&#22352;&#26631;&#65288;GCC&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20316;&#29289;&#30149;&#23475;&#20998;&#31867;&#65292;&#29992;&#20110;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26234;&#33021;&#20892;&#19994;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications. (arXiv:2311.00429v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20316;&#29289;&#30149;&#23475;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#32511;&#33394;&#33394;&#24230;&#22352;&#26631;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#20132;&#20114;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#26368;&#20339;&#30340;&#20316;&#29289;&#31181;&#26893;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20154;&#31867;&#33719;&#21462;&#33021;&#37327;&#12289;&#33829;&#20859;&#21644;&#33647;&#29992;&#20215;&#20540;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#26893;&#29289;&#30149;&#23475;&#21487;&#20197;&#22312;&#20892;&#19994;&#26685;&#22521;&#36807;&#31243;&#20013;&#23545;&#21494;&#29255;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#20316;&#29289;&#20135;&#37327;&#21644;&#32463;&#27982;&#20215;&#20540;&#30340;&#26174;&#33879;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#20892;&#27665;&#33021;&#22815;&#35782;&#21035;&#20316;&#29289;&#30149;&#23475;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36763;&#21220;&#24037;&#20316;&#12289;&#22823;&#37327;&#35745;&#21010;&#21644;&#23545;&#26893;&#29289;&#30149;&#21407;&#20307;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#37492;&#20110;&#36825;&#20123;&#38556;&#30861;&#65292;&#25552;&#20379;&#33021;&#22815;&#36731;&#26494;&#19982;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#20132;&#20114;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20892;&#27665;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#21644;&#30740;&#31350;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#29992;&#20110;&#26893;&#29289;&#30149;&#23475;&#26816;&#27979;&#30340;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#37325;&#35201;&#19988;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#12289;RGB&#21644;GCC&#12290;
&lt;/p&gt;
&lt;p&gt;
Crops hold paramount significance as they serve as the primary provider of energy, nutrition, and medicinal benefits for the human population. Plant diseases, however, can negatively affect leaves during agricultural cultivation, resulting in significant losses in crop output and economic value. Therefore, it is crucial for farmers to identify crop diseases. However, this method frequently necessitates hard work, a lot of planning, and in-depth familiarity with plant pathogens. Given these numerous obstacles, it is essential to provide solutions that can easily interface with mobile and IoT devices so that our farmers can guarantee the best possible crop development. Various machine learning (ML) as well as deep learning (DL) algorithms have been created &amp; studied for the identification of plant disease detection, yielding substantial and promising results. This article presents a novel classification method that builds on prior work by utilising attention-based feature extraction, RGB
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;robust Bayesian Optimization&#31639;&#27861;&#65292;AIRBO&#65292;&#23427;&#33021;&#22815;&#22312;&#20219;&#24847;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#26377;&#25928;&#35782;&#21035;&#20986;&#34920;&#29616;&#19968;&#33268;&#33391;&#22909;&#30340;&#40065;&#26834;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.20145</link><description>&lt;p&gt;
&#39640;&#25928;robust Bayesian Optimization&#23545;&#20110;&#20219;&#24847;&#19981;&#30830;&#23450;&#36755;&#20837;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs. (arXiv:2310.20145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;robust Bayesian Optimization&#31639;&#27861;&#65292;AIRBO&#65292;&#23427;&#33021;&#22815;&#22312;&#20219;&#24847;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#26377;&#25928;&#35782;&#21035;&#20986;&#34920;&#29616;&#19968;&#33268;&#33391;&#22909;&#30340;&#40065;&#26834;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) &#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;BO&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#38543;&#26426;&#24615;&#65292;&#22914;&#21152;&#24037;&#35823;&#24046;&#12289;&#25191;&#34892;&#22122;&#22768;&#25110;&#19978;&#19979;&#25991;&#21464;&#24322;&#65292;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#20250;&#20986;&#29616;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#20250;&#20351;&#36755;&#20837;&#22312;&#35780;&#20272;&#20043;&#21069;&#20559;&#31163;&#39044;&#26399;&#20540;&#65292;&#23548;&#33268;&#26368;&#32456;&#32467;&#26524;&#30340;&#24615;&#33021;&#27874;&#21160;&#36739;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;robust Bayesian Optimization&#31639;&#27861;&#65292;AIRBO&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#35782;&#21035;&#22312;&#20219;&#24847;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#19979;&#34920;&#29616;&#19968;&#33268;&#33391;&#22909;&#30340;&#40065;&#26834;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;(MMD)&#36171;&#33021;&#39640;&#26031;&#36807;&#31243;&#65292;&#30452;&#25509;&#24314;&#27169;&#20219;&#24847;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;Nystrom&#36924;&#36817;&#21152;&#36895;&#21518;&#39564;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;MMD&#20272;&#35745;&#35823;&#24046;&#19979;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21512;&#25104;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic function
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;&#65292;&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#21487;&#20197;&#25913;&#21892;&#36716;&#35786;&#65292;&#20026;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;&#30149;&#24773;&#24694;&#21270;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.19967</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#20197;&#25913;&#21892;&#36716;&#35786;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records. (arXiv:2310.19967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;&#65292;&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#21487;&#20197;&#25913;&#21892;&#36716;&#35786;&#65292;&#20026;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;&#30149;&#24773;&#24694;&#21270;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#65288;IA&#65289;&#23545;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#21307;&#38498;&#36716;&#35786;&#20998;&#27969;&#20197;&#21450;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;IA&#30142;&#30149;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#38480;&#30340;&#21307;&#30103;&#36164;&#28304;&#19979;&#12290;&#25163;&#21160;&#35780;&#20272;&#27969;&#31243;&#26159;&#30446;&#21069;&#23454;&#36341;&#20013;&#26368;&#24120;&#35265;&#30340;&#26089;&#26399;&#26816;&#27979;IA&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#21644;&#20302;&#25928;&#29575;&#12290;&#27599;&#20010;&#20174;&#19968;&#33324;&#23454;&#36341;&#65288;GP&#65289;&#21040;&#21307;&#38498;&#30340;&#36716;&#35786;&#37117;&#38656;&#35201;&#35780;&#20272;&#22823;&#37327;&#30340;&#20020;&#24202;&#20449;&#24687;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#37325;&#22797;&#35780;&#20272;&#20219;&#21153;&#21644;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;IA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IA&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#34880;&#28082;&#27979;&#35797;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#34880;&#28082;&#27979;&#35797;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#22312;&#36716;&#35786;&#26102;&#21487;&#29992;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;IA&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34701;&#21512;&#21644;
&lt;/p&gt;
&lt;p&gt;
Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24310;&#36831;&#21453;&#39304;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18919</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#30340;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation. (arXiv:2310.18919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24310;&#36831;&#21453;&#39304;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#20989;&#25968;&#36924;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#39640;&#25928;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21363;&#26102;&#21453;&#39304;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#25361;&#25112;&#65292;&#39318;&#20808;&#20171;&#32461;&#20102;Delayed-PSVI&#31639;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#20013;&#30340;&#22122;&#22768;&#25200;&#21160;&#26377;&#25928;&#22320;&#25506;&#32034;&#20215;&#20540;&#20989;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24310;&#36831;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#20013;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#30340;&#39318;&#27425;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#24773;&#20917;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $
&lt;/p&gt;</description></item><item><title>ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18208</link><description>&lt;p&gt;
ArcheType&#65306;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18208
&lt;/p&gt;
&lt;p&gt;
ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35821;&#20041;&#21015;&#31867;&#22411;&#27880;&#37322;&#65288;CTA&#65289;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#26102;&#22266;&#23450;&#30340;&#35821;&#20041;&#31867;&#22411;&#65307;&#38656;&#35201;&#22823;&#37327;&#30340;&#27599;&#20010;&#31867;&#22411;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#20135;&#29983;&#22823;&#37327;&#36816;&#34892;&#26102;&#25512;&#26029;&#25104;&#26412;&#65307;&#21363;&#20351;&#31867;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#21487;&#33021;&#22312;&#35780;&#20272;&#26032;&#25968;&#25454;&#38598;&#26102;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;CTA&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ArcheType&#65292;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#37319;&#26679;&#12289;&#25552;&#31034;&#24207;&#21015;&#21270;&#12289;&#27169;&#22411;&#26597;&#35810;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#20840;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#35299;&#20915;&#21015;&#31867;&#22411;&#27880;&#37322;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#21035;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20986;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#25552;&#20379;&#20102;&#26368;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.14421</link><description>&lt;p&gt;
&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#65292;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#65288;&#23616;&#37096;&#65289;&#21807;&#19968;&#21487;&#36870;&#20998;&#31867;&#22120;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#21644;&#29109;AI&#65288;EAI&#65289;&#20855;&#26377;&#26368;&#23567;&#23545;&#25239;&#36335;&#24452;&#65288;MAP&#65289;&#21644;&#26368;&#23567;&#23545;&#25239;&#36317;&#31163;&#65288;MAD&#65289;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#26126;&#30830;&#30340;&#20998;&#26512;&#35745;&#31639;&#30340;&#31616;&#21333;&#21487;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#12290;&#22312;&#24120;&#35265;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#12289;&#25552;&#21319;&#38543;&#26426;&#26862;&#26519;&#12289;GLM&#21644;EAI&#31561;&#21508;&#31867;AI&#24037;&#20855;&#36827;&#34892;MAP&#21644;MAD&#30340;&#23454;&#38469;&#35745;&#31639;&#12289;&#27604;&#36739;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#21452;&#21367;&#29366;&#34746;&#26059;&#32447;&#21450;&#20854;&#25193;&#23637;&#20197;&#21450;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38382;&#39064;&#65288;&#29992;&#20110;&#20581;&#24247;&#20445;&#38505;&#29702;&#36180;&#39044;&#27979;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#33268;&#27515;&#29575;&#20998;&#31867;&#65289;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#23637;&#31034;&#20102;MAP&#22914;&#20309;&#22312;&#39044;&#23450;&#20041;&#30340;&#21487;&#35775;&#38382;&#25511;&#21046;&#21464;&#37327;&#23376;&#38598;&#20013;&#25552;&#20379;&#21807;&#19968;&#30340;&#26368;&#23567;&#24739;&#32773;&#29305;&#23450;&#39118;&#38505;&#32531;&#35299;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14017</link><description>&lt;p&gt;
&#20840;&#38754;&#23545;&#27604;&#65306;&#38754;&#21521;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series. (arXiv:2310.14017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20943;&#23569;&#20102;&#23545;&#21171;&#21160;&#23494;&#38598;&#12289;&#39046;&#22495;&#29305;&#23450;&#21644;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#25968;&#25454;&#23618;&#38754;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COMET&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20013;&#25152;&#26377;&#20869;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22411;&#31995;&#32479;&#22320;&#25429;&#25417;&#20102;&#26469;&#33258;&#22235;&#20010;&#28508;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#65306;&#35266;&#27979;&#12289;&#26679;&#26412;&#12289;&#35797;&#39564;&#21644;&#24739;&#32773;&#23618;&#32423;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#21040;&#20445;&#25345;&#20840;&#38754;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#23454;&#29616;&#33258;&#30417;&#30563;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#26368;&#22823;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#31435;&#24739;&#32773;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#23558;COMET&#19982;&#20845;&#20010;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>VQ-NeRF&#26159;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#35299;&#21644;&#32534;&#36753;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#26448;&#26009;&#31163;&#25955;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#22122;&#22768;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#21270;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2310.11864</link><description>&lt;p&gt;
VQ-NeRF: &#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#21453;&#23556;&#20998;&#35299;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11864
&lt;/p&gt;
&lt;p&gt;
VQ-NeRF&#26159;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#35299;&#21644;&#32534;&#36753;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#26448;&#26009;&#31163;&#25955;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#22122;&#22768;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#21270;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VQ-NeRF&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#21452;&#20998;&#25903;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#36827;&#34892;&#20998;&#35299;&#21644;&#32534;&#36753;&#12290;&#20256;&#32479;&#30340;&#31070;&#32463;&#21453;&#23556;&#22330;&#20165;&#20351;&#29992;&#36830;&#32493;&#34920;&#31034;&#26469;&#24314;&#27169;3D&#22330;&#26223;&#65292;&#23613;&#31649;&#29616;&#23454;&#20013;&#30340;&#29289;&#20307;&#36890;&#24120;&#30001;&#31163;&#25955;&#26448;&#26009;&#32452;&#25104;&#12290;&#36825;&#31181;&#32570;&#20047;&#31163;&#25955;&#21270;&#21487;&#33021;&#23548;&#33268;&#26448;&#26009;&#20998;&#35299;&#22122;&#22768;&#21644;&#22797;&#26434;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#36830;&#32493;&#20998;&#25903;&#21644;&#19968;&#20010;&#31163;&#25955;&#20998;&#25903;&#12290;&#36830;&#32493;&#20998;&#25903;&#25353;&#29031;&#20256;&#32479;&#27969;&#31243;&#39044;&#27979;&#20998;&#35299;&#30340;&#26448;&#26009;&#65292;&#32780;&#31163;&#25955;&#20998;&#25903;&#20351;&#29992;VQ&#26426;&#21046;&#23558;&#36830;&#32493;&#26448;&#26009;&#37327;&#21270;&#20026;&#21333;&#29420;&#30340;&#26448;&#26009;&#12290;&#36890;&#36807;&#31163;&#25955;&#21270;&#26448;&#26009;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#65292;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#12290;&#21487;&#20197;&#36890;&#36807;&#28857;&#20987;&#20998;&#21106;&#32467;&#26524;&#30340;&#30456;&#24212;&#21306;&#22495;&#26469;&#36731;&#26494;&#36873;&#25321;&#29305;&#23450;&#26448;&#26009;&#36827;&#34892;&#36827;&#19968;&#27493;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Ad
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#38750;&#21442;&#25968;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;k&#26368;&#36817;&#37051;&#30340;CMI&#20272;&#35745;&#22120;&#21644;&#26412;&#22320;&#32622;&#25442;&#26041;&#26696;&#65292;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#21253;&#21547;&#25968;&#20540;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.11132</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#30340;&#28151;&#21512;&#36830;&#32493;-&#20998;&#31867;&#21464;&#37327;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65306;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#25968;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation. (arXiv:2310.11132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#38750;&#21442;&#25968;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;k&#26368;&#36817;&#37051;&#30340;CMI&#20272;&#35745;&#22120;&#21644;&#26412;&#22320;&#32622;&#25442;&#26041;&#26696;&#65292;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#21253;&#21547;&#25968;&#20540;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;CIT&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#21464;&#37327;&#36873;&#25321;&#65292;&#20063;&#26159;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22823;&#22810;&#25968;&#24403;&#19979;&#30340;CIT&#26041;&#27861;&#20551;&#35774;&#25152;&#26377;&#21464;&#37327;&#37117;&#26159;&#25968;&#20540;&#25110;&#25152;&#26377;&#21464;&#37327;&#37117;&#26159;&#20998;&#31867;&#30340;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#28041;&#21450;&#21253;&#21547;&#25968;&#20540;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#12290;&#38750;&#21442;&#25968;&#30340;CIT&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#20272;&#35745;&#22120;&#32467;&#21512;&#26412;&#22320;&#32622;&#25442;&#26041;&#26696;&#36827;&#34892;&#12290;&#26368;&#36817;&#65292;&#24050;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#30340;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;CMI&#20272;&#35745;&#22120;&#12290;&#19982;&#20219;&#20309;k-NN&#26041;&#27861;&#19968;&#26679;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#20381;&#36182;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#23450;&#20041;&#12290;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#23545;&#20998;&#31867;&#21464;&#37327;&#36827;&#34892;one-hot&#32534;&#30721;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#20174;&#32780;&#23558;&#20998;&#31867;&#21464;&#37327;&#35270;&#20026;&#31163;&#25955;&#25968;&#20540;&#21464;&#37327;&#65292;&#32780;&#21478;&#19968;&#31181;&#26041;&#27861;&#21017;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#21464;&#37327;&#20316;&#20026;&#26465;&#20214;&#26469;&#36890;&#36807;&#29109;&#26469;&#34920;&#31034;CMI&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#30340;&#25216;&#26415;&#25351;&#26631;&#32452;&#21512;&#26469;&#23454;&#29616;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.09903</link><description>&lt;p&gt;
&#35780;&#20272;&#29305;&#24449;&#36873;&#25321;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction. (arXiv:2310.09903v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#30340;&#25216;&#26415;&#25351;&#26631;&#32452;&#21512;&#26469;&#23454;&#29616;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25216;&#26415;&#25351;&#26631;&#23545;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#29305;&#24449;&#36873;&#25321;&#23545;&#36873;&#25321;&#26368;&#20339;&#25351;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#32771;&#34385;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#27169;&#22411;&#24615;&#33021;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26159;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#37492;&#23450;&#20986;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#32929;&#24066;&#20215;&#26684;&#30340;&#26368;&#20339;&#32929;&#24066;&#25351;&#26631;&#32452;&#21512;&#12290;&#20026;&#35780;&#20272;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#23545;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#22312;&#36807;&#21435;10&#24180;&#33529;&#26524;&#20844;&#21496;&#30340;&#25968;&#25454;&#19978;&#20351;&#29992;&#20102;10&#20010;&#35780;&#20272;&#22120;&#21644;123&#20010;&#25216;&#26415;&#25351;&#26631;&#36827;&#34892;&#20102;SFS&#21644;SBS&#30340;&#32771;&#23519;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23558;&#30001;3&#22825;&#26102;&#38388;&#31383;&#21475;&#21019;&#24314;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#29992;&#20110;&#22238;&#24402;&#26041;&#27861;&#30340;&#36755;&#20837;&#12290;&#20174;&#35266;&#23519;&#32467;&#26524;&#21487;&#20197;&#24471;&#20986;&#65306;&#65288;1&#65289;&#27599;&#31181;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the influence of many factors, including technical indicators on stock market prediction, feature selection is important to choose the best indicators. One of the feature selection methods that consider the performance of models during feature selection is the wrapper feature selection method. The aim of this research is to identify a combination of the best stock market indicators through feature selection to predict the stock market price with the least error. In order to evaluate the impact of wrapper feature selection techniques on stock market prediction, in this paper SFS and SBS with 10 estimators and 123 technical indicators have been examined on the last 10 years of Apple Company. Also, by the proposed method, the data created by the 3-day time window were converted to the appropriate input for regression methods. Based on the results observed: (1) Each wrapper feature selection method has different results with different machine learning methods, and each method is mor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08812</link><description>&lt;p&gt;
&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model. (arXiv:2310.08812v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27169;&#24577;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#24182;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20869;&#22312;&#27169;&#24577;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#26263;&#21547;&#27874;&#21160;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#24403;&#21069;&#12289;&#24555;&#36895;&#28436;&#21464;&#21644;&#27874;&#21160;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;-&#38598;&#25104;&#33539;&#24335;&#65292;&#21363;VMD-LSTM-GARCH&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#31639;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#20026;K&#20010;&#23376;&#27169;&#24577;&#12290;&#38543;&#21518;&#65292;GARCH&#27169;&#22411;&#20174;&#36825;&#20123;&#23376;&#27169;&#24577;&#20013;&#25552;&#21462;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;LSTM&#30340;&#36755;&#20837;&#12290;&#27599;&#20010;&#23376;&#27169;&#24577;&#30340;&#25968;&#20540;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#39044;&#27979;&#23376;&#27169;&#24577;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#25152;&#26377;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting represents a significant and challenging task across various fields. Recently, methods based on mode decomposition have dominated the forecasting of complex time series because of the advantages of capturing local characteristics and extracting intrinsic modes from data. Unfortunately, most models fail to capture the implied volatilities that contain significant information. To enhance the forecasting of current, rapidly evolving, and volatile time series, we propose a novel decomposition-ensemble paradigm, the VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed to decompose the time series into K sub-modes. Subsequently, the GARCH model extracts the volatility information from these sub-modes, which serve as the input for the LSTM. The numerical and volatility information of each sub-mode is utilized to train a Long Short-Term Memory network. This network predicts the sub-mode, and then we aggregate the predictions from all sub-modes 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#24341;&#20837;&#32852;&#31995;&#22270;&#20449;&#24687;&#65292;&#21487;&#20197;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04017</link><description>&lt;p&gt;
PGraphDTA: &#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps. (arXiv:2310.04017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#24341;&#20837;&#32852;&#31995;&#22270;&#20449;&#24687;&#65292;&#21487;&#20197;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#21644;&#24320;&#21457;&#26032;&#33647;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#24037;&#20316;&#65292;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#25104;&#26412;&#12289;&#26102;&#38388;&#25237;&#20837;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#30830;&#23450;&#26032;&#39062;&#30340;&#33647;&#29289;-&#38774;&#26631;&#65288;DT&#65289;&#30456;&#20114;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#39044;&#27979;DT&#30456;&#20114;&#20316;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#26088;&#22312;&#30830;&#23450;DT&#23545;&#26159;&#21542;&#23384;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#34920;&#29616;&#20986;&#19968;&#31995;&#21015;&#19981;&#21516;&#32467;&#21512;&#24378;&#24230;&#65292;&#21363;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#36825;&#23545;&#20934;&#30830;&#39044;&#27979;&#36896;&#25104;&#20102;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#39044;&#27979;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#25972;&#21512;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#23558;&#25509;&#35302;&#22270;&#20449;&#24687;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03906</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;PyDCM&#65306;&#20026;&#21487;&#25345;&#32493;&#24615;&#23450;&#21046;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03906
&lt;/p&gt;
&lt;p&gt;
PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#21487;&#25345;&#32493;&#24615;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#24378;&#35843;&#26085;&#30410;&#22686;&#21152;&#65292;&#20419;&#20351;&#25919;&#24220;&#21644;&#20225;&#19994;&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#20013;&#24515;&#30340;&#35774;&#35745;&#21644;&#36816;&#33829;&#26041;&#27861;&#12290;&#37492;&#20110;&#25968;&#25454;&#20013;&#24515;&#30340;&#39640;&#33021;&#32791;&#21644;&#25351;&#25968;&#32423;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20248;&#21270;&#33021;&#32791;&#29305;&#21035;&#26159;&#22312;&#20919;&#21364;&#21644;IT&#33021;&#28304;&#20351;&#29992;&#26041;&#38754;&#65292;&#25968;&#25454;&#20013;&#24515;&#26159;&#20248;&#21270;&#30005;&#21147;&#28040;&#32791;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#21487;&#37197;&#32622;&#21644;&#21487;&#25193;&#23637;&#30340;&#28909;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31649;&#36947;&#12290;&#25968;&#25454;&#20013;&#24515;&#30001;&#22810;&#20010;IT&#32452;&#20214;&#32452;&#25104;&#65292;&#20854;&#20960;&#20309;&#37197;&#32622;&#21644;&#25955;&#28909;&#20351;&#24471;&#28909;&#24314;&#27169;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyDCM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;Python&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#26381;&#21153;&#22120;&#35268;&#26684;&#21644;IT&#26426;&#26588;&#30340;&#20960;&#20309;&#24067;&#32622;&#21019;&#24314;&#29420;&#29305;&#30340;&#37197;&#32622;&#12290;&#20351;&#29992;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#20351;&#24471;PyDCM&#27604;&#24403;&#21069;&#26041;&#27861;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65288;30&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#27969;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#21521;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#33021;&#37327;&#27867;&#20989;&#30340;&#29420;&#29305;&#23646;&#24615;&#12289;&#24230;&#37327;&#30340;&#36873;&#25321;&#19982;&#19981;&#21464;&#24615;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03597</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling via Gradient Flows in the Space of Probability Measures. (arXiv:2310.03597v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03597
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#27969;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#21521;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#33021;&#37327;&#27867;&#20989;&#30340;&#29420;&#29305;&#23646;&#24615;&#12289;&#24230;&#37327;&#30340;&#36873;&#25321;&#19982;&#19981;&#21464;&#24615;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#20351;&#29992;&#26410;&#30693;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#25277;&#26679;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32771;&#34385;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#27969;&#27966;&#29983;&#30340;&#31639;&#27861;&#20026;&#31639;&#27861;&#24320;&#21457;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#36890;&#36807;&#23457;&#26597;&#36825;&#31181;&#26799;&#24230;&#27969;&#30340;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#36825;&#31181;&#25277;&#26679;&#26041;&#27861;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#25277;&#26679;&#30340;&#20219;&#20309;&#23454;&#20363;&#21270;&#37117;&#38656;&#35201;&#19968;&#20010;&#33021;&#37327;&#27867;&#20989;&#21644;&#19968;&#20010;&#24230;&#37327;&#26469;&#30830;&#23450;&#27969;&#21160;&#65292;&#20197;&#21450;&#27969;&#21160;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#25512;&#23548;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;Kullback-Leibler&#25955;&#24230;&#20316;&#20026;&#19968;&#20010;&#33021;&#37327;&#27867;&#20989;&#20855;&#26377;&#21807;&#19968;&#30340;&#29305;&#24449;&#65288;&#22312;&#25152;&#26377;f-&#25955;&#24230;&#20013;&#65289;&#65292;&#21363;&#30001;&#20854;&#24471;&#21040;&#30340;&#26799;&#24230;&#27969;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#20174;&#19981;&#21464;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#24230;&#37327;&#30340;&#36873;&#25321;&#12290;Fisher-Rao&#24230;&#37327;&#34987;&#31216;&#20026;t
&lt;/p&gt;
&lt;p&gt;
Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#25968;&#25454;&#20013;&#24674;&#22797;&#32806;&#21512;&#21160;&#21147;&#31995;&#32479;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#24182;&#39044;&#27979;&#20010;&#20307;&#20195;&#29702;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.03378</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#32806;&#21512;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Machine learning the interaction network in coupled dynamical systems. (arXiv:2310.03378v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#25968;&#25454;&#20013;&#24674;&#22797;&#32806;&#21512;&#21160;&#21147;&#31995;&#32479;&#30340;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#24182;&#39044;&#27979;&#20010;&#20307;&#20195;&#29702;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#21160;&#21147;&#31995;&#32479;&#30340;&#30740;&#31350;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#21508;&#20010;&#39046;&#22495;&#20173;&#28982;&#20855;&#26377;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#19968;&#32452;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#20013;&#65292;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#21253;&#21547;&#20102;&#21508;&#20010;&#32452;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20449;&#24687;&#12290;&#20174;&#20195;&#29702;&#30340;&#21160;&#24577;&#20013;&#25512;&#26029;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#32467;&#26524;&#65306;&#24674;&#22797;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#21644;&#39044;&#27979;&#20010;&#20307;&#20195;&#29702;&#30340;&#21160;&#24577;&#12290;&#36825;&#20123;&#20449;&#24687;&#37117;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#26412;&#30740;&#31350;&#23558;&#31070;&#32463;&#20851;&#31995;&#25512;&#29702;&#27169;&#22411;&#24212;&#29992;&#20110;&#20004;&#20010;&#21160;&#21147;&#31995;&#32479;&#65306;&#36890;&#36807;&#32993;&#20811;&#23450;&#24459;&#30456;&#20114;&#20316;&#29992;&#30340;&#32806;&#21512;&#31890;&#23376;&#21644;&#32806;&#21512;&#30456;&#20301;&#65288;Kuramoto&#65289;&#25391;&#33633;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of interacting dynamical systems continues to attract research interest in various fields of science and engineering. In a collection of interacting particles, the interaction network contains information about how various components interact with one another. Inferring the information about the interaction network from the dynamics of agents is a problem of long-standing interest. In this work, we employ a self-supervised neural network model to achieve two outcomes: to recover the interaction network and to predict the dynamics of individual agents. Both these information are inferred solely from the observed trajectory data. This work presents an application of the Neural Relational Inference model to two dynamical systems: coupled particles mediated by Hooke's law interaction and coupled phase (Kuramoto) oscillators.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02066</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;Transformer&#36827;&#34892;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#38656;&#35201;&#21516;&#26102;&#29983;&#25104;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#20854;&#30446;&#26631;&#23646;&#24615;&#65292;&#36825;&#23545;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#39033;&#33392;&#24040;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#65292;&#23427;&#23558;Transformer&#30340;&#35299;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;&#21644;&#39044;&#27979;&#22120;&#32467;&#21512;&#20026;&#19968;&#20010;&#20855;&#26377;&#20849;&#20139;&#26435;&#37325;&#30340;&#32852;&#21512;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#26469;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;Transformer&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30446;&#26631;&#23646;&#24615;&#30340;&#26032;&#39062;&#20998;&#23376;&#65292;&#30456;&#27604;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
&lt;/p&gt;</description></item><item><title>DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02025</link><description>&lt;p&gt;
DeepZero: &#23558;&#38646;&#38454;&#20248;&#21270;&#24212;&#29992;&#20110;&#28145;&#24230;&#27169;&#22411;&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02025
&lt;/p&gt;
&lt;p&gt;
DeepZero&#26159;&#19968;&#20010;&#25193;&#23637;&#38646;&#38454;&#20248;&#21270;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22352;&#26631;&#26799;&#24230;&#20272;&#35745;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#38646;&#38454;&#35757;&#32451;&#21327;&#35758;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#27861;&#33719;&#21462;&#19968;&#38454;&#20449;&#24687;&#26102;&#65292;&#38646;&#38454;&#20248;&#21270;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#30340;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20854;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#22312;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;DeepZero&#65292;&#19968;&#20010;&#22522;&#20110;&#38646;&#38454;&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#23558;&#38646;&#38454;&#20248;&#21270;&#25193;&#23637;&#21040;&#20174;&#38646;&#24320;&#22987;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01210</link><description>&lt;p&gt;
&#23454;&#29616;&#31283;&#20581;&#30340;&#24515;&#33039;&#20998;&#21106;&#65306;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#33258;&#21160;&#21270;&#30340;&#24515;&#33039;&#20998;&#21106;&#21487;&#20197;&#24555;&#36895;&#12289;&#21487;&#37325;&#22797;&#22320;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#26816;&#26597;&#20013;&#25552;&#21462;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;U-Net&#32467;&#26500;&#26159;&#30446;&#21069;&#21307;&#23398;&#20998;&#21106;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#26102;&#20998;&#21106;&#24515;&#33039;&#32467;&#26500;&#65292;&#24182;&#19988;&#24179;&#22343;&#35823;&#24046;&#21487;&#19982;&#35266;&#27979;&#32773;&#38388;&#21464;&#24322;&#24615;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#35813;&#26550;&#26500;&#20173;&#28982;&#20250;&#29983;&#25104;&#35768;&#22810;&#35299;&#31163;&#24322;&#24120;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#39044;&#27979;&#20986;&#24863;&#20852;&#36259;&#32467;&#26500;&#30340;&#36718;&#24275;&#28857;&#65292;&#32780;&#19981;&#26159;&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#33039;&#35299;&#21078;&#23398;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#36825;&#28040;&#38500;&#20102;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;CAMUS&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#32467;&#26500;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#22312;&#20020;&#24202;HUNT4&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully automatic cardiac segmentation can be a fast and reproducible method to extract clinical measurements from an echocardiography examination. The U-Net architecture is the current state-of-the-art deep learning architecture for medical segmentation and can segment cardiac structures in real-time with average errors comparable to inter-observer variability. However, this architecture still generates large outliers that are often anatomically incorrect. This work uses the concept of graph convolutional neural networks that predict the contour points of the structures of interest instead of labeling each pixel. We propose a graph architecture that uses two convolutional rings based on cardiac anatomy and show that this eliminates anatomical incorrect multi-structure segmentations on the publicly available CAMUS dataset. Additionally, this work contributes with an ablation study on the graph convolutional architecture and an evaluation of clinical measurements on the clinical HUNT4 dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17425</link><description>&lt;p&gt;
&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35757;&#32451;&#38598;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#24182;&#20026;&#35821;&#35328;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#37319;&#38598;&#20173;&#28982;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33539;&#24335;&#65292;&#20294;&#25968;&#25454;&#31574;&#21010;&#24448;&#24448;&#20173;&#28982;&#26159;&#20020;&#26102;&#30340;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#27492;&#20505;&#36873;&#27744;&#31579;&#36873;&#21040;&#23454;&#38469;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65288;DFN&#65289;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#29992;&#20110;&#31579;&#36873;&#30340;&#32593;&#32476;&#30340;&#36136;&#37327;&#19982;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26159;&#19981;&#21516;&#30340;&#65306;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#27604;&#19968;&#20010;&#22312;ImageNet&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#20294;&#22312;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#24046;&#30340;&#35757;&#32451;&#38598;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#25968;&#25454;&#38598;DFN-5B&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;3D&#32593;&#26684;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#38236;&#20687;&#37325;&#37327;&#23545;&#31216;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#21487;&#20197;&#25552;&#39640;1&#33267;3&#65285;&#30340;&#39069;&#22806;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#39640;&#36798;8&#20493;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17076</link><description>&lt;p&gt;
&#38236;&#20687;&#37325;&#37327;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;3D&#32593;&#26684;&#20998;&#21106;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications. (arXiv:2309.17076v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;3D&#32593;&#26684;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#38236;&#20687;&#37325;&#37327;&#23545;&#31216;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#21487;&#20197;&#25552;&#39640;1&#33267;3&#65285;&#30340;&#39069;&#22806;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#39640;&#36798;8&#20493;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#32593;&#26684;&#20998;&#21106;&#26159;&#20855;&#26377;&#35768;&#22810;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20154;&#20307;&#20855;&#26377;&#23545;&#31216;&#21644;&#19968;&#20123;&#22120;&#23448;&#20301;&#32622;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#21487;&#20197;&#39044;&#26399;&#22312;&#25191;&#34892;&#29983;&#29289;&#21307;&#23398;&#20998;&#21106;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26059;&#36716;&#21644;&#21453;&#36716;&#19981;&#21464;&#30340;&#23618;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25191;&#34892;3D&#32593;&#26684;&#20998;&#21106;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#30149;&#29702;&#34880;&#31649;&#32467;&#26500;&#65288;&#21160;&#33033;&#30244;&#65289;&#21644;&#20256;&#32479;&#35299;&#21078;&#32467;&#26500;&#65288;&#24515;&#23460;&#30340;&#20869;&#33180;&#21644;&#22806;&#33180;&#65289;&#30340;3D&#32593;&#26684;&#20998;&#21106;&#38382;&#39064;&#12290;&#23616;&#37096;&#20960;&#20309;&#29305;&#24449;&#34987;&#32534;&#30721;&#20026;&#20174;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#20013;&#37319;&#26679;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#32593;&#26684;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#37325;&#37327;&#23545;&#31216;&#24615;&#21487;&#20197;&#22686;&#21152;1&#33267;3&#65285;&#30340;&#39069;&#22806;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#21487;&#20197;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#39640;&#36798;8&#20493;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#21069;&#25552;&#26159;&#31070;&#32463;&#32593;&#32476;&#33267;&#23569;&#20855;&#26377;&#19977;&#20010;&#21367;&#31215;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D mesh segmentation is an important task with many biomedical applications. The human body has bilateral symmetry and some variations in organ positions. It allows us to expect a positive effect of rotation and inversion invariant layers in convolutional neural networks that perform biomedical segmentations. In this study, we show the impact of weight symmetry in neural networks that perform 3D mesh segmentation. We analyze the problem of 3D mesh segmentation for pathological vessel structures (aneurysms) and conventional anatomical structures (endocardium and epicardium of ventricles). Local geometrical features are encoded as sampling from the signed distance function, and the neural network performs prediction for each mesh node. We show that weight symmetry gains from 1 to 3% of additional accuracy and allows decreasing the number of trainable parameters up to 8 times without suffering the performance loss if neural networks have at least three convolutional layers. This also work
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16223</link><description>&lt;p&gt;
GInX-Eval: &#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#20102;&#31361;&#20986;&#22270;&#20013;&#23545;&#27169;&#22411;&#39044;&#27979;&#26368;&#26377;&#36129;&#29486;&#30340;&#36793;&#21644;&#33410;&#28857;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20174;&#20154;&#31867;&#25110;&#27169;&#22411;&#30340;&#35282;&#24230;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#12290;&#24403;&#21069;&#35780;&#20272;&#36807;&#31243;&#20013;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#29942;&#39048;&#38382;&#39064;&#26159;&#35299;&#37322;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#20250;&#24433;&#21709;&#21040;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#27969;&#34892;&#30340;&#24544;&#23454;&#24230;&#25110;&#20445;&#30495;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24544;&#23454;&#24230;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GInX-Eval&#65288;&#22270;&#20869;&#20998;&#24067;&#35299;&#37322;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#35299;&#37322;&#30340;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#24544;&#23454;&#24230;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35299;&#37322;&#26041;&#27861;&#30340;&#26032;&#35265;&#35299;&#12290;&#20351;&#29992;&#37325;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;GInX&#24471;&#20998;&#21487;&#34913;&#37327;&#24050;&#31227;&#38500;&#36793;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#20197;&#21450;&#36793;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15669</link><description>&lt;p&gt;
&#20851;&#20110;&#35745;&#31639;&#32416;&#32544;&#21450;&#20854;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27450;&#39575;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#27492;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#24615;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#65292;&#36890;&#36807;&#32416;&#32544;&#30340;&#27010;&#24565;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#32416;&#32544;&#36827;&#34892;&#20102;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#65292;&#31867;&#20284;&#20110;&#37327;&#23376;&#39046;&#22495;&#20013;&#30340;&#32416;&#32544;&#12290;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;&#23545;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#20219;&#24847;&#36755;&#20837;&#22270;&#20687;&#21518;&#39564;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.15533</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#21518;&#39564;&#20027;&#25104;&#20998;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#20219;&#24847;&#36755;&#20837;&#22270;&#20687;&#21518;&#39564;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#29983;&#29289;&#25104;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#37096;&#32626;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27599;&#20687;&#32032;&#20272;&#35745;&#19978;&#12290;&#28982;&#32780;&#65292;&#27599;&#20687;&#32032;&#26041;&#24046;&#30340;&#28909;&#22270;&#36890;&#24120;&#22312;&#23454;&#38469;&#20013;&#29992;&#36884;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#25429;&#25417;&#20687;&#32032;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#26356;&#33258;&#28982;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#23545;&#24212;&#20110;&#21518;&#39564;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#65288;PCs&#65289;&#19978;&#30340;&#26041;&#24046;&#12290;&#29702;&#35770;&#19978;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#24212;&#29992;PCA&#26469;&#35745;&#31639;PCs&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#29983;&#25104;&#22823;&#37327;&#30340;&#26679;&#26412;&#65292;&#32780;&#22312;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#65288;&#25193;&#25955;&#65289;&#27169;&#22411;&#19979;&#38750;&#24120;&#32531;&#24930;&#12290;&#22312;&#35813;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39044;&#27979;&#21518;&#39564;&#20998;&#24067;&#30340;PCs&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#36755;&#20837;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. However, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20020;&#30028;&#24102;&#23631;&#34109;&#23454;&#39564;&#25581;&#31034;&#20102;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#21644;&#31070;&#32463;&#32593;&#32476;&#29289;&#20307;&#35782;&#21035;&#22312;&#31354;&#38388;&#39057;&#29575;&#36890;&#36947;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#35782;&#21035;&#29289;&#20307;&#25152;&#20351;&#29992;&#30340;&#36890;&#36947;&#19982;&#23383;&#27597;&#21644;&#20809;&#26629;&#35782;&#21035;&#30456;&#21516;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19981;&#21516;&#30340;&#39057;&#29575;&#36890;&#36947;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13190</link><description>&lt;p&gt;
&#31354;&#38388;&#39057;&#29575;&#36890;&#36947;&#12289;&#24418;&#29366;&#20559;&#20506;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Spatial-frequency channels, shape bias, and adversarial robustness. (arXiv:2309.13190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20020;&#30028;&#24102;&#23631;&#34109;&#23454;&#39564;&#25581;&#31034;&#20102;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#21644;&#31070;&#32463;&#32593;&#32476;&#29289;&#20307;&#35782;&#21035;&#22312;&#31354;&#38388;&#39057;&#29575;&#36890;&#36947;&#19978;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#35782;&#21035;&#29289;&#20307;&#25152;&#20351;&#29992;&#30340;&#36890;&#36947;&#19982;&#23383;&#27597;&#21644;&#20809;&#26629;&#35782;&#21035;&#30456;&#21516;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19981;&#21516;&#30340;&#39057;&#29575;&#36890;&#36947;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#29289;&#20307;&#26102;&#20351;&#29992;&#20102;&#21738;&#20123;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#65311;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#20020;&#30028;&#24102;&#23631;&#34109;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25581;&#31034;&#29992;&#20110;&#29289;&#20307;&#35782;&#21035;&#30340;&#39057;&#29575;&#36873;&#25321;&#24615;&#28388;&#27874;&#22120;&#12290;&#20020;&#30028;&#24102;&#23631;&#34109;&#27979;&#37327;&#20102;&#35782;&#21035;&#24615;&#33021;&#23545;&#19981;&#21516;&#31354;&#38388;&#39057;&#29575;&#28155;&#21152;&#22122;&#22768;&#30340;&#25935;&#24863;&#24230;&#12290;&#29616;&#26377;&#30340;&#20020;&#30028;&#24102;&#23631;&#34109;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#36890;&#36807;&#31354;&#38388;&#39057;&#29575;&#28388;&#27874;&#22120;&#65288;&#25110;&#8220;&#36890;&#36947;&#8221;&#65289;&#35782;&#21035;&#21608;&#26399;&#24615;&#27169;&#24335;&#65288;&#20809;&#26629;&#65289;&#21644;&#23383;&#27597;&#65292;&#35813;&#28388;&#27874;&#22120;&#30340;&#39057;&#24102;&#23485;&#24230;&#20026;&#19968;&#20010;&#20843;&#24230;&#65288;&#39057;&#29575;&#32763;&#20493;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#20020;&#30028;&#24102;&#23631;&#34109;&#24341;&#20837;&#20026;&#32593;&#32476;&#19982;&#20154;&#31867;&#23545;&#27604;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#31364;&#24102;&#22122;&#22768;&#23384;&#22312;&#19979;&#27979;&#35797;&#20102;14&#21517;&#20154;&#31867;&#21644;76&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;16&#36335;ImageNet&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#20351;&#29992;&#19982;&#23383;&#27597;&#21644;&#20809;&#26629;&#30456;&#21516;&#30340;&#19968;&#20010;&#20843;&#24230;&#23485;&#30340;&#36890;&#36947;&#26469;&#35782;&#21035;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#65292;&#20351;&#20854;&#25104;&#20026;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#20856;&#22411;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#39057;&#29575;&#36890;&#36947;&#65292;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#39057;&#29575;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or "channel'') that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. On the other hand, the neural netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08549</link><description>&lt;p&gt;
&#22522;&#20110;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#30340;&#35757;&#32451;&#26469;&#25269;&#24481;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#26469;&#38450;&#27490;&#26469;&#33258;&#19981;&#21487;&#20449;&#25968;&#25454;&#28304;&#30340;&#28508;&#22312;&#27745;&#26579;&#25915;&#20987;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#36825;&#32473;&#20102;&#25915;&#20987;&#32773;&#35768;&#22810;&#21487;&#21033;&#29992;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#26377;&#25928;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#22914;&#20960;&#31181;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#37027;&#26679;&#21521;&#25152;&#26377;&#31034;&#20363;&#28155;&#21152;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#30340;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#26368;&#26032;&#25915;&#20987;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;H
&lt;/p&gt;
&lt;p&gt;
While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07760</link><description>&lt;p&gt;
PRE: &#35270;&#35273;-&#35821;&#35328;&#25552;&#31034;&#23398;&#20064;&#19982;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#25552;&#31034;&#20197;&#25913;&#36827;&#19979;&#28216;&#22270;&#20687;&#20998;&#24067;&#21644;&#25991;&#26412;&#31867;&#25551;&#36848;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#36341;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#19988;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#36991;&#20813;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#26368;&#36817;&#30340;CoOp&#24037;&#20316;&#24341;&#20837;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#20351;&#29992;&#21487;&#25511;&#25991;&#26412;&#26631;&#35760;&#30340;&#25552;&#31034;&#23398;&#20064;&#27010;&#24565;&#12290;&#34429;&#28982;CoOp&#21487;&#20197;&#22312;&#25163;&#21160;&#25552;&#31034;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20854;&#23398;&#21040;&#30340;&#19978;&#19979;&#25991;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#26356;&#24191;&#27867;&#30340;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Learning with Reparameterization Encoder (PRE) &#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PolyLUT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;FPGA&#19978;&#36827;&#34892;&#37096;&#32626;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#21464;&#37327;&#22810;&#39033;&#24335;&#20316;&#20026;&#22522;&#26412;&#27169;&#22359;&#65292;&#24182;&#21033;&#29992;&#36719;&#36923;&#36753;&#23558;&#22810;&#39033;&#24335;&#35780;&#20272;&#38544;&#34255;&#22312;FPGA&#30340;&#26597;&#25214;&#34920;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#36229;&#20302;&#24310;&#36831;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#20102;&#36719;&#20214;&#36923;&#36753;&#30340;&#23618;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.02334</link><description>&lt;p&gt;
PolyLUT: &#29992;&#20110;&#36229;&#20302;&#24310;&#36831;FPGA&#22522;&#20110;&#26597;&#25214;&#34920;&#25512;&#29702;&#30340;&#20998;&#27573;&#22810;&#39033;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference. (arXiv:2309.02334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02334
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PolyLUT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;FPGA&#19978;&#36827;&#34892;&#37096;&#32626;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#21464;&#37327;&#22810;&#39033;&#24335;&#20316;&#20026;&#22522;&#26412;&#27169;&#22359;&#65292;&#24182;&#21033;&#29992;&#36719;&#36923;&#36753;&#23558;&#22810;&#39033;&#24335;&#35780;&#20272;&#38544;&#34255;&#22312;FPGA&#30340;&#26597;&#25214;&#34920;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#36229;&#20302;&#24310;&#36831;&#25512;&#29702;&#65292;&#24182;&#20943;&#23569;&#20102;&#36719;&#20214;&#36923;&#36753;&#30340;&#23618;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#12290;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#28041;&#21450;&#20132;&#38169;&#32447;&#24615;&#26144;&#23556;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#35745;&#31639;&#12290;&#20197;&#24448;&#30340;&#36229;&#20302;&#24310;&#36831;&#23454;&#29616;&#24037;&#20316;&#22312;FPGA&#26597;&#25214;&#34920;&#65288;LUT&#65289;&#20013;&#30828;&#32534;&#30721;&#20102;&#32447;&#24615;&#26144;&#23556;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#21040;&#36825;&#20010;&#24819;&#27861;&#30340;&#21551;&#21457;&#65292;&#21363;FPGA&#20013;&#30340;LUT&#21487;&#20197;&#29992;&#26469;&#23454;&#29616;&#27604;&#36825;&#26356;&#22810;&#26679;&#21270;&#30340;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#29992;&#20110;FPGA&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22810;&#21464;&#37327;&#22810;&#39033;&#24335;&#20316;&#20026;&#22522;&#26412;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36719;&#20214;&#36923;&#36753;&#25552;&#20379;&#30340;&#28789;&#27963;&#24615;&#65292;&#23558;&#22810;&#39033;&#24335;&#35780;&#20272;&#38544;&#34255;&#22312;LUT&#20013;&#19988;&#27809;&#26377;&#20219;&#20309;&#24320;&#38144;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#27169;&#22359;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#20351;&#29992;&#30340;&#36719;&#20214;&#36923;&#36753;&#23618;&#25968;&#35201;&#27604;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#35201;&#23569;&#24471;&#22810;&#65292;&#20174;&#32780;&#24102;&#26469;&#26174;&#33879;&#30340;&#24310;&#36831;&#21644;&#38754;&#31215;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13670</link><description>&lt;p&gt;
&#32447;&#24615;&#25391;&#21160;&#65306;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#30340;&#22256;&#24785;&#32654;&#23398;
&lt;/p&gt;
&lt;p&gt;
Linear Oscillation: The Aesthetics of Confusion for Vision Transformer. (arXiv:2308.13670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13670
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#65292;&#28145;&#21051;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#23427;&#20204;&#19981;&#20165;&#22609;&#36896;&#20102;&#34920;&#31034;&#30340;&#24615;&#36136;&#65292;&#36824;&#20248;&#21270;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#19968;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23450;&#20041;&#20026;$f(x) = x \times \sin(\alpha x + \beta)$&#12290;&#19982;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#19981;&#21516;&#65292;LoC&#23558;&#32447;&#24615;&#36712;&#36857;&#19982;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#12290;&#21629;&#21517;&#20026;&#8220;&#32447;&#24615;&#25391;&#21160;&#8221;&#26159;&#23545;&#20854;&#29420;&#29305;&#23646;&#24615;&#30340;&#33268;&#25964;&#65292;&#21363;&#36890;&#36807;&#21644;&#35856;&#30340;&#25391;&#21160;&#34701;&#20837;&#32447;&#24615;&#28608;&#27963;&#65292;&#25429;&#25417;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#32593;&#32476;&#28608;&#27963;&#20013;&#30340;&#8220;&#25511;&#21046;&#24615;&#22256;&#24785;&#8221;&#27010;&#24565;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#26356;&#31283;&#20581;&#30340;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#32593;&#32476;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#38544;&#34255;&#30340;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are the linchpins of deep learning, profoundly influencing both the representational capacity and training dynamics of neural networks. They shape not only the nature of representations but also optimize convergence rates and enhance generalization potential. Appreciating this critical role, we present the Linear Oscillation (LoC) activation function, defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional activation functions which primarily introduce non-linearity, LoC seamlessly blends linear trajectories with oscillatory deviations. The nomenclature ``Linear Oscillation'' is a nod to its unique attribute of infusing linear activations with harmonious oscillations, capturing the essence of the 'Importance of Confusion'. This concept of ``controlled confusion'' within network activations is posited to foster more robust learning, particularly in contexts that necessitate discerning subtle patterns. Our empirical studies reveal that, when i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32500;&#25252;&#25805;&#20316;&#20013;&#65292;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#30340;&#26368;&#20248;&#25968;&#25454;&#27719;&#32858;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12670</link><description>&lt;p&gt;
&#32500;&#25252;&#25805;&#20316;&#20013;&#25968;&#25454;&#20849;&#20139;&#23398;&#20064;&#30340;&#26368;&#20248;&#25968;&#25454;&#27719;&#32858;
&lt;/p&gt;
&lt;p&gt;
Optimal data pooling for shared learning in maintenance operations. (arXiv:2308.12670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32500;&#25252;&#25805;&#20316;&#20013;&#65292;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#30340;&#26368;&#20248;&#25968;&#25454;&#27719;&#32858;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32500;&#25252;&#25805;&#20316;&#20013;&#27719;&#32858;&#25968;&#25454;&#36827;&#34892;&#20849;&#20139;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#32452;&#36890;&#36807;&#20808;&#39564;&#26410;&#30693;&#36895;&#29575;&#30456;&#20114;&#32806;&#21512;&#30340;&#27850;&#26494;&#36864;&#21270;&#31995;&#32479;&#12290;&#28041;&#21450;&#36825;&#20123;&#31995;&#32479;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;&#39640;&#32500;&#24230;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#36825;&#26679;&#30340;MDP&#20998;&#35299;&#20026;&#20108;&#32500;MDP&#30340;&#20998;&#35299;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#32467;&#26500;&#20998;&#26512;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#20998;&#35299;&#32467;&#26524;&#35777;&#26126;&#20102;&#25968;&#25454;&#20849;&#20139;&#21487;&#20197;&#30456;&#23545;&#20110;&#19981;&#20849;&#20139;&#26102;&#26174;&#33879;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the benefits of pooling data for shared learning in maintenance operations. We consider a set of systems subject to Poisson degradation that are coupled through an a-priori unknown rate. Decision problems involving these systems are high-dimensional Markov decision processes (MDPs). We present a decomposition result that reduces such an MDP to two-dimensional MDPs, enabling structural analyses and computations. We leverage this decomposition to demonstrate that pooling data can lead to significant cost reductions compared to not pooling.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24352;&#37327;&#31209;&#28436;&#21270;&#26469;&#29702;&#35299;&#31070;&#32463;&#20803;&#36830;&#25509;&#22312;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#21464;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#36807;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#36890;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#32780;&#36825;&#31181;&#32467;&#26500;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#12290;&#23545;&#30495;&#23454;&#26435;&#37325;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11567</link><description>&lt;p&gt;
&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#20302;&#38454;&#24352;&#37327;&#31209;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low Tensor Rank Learning of Neural Dynamics. (arXiv:2308.11567v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#24352;&#37327;&#31209;&#28436;&#21270;&#26469;&#29702;&#35299;&#31070;&#32463;&#20803;&#36830;&#25509;&#22312;&#23398;&#20064;&#20013;&#30340;&#21327;&#35843;&#21464;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#36807;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#36890;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#32780;&#36825;&#31181;&#32467;&#26500;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#12290;&#23545;&#30495;&#23454;&#26435;&#37325;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20381;&#36182;&#20110;&#31070;&#32463;&#20803;&#32676;&#20307;&#20013;&#30340;&#21327;&#35843;&#31361;&#35302;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#23398;&#20064;&#36807;&#31243;&#20013;&#31361;&#35302;&#36830;&#25509;&#30340;&#38598;&#20307;&#28436;&#21270;&#26159;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#26435;&#37325;&#30697;&#38453;&#36890;&#24120;&#26159;&#20302;&#31209;&#30340;&#65292;&#20294;&#26159;&#36825;&#31181;&#20302;&#31209;&#32467;&#26500;&#22914;&#20309;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23637;&#24320;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#30001;&#26435;&#37325;&#30697;&#38453;&#24418;&#25104;&#30340;3&#38454;&#24352;&#37327;&#30340;&#31209;&#12290;&#36890;&#36807;&#29992;&#19981;&#21516;&#31209;&#30340;RNN&#25311;&#21512;&#22823;&#35268;&#27169;&#31070;&#32463;&#35760;&#24405;&#30340;&#36816;&#21160;&#23398;&#20064;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#25512;&#26029;&#30340;&#26435;&#37325;&#26159;&#20302;&#38454;&#24352;&#37327;&#31209;&#30340;&#65292;&#22240;&#27492;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#28436;&#21270;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#26435;&#37325;&#19978;&#30452;&#25509;&#36827;&#34892;&#20302;&#38454;&#24352;&#37327;&#31209;&#20998;&#35299;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20302;&#38454;&#24352;&#37327;&#31209;&#23398;&#20064;&#30340;&#35266;&#23519;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applie
&lt;/p&gt;</description></item><item><title>Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.09883</link><description>&lt;p&gt;
Flamingo: &#22810;&#36718;&#21333;&#26381;&#21153;&#22120;&#23433;&#20840;&#32858;&#21512;&#21450;&#20854;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning. (arXiv:2308.09883v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09883
&lt;/p&gt;
&lt;p&gt;
Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Flamingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#22312;&#23433;&#20840;&#32858;&#21512;&#20013;&#65292;&#26381;&#21153;&#22120;&#23545;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#36755;&#20837;&#36827;&#34892;&#27714;&#21644;&#65292;&#24182;&#22312;&#19981;&#20102;&#35299;&#20010;&#20307;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#32467;&#26524;&#65292;&#20165;&#33021;&#25512;&#26029;&#20986;&#26368;&#32456;&#24635;&#21644;&#12290;Flamingo&#19987;&#27880;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#36718;&#35774;&#32622;&#65292;&#20854;&#20013;&#25191;&#34892;&#22810;&#20010;&#36830;&#32493;&#30340;&#27169;&#22411;&#26435;&#37325;&#27714;&#21644;&#65288;&#24179;&#22343;&#65289;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#21327;&#35758;&#65288;&#20363;&#22914;Bell&#31561;&#20154;&#30340;CCS '20&#65289;&#20165;&#36866;&#29992;&#20110;&#21333;&#36718;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#37325;&#22797;&#35813;&#21327;&#35758;&#26469;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#30340;&#35774;&#32622;&#12290;Flamingo&#28040;&#38500;&#20102;&#20043;&#21069;&#21327;&#35758;&#27599;&#36718;&#35774;&#32622;&#30340;&#38656;&#27714;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;&#20197;&#30830;&#20445;&#22914;&#26524;&#23458;&#25143;&#31471;&#22312;&#27714;&#21644;&#36807;&#31243;&#20013;&#31163;&#24320;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;Flamingo&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#25152;&#35859;&#30340;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#65292;&#27492;&#27010;&#24565;&#30001;Bell&#31561;&#20154;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09544</link><description>&lt;p&gt;
&#36866;&#24212;&#24744;&#30340;&#25945;&#24072;: &#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#26088;&#22312;&#38450;&#27490;&#36951;&#24536;&#12290;KD&#26041;&#27861;&#22312;CIL&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#35775;&#38382;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#24456;&#38590;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#26102;&#25945;&#24072;&#32593;&#32476;&#20013;&#30340;&#26174;&#33879;&#34920;&#31034;&#36716;&#25442;&#12290;&#36825;&#23548;&#33268;KD&#25439;&#22833;&#25104;&#20998;&#20013;&#20986;&#29616;&#36739;&#22823;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#23548;&#33268;CIL&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25945;&#24072;&#36866;&#24212;&#65288;TA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#26356;&#26032;&#25945;&#24072;&#21644;&#20027;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;KD&#30340;CIL&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#26080;&#26679;&#26412;CIL&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.06197</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#22522;&#26412;&#29305;&#24449;&#36827;&#34892;&#22797;&#26434;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06197
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#35748;&#30693;&#20219;&#21153;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20854;&#34920;&#29616;&#22312;&#19982;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#30456;&#31561;&#25110;&#20197;&#19978;&#30340;&#20854;&#20182;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#30340;&#31243;&#24230;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#36739;&#20026;&#22256;&#38590;&#30340;&#12290;&#30001;&#20110;&#20154;&#33080;&#34920;&#36798;&#30340;&#24773;&#32490;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#38754;&#37096;&#34920;&#24773;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#29305;&#21035;&#22256;&#38590;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#22312;&#36825;&#19968;&#39046;&#22495;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#23427;&#21487;&#33021;&#38656;&#35201;&#23454;&#26102;&#21512;&#25104;&#30693;&#35782;&#24182;&#29702;&#35299;&#26032;&#27010;&#24565;&#12290;&#20154;&#31867;&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#20960;&#20010;&#20363;&#23376;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20174;&#35760;&#24518;&#20013;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#24182;&#20002;&#24323;&#20854;&#20313;&#20449;&#24687;&#12290;&#21516;&#26679;&#65292;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#30041;&#24050;&#30693;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#32780;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#21644;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#20934;&#30830;&#22320;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05061</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#25512;&#29702;&#38454;&#27573;&#20174;&#25552;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#36816;&#31639;&#31526;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#36816;&#31639;&#31526;&#30340;&#23453;&#36149;&#30340;&#20154;&#31867;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31181;&#22810;&#27169;&#24335;&#33539;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#8220;&#26631;&#39064;&#8221;&#26469;&#25972;&#21512;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#26041;&#31243;&#24335;&#34920;&#36798;&#30340;&#36816;&#31639;&#31526;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36941;&#24615;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#19978;&#19979;&#25991;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;ICON-LM&#8221;&#65292;&#22522;&#20110;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#22312;&#32447;&#37325;&#21472;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$&#21644;$O\big(\sqrt{d}\,n^{-1/8}\big)$&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#29366;&#24577;&#30456;&#20851;&#21644;&#29366;&#24577;&#26080;&#20851;&#30340;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#12290;&#36825;&#20123;&#36895;&#29575;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#20811;&#26381;&#20102;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.01481</link><description>&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#65292;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22312;&#32447;&#21327;&#26041;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#22312;&#32447;&#37325;&#21472;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$&#21644;$O\big(\sqrt{d}\,n^{-1/8}\big)$&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#29366;&#24577;&#30456;&#20851;&#21644;&#29366;&#24577;&#26080;&#20851;&#30340;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#12290;&#36825;&#20123;&#36895;&#29575;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#20811;&#26381;&#20102;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#22312;&#32447;&#37325;&#21472;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$&#21644;$O\big(\sqrt{d}\,n^{-1/8}\big)$&#65292;&#20854;&#20013;$d$&#20195;&#34920;&#32500;&#24230;&#65292;$n$&#34920;&#31034;&#35266;&#27979;&#25968;&#37327;&#25110;SGD&#36845;&#20195;&#27425;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#36895;&#29575;&#19982;&#20808;&#21069;&#30001;\cite{zhu2021online}&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;($\iid$)&#24773;&#20917;&#19979;&#24314;&#31435;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#38500;&#20102;&#23545;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20811;&#26381;&#20102;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#32780;&#20135;&#29983;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#35823;&#24046;&#39033;&#21644;&#25209;&#27425;&#22343;&#20540;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;SGD&#21160;&#24577;&#35823;&#24046;$\ell_2$&#33539;&#25968;&#30340;&#21069;&#22235;&#38454;&#30697;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#27169;&#25311;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;Murski{i}&#30340;&#36890;&#29992;&#20195;&#25968;&#23450;&#29702;&#21644;Cybenko&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#32479;&#19968;&#36215;&#26469;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#32467;&#26500;&#30340;&#22810;&#24577;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32463;&#20856;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00677</link><description>&lt;p&gt;
&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Discrete neural nets and polymorphic learning. (arXiv:2308.00677v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#27169;&#25311;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;Murski{i}&#30340;&#36890;&#29992;&#20195;&#25968;&#23450;&#29702;&#21644;Cybenko&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#32479;&#19968;&#36215;&#26469;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#32467;&#26500;&#30340;&#22810;&#24577;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32463;&#20856;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
20&#19990;&#32426;70&#24180;&#20195;Murski{i}&#25552;&#20986;&#30340;&#36890;&#29992;&#20195;&#25968;&#23450;&#29702;&#19982;20&#19990;&#32426;80&#24180;&#20195;Cybenko&#31561;&#20154;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#26377;&#30528;&#24778;&#20154;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;&#27169;&#25311;&#65292;&#23558;&#36825;&#20123;&#32467;&#26524;&#25918;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#32467;&#26500;&#22810;&#24577;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#29992;&#20110;&#32463;&#20856;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theorems from universal algebra such as that of Murski\u{i} from the 1970s have a striking similarity to universal approximation results for neural nets along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural net which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15936</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#21512;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#25193;&#22823;&#26102;&#65292;&#26032;&#30340;&#25216;&#33021;&#23558;&#22312; AI &#20135;&#21697;&#20013;&#20986;&#29616;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#31181;&#29616;&#35937;&#23578;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#20110;&#26799;&#24230;&#35757;&#32451;&#30340;&#25968;&#23398;&#20998;&#26512;&#25552;&#20379;&#26426;&#26800;&#35299;&#37322;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33879;&#21517;&#30340;&#65288;&#21644;&#32463;&#39564;&#24615;&#30340;&#65289;LLM&#25193;&#23637;&#23450;&#24459;&#21644;&#31616;&#21333;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#20998;&#26512;&#20986;&#29616;&#12290;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;a&#65289;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#35821;&#35328;&#20219;&#21153;&#22522;&#26412;&#25216;&#33021;&#30340;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#65288;b&#65289;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#25193;&#23637;&#23450;&#24459;&#24847;&#21619;&#30528;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#38750;&#27491;&#24335;&#22320;&#31216;&#20043;&#20026;&#8220;&#24377;&#24339;&#27867;&#21270;&#8221;&#65292;&#22240;&#20026;&#34920;&#38754;&#19978;&#30475;&#65292;&#23427;&#20284;&#20046;&#25552;&#20379;&#20102;&#22312;&#25216;&#33021;&#27700;&#24179;&#19978;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;&#65288;c&#65289;&#24377;&#24339;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#20363;&#23376;&#65292;&#21363;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07320</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25968;&#25454;&#25910;&#38598;&#24050;&#25104;&#20026;&#22686;&#24378;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#25928;&#29575;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#26426;&#21046;&#24120;&#24120;&#32473;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#24341;&#20837;&#22797;&#26434;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#20272;&#35745;&#37327;&#21487;&#33021;&#34920;&#29616;&#20986;&#38750;&#27491;&#24577;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#20174;&#32780;&#23545;&#20934;&#30830;&#30340;&#25512;&#26029;&#21644;&#35299;&#37322;&#25552;&#20986;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#30340;&#24605;&#24819;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#28176;&#36817;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#20445;&#30041;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#32479;&#35745;&#25512;&#26029;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.06870</link><description>&lt;p&gt;
&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#38271;&#26102;&#38388;&#20869;&#37096;&#32626;&#22312;&#23478;&#20013;&#30340;&#26426;&#22120;&#20154;&#38754;&#20020;&#30528;&#30495;&#27491;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#12290;&#20316;&#20026;&#26426;&#22120;&#20154;&#23547;&#27714;&#20026;&#29992;&#25143;&#25552;&#20379;&#24110;&#21161;&#65292;&#23427;&#24212;&#35813;&#21033;&#29992;&#20219;&#20309;&#31215;&#32047;&#30340;&#32463;&#39564;&#26469;&#25913;&#36827;&#33258;&#24049;&#30340;&#30693;&#35782;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#29087;&#32451;&#30340;&#21161;&#25163;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#36825;&#31181;&#24773;&#20917;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#24314;&#27169;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#65292;&#20026;&#35268;&#21010;&#22120;&#29983;&#25104;&#20505;&#36873;&#36830;&#32493;&#21442;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#25968;&#25454;&#22914;&#20309;&#22312;&#20219;&#21153;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#20915;&#23450;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#22312;&#32447;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#65292;&#36825;&#20123;&#20219;&#21153;&#20316;&#20026;&#27599;&#20010;&#27169;&#22411;&#23545;&#29366;&#24577;&#30340;&#29702;&#35299;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.04075</link><description>&lt;p&gt;
&#22522;&#20110;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#30284;&#30151;&#26032;&#20122;&#22411;&#21644;&#27835;&#30103;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30284;&#30151;&#30340;&#39640;&#24322;&#36136;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#19981;&#21516;&#30284;&#30151;&#20122;&#22411;&#20043;&#38388;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#20020;&#24202;&#29305;&#24449;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#30284;&#30151;&#20122;&#22411;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#23545;&#20110;&#30284;&#30151;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20174;&#32780;&#35782;&#21035;&#21644;&#34920;&#24449;&#30284;&#30151;&#20122;&#22411;&#12290;AMUCL&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#21644;&#32858;&#31867;&#65292;&#24182;&#35782;&#21035;&#26032;&#30340;&#30284;&#30151;&#20122;&#22411;&#12290;&#36825;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#32858;&#31867;&#20122;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyMat&#30340;&#26032;&#30340;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#23545;&#31216;&#24615;&#36827;&#34892;&#24863;&#30693;&#65292;&#24182;&#33021;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02707</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#23545;&#31216;&#24863;&#30693;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Symmetry-Aware Generation of Periodic Materials. (arXiv:2307.02707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyMat&#30340;&#26032;&#30340;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#23545;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#23545;&#31216;&#24615;&#36827;&#34892;&#24863;&#30693;&#65292;&#24182;&#33021;&#22312;&#29983;&#25104;&#21644;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#28145;&#24230;&#27169;&#22411;&#29983;&#25104;&#21608;&#26399;&#24615;&#26448;&#26009;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23545;&#20110;&#23545;&#31216;&#24863;&#30693;&#30340;&#20998;&#23376;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#21608;&#26399;&#24615;&#26448;&#26009;&#20855;&#26377;&#19981;&#21516;&#30340;&#23545;&#31216;&#24615;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#21040;&#36825;&#20123;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;SyMat&#65292;&#33021;&#22815;&#25429;&#25417;&#21608;&#26399;&#24615;&#26448;&#26009;&#32467;&#26500;&#30340;&#29289;&#29702;&#23545;&#31216;&#24615;&#12290;SyMat&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#29983;&#25104;&#26448;&#26009;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#26230;&#26684;&#65292;&#36890;&#36807;&#29983;&#25104;&#21407;&#23376;&#31867;&#22411;&#38598;&#12289;&#26230;&#26684;&#38271;&#24230;&#21644;&#26230;&#26684;&#35282;&#24230;&#12290;&#27492;&#22806;&#65292;SyMat&#37319;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26448;&#26009;&#30340;&#21407;&#23376;&#22352;&#26631;&#65292;&#22312;&#22352;&#26631;&#25193;&#25955;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#24863;&#30693;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;SyMat&#22312;&#25152;&#26377;&#26448;&#26009;&#30340;&#23545;&#31216;&#21464;&#25442;&#19978;&#29702;&#35770;&#19978;&#26159;&#19981;&#21464;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;SyMat&#22312;&#38543;&#26426;&#29983;&#25104;&#21644;&#24615;&#33021;&#20248;&#21270;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods. In this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01050</link><description>&lt;p&gt;
&#36816;&#36755;&#12289;&#21464;&#20998;&#25512;&#26029;&#21644;&#25193;&#25955;&#65306;&#24212;&#29992;&#20110;&#22238;&#28779;&#27969;&#21644;&#34203;&#23450;&#35860;&#26725;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36816;&#36755;&#19982;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#27491;&#21521;&#21644;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#21450;Girsanov&#21464;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26368;&#32456;&#21457;&#23637;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#65288;&#19982;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;Jarzynski&#21644;Crooks&#24658;&#31561;&#24335;&#26377;&#20851;&#65289;&#21644;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#65288;IPF&#65289;&#22411;&#30446;&#26631;&#65292;&#19981;&#21516;&#20110;&#26631;&#20934;IPF&#30340;&#39034;&#24207;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#29983;&#25104;&#24314;&#27169;&#31034;&#20363;&#21644;&#22522;&#20110;&#21452;&#20117;&#30340;&#31232;&#26377;&#20107;&#20214;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.14111</link><description>&lt;p&gt;
RLHF&#26159;&#21542;&#27604;&#26631;&#20934;RL&#26356;&#22256;&#38590;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#20174;&#20559;&#22909;&#20449;&#21495;&#23398;&#20064;&#65292;&#32780;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21017;&#30452;&#25509;&#20174;&#22870;&#21169;&#20449;&#21495;&#23398;&#20064;&#12290;&#20559;&#22909;&#20449;&#21495;&#21487;&#33021;&#21253;&#21547;&#30340;&#20449;&#24687;&#27604;&#22870;&#21169;&#20449;&#21495;&#23569;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#20284;&#20046;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#31867;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22870;&#21169;&#27010;&#29575;&#27169;&#22411;&#30340;&#20559;&#22909;&#65292;&#27492;&#26102;&#21487;&#20197;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#23481;&#24525;&#22870;&#21169;&#23567;&#35823;&#24046;&#30340;&#40065;&#26834;&#22870;&#21169;RL&#38382;&#39064;&#65307;&#65288;2&#65289;&#23545;&#20110;&#19968;&#33324;&#30340;&#20219;&#24847;&#20559;&#22909;&#19988;&#30446;&#26631;&#26159;&#25214;&#21040;von Neumann&#33719;&#32988;&#32773;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#22810;&#26234;&#33021;&#20307;&#22870;&#21169;RL&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#22312;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#19979;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22240;&#23376;&#32435;&#20160;&#24179;&#34913;&#35299;&#12290;&#21518;&#19968;&#31181;&#24773;&#20917;&#21487;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302;&#25104;&#23545;&#20851;&#31995;&#30340;MDP&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#21464;&#24418;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;SE&#65288;3&#65289;&#26426;&#22120;&#20154;&#25805;&#32437;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12392</link><description>&lt;p&gt;
&#20132;&#20114;&#21464;&#24418;&#30340;&#19968;&#27425;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-shot Imitation Learning via Interaction Warping. (arXiv:2306.12392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#21464;&#24418;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;SE&#65288;3&#65289;&#26426;&#22120;&#20154;&#25805;&#32437;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#24335;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#23398;&#20064;&#26426;&#22120;&#20154;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20132;&#20114;&#21464;&#24418;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;SE&#65288;3&#65289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#24418;&#29366;&#21464;&#24418;&#25216;&#26415;&#25512;&#26029;&#29615;&#22659;&#20013;&#27599;&#20010;&#29289;&#20307;&#30340;&#19977;&#32500;&#32593;&#26684;&#65292;&#24182;&#23558;&#25805;&#32437;&#21160;&#20316;&#34920;&#31034;&#20026;&#29289;&#20307;&#19978;&#30340;&#20851;&#38190;&#28857;&#65292;&#21487;&#20197;&#29992;&#29289;&#20307;&#30340;&#24418;&#29366;&#36827;&#34892;&#21464;&#24418;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#20013;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#19968;&#27425;&#24615;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37326;&#22806;&#39044;&#27979;&#29289;&#20307;&#32593;&#26684;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning of robot policies from few demonstrations is crucial in open-ended applications. We propose a new method, Interaction Warping, for learning SE(3) robotic manipulation policies from a single demonstration. We infer the 3D mesh of each object in the environment using shape warping, a technique for aligning point clouds across object instances. Then, we represent manipulation actions as keypoints on objects, which can be warped with the shape of the object. We show successful one-shot imitation learning on three simulated and real-world object re-arrangement tasks. We also demonstrate the ability of our method to predict object meshes and robot grasps in the wild.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11197</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22359;&#28608;&#27963;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411; (SSM) &#22312;&#21508;&#31181;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#24490;&#29615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#32508;&#21512;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;SSM&#12290;&#21516;&#26102;&#20351;&#29992;SSM&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#27169;&#22359;&#38745;&#24577;&#19988;&#22343;&#21248;&#22320;&#24212;&#29992;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#65292;&#23548;&#33268;&#20102;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#31232;&#30095;&#22320;&#21160;&#24577;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#12290;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#20803;&#32032;&#36339;&#36807;&#38750;&#28608;&#27963;&#30340;&#23376;&#27169;&#22359;&#65292;SMA&#21487;&#20197;&#22312;&#24207;&#21015;&#24314;&#27169;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#20316;&#20026;SMA&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#20851;&#31995;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65288;RCNPs&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#23558;&#31561;&#21464;&#24615;&#32435;&#20837;&#20219;&#20309;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;&#31561;&#21464;&#31070;&#32463;&#36807;&#31243;&#30340;&#36866;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#21040;&#26356;&#39640;&#30340;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.10915</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#31995;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#23454;&#29616;&#23454;&#29992;&#30340;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Practical Equivariances via Relational Conditional Neural Processes. (arXiv:2306.10915v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#20851;&#31995;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65288;RCNPs&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#23558;&#31561;&#21464;&#24615;&#32435;&#20837;&#20219;&#20309;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;&#31561;&#21464;&#31070;&#32463;&#36807;&#31243;&#30340;&#36866;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#21040;&#26356;&#39640;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65288;CNPs&#65289;&#26159;&#19968;&#31867;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20854;&#32508;&#21512;&#36816;&#34892;&#26102;&#25928;&#29575;&#21644;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32780;&#21463;&#27426;&#36814;&#12290;&#35768;&#22810;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#26102;&#31354;&#24314;&#27169;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#36830;&#32493;&#25511;&#21046;&#65292;&#21253;&#21547;&#31561;&#21464;&#24615;&#65292;&#20363;&#22914;&#23545;&#20110;&#24179;&#31227;&#65292;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#26368;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35797;&#22270;&#22312;CNPs&#20013;&#21253;&#21547;&#31561;&#21464;&#24615;&#22312;&#36229;&#36807;&#20004;&#20010;&#36755;&#20837;&#32500;&#24230;&#20043;&#22806;&#30340;&#23610;&#24230;&#19978;&#26080;&#27861;&#26377;&#25928;&#25193;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65288;RCNPs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#23558;&#31561;&#21464;&#24615;&#32435;&#20837;&#20219;&#20309;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#31561;&#21464;&#31070;&#32463;&#36807;&#31243;&#30340;&#36866;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#21040;&#26356;&#39640;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#21253;&#21547;&#31561;&#21464;&#24615;&#20219;&#21153;&#30340;&#22823;&#37327;&#20219;&#21153;&#19978;&#32463;&#39564;&#35777;&#23454;&#20102;RCNPs&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Neural Processes (CNPs) are a class of metalearning models popular for combining the runtime efficiency of amortized inference with reliable uncertainty quantification. Many relevant machine learning tasks, such as spatio-temporal modeling, Bayesian Optimization and continuous control, contain equivariances -- for example to translation -- which the model can exploit for maximal performance. However, prior attempts to include equivariances in CNPs do not scale effectively beyond two input dimensions. In this work, we propose Relational Conditional Neural Processes (RCNPs), an effective approach to incorporate equivariances into any neural process model. Our proposed method extends the applicability and impact of equivariant neural processes to higher dimensions. We empirically demonstrate the competitive performance of RCNPs on a large array of tasks naturally containing equivariances.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;VillanDiffusion&#65292;&#19968;&#20010;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#28085;&#30422;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.06874</link><description>&lt;p&gt;
VillanDiffusion: &#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;VillanDiffusion&#65292;&#19968;&#20010;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#28085;&#30422;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#23398;&#20064;&#21487;&#36870;&#30340;&#25439;&#22351;&#36807;&#31243;&#12290;&#23427;&#20204;&#26159;&#35768;&#22810;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20027;&#24178;&#65292;&#20363;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#26377;&#26465;&#20214;&#29983;&#25104;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22522;&#26412;&#26080;&#26465;&#20214;DM&#65288;&#20363;&#22914;DDPM&#21644;DDIM&#65289;&#26131;&#21463;&#21518;&#38376;&#27880;&#20837;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#20110;&#24694;&#24847;&#23884;&#20837;&#27169;&#22411;&#36755;&#20837;&#30340;&#27169;&#24335;&#32780;&#35302;&#21457;&#30340;&#36755;&#20986;&#25805;&#32437;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65288;VillanDiffusion&#65289;&#65292;&#20197;&#25193;&#23637;&#24403;&#21069;&#30340;DM&#21518;&#38376;&#20998;&#26512;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65288;&#22522;&#20110;&#21435;&#22122;&#21644;&#22522;&#20110;&#35780;&#20998;&#65289;&#65292;&#20197;&#21450;&#21508;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#37319;&#26679;&#22120;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.06203</link><description>&lt;p&gt;
&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;FLSL
&lt;/p&gt;
&lt;p&gt;
FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;SimCLR&#12289;DINO&#12289;VICReg&#12289;MOCOv3&#65289;&#20027;&#35201;&#38024;&#23545;&#23454;&#20363;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#19981;&#36866;&#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;Vision Transformers&#65288;ViT&#65289;&#30340;&#22522;&#30784;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#36807;&#31243;&#33021;&#22815;&#33391;&#22909;&#22320;&#19982;&#33258;&#28982;&#22270;&#20687;&#35821;&#20041;&#65288;&#20363;&#22914;&#29289;&#20307;&#21644;&#22330;&#26223;&#65289;&#23545;&#40784;&#12290;&#36890;&#36807;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#32423;&#29305;&#24449;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;FLSL&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FLSL&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#20174;&#22343;&#20540;&#28418;&#31227;&#21644;k-means&#30340;&#35282;&#24230;&#26500;&#24314;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLSL&#20419;&#36827;&#20102;&#26174;&#33879;&#30340;&#35821;&#20041;&#31867;&#31751;&#34920;&#31034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#30340;&#23884;&#20837;&#26041;&#26696;&#12290;FLSL&#30340;&#36816;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2306.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#32479;&#35745;&#23398;&#65306;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#23637;&#29616;&#20102;&#20135;&#29983;&#36924;&#30495;&#22270;&#20687;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#31070;&#31192;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#26174;&#24335;&#28145;&#24230;&#20449;&#24687;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#36890;&#24120;&#20063;&#20250;&#36755;&#20986;&#19968;&#33268;&#30340;3D&#22330;&#26223;&#22270;&#29255;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65306;LDM&#26159;&#21542;&#21019;&#24314;&#24182;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65311;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#65292;&#25105;&#20204;&#21457;&#29616;LDM&#30340;&#20869;&#37096;&#28608;&#27963;&#32534;&#30721;&#20102;&#32447;&#24615;&#34920;&#31034;&#65292;&#26082;&#21253;&#25324;3D&#28145;&#24230;&#25968;&#25454;&#65292;&#21448;&#21253;&#25324;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#21306;&#21035;&#12290;&#36825;&#20123;&#34920;&#31034;&#20284;&#20046;&#22312;&#21435;&#22122;&#36807;&#31243;&#30340;&#26089;&#26399;&#23601;&#20986;&#29616;&#20102;&#8212;&#8212;&#22312;&#20154;&#31867;&#33021;&#36731;&#26131;&#29702;&#35299;&#22024;&#26434;&#30340;&#22270;&#20687;&#20043;&#21069;&#12290;&#24178;&#39044;&#24615;&#23454;&#39564;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36825;&#20123;&#34920;&#31034;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#25198;&#28436;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#19988;&#21487;&#33021;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23567;&#37327;&#23376;&#24577;&#19978;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.05694</link><description>&lt;p&gt;
&#23567;&#37327;&#23376;&#24577;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Representation Learning of Small Quantum States. (arXiv:2306.05694v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23567;&#37327;&#23376;&#24577;&#19978;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#30340;&#21487;&#29702;&#35299;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#20154;&#31867;&#25351;&#23548;&#25110;&#29305;&#24449;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435;&#36215;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;&#21040;&#20851;&#20110;&#22788;&#29702;&#20219;&#21153;&#25152;&#38656;&#30340;&#25968;&#25454;&#29305;&#24449;&#20449;&#24687;&#12290;&#22312;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#32972;&#26223;&#19979;&#65292;&#35757;&#32451;&#27169;&#22411;&#20197;&#25551;&#36848;&#37327;&#23376;&#24577;&#21448;&#26410;&#32463;&#20154;&#31867;&#24178;&#39044;&#22320;&#23398;&#20064;&#20449;&#24687;&#26159;&#33719;&#24471;&#28145;&#20837;&#20102;&#35299;&#26426;&#22120;&#22914;&#20309;&#21576;&#29616;&#22797;&#26434;&#37327;&#23376;&#24577;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#23558;&#20026;&#38750;&#24179;&#20961;&#30340;&#37327;&#23376;&#31995;&#32479;&#21450;&#20854;&#26377;&#25928;&#34920;&#31034;&#24102;&#26469;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#38024;&#23545;&#30001;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#29983;&#25104;&#30340;&#20004;&#37327;&#23376;&#27604;&#29305;&#23494;&#24230;&#30697;&#38453;&#35757;&#32451;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35745;&#31639;&#23454;&#39564;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35813;&#27169;&#22411;&#25152;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20197;&#21450;&#20854;&#20869;&#37096;&#23545;&#25968;&#25454;&#20449;&#24687;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#24182;&#23558;&#37327;&#23376;&#24577;&#19982;&#20854;&#28508;&#22312;&#32467;&#26500;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning models build an internal representation of their training data without the need for explicit human guidance or feature engineering. This learned representation provides insights into which features of the data are relevant for the task at hand. In the context of quantum physics, training models to describe quantum states without human intervention offers a promising approach to gaining insight into how machines represent complex quantum states. The ability to interpret the learned representation may offer a new perspective on non-trivial features of quantum systems and their efficient representation. We train a generative model on two-qubit density matrices generated by a parameterized quantum circuit. In a series of computational experiments, we investigate the learned representation of the model and its internal understanding of the data. We observe that the model learns an interpretable representation which relates the quantum states to their underlying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;</title><link>http://arxiv.org/abs/2306.04181</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#32771;&#23448;&#8221;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23427;&#26159;&#27979;&#35797;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;&#20840;&#38754;&#27979;&#35797;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30475;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65288;LMAE&#65289;&#65292;&#20854;&#20013;LM&#20316;&#20026;&#30693;&#35782;&#28170;&#21338;&#30340;&#32771;&#23448;&#65292;&#26681;&#25454;&#20854;&#30693;&#35782;&#21046;&#23450;&#38382;&#39064;&#24182;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#22240;&#20026;&#21487;&#20197;&#37319;&#29992;&#21508;&#31181;LM&#20316;&#20026;&#32771;&#23448;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#26029;&#26356;&#26032;&#38382;&#39064;&#65292;&#32473;&#20104;&#26356;&#22810;&#26679;&#21270;&#30340;&#35302;&#21457;&#20027;&#39064;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#21644;&#20844;&#27491;&#22320;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#31574;&#30053;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25351;&#31034;LM&#32771;&#23448;&#22312;&#35768;&#22810;&#39046;&#22495;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03937</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Fl)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#20801;&#35768;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36328;&#22810;&#20010;&#21442;&#19982;&#32773;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24320;&#22987;&#32771;&#34385;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#29616;&#26377;FL&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#28857;&#30340;&#24433;&#21709;; &#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#38598;&#20013;&#24335;&#23398;&#20064;&#35774;&#32622;&#20013;&#22823;&#37327;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#25991;&#29486;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#32771;&#34385;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;FL&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#19968;&#32452;&#35745;&#31639;&#26426;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20165;&#25311;&#21512;&#32447;&#24615;&#20998;&#31867;&#22836;&#26159;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#25311;&#21512;&#20998;&#31867;&#22120;&#21487;&#20197;&#31934;&#30830;&#22320;&#19988;&#27604;&#29616;&#26377;&#25552;&#35758;&#39640;&#25928;&#22320;&#23436;&#25104;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#33719;&#24471;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel
&lt;/p&gt;</description></item><item><title>SALE&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20302;&#32423;&#29366;&#24577;&#20013;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#65292;TD7&#31639;&#27861;&#24341;&#20837;&#20102;&#35813;&#26041;&#27861;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.02451</link><description>&lt;p&gt;
&#24453;&#21806;&#65306;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
For SALE: State-Action Representation Learning for Deep Reinforcement Learning. (arXiv:2306.02451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02451
&lt;/p&gt;
&lt;p&gt;
SALE&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20302;&#32423;&#29366;&#24577;&#20013;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#65292;TD7&#31639;&#27861;&#24341;&#20837;&#20102;&#35813;&#26041;&#27861;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#34920;&#31034;&#23398;&#20064;&#26159;&#22788;&#29702;&#22797;&#26434;&#22522;&#20110;&#22270;&#20687;&#20219;&#21153;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#20294;&#36890;&#24120;&#34987;&#24573;&#30053;&#20102;&#20302;&#32423;&#29366;&#24577;&#65288;&#20363;&#22914;&#29289;&#29702;&#25511;&#21046;&#38382;&#39064;&#65289;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SALE&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#23884;&#20837;&#26469;&#24314;&#27169;&#29366;&#24577;&#21644;&#21160;&#20316;&#20043;&#38388;&#24494;&#22937;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#20302;&#32423;&#29366;&#24577;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#36825;&#20123;&#23884;&#20837;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#24378;&#35843;&#20102;&#37325;&#35201;&#30340;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;SALE&#21644;RL&#30340;&#26816;&#26597;&#28857;&#33258;&#36866;&#24212;&#26041;&#27861;&#25972;&#21512;&#21040;TD3&#20013;&#65292;&#24418;&#25104;TD7&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#22312;OpenAI gym&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;TD7&#22312;300k&#21644;5M&#26102;&#38388;&#27493;&#39588;&#19979;&#30340;&#24179;&#22343;&#24615;&#33021;&#22686;&#30410;&#20998;&#21035;&#20026;276.7&#65285;&#21644;50.7&#65285;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of reinforcement learning (RL), representation learning is a proven tool for complex image-based tasks, but is often overlooked for environments with low-level states, such as physical control problems. This paper introduces SALE, a novel approach for learning embeddings that model the nuanced interaction between state and action, enabling effective representation learning from low-level states. We extensively study the design space of these embeddings and highlight important design considerations. We integrate SALE and an adaptation of checkpoints for RL into TD3 to form the TD7 algorithm, which significantly outperforms existing continuous control algorithms. On OpenAI gym benchmark tasks, TD7 has an average performance gain of 276.7% and 50.7% over TD3 at 300k and 5M time steps, respectively, and works in both the online and offline settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2306.01951</link><description>&lt;p&gt;
GAD-NR: &#36890;&#36807;&#37051;&#22495;&#37325;&#26500;&#23454;&#29616;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#31038;&#20132;&#23186;&#20307;&#22403;&#22334;&#26816;&#27979;&#21644;&#20854;&#20182;&#21508;&#31181;&#39046;&#22495;&#20013;&#26377;&#24212;&#29992;&#12290;GAD&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAEs&#65289;&#65292;&#23427;&#23558;&#22270;&#24418;&#25968;&#25454;&#32534;&#30721;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#34920;&#31034;&#26469;&#35780;&#20272;&#22270;&#24418;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#20027;&#35201;&#38024;&#23545;&#30452;&#25509;&#38142;&#25509;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36830;&#25509;&#22270;&#20013;&#30340;&#33410;&#28857;&#34987;&#32858;&#31867;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25797;&#38271;&#26816;&#27979;&#32858;&#31867;&#22411;&#32467;&#26500;&#24322;&#24120;&#65292;&#20294;&#23545;&#19981;&#31526;&#21512;&#32858;&#31867;&#30340;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#24322;&#24120;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;GAD-NR&#65292;&#23427;&#26159;GAE&#30340;&#19968;&#20010;&#26032;&#21464;&#20307;&#65292;&#34701;&#21512;&#37051;&#22495;&#37325;&#26500;&#36827;&#34892;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#12290;GAD-NR&#30340;&#30446;&#26631;&#26159;&#37325;&#26500;&#33410;&#28857;&#30340;&#25972;&#20010;&#37051;&#22495;&#65292;&#28085;&#30422;&#26412;&#22320;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#35780;&#20272;LLMs&#30340;&#21407;&#22411;&#24179;&#21488;CheckMate&#65292;&#24182;&#21033;&#29992;&#35813;&#24179;&#21488;&#35780;&#20272;&#20102;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#25968;&#23398;&#35777;&#26126;&#21161;&#25163;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#24067;&#20102;&#32467;&#26524;&#25968;&#25454;&#38598;MathConverse&#65292;&#24182;&#24471;&#20986;&#20102;&#20154;&#31867;&#34892;&#20026;&#30340;&#21021;&#27493;&#20998;&#31867;&#21644;LMM&#29983;&#25104;&#27491;&#30830;&#24615;&#19982;&#24863;&#30693;&#24110;&#21161;&#24615;&#20998;&#27495;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01694</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#35780;&#20272;&#25968;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Models for Mathematics through Interactions. (arXiv:2306.01694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#35780;&#20272;LLMs&#30340;&#21407;&#22411;&#24179;&#21488;CheckMate&#65292;&#24182;&#21033;&#29992;&#35813;&#24179;&#21488;&#35780;&#20272;&#20102;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#25968;&#23398;&#35777;&#26126;&#21161;&#25163;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#24067;&#20102;&#32467;&#26524;&#25968;&#25454;&#38598;MathConverse&#65292;&#24182;&#24471;&#20986;&#20102;&#20154;&#31867;&#34892;&#20026;&#30340;&#21021;&#27493;&#20998;&#31867;&#21644;LMM&#29983;&#25104;&#27491;&#30830;&#24615;&#19982;&#24863;&#30693;&#24110;&#21161;&#24615;&#20998;&#27495;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#38745;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#24320;&#21457;&#21161;&#25163;&#65306;&#36825;&#31181;&#35780;&#20272;&#26410;&#33021;&#32771;&#34385;&#37096;&#32626;&#20013;&#30340;&#22522;&#26412;&#20132;&#20114;&#24615;&#36136;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CheckMate&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#24212;&#24615;&#30340;&#20154;&#26426;&#20132;&#20114;&#21407;&#22411;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#21033;&#29992;CheckMate&#23545;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#12289;ChatGPT&#21644;GPT-4&#65289;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35780;&#20272;&#23427;&#20204;&#20316;&#20026;&#22823;&#23398;&#32423;&#25968;&#23398;&#35777;&#26126;&#21161;&#25163;&#30340;&#33021;&#21147;&#65292;&#21442;&#19982;&#32773;&#28151;&#21512;&#20102;&#20174;&#26412;&#31185;&#29983;&#21040;&#25968;&#23398;&#25945;&#25480;&#30340;&#21508;&#23618;&#27425;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#32467;&#26524;&#20132;&#20114;&#21644;&#35780;&#20998;&#25968;&#25454;&#38598;MathConverse&#12290;&#36890;&#36807;&#23545;MathConverse&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20154;&#31867;&#34892;&#20026;&#30340;&#21021;&#27493;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#26222;&#36941;&#23384;&#22312;&#31215;&#26497;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;LLM&#29983;&#25104;&#30340;&#27491;&#30830;&#24615;&#21644;&#24863;&#30693;&#24110;&#21161;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01589</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#21407;&#23376;&#27169;&#25311;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26680;&#22343;&#20540;&#23884;&#20837;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#20102;&#21183;&#33021;&#34920;&#38754;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#21487;&#27010;&#25324;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#21407;&#23376;&#27169;&#25311;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#32780;&#29983;&#25104;&#21442;&#32771;&#35745;&#31639;&#26159;&#35745;&#31639;&#19978;&#35201;&#27714;&#24456;&#39640;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36882;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25551;&#36848;&#21270;&#23398;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#26680;&#22343;&#20540;&#23884;&#20837;&#12290;&#25105;&#20204;&#20174;&#39044;&#20808;&#22312;OC20&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#30340;GNN&#20013;&#25552;&#21462;&#29305;&#24449;&#26144;&#23556;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20174;&#20652;&#21270;&#36807;&#31243;&#30340;&#31995;&#32479;&#29305;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21183;&#33021;&#34920;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#36890;&#36807;&#28789;&#27963;&#30340;&#26680;&#20989;&#25968;&#26469;&#22686;&#24378;&#65292;&#35813;&#26680;&#20989;&#25968;&#21253;&#25324;&#21270;&#23398;&#29289;&#31181;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#36880;&#28176;&#22797;&#26434;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#21487;&#36716;&#31227;&#24615;&#33021;&#65292;&#25913;&#36827;&#20102;&#20381;&#36182;GNNs&#25110;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;Wasserstein&#20998;&#24067;&#24335;&#24378;&#40065;&#26834;&#20272;&#35745;&#22120;&#30340;&#27867;&#21270;&#20445;&#35777;&#36866;&#29992;&#20110;&#19968;&#33324;&#27169;&#22411;&#31867;&#21035;&#65292;&#19981;&#21463;&#32500;&#25968;&#28798;&#38590;&#25152;&#22256;&#25200;&#65292;&#29978;&#33267;&#21487;&#20197;&#28085;&#30422;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.17076</link><description>&lt;p&gt;
&#65288;&#27491;&#21017;&#21270;&#65289;Wasserstein&#20998;&#24067;&#24335;&#24378;&#26368;&#20248;&#27169;&#22411;&#30340;&#30830;&#20999;&#27867;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models. (arXiv:2305.17076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;Wasserstein&#20998;&#24067;&#24335;&#24378;&#40065;&#26834;&#20272;&#35745;&#22120;&#30340;&#27867;&#21270;&#20445;&#35777;&#36866;&#29992;&#20110;&#19968;&#33324;&#27169;&#22411;&#31867;&#21035;&#65292;&#19981;&#21463;&#32500;&#25968;&#28798;&#38590;&#25152;&#22256;&#25200;&#65292;&#29978;&#33267;&#21487;&#20197;&#28085;&#30422;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wasserstein&#20998;&#24067;&#24335;&#24378;&#40065;&#26834;&#20272;&#35745;&#22120;&#24050;&#32463;&#25104;&#20026;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#30340;&#24378;&#22823;&#27169;&#22411;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#27867;&#21270;&#20445;&#35777;&#65306;&#35757;&#32451;&#20998;&#24067;&#24471;&#21040;&#30340;&#24378;&#40065;&#26834;&#30446;&#26631;&#26159;&#30495;&#23454;&#39118;&#38505;&#30340;&#19968;&#20010;&#31934;&#30830;&#19978;&#30028;&#65292;&#24182;&#19988;&#39640;&#27010;&#29575;&#25104;&#31435;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20445;&#35777;&#35201;&#20040;&#21463;&#21040;&#32500;&#25968;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#35201;&#20040;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#35774;&#32622;&#65292;&#25110;&#32773;&#20250;&#23548;&#33268;&#34394;&#20551;&#30340;&#38169;&#35823;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#27867;&#21270;&#20445;&#35777;&#23454;&#38469;&#19978;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#19981;&#21463;&#32500;&#25968;&#28798;&#38590;&#25152;&#22256;&#25200;&#65292;&#29978;&#33267;&#21487;&#20197;&#28085;&#30422;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#24341;&#20837;&#30340;Wasserstein&#20998;&#24067;&#24335;&#24378;&#26368;&#20248;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein distributionally robust estimators have emerged as powerful models for prediction and decision-making under uncertainty. These estimators provide attractive generalization guarantees: the robust objective obtained from the training distribution is an exact upper bound on the true risk with high probability. However, existing guarantees either suffer from the curse of dimensionality, are restricted to specific settings, or lead to spurious error terms. In this paper, we show that these generalization guarantees actually hold on general classes of models, do not suffer from the curse of dimensionality, and can even cover distribution shifts at testing. We also prove that these results carry over to the newly-introduced regularized versions of Wasserstein distributionally robust problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35757;&#32451;&#35270;&#22270;&#30340;&#36807;&#25311;&#21512;&#23548;&#33268;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16914</link><description>&lt;p&gt;
PlaNeRF&#65306;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#29992;&#20110;NeRF&#22823;&#35268;&#27169;&#22330;&#26223;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction. (arXiv:2305.16914v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SVD&#26080;&#30417;&#30563;&#19977;&#32500;&#24179;&#38754;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35757;&#32451;&#35270;&#22270;&#30340;&#36807;&#25311;&#21512;&#23548;&#33268;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21033;&#29992;2D&#22270;&#20687;&#21644;&#30456;&#26426;&#23039;&#24577;&#36827;&#34892;3D&#22330;&#26223;&#37325;&#24314;&#20197;&#36827;&#34892;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#23613;&#31649;NeRF&#33021;&#20135;&#29983;&#36924;&#30495;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#32463;&#24120;&#36973;&#21463;&#36807;&#25311;&#21512;&#20110;&#35757;&#32451;&#35270;&#22270;&#30340;&#22256;&#25200;&#65292;&#23548;&#33268;&#20960;&#20309;&#37325;&#24314;&#19981;&#20339;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#32441;&#29702;&#21306;&#22495;&#12290;&#36825;&#31181;&#38480;&#21046;&#38480;&#21046;&#20102;&#35768;&#22810;&#38656;&#35201;&#20934;&#30830;&#20960;&#20309;&#24418;&#24577;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#20363;&#22914;&#22806;&#25512;NVS&#65292;&#39640;&#28165;&#26144;&#23556;&#21644;&#22330;&#26223;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#21644;&#35821;&#20041;&#22320;&#22270;&#21363;&#21487;&#25913;&#21892;NeRF&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#39062;&#24179;&#38754;&#27491;&#21017;&#21270;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25439;&#22833;&#35774;&#35745;&#20013;&#21033;&#29992;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#65288;SSIM&#65289;&#26469;&#27491;&#30830;&#21021;&#22987;&#21270;NeRF&#30340;&#20307;&#31215;&#34920;&#31034;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20960;&#20309;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstructi
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16358</link><description>&lt;p&gt;
&#24102;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16358
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26435;&#37325;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#26641;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#22810;&#20010;&#36830;&#36890;&#20998;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20197;&#23454;&#29616;&#24179;&#28369;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27969;&#27700;&#32447;&#20013;&#21253;&#21547;&#32858;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20960;&#20309;&#29615;&#22659;&#19979;&#20063;&#33021;&#33391;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#29305;&#21035;&#30340;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#37096;&#20998;&#32858;&#31867;&#25968;&#25454;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#37319;&#26679;&#35268;&#21017;EB-TC $\varepsilon$&#65292;&#29992;&#20110;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;$\varepsilon$-&#26368;&#20339;&#33218;&#30340;&#36776;&#35782;&#12290;&#35813;&#35268;&#21017;&#21487;&#29992;&#20110;&#30830;&#23450;&#22266;&#23450;&#32622;&#20449;&#24230;&#25110;&#22266;&#23450;&#39044;&#31639;&#26631;&#35782;&#19988;&#20855;&#22791;&#33258;&#36866;&#24212;&#35843;&#25972;&#21208;&#25506;&#21442;&#25968;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.16041</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30830;&#23450;&#22266;&#23450;&#32622;&#20449;&#24230;&#21644;&#20197;&#19978;&#30340; $\varepsilon$-&#26368;&#20339;&#33218;&#36776;&#35782;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond. (arXiv:2305.16041v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#37319;&#26679;&#35268;&#21017;EB-TC $\varepsilon$&#65292;&#29992;&#20110;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;$\varepsilon$-&#26368;&#20339;&#33218;&#30340;&#36776;&#35782;&#12290;&#35813;&#35268;&#21017;&#21487;&#29992;&#20110;&#30830;&#23450;&#22266;&#23450;&#32622;&#20449;&#24230;&#25110;&#22266;&#23450;&#39044;&#31639;&#26631;&#35782;&#19988;&#20855;&#22791;&#33258;&#36866;&#24212;&#35843;&#25972;&#21208;&#25506;&#21442;&#25968;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37319;&#26679;&#35268;&#21017;EB-TC $\varepsilon$&#65292;&#35813;&#35268;&#21017;&#29992;&#20110;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;$\varepsilon$-&#26368;&#20339;&#33218;&#30340;&#36776;&#35782;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;&#26368;&#20339;&#33218;&#36776;&#35782;&#30340;Top Two&#31639;&#27861;&#20998;&#26512;&#23454;&#20363;&#12290; EB-TC $\varepsilon$ &#26159;&#19968;&#31181;&#8220;&#38543;&#26102;&#21487;&#29992;&#8221;&#30340;&#37319;&#26679;&#35268;&#21017;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#27809;&#26377;&#39044;&#31639;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#29992;&#20110;&#30830;&#23450;&#22266;&#23450;&#32622;&#20449;&#24230;&#25110;&#22266;&#23450;&#39044;&#31639;&#26631;&#35782;&#65288;&#26080;&#38656;&#20462;&#25913;&#65289;&#12290;&#25105;&#20204;&#20026;EB-TC $\varepsilon$ &#25552;&#20379;&#20102;&#19977;&#31181;&#29702;&#35770;&#20445;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#22312;&#22266;&#23450;&#32622;&#20449;&#24230;&#35774;&#32622;&#20013;&#39044;&#26399;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#26377;&#30028;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#21208;&#25506;&#21442;&#25968;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#19982;&#32452;&#21512;&#30340;&#24773;&#20917;&#19979;&#21576;&#29616;&#20854;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20219;&#20309;&#26102;&#38388;&#21644;&#23545;&#20110;&#20219;&#20309;&#35823;&#24046;&#21442;&#25968;&#30340;&#27010;&#29575;&#19978;&#30028;&#26469;&#34917;&#20805;&#36825;&#20123;&#21457;&#29616;&#65292;&#36825;&#36827;&#19968;&#27493;&#20135;&#29983;&#20854;&#20219;&#20309;&#26102;&#38388;&#30340;&#31616;&#21333;&#36951;&#25022;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#65292;EB-TC $\varepsilon$ &#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EB-TC$\varepsilon$, a novel sampling rule for $\varepsilon$-best arm identification in stochastic bandits. It is the first instance of Top Two algorithm analyzed for approximate best arm identification. EB-TC$\varepsilon$ is an *anytime* sampling rule that can therefore be employed without modification for fixed confidence or fixed budget identification (without prior knowledge of the budget). We provide three types of theoretical guarantees for EB-TC$\varepsilon$. First, we prove bounds on its expected sample complexity in the fixed confidence setting, notably showing its asymptotic optimality in combination with an adaptive tuning of its exploration parameter. We complement these findings with upper bounds on its probability of error at any time and for any error parameter, which further yield upper bounds on its simple regret at any time. Finally, we show through numerical simulations that EB-TC$\varepsilon$ performs favorably compared to existing algorithms, in different
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#22312;&#24179;&#34913;&#23545;&#25239;&#27169;&#22411;&#19979;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#65292;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#24179;&#34913;&#29366;&#24577;&#19979;&#65292;&#22238;&#31572;&#37027;&#20123;&#20197;&#21069;&#34987;&#35748;&#20026;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#33258;&#36866;&#24212;&#26597;&#35810;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15452</link><description>&lt;p&gt;
&#24179;&#34913;&#23545;&#25239;&#27169;&#22411;&#19979;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Adaptive Data Analysis in a Balanced Adversarial Model. (arXiv:2305.15452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#22312;&#24179;&#34913;&#23545;&#25239;&#27169;&#22411;&#19979;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#65292;&#22312;&#35813;&#27169;&#22411;&#19979;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#24179;&#34913;&#29366;&#24577;&#19979;&#65292;&#22238;&#31572;&#37027;&#20123;&#20197;&#21069;&#34987;&#35748;&#20026;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#33258;&#36866;&#24212;&#26597;&#35810;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#26426;&#21046;&#33719;&#21462;n&#20010;&#26469;&#33258;&#26410;&#30693;&#20998;&#24067;D&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#65292;&#24182;&#38656;&#35201;&#23545;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#32479;&#35745;&#26597;&#35810;&#25552;&#20379;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#22312;&#19968;&#20010;&#25317;&#26377;&#21333;&#21521;&#20989;&#25968;&#30340;&#22522;&#30784;&#19978;&#65292;Hardt&#21644;Ullman(FOCS 2014)&#20197;&#21450;Steinke&#21644;Ullman(COLT 2015)&#34920;&#26126;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22238;&#31572;&#36229;&#36807;&#920;(n^2)&#20010;&#36866;&#24212;&#24615;&#26597;&#35810;&#26159;&#35745;&#31639;&#19978;&#22256;&#38590;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36127;&#38754;&#32467;&#26524;&#24378;&#28872;&#20381;&#36182;&#20110;&#19968;&#20010;&#23545;&#25239;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#24471;&#23545;&#25239;&#20998;&#26512;&#21592;&#27604;&#26426;&#21046;&#26126;&#26174;&#22788;&#20110;&#20248;&#21183;&#22320;&#20301;&#65292;&#22240;&#20026;&#36873;&#25321;&#33258;&#36866;&#24212;&#26597;&#35810;&#30340;&#20998;&#26512;&#21592;&#20063;&#36873;&#25321;&#20102;&#22522;&#30784;&#20998;&#24067;D&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#20250;&#23545;&#25152;&#24471;&#21040;&#30340;&#38590;&#24230;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#38382;&#39064;&#8212;&#8212;&#20855;&#26377;&#22522;&#30784;&#20998;&#24067;D&#30340;&#23436;&#20840;&#30693;&#35782;&#30340;&#20998;&#26512;&#21592;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#24517;&#35201;&#21521;&#20165;&#25345;&#26377;&#26377;&#38480;&#25968;&#37327;D&#26679;&#26412;&#30340;&#26426;&#21046;&#21457;&#20986;&#32479;&#35745;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an unknown distribution $D$, and is required to provide accurate estimations to a sequence of adaptively chosen statistical queries with respect to $D$. Hardt and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in general, it is computationally hard to answer more than $\Theta(n^2)$ adaptive queries, assuming the existence of one-way functions.  However, these negative results strongly rely on an adversarial model that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses the adaptive queries, also chooses the underlying distribution $D$. This imbalance raises questions with respect to the applicability of the obtained hardness results -- an analyst who has complete knowledge of the underlying distribution $D$ would have little need, if at all, to issue statistical queries to a mechanism which only holds a finite number of samples from $D$.  We consider more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;&#65292;&#31639;&#27861;&#26174;&#31034;&#20122;&#32447;&#24615;&#36951;&#25022;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#8220;&#23481;&#26131;&#8221;&#26102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2305.13946</link><description>&lt;p&gt;
&#26080;&#38656;Lipschitzness&#21644;Smoothness&#30340;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#30340;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness. (arXiv:2305.13946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;&#65292;&#31639;&#27861;&#26174;&#31034;&#20122;&#32447;&#24615;&#36951;&#25022;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#8220;&#23481;&#26131;&#8221;&#26102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#20013;&#30340;&#31532;&#19968;&#31181;&#23567;&#25439;&#22833;&#21644;&#24179;&#31283;&#21464;&#21270;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#26631;&#24535;&#30528;&#22312;&#32447;&#20984;&#20248;&#21270;&#20855;&#26377;&#38750;Lipschitz&#12289;&#38750;&#20809;&#28369;&#25439;&#22833;&#30340;&#25968;&#25454;&#30456;&#20851;&#19978;&#30028;&#30340;&#39318;&#27425;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#20122;&#32447;&#24615;&#36951;&#25022;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#8220;&#23481;&#26131;&#8221;&#26102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#26102;&#38388;&#20960;&#20046;&#26159;&#25237;&#36164;&#36873;&#25321;&#25968;&#37327;&#30340;&#32447;&#24615;&#12290;&#36951;&#25022;&#19978;&#30028;&#26159;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#30340;&#26032;&#22411;&#20809;&#28369;&#24615;&#34920;&#24449;&#12289;&#36981;&#24490;&#20855;&#26377;&#33258;&#20849;&#36717;&#27491;&#21017;&#21270;&#22120;&#30340;&#27491;&#21017;&#21270;&#39046;&#34966;&#65288;FTRL&#65289;&#30340;&#23616;&#37096;&#33539;&#25968;&#20998;&#26512;&#12289;&#23427;&#20204;&#19981;&#19968;&#23450;&#26159;&#38556;&#30861;&#30340;&#21644;&#20855;&#26377;log&#38556;&#30861;&#30340;&#20048;&#35266;FTRL&#30340;&#38544;&#24335;&#21464;&#20307;&#26469;&#25512;&#23548;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first small-loss and gradual-variation regret bounds for online portfolio selection, marking the first instances of data-dependent bounds for online convex optimization with non-Lipschitz, non-smooth losses. The algorithms we propose exhibit sublinear regret rates in the worst cases and achieve logarithmic regrets when the data is "easy," with per-iteration time almost linear in the number of investment alternatives. The regret bounds are derived using novel smoothness characterizations of the logarithmic loss, a local norm-based analysis of following the regularized leader (FTRL) with self-concordant regularizers, which are not necessarily barriers, and an implicit variant of optimistic FTRL with the log-barrier.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#28145;&#24230;RKHM&#65292;&#36890;&#36807;&#20351;&#29992;$C^*$&#20195;&#25968;&#33719;&#24471;&#26356;&#28201;&#21644;&#30340;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.13588</link><description>&lt;p&gt;
&#36890;&#36807;RKHM&#21644;Perron-Frobenius&#31639;&#23376;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Kernels through RKHM and the Perron-Frobenius Operator. (arXiv:2305.13588v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#28145;&#24230;RKHM&#65292;&#36890;&#36807;&#20351;&#29992;$C^*$&#20195;&#25968;&#33719;&#24471;&#26356;&#28201;&#21644;&#30340;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;$C^*$-&#27169;(RKHM)&#36890;&#36807;$C^*$&#20195;&#25968;&#23545;&#37325;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#36827;&#34892;&#20102;&#27867;&#21270;&#65292;&#32780;Perron-Frobenius&#31639;&#23376;&#26159;&#19982;&#20989;&#25968;&#32452;&#21512;&#30456;&#20851;&#30340;&#32447;&#24615;&#31639;&#23376;&#12290;&#23558;&#36825;&#20004;&#20010;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;RKHM&#65292;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#30340;Rademacher&#24191;&#20041;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;Perron-Frobenius&#31639;&#23376;&#25552;&#20379;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#30001;&#20110;$C^*$&#20195;&#25968;&#30340;&#20248;&#21183;&#65292;&#35813;&#30028;&#38480;&#23545;&#36755;&#20986;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#36739;&#29616;&#26377;&#30028;&#38480;&#26356;&#21152;&#28201;&#21644;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;$C^*$&#20195;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#26680;&#24515;&#24037;&#20855;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#31639;&#23376;&#30340;&#20056;&#31215;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26126;&#30830;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#28145;&#24230;&#26680;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing kernel Hilbert $C^*$-module (RKHM) is a generalization of reproducing kernel Hilbert space (RKHS) by means of $C^*$-algebra, and the Perron-Frobenius operator is a linear operator related to the composition of functions. Combining these two concepts, we present deep RKHM, a deep learning framework for kernel methods. We derive a new Rademacher generalization bound in this setting and provide a theoretical interpretation of benign overfitting by means of Perron-Frobenius operators. By virtue of $C^*$-algebra, the dependency of the bound on output dimension is milder than existing bounds. We show that $C^*$-algebra is a suitable tool for deep learning with kernels, enabling us to take advantage of the product structure of operators and to provide a clear connection with convolutional neural networks. Our theoretical analysis provides a new lens through which one can design and analyze deep kernel methods.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;NTK&#36924;&#36817;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#27169;&#22411;&#32553;&#25918;&#22240;&#23376;$\alpha=O(T)$&#26102;&#22312;&#35757;&#32451;&#26102;&#38388;$T$&#20043;&#21069;&#21487;&#20197;&#20351;&#24471;NTK&#36924;&#36817;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.13141</link><description>&lt;p&gt;
NTK&#36924;&#36817;&#26377;&#25928;&#30340;&#32039;&#20945;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tight conditions for when the NTK approximation is valid. (arXiv:2305.13141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;NTK&#36924;&#36817;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#27169;&#22411;&#32553;&#25918;&#22240;&#23376;$\alpha=O(T)$&#26102;&#22312;&#35757;&#32451;&#26102;&#38388;$T$&#20043;&#21069;&#21487;&#20197;&#20351;&#24471;NTK&#36924;&#36817;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#36924;&#36817;&#22312;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#27169;&#22411;&#26102;&#20309;&#26102;&#26377;&#25928;&#12290;&#22312;Chizat&#31561;&#20154;2019&#24180;&#30340;&#25042;&#24816;&#35757;&#32451;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#22240;&#23376;&#20026;$\alpha=O(T)$&#30340;&#27169;&#22411;&#32553;&#25918;&#23601;&#36275;&#20197;&#20351;NTK&#36924;&#36817;&#22312;&#35757;&#32451;&#26102;&#38388;$T$&#20043;&#21069;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#32039;&#20945;&#30340;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;Chizat&#31561;&#20154;2019&#24180;&#30340;&#20808;&#21069;&#30028;&#38480;&#65292;&#21518;&#32773;&#38656;&#35201;&#26356;&#22823;&#30340;&#32553;&#25918;&#22240;&#23376;$\alpha=O(T^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study when the neural tangent kernel (NTK) approximation is valid for training a model with the square loss. In the lazy training setting of Chizat et al. 2019, we show that rescaling the model by a factor of $\alpha = O(T)$ suffices for the NTK approximation to be valid until training time $T$. Our bound is tight and improves on the previous bound of Chizat et al. 2019, which required a larger rescaling factor of $\alpha = O(T^2)$.
&lt;/p&gt;</description></item><item><title>DreamWaltz&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#23548;&#21644;&#21442;&#25968;&#21270;&#30340;&#20154;&#20307;&#20808;&#39564;&#29983;&#25104;&#21644;&#21160;&#30011;&#21270;&#22797;&#26434;&#30340;3D&#35282;&#33394;&#12290;&#23427;&#25552;&#20986;&#20102;&#19977;&#32500;&#19968;&#33268;&#30340;&#36974;&#25377;&#24863;&#30693;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26469;&#20248;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#35268;&#33539;&#23039;&#21183;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#21160;&#30011;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#21019;&#24314;3D&#35282;&#33394;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12529</link><description>&lt;p&gt;
DreamWaltz&#65306;&#20351;&#29992;&#22797;&#26434;3D&#21160;&#30011;&#35282;&#33394;&#21046;&#20316;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
DreamWaltz: Make a Scene with Complex 3D Animatable Avatars. (arXiv:2305.12529v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12529
&lt;/p&gt;
&lt;p&gt;
DreamWaltz&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#23548;&#21644;&#21442;&#25968;&#21270;&#30340;&#20154;&#20307;&#20808;&#39564;&#29983;&#25104;&#21644;&#21160;&#30011;&#21270;&#22797;&#26434;&#30340;3D&#35282;&#33394;&#12290;&#23427;&#25552;&#20986;&#20102;&#19977;&#32500;&#19968;&#33268;&#30340;&#36974;&#25377;&#24863;&#30693;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26469;&#20248;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#35268;&#33539;&#23039;&#21183;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#21160;&#30011;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#21019;&#24314;3D&#35282;&#33394;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DreamWaltz&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#23548;&#21644;&#21442;&#25968;&#21270;&#30340;&#20154;&#20307;&#20808;&#39564;&#29983;&#25104;&#21644;&#21160;&#30011;&#21270;&#22797;&#26434;&#30340;3D&#35282;&#33394;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#23558;&#25991;&#26412;&#36716;&#21270;&#20026;&#26222;&#36890;&#29289;&#20307;&#30340;3D&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21019;&#24314;&#39640;&#36136;&#37327;&#19988;&#21487;&#21160;&#30011;&#30340;3D&#35282;&#33394;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;3D&#35282;&#33394;&#65292;DreamWaltz&#25552;&#20986;&#20102;&#19977;&#32500;&#19968;&#33268;&#30340;&#36974;&#25377;&#24863;&#30693;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26469;&#20248;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#35268;&#33539;&#23039;&#21183;&#12290;&#23427;&#36890;&#36807;&#19977;&#32500;&#24863;&#30693;&#30340;&#39592;&#26550;&#35843;&#33410;&#25552;&#20379;&#35270;&#35282;&#23545;&#40784;&#30340;&#30417;&#30563;&#65292;&#20351;&#24471;&#22797;&#26434;&#30340;&#35282;&#33394;&#29983;&#25104;&#19981;&#20250;&#20135;&#29983;&#20266;&#24433;&#21644;&#22810;&#20010;&#38754;&#12290;&#23545;&#20110;&#21160;&#30011;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#21160;&#30011;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35282;&#33394;&#34920;&#31034;&#65292;&#21487;&#20197;&#23558;&#20219;&#24847;&#23039;&#21183;&#26144;&#23556;&#21040;&#35268;&#33539;&#23039;&#21183;&#34920;&#31034;&#20013;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DreamWaltz&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#31283;&#20581;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#24418;&#29366;&#21644;&#22806;&#35266;&#20197;&#21450;&#21160;&#30011;&#30340;3D&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable and generalizable avatar representation which could map arbitrary poses to the canonical pose representation. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Segment Training&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27835;&#27861;&#20801;&#35768;&#20351;&#29992;&#24658;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#26469;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#34987;&#35780;&#20272;&#22312;&#20960;&#39033;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#22522;&#20934;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12322</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#24418;&#27573;&#35757;&#32451;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Large Graph Property Prediction via Graph Segment Training. (arXiv:2305.12322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Segment Training&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27835;&#27861;&#20801;&#35768;&#20351;&#29992;&#24658;&#23450;&#30340;&#20869;&#23384;&#28040;&#32791;&#26469;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#34987;&#35780;&#20272;&#22312;&#20960;&#39033;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#22522;&#20934;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39044;&#27979;&#22823;&#22411;&#22270;&#30340;&#23646;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#39044;&#27979;&#37117;&#38656;&#35201;&#25972;&#20010;&#22270;&#30340;&#30693;&#35782;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#29992;&#30340;&#20869;&#23384;&#37327;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Segment Training&#65288;GST&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#27835;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#24658;&#23450;&#30340;&#20869;&#23384;&#21344;&#29992;&#37327;&#26469;&#23398;&#20064;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#12290; GST&#39318;&#20808;&#23558;&#22823;&#22411;&#22270;&#24418;&#21010;&#20998;&#20026;&#27573;&#65292;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#36845;&#20195;&#20013;&#20165;&#23545;&#20960;&#20010;&#27573;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21382;&#21490;&#23884;&#20837;&#34920;&#26469;&#25913;&#36827;GST&#33539;&#20363;&#65292;&#20197;&#26377;&#25928;&#22320;&#33719;&#21462;&#26410;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#30340;&#27573;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#20943;&#36731;&#21382;&#21490;&#23884;&#20837;&#30340;&#36807;&#26102;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#27979;&#22836;&#20197;&#20462;&#22797;&#36755;&#20837;&#20998;&#24067;&#31227;&#20301;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#8220;Stale Embedding Dropout&#8221;&#26469;&#22312;&#35757;&#32451;&#26399;&#38388;&#38477;&#20302;&#20559;&#24046;&#65292;&#20174;&#32780;&#20002;&#24323;&#19968;&#20123;&#36807;&#26102;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#22270;&#24418;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#21270;&#21512;&#29289;&#20998;&#31867;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;GST-EFD&#65288;&#21253;&#21547;&#25152;&#26377;&#25216;&#26415;&#65289;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#65292;&#23384;&#20648;&#22120;&#28040;&#32791;&#21644;&#36816;&#34892;&#26102;&#25928;&#29575;&#26041;&#38754;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to predict properties of large graphs is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST-EFD (with all the techniques 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2305.11650</link><description>&lt;p&gt;
&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBMs&#65289;&#20026;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;EBMs &#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#29992;&#20110;&#21487;&#25193;&#23637; EBM &#35757;&#32451;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#65288;DSM&#65289;&#26041;&#27861;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#21040;&#8220;&#22024;&#26434;&#8221;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65306;&#65288;&#20266;&#65289;Gibbs&#37319;&#26679;&#19982;&#21160;&#37327;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#32463;&#36807;DSM&#35757;&#32451;&#33391;&#22909;&#30340;&#8220;&#22024;&#26434;&#8221;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22522;&#30784;&#8220;&#24178;&#20928;&#8221;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#30456;&#20851;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; NODE-ImgNet&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992; PDE &#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#28982;&#22320;&#36991;&#20813;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20266;&#20687;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.11049</link><description>&lt;p&gt;
NODE-ImgNet: &#19968;&#31181;&#20351;&#29992; PDE &#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NODE-ImgNet: a PDE-informed effective and robust model for image denoising. (arXiv:2305.11049v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; NODE-ImgNet&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992; PDE &#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#28982;&#22320;&#36991;&#20813;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20266;&#20687;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20256;&#32479;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#26694;&#26550; NODE-ImgNet&#65292;&#23558;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODEs&#65289;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22359;&#30456;&#32467;&#21512;&#12290; NODE-ImgNet &#26159;&#26412;&#36136;&#19978;&#30340; PDE &#27169;&#22411;&#65292;&#21160;&#24577;&#31995;&#32479;&#26159;&#38544;&#24335;&#23398;&#20064;&#30340;&#65292;&#32780;&#19981;&#26159;&#26174;&#24335;&#25351;&#23450; PDE&#65292;&#33258;&#28982;&#22320;&#36991;&#20813;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20266;&#20687;&#30340;&#20856;&#22411;&#38382;&#39064;&#12290;&#36890;&#36807;&#35843;&#29992;&#36825;&#31181; NODE &#32467;&#26500;&#65292;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#30340;&#36830;&#32493;&#21464;&#20307;&#65292;&#32487;&#25215;&#20854;&#22312;&#22270;&#20687;&#21435;&#22122;&#20013;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#65292;&#21253;&#25324;&#21463;&#39640;&#26031;&#22122;&#22768;&#24178;&#25200;&#30340;&#28784;&#24230;&#21644;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#65292;&#20197;&#21450;&#30495;&#23454;&#22122;&#22768;&#22270;&#20687;&#23398;&#20064;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;&#23567;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the traditional partial differential equation (PDE) approach for image denoising, we propose a novel neural network architecture, referred as NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE model, where the dynamic system is learned implicitly without the explicit specification of the PDE. This naturally circumvents the typical issues associated with introducing artifacts during the learning process. By invoking such a NODE structure, which can also be viewed as a continuous variant of a residual network (ResNet) and inherits its advantage in image denoising, our model achieves enhanced accuracy and parameter efficiency. In particular, our model exhibits consistent effectiveness in different scenarios, including denoising gray and color images perturbed by Gaussian noise, as well as real-noisy images, and demonstrates superiority in learning from small image datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.05065</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20351;&#29992;&#22823;&#35268;&#27169;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#35757;&#32451;&#21452;&#32534;&#30721;&#27169;&#22411;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#39033;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26469;&#36873;&#25321;&#32473;&#23450;&#26597;&#35810;&#23884;&#20837;&#30340;&#39030;&#37096;&#20505;&#36873;&#39033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#33539;&#20363;&#65306;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#35299;&#30721;&#30446;&#26631;&#20505;&#36873;&#39033;&#30340;&#26631;&#35782;&#31526;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#20026;&#27599;&#20010;&#39033;&#30446;&#20998;&#37197;&#38543;&#26426;&#29983;&#25104;&#30340;&#21407;&#23376;ID&#65292;&#32780;&#26159;&#29983;&#25104;&#35821;&#20041;ID&#65306;&#27599;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20803;&#32452;&#32534;&#30721;&#35789;&#65292;&#23427;&#20316;&#20026;&#20854;&#21807;&#19968;&#26631;&#35782;&#31526;&#12290;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;RQ-VAE&#30340;&#20998;&#23618;&#26041;&#27861;&#29983;&#25104;&#36825;&#20123;&#32534;&#30721;&#35789;&#12290;&#19968;&#26086;&#25105;&#20204;&#23545;&#25152;&#26377;&#39033;&#30446;&#37117;&#26377;&#20102;&#35821;&#20041;ID&#65292;&#23601;&#20250;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;ID&#12290;&#30001;&#20110;&#36825;&#20010;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#30452;&#25509;&#39044;&#27979;&#26631;&#35782;&#19979;&#19968;&#20010;&#39033;&#30340;&#32534;&#30721;&#35789;&#20803;&#32452;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.02386</link><description>&lt;p&gt;
&#29992;Transformer&#36924;&#36817;CKY&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#30452;&#25509;&#39044;&#27979;&#21477;&#23376;&#30340;&#35299;&#26512;&#65292;&#36991;&#20813;&#20102;CKY&#31639;&#27861;&#23545;&#21477;&#23376;&#38271;&#24230;&#30340;&#19977;&#27425;&#20381;&#36182;&#12290;&#22312;&#26631;&#20934;&#30340;&#32452;&#25104;&#21477;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20351;&#29992;CKY&#30340;&#21487;&#27604;&#20998;&#26512;&#22120;&#21462;&#24471;&#20102;&#31454;&#20105;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;&#38543;&#26426;PCFG&#19979;&#36827;&#34892;&#35299;&#26512;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35821;&#27861;&#21464;&#24471;&#26356;&#21152;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;Transformer&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;CKY&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#65292;&#32467;&#21512;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20110;&#22270;&#34920;&#34920;&#31034;&#30340;&#26799;&#24230;&#26469;&#39044;&#27979;&#35299;&#26512;&#65292;&#31867;&#27604;&#20110;CKY&#31639;&#27861;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#19968;&#20010;&#20998;&#21306;&#20989;&#25968;&#21464;&#20307;&#30340;&#23376;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a parse and thus avoid the CKY algorithm's cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under random PCFGs. Here we find that performance declines as the grammar becomes more ambiguous, suggesting that the transformer is not fully capturing the CKY computation. However, we also find that incorporating additional inductive bias is helpful, and we propose a novel approach that makes use of gradients with respect to chart representations in predicting the parse, in analogy with the CKY algorithm being the subgradient of a partition function variant with respect to the chart.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02305</link><description>&lt;p&gt;
&#26657;&#20934;&#21270;&#35299;&#37322;&#65306;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21644;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#39046;&#22495;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21487;&#33021;&#23548;&#33268;&#28389;&#29992;&#25110;&#19981;&#20351;&#29992;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#21019;&#24314;&#21487;&#20197;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20010;&#21035;&#39044;&#27979;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#20294;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#26657;&#20934;&#21270;&#35299;&#37322;(Calibrated Explanations&#65292;CE)&#65292;&#23427;&#22522;&#20110; Venn-Abers&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#30340;&#21516;&#26102;&#26657;&#20934;&#24213;&#23618;&#27169;&#22411;&#12290;CE&#19981;&#20165;&#25552;&#20379;&#24555;&#36895;&#12289;&#21487;&#38752;&#12289;&#31283;&#23450;&#21644;&#24378;&#20581;&#30340;&#35299;&#37322;&#65292;&#36824;&#25552;&#20379;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#26465;&#20214;&#35268;&#21017;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102; ZRG &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#26679;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#27491;&#20132;&#22270;&#20687;&#25340;&#25509;&#12289;&#23545;&#24212; DSM&#12289;3D &#23627;&#39030;&#26694;&#26550;&#21644;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#30340;&#28857;&#20113;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20303;&#23429;&#23627;&#39030;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#22330;&#26223;&#29702;&#35299;&#65292;&#22914;&#23627;&#39030;&#36718;&#24275;&#25552;&#21462;&#12289;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#21644;&#24179;&#38754;&#23627;&#39030;&#32467;&#26500;&#25552;&#21462;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.13219</link><description>&lt;p&gt;
ZRG: &#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#19977;&#32500;&#20960;&#20309;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning. (arXiv:2304.13219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102; ZRG &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#26679;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#27491;&#20132;&#22270;&#20687;&#25340;&#25509;&#12289;&#23545;&#24212; DSM&#12289;3D &#23627;&#39030;&#26694;&#26550;&#21644;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#30340;&#28857;&#20113;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20303;&#23429;&#23627;&#39030;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#22330;&#26223;&#29702;&#35299;&#65292;&#22914;&#23627;&#39030;&#36718;&#24275;&#25552;&#21462;&#12289;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#21644;&#24179;&#38754;&#23627;&#39030;&#32467;&#26500;&#25552;&#21462;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Zeitview Rooftop Geometry (ZRG) &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#39640;&#20998;&#36776;&#29575;&#20303;&#23429;&#23627;&#39030;&#27491;&#20132;&#22270;&#20687;&#25340;&#25509;&#12289;&#23545;&#24212;&#25968;&#23383;&#34920;&#38754;&#27169;&#22411; (DSM)&#12289;3D &#23627;&#39030;&#26694;&#26550;&#21644;&#22810;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#30340;&#28857;&#20113;&#29992;&#20110;&#20303;&#23429;&#23627;&#39030;&#20960;&#20309;&#21644;&#22330;&#26223;&#29702;&#35299;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#27492;&#25968;&#25454;&#38598;&#35299;&#38145;&#30340;&#20247;&#22810;&#24212;&#29992;&#65292;&#24182;&#20026;&#23627;&#39030;&#36718;&#24275;&#25552;&#21462;&#12289;&#21333;&#30446;&#39640;&#24230;&#20272;&#35745;&#21644;&#24179;&#38754;&#23627;&#39030;&#32467;&#26500;&#25552;&#21462;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset. ZRG contains thousands of samples of high resolution orthomosaics of aerial imagery of residential rooftops with corresponding digital surface models (DSM), 3D rooftop wireframes, and multiview imagery generated point clouds for the purpose of residential rooftop geometry and scene understanding. We perform thorough benchmarks to illustrate the numerous applications unlocked by this dataset and provide baselines for the tasks of roof outline extraction, monocular height estimation, and planar roof structure extraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12958</link><description>&lt;p&gt;
&#23545;&#39640;&#27700;&#24179;&#26426;&#22120;&#20154;&#35299;&#37322;&#20013;&#30340;&#22870;&#21169;&#20998;&#35299;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20154;&#31867;&#35299;&#37322;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#38590;&#20197;&#29702;&#35299;&#30340;&#33258;&#20307;&#24863;&#29366;&#24577;&#12289;&#22810;&#21464;&#30340;&#20013;&#38388;&#30446;&#26631;&#21644;&#20854;&#32467;&#26524;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#19968;&#27493;&#35299;&#37322;&#21487;&#33021;&#26159;&#27169;&#31946;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#22312;&#27599;&#20010;&#36716;&#25442;&#26102;&#32771;&#34385;&#21040;&#20195;&#29702;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26144;&#23556;&#21040;&#20219;&#21153;&#29305;&#23450;&#22522;&#20803;&#30340;&#25277;&#35937;&#21160;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#31227;&#21160;&#23618;&#38754;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#22870;&#21169;&#20998;&#35299;&#65288;RD&#65289;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#20219;&#21153;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#26126;&#30830;&#30340;&#39640;&#23618;&#27425;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#26469;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;RD&#35299;&#37322;&#30340;&#36755;&#20986;&#25991;&#29289;&#20013;&#30340;&#21487;&#35270;&#21644;&#25991;&#26412;&#35299;&#37322;&#65292;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#24120;&#35265;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#22522;&#20934;&#20013;155&#20010;MDP&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;Q&#20540;&#26368;&#39640;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#21017;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09853</link><description>&lt;p&gt;
&#29992;&#26377;&#25928;&#30340;&#35270;&#37326;&#36830;&#25509;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#24120;&#35265;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#22522;&#20934;&#20013;155&#20010;MDP&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;Q&#20540;&#26368;&#39640;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#21017;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20854;&#20182;&#29615;&#22659;&#20013;&#21364;&#22833;&#36133;&#24471;&#38750;&#24120;&#20005;&#37325;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#24212;&#35813;&#33021;&#22815;&#35299;&#37322;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#20379;&#39044;&#27979;&#23454;&#38469;&#24615;&#33021;&#30340;&#30028;&#38480;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;&#29702;&#35770;&#36824;&#27809;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;155&#20010;MDP&#30340;&#26032;&#25968;&#25454;&#38598;BRIDGE&#65292;&#23558;&#26631;&#20934;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#20043;&#21069;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20808;&#21069;&#30028;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#24847;&#24819;&#19981;&#21040;&#30340;&#24615;&#36136;&#65306;&#24403;&#26368;&#39640;Q&#20540;&#30340;&#21160;&#20316;&#22312;&#38543;&#26426;&#31574;&#30053;&#19979;&#30340;Q&#20540;&#20063;&#26159;&#26368;&#39640;&#30340;&#26102;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24448;&#24448;&#20250;&#25104;&#21151;&#65307;&#21453;&#20043;&#65292;&#22833;&#36133;&#30340;&#21487;&#33021;&#24615;&#36739;&#39640;&#12290;&#22522;&#20110;&#36825;&#19968;&#24615;&#36136;&#65292;&#25105;&#20204;&#23558;&#20854;&#27010;&#25324;&#20026;&#19968;&#20010;&#26032;&#30340;MDP&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#31216;&#20026;&#26377;&#25928;&#30340;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.08897</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#25913;&#36827;&#30828;&#32422;&#26463;&#30340;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#30828;&#32422;&#26463;&#20445;&#35777;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#26368;&#26377;&#21069;&#36884;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#21521;&#12290;&#23427;&#21482;&#38656;&#35201;&#22312;&#29615;&#22659;&#29305;&#23450;&#30340;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#19978;&#39044;&#20808;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#27169;&#22411;&#65288;&#21363;&#26893;&#29289;&#65292;&#24178;&#25200;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#20197;&#21450;&#26410;&#21253;&#25324;&#22312;&#26893;&#29289;&#27169;&#22411;&#20013;&#30340;&#29366;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411; - &#20363;&#22914;&#38656;&#27714;&#65292;&#22825;&#27668;&#21644;&#20215;&#26684;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#21487;&#20943;&#23569;&#39033;&#30446;&#29305;&#23450;&#30340;&#21069;&#26399;&#21644;&#25345;&#32493;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#20173;&#21487;&#20197;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#22522;&#30784;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#20351;&#24314;&#27169;&#20559;&#24046;&#26368;&#23567;&#21270;&#65288;&#26080;&#22522;&#20110;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20165;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26377;&#26102;&#20063;&#19981;&#24635;&#26159;&#23481;&#26131;&#25552;&#20379;&#20934;&#30830;&#30340;&#20808;&#39564;&#65288;&#20363;&#22914;&#33021;&#37327;&#24179;&#34913;&#32422;&#26463;&#38656;&#35201;&#35814;&#32454;&#30830;&#23450;&#25152;&#26377;&#33021;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36827;&#23637;&#65306;&#65288;I&#65289;&#23558;Optlayer&#21644;SafeFallback&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21629;&#21517;&#20026;O
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.06813</link><description>&lt;p&gt;
&#27169;&#22411;&#29305;&#23450;&#35270;&#35282;&#19979;&#30340;&#32479;&#19968;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#19981;&#23646;&#20110;&#35757;&#32451;&#20998;&#24067;&#24182;&#19981;&#21487;&#38752;&#39044;&#27979;&#30340;&#27979;&#35797;&#26679;&#20363;&#12290;&#34429;&#28982;&#24050;&#26377;&#22823;&#37327;&#30456;&#20851;&#24037;&#20316;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#21482;&#20851;&#27880;&#26469;&#33258;&#35821;&#20041;&#36716;&#25442;&#65288;&#22914;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#65289;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21487;&#33021;&#30340;&#21407;&#22240;&#65288;&#22914;&#21327;&#21464;&#37327;&#36716;&#25442;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#30740;&#31350;OOD&#26816;&#27979;&#12290;&#25105;&#20204;&#24314;&#35758;&#19981;&#26159;&#26816;&#27979;&#29305;&#23450;&#21407;&#22240;&#23548;&#33268;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#26159;&#26816;&#27979;&#24050;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#22120;&#65289;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26159;&#21542;&#24212;&#35813;&#26816;&#27979;&#21644;&#25298;&#32477;&#27979;&#35797;&#20363;&#23376;&#26159;&#8220;&#27169;&#22411;&#29305;&#23450;&#8221;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#30001;&#35821;&#20041;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21464;&#21270;&#24341;&#36215;&#30340;OOD&#20363;&#23376;&#30340;&#26816;&#27979;&#65292;&#24182;&#23494;&#20999;&#20851;&#27880;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.16604</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26159;&#26681;&#25454;&#21253;&#21547;&#21442;&#32771;&#22270;&#20687;&#21644;&#25551;&#36848;&#25152;&#38656;&#26356;&#25913;&#30340;&#20462;&#25913;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;&#26469;&#25628;&#32034;&#30446;&#26631;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#30340;&#26041;&#27861;&#23398;&#20064;&#20174;&#65288;&#21442;&#32771;&#22270;&#20687;&#65292;&#20462;&#25913;&#25991;&#26412;&#65289;&#23545;&#21040;&#22270;&#20687;&#23884;&#20837;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#22823;&#22411;&#22270;&#20687;&#35821;&#26009;&#24211;&#36827;&#34892;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#35757;&#32451;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#36825;&#31181;&#21453;&#21521;&#26597;&#35810;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#12290;&#20026;&#20102;&#32534;&#30721;&#21452;&#21521;&#26597;&#35810;&#65292;&#25105;&#20204;&#22312;&#20462;&#25913;&#25991;&#26412;&#21069;&#38754;&#28155;&#21152;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20196;&#29260;&#65292;&#25351;&#23450;&#26597;&#35810;&#30340;&#26041;&#21521;&#65292;&#28982;&#21518;&#24494;&#35843;&#25991;&#26412;&#23884;&#20837;&#27169;&#22359;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#27809;&#26377;&#23545;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20854;&#20182;&#26356;&#25913;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21452;&#21521;&#35757;&#32451;&#22312;&#25552;&#39640;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;&#20219;&#20309;&#27969;&#24418;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#20219;&#24847;&#32676;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26377;&#25928;&#26679;&#26412;&#25968;&#37327;&#25110;&#38477;&#20302;&#20102;&#27969;&#24418;&#30340;&#32500;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.14269</link><description>&lt;p&gt;
&#20869;&#23481;&#19981;&#21464;&#24615;&#23545;&#27969;&#24418;&#26680;&#22238;&#24402;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;
The Exact Sample Complexity Gain from Invariances for Kernel Regression on Manifolds. (arXiv:2303.14269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;&#20219;&#20309;&#27969;&#24418;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#20219;&#24847;&#32676;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26377;&#25928;&#26679;&#26412;&#25968;&#37327;&#25110;&#38477;&#20302;&#20102;&#27969;&#24418;&#30340;&#32500;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#23558;&#20869;&#23481;&#19981;&#21464;&#24615;&#32534;&#30721;&#36827;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#23545;&#20869;&#23481;&#19981;&#21464;&#24615;&#22914;&#20309;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#29702;&#35770;&#32467;&#26524;&#36827;&#34892;&#20102;&#32454;&#21270;&#21644;&#25512;&#24191;&#65292;&#29305;&#21035;&#22320;&#65292;&#22312;&#20219;&#20309;&#27969;&#24418;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#22312;&#27969;&#24418;&#19978;&#20219;&#24847;&#32676;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#65288;&#20960;&#20046;&#65289;&#20219;&#20309;&#32676;&#20316;&#29992;&#65292;&#29978;&#33267;&#26159;&#27491;&#32500;&#24230;&#30340;&#32676;&#12290;&#23545;&#20110;&#26377;&#38480;&#32676;&#65292;&#22686;&#30410;&#36890;&#36807;&#23558;&#8220;&#26377;&#25928;&#8221;&#26679;&#26412;&#25968;&#37327;&#25193;&#22823;&#21040;&#32676;&#30340;&#22823;&#23567;&#26469;&#23454;&#29616;&#12290;&#23545;&#20110;&#27491;&#32500;&#24230;&#30340;&#32676;&#65292;&#22686;&#30410;&#34920;&#29616;&#20026;&#38477;&#20302;&#27969;&#24418;&#30340;&#32500;&#25968;&#65292;&#21516;&#26102;&#36824;&#19982;&#21830;&#31354;&#38388;&#20307;&#31215;&#25104;&#27604;&#20363;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20174;&#24494;&#20998;&#20960;&#20309;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#19982;&#20351;&#29992;&#19981;&#21464;&#22810;&#39033;&#24335;&#30340;&#26356;&#24120;&#35265;&#31574;&#30053;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#22312;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#23398;&#20064;&#20013;&#30340;&#26032;&#20960;&#20309;&#35270;&#35282;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, encoding invariances into models helps sample complexity. In this work, we tighten and generalize theoretical results on how invariances improve sample complexity. In particular, we provide minimax optimal rates for kernel ridge regression on any manifold, with a target function that is invariant to an arbitrary group action on the manifold. Our results hold for (almost) any group action, even groups of positive dimension. For a finite group, the gain increases the "effective" number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. Hence, this new geometric viewpoint on learning with invariances may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12513</link><description>&lt;p&gt;
BERT&#26159;&#21542;&#30450;&#30446;&#65311;&#25506;&#32034;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20154;&#20351;&#29992;&#35270;&#35273;&#24819;&#35937;&#26469;&#29702;&#35299;&#21644;&#25512;&#29702;&#35821;&#35328;&#65292;&#20294;&#26159;&#20687;BERT&#36825;&#26679;&#30340;&#27169;&#22411;&#20351;&#29992;&#22312;&#20165;&#21253;&#25324;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#25512;&#29702;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22312;&#28041;&#21450;&#38544;&#21547;&#35270;&#35273;&#25512;&#29702;&#30340;&#20165;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#37325;&#28857;&#26159;&#38646;&#26679;&#26412;&#25506;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#25506;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#27169;&#22411;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65288;VLU&#65289;&#20219;&#21153;&#65292;&#20197;&#21450;&#21508;&#31181;&#38750;&#35270;&#35273;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#29992;&#20110;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#65292;Stroop probing&#65292;&#29992;&#20110;&#23558;&#20687;CLIP&#36825;&#26679;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#20165;&#25991;&#26412;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;BERT&#27169;&#22411;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22836;&#37027;&#26679;&#30340;&#39044;&#27979;&#22836;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SOTA&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;VLU&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;NLU&#20219;&#21153;&#19978;&#19981;&#21450;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lendin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#32479;&#19968;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#29992;&#20110;&#23558;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.08797</link><description>&lt;p&gt;
&#38543;&#26426;&#25554;&#20540;&#65306;&#27969;&#21644;&#25193;&#25955;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Stochastic Interpolants: A Unifying Framework for Flows and Diffusions. (arXiv:2303.08797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#32479;&#19968;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#29992;&#20110;&#23558;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#24314;&#31435;&#22312;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#26159;&#22522;&#20110;Albergo&#65286;Vanden-Eijnden&#65288;2023&#65289;&#25552;&#20986;&#30340;&#65292;&#22312;&#27969;&#21644;&#25193;&#25955;&#26041;&#27861;&#19978;&#23454;&#29616;&#32479;&#19968;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#26500;&#24314;&#19968;&#31867;&#24191;&#27867;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#65292;&#20854;&#26102;&#38388;&#20381;&#36182;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#31934;&#30830;&#22320;&#36830;&#25509;&#20004;&#20010;&#20219;&#24847;&#30340;&#23494;&#24230;&#12290;&#36825;&#20123;&#8220;&#38543;&#26426;&#25554;&#20540;&#22120;&#8221;&#26159;&#36890;&#36807;&#23558;&#26469;&#33258;&#20004;&#20010;&#23494;&#24230;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182;&#28508;&#22312;&#21464;&#37327;&#30456;&#32467;&#21512;&#26500;&#24314;&#30340;&#65292;&#24182;&#19988;&#26500;&#36896;&#30340;&#20855;&#20307;&#32454;&#33410;&#21487;&#20197;&#28789;&#27963;&#22320;&#22609;&#36896;&#23548;&#33268;&#30340;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#25554;&#20540;&#22120;&#30340;&#26102;&#38388;&#20381;&#36182;&#23494;&#24230;&#28385;&#36275;&#19968;&#38454;&#36755;&#36816;&#26041;&#31243;&#20197;&#21450;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#35843;&#25193;&#25955;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;Fokker-Planck&#26041;&#31243;&#26063;; &#22312;&#32771;&#34385;&#21333;&#20010;&#26679;&#26412;&#30340;&#26102;&#38388;&#28436;&#21270;&#26102;&#65292;&#36825;&#20010;&#35266;&#28857;&#31435;&#21363;&#23548;&#33268;&#20102;&#22522;&#20110;&#27010;&#29575;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a class of generative models based on the stochastic interpolant framework proposed in Albergo &amp; Vanden-Eijnden (2023) that unifies flow-based and diffusion-based methods. We first show how to construct a broad class of continuous-time stochastic processes whose time-dependent probability density function bridges two arbitrary densities exactly in finite time. These `stochastic interpolants' are built by combining data from the two densities with an additional latent variable, and the specific details of the construction can be leveraged to shape the resulting time-dependent density in a flexible way. We then show that the time-dependent density of the stochastic interpolant satisfies a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion; upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on proba
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05445</link><description>&lt;p&gt;
&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65306;&#22797;&#26434;&#32593;&#32476;&#19978;&#24322;&#26500;&#36172;&#21338;&#26426;&#30340;&#39640;&#25928;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#19978;&#30340;&#24322;&#26500;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#20915;&#31574;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22914;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#21644;&#26080;&#32447;&#32593;&#32476;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22810;&#20195;&#29702;&#30340;&#22330;&#26223;&#65292;&#27599;&#20010;&#20195;&#29702;&#35299;&#20915;&#33258;&#24049;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#36172;&#21338;&#26426;&#25317;&#26377;&#19981;&#21516;&#30340;&#33218;&#12290;&#20182;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#36890;&#20449;&#21327;&#35758;&#21327;&#20316;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#38598;&#20307;&#36951;&#25022;&#12290;&#20808;&#21069;&#20851;&#20110;&#27492;&#38382;&#39064;&#30340;&#25991;&#29486;&#21482;&#32771;&#34385;&#20102;&#33218;&#30340;&#24322;&#36136;&#24615;&#21644;&#32593;&#32476;&#21270;&#20195;&#29702;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21516;&#26102;&#21253;&#21547;&#36825;&#20004;&#20010;&#29305;&#24615;&#30340;&#35774;&#32622;&#12290;&#38024;&#23545;&#36825;&#19968;&#26032;&#39062;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;&#27867;&#27946;&#21327;&#35758;&#32467;&#21512;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#31574;&#30053;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#36731;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#27867;&#27946;&#36896;&#25104;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#35758;&#65292;&#31216;&#20026;&#24102;&#21560;&#25910;&#30340;&#27867;&#27946;&#65288;FwA&#65289;&#12290;&#25105;&#20204;&#23545;&#30001;&#27492;&#20135;&#29983;&#30340;&#36951;&#25022;&#19978;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#21327;&#35758;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.04772</link><description>&lt;p&gt;
&#22810;&#32423;&#25193;&#25955;&#65306;&#22270;&#20687;&#29983;&#25104;&#30340;&#26080;&#38480;&#32500;&#24230;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#36817;&#24180;&#26469;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#22312;&#26377;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#34920;&#36848;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#23610;&#23544;&#30340;&#24352;&#37327;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#24320;&#21457;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#25968;&#25454;&#24314;&#27169;&#20026;&#25903;&#25745;&#22312;&#30697;&#24418;&#22495;&#19978;&#30340;&#20989;&#25968;&#12290;&#38500;&#20102;&#36861;&#27714;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#22270;&#20687;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21019;&#24314;&#19968;&#20010;&#33391;&#22909;&#23450;&#20041;&#30340;&#26080;&#38480;&#32500;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#19968;&#33268;&#22320;&#31163;&#25955;&#21270;&#23427;&#12290;&#25105;&#20204;&#24076;&#26395;&#33719;&#24471;&#33021;&#22815;&#27178;&#36328;&#19981;&#21516;&#20998;&#36776;&#29575;&#32423;&#21035;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20811;&#26381;&#24403;&#21069;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#21069;&#21521;&#36807;&#31243;&#20197;&#30830;&#20445;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#28508;&#22312;&#20998;&#24067;&#26159;&#33391;&#22909;&#23450;&#20041;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#32423;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10798</link><description>&lt;p&gt;
&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#20197;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;: &#19968;&#31181;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#21152;&#22797;&#26434;&#65292;&#32511;&#33394;AI&#24050;&#32463;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#23545;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#35009;&#21098;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#35009;&#21098;&#26041;&#26696;&#36890;&#24120;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#21644;&#24494;&#35843;&#25110;&#37325;&#22797;&#35745;&#31639;&#21160;&#24577;&#35009;&#21098;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#23376;&#32593;&#32476;&#65292;&#26082;&#33021;&#26368;&#23567;&#21270;&#33021;&#32791;&#65292;&#21448;&#33021;&#22312;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#19982;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#20445;&#25345;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#26696;&#20197;&#32511;&#33394;&#20026;&#23548;&#21521;&#65292;&#22240;&#20026;&#23427;&#20165;&#38656;&#35201;&#21160;&#24577;&#35009;&#21098;&#26041;&#27861;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#26469;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#12290;&#35813;&#26041;&#26696;&#30001;&#19968;&#20010;&#20108;&#36827;&#21046;&#38376;&#25511;&#27169;&#22359;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#32452;&#25104;&#65292;&#20197;&#21457;&#29616;&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#31232;&#30095;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#31561;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35009;&#21098;&#21644;&#36716;&#25442;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07425</link><description>&lt;p&gt;
&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#20195;&#29702;&#25353;&#29031;&#31616;&#21333;&#30340;&#22810;&#33218;&#21163;&#21290;&#21327;&#35758;&#20849;&#21516;&#34892;&#21160;&#12290;&#20195;&#29702;&#20197;&#39034;&#24207;&#26041;&#24335;&#21040;&#36798;&#65292;&#36873;&#25321;&#27494;&#22120;&#24182;&#25509;&#25910;&#30456;&#20851;&#22870;&#21169;&#12290;&#27599;&#20010;&#20195;&#29702;&#35266;&#23519;&#20808;&#21069;&#20195;&#29702;&#30340;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#65288;&#27494;&#22120;&#21644;&#22870;&#21169;&#65289;&#65292;&#19981;&#23384;&#22312;&#31169;&#26377;&#20449;&#21495;&#12290;&#23613;&#31649;&#20195;&#29702;&#20849;&#21516;&#38754;&#20020;&#24320;&#21457;&#21644;&#21033;&#29992;&#30340;&#25506;&#32034;&#25240;&#34935;&#65292;&#20294;&#27599;&#20010;&#20195;&#29702;&#20154;&#37117;&#26159;&#19968;&#35265;&#38047;&#24773;&#30340;&#65292;&#26080;&#38656;&#32771;&#34385;&#25506;&#32034;&#12290;&#25105;&#20204;&#20801;&#35768;&#19968;&#31995;&#21015;&#19982;&#65288;&#21442;&#25968;&#21270;&#65289;&#32622;&#20449;&#21306;&#38388;&#19968;&#33268;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#21253;&#25324;&#8220;&#26080;&#20559;&#8221;&#34892;&#20026;&#21644;&#21508;&#31181;&#34892;&#20026;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20123;&#34892;&#20026;&#30340;&#26497;&#31471;&#29256;&#26412;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#21163;&#21290;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#26356;&#28201;&#21644;&#30340;&#29256;&#26412;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#25506;&#32034;&#22833;&#36133;&#65292;&#22240;&#27492;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#8220;&#28201;&#21644;&#20048;&#35266;&#8221;&#30340;&#20195;&#29702;&#25552;&#20379;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25506;&#32034;&#28608;&#21169;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65306;&#27494;&#22120;&#25506;&#32034;&#26159;&#22266;&#26377;&#20110;&#21163;&#21290;&#38382;&#39064;&#30340;&#65292;&#21482;&#21463;&#24403;&#21069;&#20195;&#29702;&#30340;&#34892;&#21160;&#24433;&#21709;&#65292;&#32780;&#31038;&#20132;&#25506;&#32034;&#26159;&#30001;&#20808;&#21069;&#20195;&#29702;&#34892;&#20026;&#39537;&#21160;&#30340;&#65292;&#22240;&#27492;&#26377;&#21033;&#20110;&#26410;&#26469;&#20195;&#29702;&#12290;&#30001;&#20110;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#38480;&#21046;&#20102;&#31038;&#20132;&#25506;&#32034;&#65292;&#36825;&#31181;&#26435;&#34913;&#34987;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
&lt;/p&gt;</description></item><item><title>&#35780;&#20998;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#22833;&#36133;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#30340;&#26657;&#20934;&#21327;&#35758;&#21487;&#20197;&#25552;&#39640;&#29616;&#26377;&#20462;&#21098;&#31639;&#27861;&#22312;&#35813;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06960</link><description>&lt;p&gt;
&#25968;&#25454;&#20462;&#21098;&#21644;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65306;&#22522;&#20110;&#35780;&#20998;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Data pruning and neural scaling laws: fundamental limitations of score-based algorithms. (arXiv:2302.06960v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06960
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20998;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#22833;&#36133;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#30340;&#26657;&#20934;&#21327;&#35758;&#21487;&#20197;&#25552;&#39640;&#29616;&#26377;&#20462;&#21098;&#31639;&#27861;&#22312;&#35813;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#24120;&#29992;&#20110;&#20943;&#23569;&#20248;&#21270;&#36807;&#31243;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#25968;&#25454;&#20462;&#21098;&#20173;&#28982;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#20248;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;&#20102;&#19981;&#21040;&#25968;&#25454;&#30340;30&#65285;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#21387;&#32553;&#21306;&#22495;&#26368;&#36817;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#25968;&#25454;&#20462;&#21098;&#22312;&#25552;&#39640;&#25152;&#35859;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#20013;&#30340;&#20316;&#29992;&#65307;&#22312;[Sorscher et al.]&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#25165;&#33021;&#20987;&#36133;&#26679;&#26412;&#21183;&#24459;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#35780;&#20998;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#38469;&#19978;&#23637;&#31034;&#20102;&#20026;&#20160;&#20040;&#36825;&#26679;&#30340;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#22833;&#36133;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#25454;&#20462;&#21098;&#30340;&#8220;&#27809;&#26377;&#20813;&#36153;&#21320;&#39184;&#8221;&#23450;&#29702;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#25552;&#20986;&#20102;&#26657;&#20934;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;&#20462;&#21098;&#31639;&#27861;&#22312;&#39640;&#21387;&#32553;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of $30\%$ or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; in [Sorscher et al.], the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law.  In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate ``No Free Lunch" theorems for data pruning and present calibration protocols that enhance the performance of existing pruning algorithms in this high compression regime using randomization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.03421</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#25512;&#23548;&#65288;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;PAC-Bayes&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22266;&#23450;&#26679;&#26412;&#22823;&#23567;&#26041;&#24335;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#23548;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#12290;&#19982;&#22823;&#22810;&#25968;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#20219;&#20309;&#26102;&#38388;&#37117;&#26377;&#25928;&#30340;&#65288;&#21363;&#26102;&#38388;&#22343;&#21248;&#30340;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#36866;&#29992;&#20110;&#25152;&#26377;&#20572;&#27490;&#26102;&#38388;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22266;&#23450;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25353;&#29031;&#20197;&#19979;&#39034;&#24207;&#32467;&#21512;&#20102;&#22235;&#31181;&#24037;&#20855;&#65306;&#65288;a&#65289;&#38750;&#36127;&#36229;&#39532;&#19969;&#26684;&#23572;&#25110;&#21453;&#21521;&#20122;&#39532;&#36874;&#65292;&#65288;b&#65289;&#28151;&#21512;&#27861;&#65292;&#65288;c&#65289;Donsker-Varadhan&#20844;&#24335;&#65288;&#25110;&#20854;&#23427;&#20984;&#24615;&#23545;&#20598;&#21407;&#29702;&#65289;&#21644;&#65288;d&#65289;Ville&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25104;&#26524;&#26159;&#19968;&#20010;PAC-Bayes&#23450;&#29702;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31163;&#25955;&#38543;&#26426;&#36807;&#31243;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#32467;&#26524;&#22914;&#20309;&#25512;&#20986;&#30693;&#21517;&#30340;&#32463;&#20856;PAC-Bayes&#30028;&#38480;&#65292;&#20363;&#22914;Seeger&#12289;McAllester&#12289;Maurer&#21644;Catoni&#30340;&#30028;&#38480;&#65292;&#20197;&#21450;&#35768;&#22810;&#26368;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#25918;&#26494;&#20256;&#32479;&#30340;&#20551;&#35774;&#65307;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#38750;&#24179;&#31283;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21518;&#39564;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.02713</link><description>&lt;p&gt;
&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21518;&#39564;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36890;&#36807;&#23545;&#27169;&#22411;&#21442;&#25968;&#26045;&#21152;&#20808;&#39564;&#20998;&#24067;&#24182;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#12290;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#25552;&#20379;&#38598;&#25104;&#39044;&#27979;&#21644;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20855;&#26377;&#36739;&#20302;&#23574;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21518;&#39564;&#25512;&#35770;&#23545;&#20110;&#23574;&#24230;/&#25153;&#24179;&#21270;&#24182;&#19981;&#20855;&#22791;&#24847;&#35782;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20174;&#20854;&#37319;&#26679;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#23574;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#25153;&#24179;&#21270;&#23545;&#21518;&#39564;&#36827;&#34892;&#20102;&#29702;&#35770;&#12289;&#36125;&#21494;&#26031;&#35774;&#23450;&#21644;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#25153;&#24179;&#21270;&#24847;&#20041;&#19978;&#25512;&#26029;&#30340;&#27169;&#22411;&#20197;&#21450;&#20272;&#35745;&#35813;&#25153;&#24179;&#21270;&#24847;&#20041;&#21518;&#39564;&#30340;&#26368;&#20339;&#36817;&#20284;&#21518;&#39564;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#22806;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ScaledGD(&#120582;)&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#26102;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.01186</link><description>&lt;p&gt;
&#39044;&#26465;&#20214;&#23545;&#36229;&#21442;&#21270;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ScaledGD(&#120582;)&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#26102;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ScaledGD(&#120582;)&#26041;&#27861;&#26469;&#35299;&#20915;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#20013;&#30697;&#38453;&#21487;&#33021;&#30149;&#24577;&#20197;&#21450;&#30495;&#23454;&#31209;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#36229;&#21442;&#24335;&#34920;&#31034;&#65292;&#20174;&#19968;&#20010;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#24418;&#24335;&#30340;&#38459;&#23612;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26469;&#23545;&#25239;&#36229;&#21442;&#21270;&#21644;&#30149;&#24577;&#26354;&#29575;&#30340;&#24433;&#21709;&#12290;&#19982;&#22522;&#20934;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30456;&#27604;&#65292;&#23613;&#31649;&#39044;&#22788;&#29702;&#38656;&#35201;&#36731;&#24494;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20294;ScaledGD&#65288;&#120582;&#65289;&#22312;&#38754;&#23545;&#30149;&#24577;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#39640;&#26031;&#35774;&#35745;&#19979;&#65292;ScaledGD($\lambda$) &#20250;&#22312;&#20165;&#36845;&#20195;&#25968;&#23545;&#25968;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
&lt;/p&gt;</description></item><item><title>AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08110</link><description>&lt;p&gt;
AtMan:&#36890;&#36807;&#33410;&#32422;&#20869;&#23384;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29702;&#35299;Transformer&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08110
&lt;/p&gt;
&lt;p&gt;
AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;Transformer&#27169;&#22411;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#25968;&#37327;&#22823;&#19988;&#20855;&#22791;&#22788;&#29702;&#22810;&#36755;&#20837;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#30340;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#21453;&#21521;&#20256;&#25773;&#20250;&#20998;&#37197;&#30340;GPU&#20869;&#23384;&#20960;&#20046;&#26159;&#21069;&#21521;&#20256;&#25773;&#30340;&#20004;&#20493;&#12290;&#36825;&#20351;&#24471;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#38750;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtMan&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AtMan&#26159;&#19968;&#31181;&#27169;&#24577;&#26080;&#20851;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#19982;&#36755;&#20986;&#39044;&#27979;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#22270;&#12290;AtMan&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#26159;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#37051;&#36817;&#24615;&#30340;&#21487;&#24182;&#34892;&#21270;&#22522;&#20110;&#35760;&#21495;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;-&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#25351;&#26631;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#19981;&#24847;&#21619;&#30528;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2212.00219</link><description>&lt;p&gt;
&#20320;&#26159;&#21542;&#27491;&#30830;&#20351;&#29992;&#20102;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are you using test log-likelihood correctly?. (arXiv:2212.00219v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00219
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#25351;&#26631;&#30456;&#30683;&#30462;&#65292;&#24182;&#19988;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#19981;&#24847;&#21619;&#30528;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#24120;&#34987;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#21516;&#19968;&#25968;&#25454;&#65292;&#25110;&#32773;&#27604;&#36739;&#25311;&#21512;&#21516;&#19968;&#27010;&#29575;&#27169;&#22411;&#30340;&#19981;&#21516;&#36817;&#20284;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#22522;&#20110;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#30340;&#27604;&#36739;&#21487;&#33021;&#19982;&#20854;&#20182;&#30446;&#26631;&#30456;&#30683;&#30462;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20363;&#23376;&#34920;&#26126;&#65306;&#65288;i&#65289;&#36798;&#21040;&#26356;&#39640;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#19981;&#24517;&#24847;&#21619;&#30528;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#65288;ii&#65289;&#22522;&#20110;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#27604;&#36739;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#32467;&#35770;&#21487;&#33021;&#19982;&#22522;&#20110;&#22343;&#26041;&#26681;&#35823;&#24046;&#30340;&#32467;&#35770;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#37325;&#23614;&#20998;&#24067;&#12290;&#26681;&#25454;&#31934;&#24515;&#36873;&#25321;&#30340;&#21487;&#36716;&#31227;&#28304;&#22495;&#24314;&#31435;&#20102;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#19968;&#27493;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2211.14578</link><description>&lt;p&gt;
&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical inference for transfer learning with high-dimensional quantile regression. (arXiv:2211.14578v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#37325;&#23614;&#20998;&#24067;&#12290;&#26681;&#25454;&#31934;&#24515;&#36873;&#25321;&#30340;&#21487;&#36716;&#31227;&#28304;&#22495;&#24314;&#31435;&#20102;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#19968;&#27493;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21033;&#29992;&#28304;&#22495;&#20013;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#39640;&#32500;&#25968;&#25454;&#26222;&#36941;&#23384;&#22312;&#24322;&#36136;&#24615;&#21644;/&#25110;&#37325;&#23614;&#20998;&#24067;&#65292;&#20294;&#30446;&#21069;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36866;&#24212;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#37325;&#23614;&#20998;&#24067;&#12290;&#25105;&#20204;&#26681;&#25454;&#31934;&#24515;&#36873;&#25321;&#30340;&#21487;&#36716;&#31227;&#28304;&#22495;&#24314;&#31435;&#20102;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#26174;&#31034;&#22312;&#20851;&#38190;&#36873;&#25321;&#26631;&#20934;&#21644;&#36739;&#22823;&#30340;&#28304;&#20219;&#21153;&#26679;&#26412;&#37327;&#19979;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#21644;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#65292;&#29992;&#20110;&#39640;&#32500;&#20998;&#20301;&#25968;&#22238;&#24402;&#31995;&#25968;&#30340;&#21508;&#20010;&#20998;&#37327;&#65292;&#36890;&#36807;&#20513;&#23548;&#21452;&#37325;&#36716;&#31227;&#23398;&#20064;&#20272;&#35745;&#37327;&#65292;&#23454;&#29616;&#19968;&#27493;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become an essential technique to exploit information from the source domain to boost performance of the target task. Despite the prevalence in high-dimensional data, heterogeneity and/or heavy tails are insufficiently accounted for by current transfer learning approaches and thus may undermine the resulting performance. We propose a transfer learning procedure in the framework of high-dimensional quantile regression models to accommodate the heterogeneity and heavy tails in the source and target domains. We establish error bounds of the transfer learning estimator based on delicately selected transferable source domains, showing that lower error bounds can be achieved for critical selection criterion and larger sample size of source tasks. We further propose valid confidence interval and hypothesis test procedures for individual component of high-dimensional quantile regression coefficients by advocating a double transfer learning estimator, which is the one-step 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.13708</link><description>&lt;p&gt;
MARLlib: &#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#35780;&#20272;&#24179;&#21488;&#21644;&#20844;&#35748;&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#38598;&#25104;&#24211;&#22871;&#20214;&#65292;&#20197;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#21487;&#38752;&#30340;MARL&#23454;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;MARLlib&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#27969;&#35774;&#35745;&#65292;&#22312;&#39640;&#24230;&#21487;&#32452;&#21512;&#30340;&#38598;&#25104;&#39118;&#26684;&#20013;&#32479;&#19968;&#20102;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;MARLlib&#36890;&#36807;&#38598;&#25104;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65307;&#36825;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#22312;&#26368;&#23567;&#30340;&#20195;&#30721;&#20462;&#25913;&#19979;&#23454;&#29616;&#21327;&#20316;&#12289;&#31454;&#20105;&#21644;&#28151;&#21512;&#20219;&#21153;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;MARLlib&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#23436;&#20840;&#35299;&#32806;&#21512;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#28857;&#30340;&#36873;&#25321;&#21644;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#21487;&#38752;&#30340;&#31232;&#30095;&#36924;&#36817;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.07893</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Cover Trees&#30340;&#26368;&#23567;&#38388;&#38548;&#23454;&#29616;&#25968;&#20540;&#31283;&#23450;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#28857;&#30340;&#36873;&#25321;&#21644;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#21487;&#38752;&#30340;&#31232;&#30095;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#24120;&#29992;&#20110;&#36739;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#20363;&#22914;&#22320;&#29702;&#31354;&#38388;&#24314;&#27169;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#25110;&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#20013;&#12290;&#22312;&#19968;&#20010;&#31995;&#32479;&#20013;&#65292;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#38656;&#35201;&#20197;&#31283;&#23450;&#21487;&#38752;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#20197;&#30830;&#20445;&#19982;&#31995;&#32479;&#30340;&#20854;&#20182;&#37096;&#20998;&#27491;&#30830;&#20132;&#20114;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24863;&#20852;&#36259;&#28857;&#30340;&#21487;&#25193;&#23637;&#31232;&#30095;&#36924;&#36817;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21487;&#33021;&#19981;&#31283;&#23450;&#30340;&#20856;&#22411;&#24773;&#20917;&#12290;&#22312;&#25554;&#20540;&#25991;&#29486;&#20013;&#21407;&#22987;&#24320;&#21457;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#24863;&#20852;&#36259;&#28857;&#36827;&#34892;&#35745;&#31639;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#21644;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#22320;&#29702;&#31354;&#38388;&#24314;&#27169;&#31561;&#20302;&#32500;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#30340;&#24863;&#20852;&#36259;&#28857;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29420;&#31435;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#30340;&#22810;&#29615;&#22659;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#27979;&#35797;&#29420;&#31435;&#24615;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2205.13935</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#29615;&#22659;&#26041;&#27861;&#26816;&#27979;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#38544;&#24335;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden confounding in observational data using multiple environments. (arXiv:2205.13935v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13935
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29420;&#31435;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#30340;&#22810;&#29615;&#22659;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#27979;&#35797;&#29420;&#31435;&#24615;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#65292;&#24120;&#35265;&#30340;&#20551;&#35774;&#26159;&#27809;&#26377;&#38544;&#24335;&#28151;&#28102;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#20013;&#19981;&#33021;&#30830;&#23450;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#29420;&#31435;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#29615;&#22659;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#21487;&#39564;&#35777;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#29702;&#35770;&#65292;&#36825;&#31181;&#29420;&#31435;&#24615;&#20165;&#24403;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#26102;&#25165;&#19981;&#23384;&#22312;&#65292;&#24182;&#26816;&#26597;&#20102;&#36829;&#21453;&#20854;&#20551;&#35774;&#30340;&#24773;&#20917;&#65306;&#36864;&#21270;&#21644;&#20381;&#36182;&#26426;&#21046;&#20197;&#21450;&#24544;&#23454;&#24230;&#36829;&#21453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31243;&#24207;&#26469;&#27979;&#35797;&#36825;&#20123;&#29420;&#31435;&#24615;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#21644;&#27169;&#25311;&#30740;&#31350;&#30740;&#31350;&#20854;&#32463;&#39564;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#30340;&#31243;&#24207;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#23384;&#22312;&#38544;&#24335;&#28151;&#28102;&#65292;&#29305;&#21035;&#26159;&#24403;&#28151;&#28102;&#20559;&#24046;&#24456;&#22823;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common assumption in causal inference from observational data is that there is no hidden confounding. Yet it is, in general, impossible to verify this assumption from a single dataset. Under the assumption of independent causal mechanisms underlying the data-generating process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only absent when there is hidden confounding and examine cases where we violate its assumptions: degenerate &amp; dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies and semi-synthetic data based on a real-world dataset. In most cases, the proposed procedure correctly predicts the presence of hidden confounding, particularly when the confounding bias is large.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#26032;&#25968;&#25454;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.05069</link><description>&lt;p&gt;
&#19981;&#21516;&#36755;&#20837;&#32500;&#24230;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case. (arXiv:2202.05069v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#26032;&#25968;&#25454;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#20256;&#24863;&#22120;&#21644;&#30417;&#27979;&#35774;&#22791;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#25968;&#25454;&#26082;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23558;&#36825;&#20123;&#26032;&#36755;&#20837;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35814;&#32454;&#30740;&#31350;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#26032;&#25968;&#25454;&#21644;&#21382;&#21490;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#19979;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23545;&#35813;&#26041;&#27861;&#30340;&#30410;&#22788;&#36827;&#34892;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#26159;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of new sensors and monitoring devices, more sources of data become available to be used as inputs for machine learning models. These can on the one hand help to improve the accuracy of a model. On the other hand however, combining these new inputs with historical data remains a challenge that has not yet been studied in enough detail. In this work, we propose a transfer-learning algorithm that combines the new and the historical data, that is especially beneficial when the new data is scarce. We focus the approach on the linear regression case, which allows us to conduct a rigorous theoretical study on the benefits of the approach. We show that our approach is robust against negative transfer-learning, and we confirm this result empirically with real and simulated data.
&lt;/p&gt;</description></item><item><title>ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2007.01777</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#20998;&#31867;&#36890;&#36807;&#21407;&#22411;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.01777
&lt;/p&gt;
&lt;p&gt;
ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;ProtoryNet&#65292;&#23427;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#26032;&#27010;&#24565;&#12290;&#21463;&#29616;&#20195;&#35821;&#35328;&#23398;&#20013;&#30340;&#21407;&#22411;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;ProtoryNet&#36890;&#36807;&#20026;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#25214;&#21040;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#65292;&#24182;&#23558;&#27599;&#20010;&#21477;&#23376;&#19982;&#30456;&#24212;&#30340;&#27963;&#21160;&#21407;&#22411;&#30340;&#25509;&#36817;&#31243;&#24230;&#36755;&#20837;&#21040;RNN&#20027;&#24178;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;RNN&#20027;&#24178;&#25429;&#25417;&#21040;&#21407;&#22411;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#36712;&#36857;&#12290;&#21407;&#22411;&#36712;&#36857;&#33021;&#22815;&#30452;&#35266;&#32780;&#32454;&#33268;&#22320;&#35299;&#37322;RNN&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#20998;&#26512;&#25991;&#26412;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21407;&#22411;&#20462;&#21098;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#20351;&#29992;&#30340;&#21407;&#22411;&#24635;&#25968;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ProtoryNet&#27604;&#22522;&#32447;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
&lt;/p&gt;</description></item></channel></rss>