<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SoftZoo&#26159;&#19968;&#20010;&#38754;&#21521;&#22810;&#31181;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#24179;&#21488;&#65292;&#25903;&#25345;&#24191;&#27867;&#12289;&#33258;&#28982;&#21551;&#21457;&#30340;&#26448;&#26009;&#32452;&#21512;&#21644;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#21487;&#24494;&#20998;&#35774;&#35745;&#34920;&#31034;&#12290;&#36890;&#36807;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;SoftZoo&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#36719;&#20307;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.09555</link><description>&lt;p&gt;
SoftZoo&#65306;&#38754;&#21521;&#22810;&#26679;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#22522;&#20934;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments. (arXiv:2303.09555v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09555
&lt;/p&gt;
&lt;p&gt;
SoftZoo&#26159;&#19968;&#20010;&#38754;&#21521;&#22810;&#31181;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#24179;&#21488;&#65292;&#25903;&#25345;&#24191;&#27867;&#12289;&#33258;&#28982;&#21551;&#21457;&#30340;&#26448;&#26009;&#32452;&#21512;&#21644;&#22810;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#21487;&#24494;&#20998;&#35774;&#35745;&#34920;&#31034;&#12290;&#36890;&#36807;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;SoftZoo&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#36719;&#20307;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#25511;&#21046;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20294;&#22312;&#21516;&#26102;&#21327;&#21516;&#20248;&#21270;&#24418;&#24577;&#26102;&#20986;&#29616;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#26159;&#20026;&#29305;&#23450;&#29615;&#22659;&#25110;&#34920;&#31034;&#37327;&#37327;&#36523;&#23450;&#21046;&#30340;&#12290;&#20026;&#20102;&#26356;&#20805;&#20998;&#22320;&#29702;&#35299;&#22266;&#26377;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#26435;&#34913;&#65292;&#24182;&#21152;&#36895;&#24320;&#21457;&#26032;&#22411;&#36719;&#20307;&#26426;&#22120;&#20154;&#65292;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;&#24050;&#24314;&#31435;&#22909;&#30340;&#20219;&#21153;&#12289;&#29615;&#22659;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#20840;&#38754;&#34394;&#25311;&#24179;&#21488;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SoftZoo&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#22810;&#26679;&#29615;&#22659;&#30340;&#36719;&#20307;&#26426;&#22120;&#20154;&#21327;&#21516;&#35774;&#35745;&#24179;&#21488;&#12290;SoftZoo&#25903;&#25345;&#24191;&#27867;&#12289;&#33258;&#28982;&#21551;&#21457;&#30340;&#26448;&#26009;&#32452;&#21512;&#65292;&#21253;&#25324;&#33021;&#22815;&#27169;&#25311;&#24179;&#22320;&#12289;&#27801;&#28448;&#12289;&#28287;&#22320;&#12289;&#40655;&#22303;&#12289;&#20912;&#12289;&#38634;&#12289;&#27973;&#27700;&#21644;&#28023;&#27915;&#31561;&#22810;&#31181;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#22810;&#20010;&#19982;&#36719;&#20307;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24555;&#36895;&#31227;&#21160;&#12289;&#28789;&#27963;&#36716;&#21521;&#21644;&#36335;&#24452;&#36319;&#38543;&#65292;&#20197;&#21450;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#21487;&#24494;&#20998;&#35774;&#35745;&#34920;&#31034;&#12290;&#32467;&#21512;&#19968;&#33268;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;SoftZoo&#20801;&#35768;&#22312;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#65292;&#24182;&#25581;&#31034;&#36719;&#20307;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
While significant research progress has been made in robot learning for control, unique challenges arise when simultaneously co-optimizing morphology. Existing work has typically been tailored for particular environments or representations. In order to more fully understand inherent design and performance tradeoffs and accelerate the development of new breeds of soft robots, a comprehensive virtual platform with well-established tasks, environments, and evaluation metrics is needed. In this work, we introduce SoftZoo, a soft robot co-design platform for locomotion in diverse environments. SoftZoo supports an extensive, naturally-inspired material set, including the ability to simulate environments such as flat ground, desert, wetland, clay, ice, snow, shallow water, and ocean. Further, it provides a variety of tasks relevant for soft robotics, including fast locomotion, agile turning, and path following, as well as differentiable design representations for morphology and control. Combi
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27969;&#30340;&#32534;&#31243;&#65288;FBP&#65289;&#65292;&#24182;&#24378;&#35843;&#20102;FBP&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#22270;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#36825;&#31181;&#32852;&#31995;&#21487;&#20197;&#25913;&#36827;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#21253;&#25324;&#25925;&#38556;&#23450;&#20301;&#12289;&#19994;&#21153;&#20998;&#26512;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.09552</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#22270;&#20316;&#20026;&#23436;&#25972;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Dataflow graphs as complete causal graphs. (arXiv:2303.09552v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27969;&#30340;&#32534;&#31243;&#65288;FBP&#65289;&#65292;&#24182;&#24378;&#35843;&#20102;FBP&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#22270;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#36825;&#31181;&#32852;&#31995;&#21487;&#20197;&#25913;&#36827;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#21253;&#25324;&#25925;&#38556;&#23450;&#20301;&#12289;&#19994;&#21153;&#20998;&#26512;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#20214;&#21270;&#24320;&#21457;&#26159;&#29616;&#20195;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#30340;&#26680;&#24515;&#21407;&#21017;&#20043;&#19968;&#12290;&#29702;&#35299;&#36719;&#20214;&#31995;&#32479;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#20026;&#24320;&#21457;&#20154;&#21592;&#24102;&#26469;&#26174;&#33879;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#20351;&#24471;&#22312;&#31995;&#32479;&#35268;&#27169;&#19979;&#36319;&#36394;&#21644;&#21457;&#29616;&#36825;&#31181;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#65292;&#36825;&#23548;&#33268;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#26234;&#21147;&#36127;&#25285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#36719;&#20214;&#35774;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27969;&#30340;&#32534;&#31243;&#65288;FBP&#65289;&#65292;&#24182;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;FBP&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#22270;&#21644;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#36890;&#36807;&#35828;&#26126;&#24615;&#31034;&#20363;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#26469;&#25913;&#36827;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#21253;&#25324;&#25925;&#38556;&#23450;&#20301;&#12289;&#19994;&#21153;&#20998;&#26512;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Component-based development is one of the core principles behind modern software engineering practices. Understanding of causal relationships between components of a software system can yield significant benefits to developers. Yet modern software design approaches make it difficult to track and discover such relationships at system scale, which leads to growing intellectual debt. In this paper we consider an alternative approach to software design, flow-based programming (FBP), and draw the attention of the community to the connection between dataflow graphs produced by FBP and structural causal models. With expository examples we show how this connection can be leveraged to improve day-to-day tasks in software projects, including fault localisation, business analysis and experimentation.
&lt;/p&gt;</description></item><item><title>WebSHAP&#26159;&#31532;&#19968;&#20010;&#23558;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;SHAP&#36866;&#24212;Web&#29615;&#22659;&#30340;&#27983;&#35272;&#22120;&#20869;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;ML&#30340;&#36151;&#27454;&#25209;&#20934;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.09545</link><description>&lt;p&gt;
WebSHAP: &#38754;&#21521;&#20219;&#24847;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
WebSHAP: Towards Explaining Any Machine Learning Models Anywhere. (arXiv:2303.09545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09545
&lt;/p&gt;
&lt;p&gt;
WebSHAP&#26159;&#31532;&#19968;&#20010;&#23558;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;SHAP&#36866;&#24212;Web&#29615;&#22659;&#30340;&#27983;&#35272;&#22120;&#20869;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;ML&#30340;&#36151;&#27454;&#25209;&#20934;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#25105;&#20204;&#26085;&#24120;&#30340;&#32593;&#32476;&#20307;&#39564;&#20013;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#38656;&#35201;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Web&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#24448;&#24448;&#38656;&#35201;&#19987;&#29992;&#30340;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21521;&#36739;&#20302;&#24310;&#36831;&#21644;&#26356;&#39640;&#38544;&#31169;&#24615;&#30340;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;Web ML&#36801;&#31227;&#26102;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23458;&#25143;&#31471;&#35299;&#37322;&#26041;&#26696;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WebSHAP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;SHAP&#36866;&#24212;Web&#29615;&#22659;&#30340;&#27983;&#35272;&#22120;&#20869;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#24037;&#20855;&#26159;&#20351;&#29992;&#29616;&#20195;Web&#25216;&#26415;&#65288;&#22914;WebGL&#65289;&#24320;&#21457;&#30340;&#65292;&#21033;&#29992;&#23458;&#25143;&#31471;&#30828;&#20214;&#33021;&#21147;&#65292;&#26131;&#20110;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;Web ML&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;WebSHAP&#22312;&#35299;&#37322;&#22522;&#20110;ML&#30340;&#36151;&#27454;&#25209;&#20934;&#20915;&#31574;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290; &#22238;&#39038;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36879;&#26126;Web ML&#30340;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;WebSHAP&#29616;&#24050;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) is increasingly integrated into our everyday Web experience, there is a call for transparent and explainable web-based ML. However, existing explainability techniques often require dedicated backend servers, which limit their usefulness as the Web community moves toward in-browser ML for lower latency and greater privacy. To address the pressing need for a client-side explainability solution, we present WebSHAP, the first in-browser tool that adapts the state-of-the-art model-agnostic explainability technique SHAP to the Web environment. Our open-source tool is developed with modern Web technologies such as WebGL that leverage client-side hardware capabilities and make it easy to integrate into existing Web ML applications. We demonstrate WebSHAP in a usage scenario of explaining ML-based loan approval decisions to loan applicants. Reflecting on our work, we discuss the opportunities and challenges for future research on transparent Web ML. WebSHAP is available
&lt;/p&gt;</description></item><item><title>SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09540</link><description>&lt;p&gt;
SemDeDup:&#36890;&#36807;&#35821;&#20041;&#21435;&#37325;&#23454;&#29616;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;arXiv:2303.09540v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09540
&lt;/p&gt;
&lt;p&gt;
SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#28023;&#37327;&#25968;&#25454;&#30340;&#22686;&#21152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#20687;LAION&#36825;&#26679;&#30340;&#22823;&#22411;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#38500;&#26597;&#25214;&#31934;&#30830;&#37325;&#22797;&#39033;&#22806;&#65292;&#22823;&#37096;&#20998;&#26410;&#32463;&#31934;&#24515;&#31579;&#36873;&#65292;&#21487;&#33021;&#23384;&#22312;&#24456;&#22810;&#20887;&#20313;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SemDeDup&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#65306;&#21363;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#25968;&#25454;&#23545;&#12290;&#21435;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#21487;&#20197;&#20445;&#25345;&#24615;&#33021;&#24182;&#21152;&#36895;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#22312;&#20998;&#24067;&#20197;&#22806;&#24471;&#21040;&#25552;&#39640;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#37096;&#20998;&#31579;&#36873;&#36807;&#30340;&#25968;&#25454;&#38598;C4&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;SemDeDup&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992;&#36136;&#37327;&#23884;&#20837;&#31616;&#21333;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#26356;&#24555;&#22320;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;CD&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#28145;&#24230;&#32593;&#32476;&#65288;D-CPG&#21644;D-FE&#65289;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#21464;&#21270;&#21644;&#25552;&#21462;&#29992;&#20110;&#21464;&#21270;&#26816;&#27979;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;CD&#26041;&#27861;&#20197;&#21450;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09536</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36965;&#24863;&#21464;&#21270;&#26816;&#27979;&#20013;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Metric Learning for Unsupervised Remote Sensing Change Detection. (arXiv:2303.09536v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09536
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;CD&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#28145;&#24230;&#32593;&#32476;&#65288;D-CPG&#21644;D-FE&#65289;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#21464;&#21270;&#21644;&#25552;&#21462;&#29992;&#20110;&#21464;&#21270;&#26816;&#27979;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;CD&#26041;&#27861;&#20197;&#21450;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#21464;&#21270;&#26816;&#27979;&#26159;&#20174;&#22810;&#26102;&#30456;&#36965;&#24863;&#22270;&#20687;&#20013;&#26816;&#27979;&#30456;&#20851;&#21464;&#21270;&#65292;&#29992;&#20110;&#21508;&#31181;&#36965;&#24863;&#24212;&#29992;&#65292;&#20363;&#22914;&#22303;&#22320;&#35206;&#30422;&#12289;&#22303;&#22320;&#21033;&#29992;&#12289;&#20154;&#31867;&#21457;&#23637;&#20998;&#26512;&#21644;&#28798;&#38590;&#21709;&#24212;&#12290;&#29616;&#26377;&#30340;&#36965;&#24863;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#24402;&#22240;&#20110;&#22823;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22521;&#35757;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#24046;&#36317;&#26102;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#38750;&#24120;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;CD&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#32473;&#23450;MT-RSI&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#20248;&#21270;&#26080;&#30417;&#30563;CD&#25439;&#22833;&#26469;&#29983;&#25104;&#23545;&#24212;&#30340;&#21464;&#21270;&#27010;&#29575;&#22270;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;CD&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#21363;Deep-Change Probability Generator&#65288;D-CPG&#65289;&#21644;Deep-Feature Extractor&#65288;D-FE&#65289;&#12290;D-CPG&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21028;&#21035;&#29305;&#24449;&#26469;&#39044;&#27979;&#21464;&#21270;&#65292;&#32780;D-FE&#21017;&#29992;&#20110;&#25552;&#21462;&#29992;&#20110;&#21464;&#21270;&#26816;&#27979;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;CD&#26041;&#27861;&#20197;&#21450;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote Sensing Change Detection (RS-CD) aims to detect relevant changes from Multi-Temporal Remote Sensing Images (MT-RSIs), which aids in various RS applications such as land cover, land use, human development analysis, and disaster response. The performance of existing RS-CD methods is attributed to training on large annotated datasets. Furthermore, most of these models are less transferable in the sense that the trained model often performs very poorly when there is a domain gap between training and test datasets. This paper proposes an unsupervised CD method based on deep metric learning that can deal with both of these issues. Given an MT-RSI, the proposed method generates corresponding change probability map by iteratively optimizing an unsupervised CD loss without training it on a large dataset. Our unsupervised CD method consists of two interconnected deep networks, namely Deep-Change Probability Generator (D-CPG) and Deep-Feature Extractor (D-FE). The D-CPG is designed to pred
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38236;&#20687;&#19979;&#38477;&#21644;&#38236;&#20687; Langevin &#21160;&#21147;&#23398;&#30340;&#21464;&#20998;&#24418;&#24335;&#65292;&#34920;&#26126;&#38236;&#20687;&#19979;&#38477;&#20316;&#20026;&#26576;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#38381;&#29615;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#24182;&#32473;&#20986;&#20102; Bellman &#20540;&#20989;&#25968;&#30340;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.09532</link><description>&lt;p&gt;
&#38236;&#20687;&#19979;&#38477;&#21644;&#38236;&#20687; Langevin &#21160;&#21147;&#23398;&#30340;&#21464;&#20998;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
Variational Principles for Mirror Descent and Mirror Langevin Dynamics. (arXiv:2303.09532v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38236;&#20687;&#19979;&#38477;&#21644;&#38236;&#20687; Langevin &#21160;&#21147;&#23398;&#30340;&#21464;&#20998;&#24418;&#24335;&#65292;&#34920;&#26126;&#38236;&#20687;&#19979;&#38477;&#20316;&#20026;&#26576;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#38381;&#29615;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#24182;&#32473;&#20986;&#20102; Bellman &#20540;&#20989;&#25968;&#30340;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38236;&#20687;&#19979;&#38477;&#26159;&#30001; Nemirovski &#21644; Yudin &#22312;&#19978;&#19990;&#32426;70&#24180;&#20195;&#24341;&#20837;&#30340;&#21407;&#22987;-&#23545;&#20598;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#24378;&#20984;&#21183;&#20989;&#25968;&#26469;&#36866;&#24212;&#20248;&#21270;&#38382;&#39064;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#23427;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#22522;&#26412;&#21407;&#35821;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#20248;&#21270;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38236;&#20687;&#19979;&#38477;&#21450;&#20854;&#38543;&#26426;&#21464;&#20307;&#38236;&#20687; Langevin &#21160;&#21147;&#23398;&#30340;&#21464;&#20998;&#24418;&#24335;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#21463; Brezis &#21644; Ekeland &#20851;&#20110;&#26799;&#24230;&#27969;&#21464;&#20998;&#21407;&#29702;&#30340;&#32463;&#20856;&#24037;&#20316;&#21551;&#21457;&#65292;&#34920;&#26126;&#38236;&#20687;&#19979;&#38477;&#20316;&#20026;&#26576;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#38381;&#29615;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#32780; Bellman &#20540;&#20989;&#25968;&#26159;&#21021;&#22987;&#26465;&#20214;&#21644;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340; Bregman &#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mirror descent, introduced by Nemirovski and Yudin in the 1970s, is a primal-dual convex optimization method that can be tailored to the geometry of the optimization problem at hand through the choice of a strongly convex potential function. It arises as a basic primitive in a variety of applications, including large-scale optimization, machine learning, and control. This paper proposes a variational formulation of mirror descent and of its stochastic variant, mirror Langevin dynamics. The main idea, inspired by the classic work of Brezis and Ekeland on variational principles for gradient flows, is to show that mirror descent emerges as a closed-loop solution for a certain optimal control problem, and the Bellman value function is given by the Bregman divergence between the initial condition and the global minimizer of the objective function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#26434;&#27874;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#26434;&#27874;&#26631;&#31614;&#12290;&#23545;&#29616;&#26377;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#21457;&#24067;&#31532;&#19968;&#20010;&#34920;&#31034;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#30340;&#33258;&#30001;&#21487;&#29992;&#38647;&#36798;&#26434;&#27874;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.09530</link><description>&lt;p&gt;
&#21033;&#29992;PointNet++&#35299;&#20915;&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#26434;&#27874; &#8212;&#8212;&#26631;&#31614;&#29983;&#25104;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tackling Clutter in Radar Data -- Label Generation and Detection Using PointNet++. (arXiv:2303.09530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#26434;&#27874;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#26434;&#27874;&#26631;&#31614;&#12290;&#23545;&#29616;&#26377;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#21457;&#24067;&#31532;&#19968;&#20010;&#34920;&#31034;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#30340;&#33258;&#30001;&#21487;&#29992;&#38647;&#36798;&#26434;&#27874;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#29615;&#22659;&#24863;&#30693;&#39046;&#22495;&#20013;&#65292;&#38647;&#36798;&#20256;&#24863;&#22120;&#36755;&#20986;&#20102;&#22823;&#37327;&#19981;&#38656;&#35201;&#30340;&#26434;&#27874;&#12290;&#36825;&#20123;&#28857;&#27809;&#26377;&#23545;&#24212;&#30340;&#30495;&#23454;&#23545;&#35937;&#65292;&#26159;&#21518;&#32493;&#22788;&#29702;&#27493;&#39588;&#65288;&#22914;&#29289;&#20307;&#26816;&#27979;&#25110;&#36319;&#36394;&#65289;&#20013;&#38169;&#35823;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#26469;&#35782;&#21035;&#26434;&#27874;&#12290;&#36755;&#20837;&#25968;&#25454;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#37197;&#32622;&#37117;&#26159;&#38024;&#23545;&#36825;&#20010;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#30340;&#12290;&#29305;&#21035;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#30001;&#22810;&#20010;&#20256;&#24863;&#22120;&#25195;&#25551;&#32452;&#25104;&#30340;&#28857;&#20113;&#30340;&#19979;&#37319;&#26679;&#12290;&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#65292;&#26032;&#30340;&#35774;&#32622;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#27809;&#26377;&#36866;&#21512;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#27880;&#37322;&#26434;&#27874;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#30456;&#24212;&#30340;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#23545;&#35937;&#27880;&#37322;&#30340;&#29616;&#26377;&#25968;&#25454;&#24182;&#21457;&#24067;&#20854;&#20195;&#30721;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20195;&#34920;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#30340;&#33258;&#30001;&#21487;&#29992;&#38647;&#36798;&#26434;&#27874;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radar sensors employed for environment perception, e.g. in autonomous vehicles, output a lot of unwanted clutter. These points, for which no corresponding real objects exist, are a major source of errors in following processing steps like object detection or tracking. We therefore present two novel neural network setups for identifying clutter. The input data, network architectures and training configuration are adjusted specifically for this task. Special attention is paid to the downsampling of point clouds composed of multiple sensor scans. In an extensive evaluation, the new setups display substantially better performance than existing approaches. Because there is no suitable public data set in which clutter is annotated, we design a method to automatically generate the respective labels. By applying it to existing data with object annotations and releasing its code, we effectively create the first freely available radar clutter data set representing real-world driving scenarios. C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DP-Fair&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#21644;&#20844;&#24179;&#32422;&#26463;&#65292;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12289;&#30830;&#20445;&#20844;&#24179;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2303.09527</link><description>&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#30340;&#24046;&#20998;&#38544;&#31169;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Differentially Private Collaborative Filtering. (arXiv:2303.09527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DP-Fair&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#21644;&#20844;&#24179;&#32422;&#26463;&#65292;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12289;&#30830;&#20445;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31639;&#27861;&#20351;&#29992;&#22312;&#31639;&#27861;&#20844;&#24179;&#24615;&#26041;&#38754;&#26377;&#25240;&#34935;&#65292;&#36825;&#19968;&#28857;&#34987;&#24191;&#27867;&#35748;&#21487;&#12290;&#26412;&#25991;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#35757;&#32451;&#30340;&#32463;&#20856;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#23548;&#33268;&#29992;&#25143;&#32676;&#20307;&#19982;&#19981;&#21516;&#29992;&#25143;&#21442;&#19982;&#27700;&#24179;&#20043;&#38388;&#23384;&#22312;&#19981;&#20844;&#24179;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;DP-Fair&#65292;&#23427;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#19982;&#20844;&#24179;&#38480;&#21046;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#30830;&#20445;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been an increasing adoption of differential privacy guided algorithms for privacy-preserving machine learning tasks. However, the use of such algorithms comes with trade-offs in terms of algorithmic fairness, which has been widely acknowledged. Specifically, we have empirically observed that the classical collaborative filtering method, trained by differentially private stochastic gradient descent (DP-SGD), results in a disparate impact on user groups with respect to different user engagement levels. This, in turn, causes the original unfair model to become even more biased against inactive users. To address the above issues, we propose \textbf{DP-Fair}, a two-stage framework for collaborative filtering based algorithms. Specifically, it combines differential privacy mechanisms with fairness constraints to protect user privacy while ensuring fair recommendations. The experimental results, based on Amazon datasets, and user history logs collected from Etsy, one of th
&lt;/p&gt;</description></item><item><title>PyVBMC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Python&#24037;&#20855;&#65292;&#29992;&#20110;&#40657;&#30418;&#35745;&#31639;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#27169;&#22411;&#36873;&#25321;&#65292;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#21442;&#25968;&#19981;&#36229;&#36807;&#32422;10-15&#20010;&#30340;&#35745;&#31639;&#25110;&#32479;&#35745;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09519</link><description>&lt;p&gt;
PyVBMC&#65306;Python&#20013;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PyVBMC: Efficient Bayesian inference in Python. (arXiv:2303.09519v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09519
&lt;/p&gt;
&lt;p&gt;
PyVBMC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Python&#24037;&#20855;&#65292;&#29992;&#20110;&#40657;&#30418;&#35745;&#31639;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#27169;&#22411;&#36873;&#25321;&#65292;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#21442;&#25968;&#19981;&#36229;&#36807;&#32422;10-15&#20010;&#30340;&#35745;&#31639;&#25110;&#32479;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyVBMC&#26159;Variational Bayesian Monte Carlo&#65288;VBMC&#65289;&#31639;&#27861;&#30340;Python&#23454;&#29616;&#65292;&#29992;&#20110;&#40657;&#30418;&#35745;&#31639;&#27169;&#22411;&#30340;&#21518;&#39564;&#21644;&#27169;&#22411;&#25512;&#26029;&#12290;VBMC&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#20272;&#35745;&#21644;&#27169;&#22411;&#35780;&#20272;&#30340;&#36817;&#20284;&#25512;&#26029;&#26041;&#27861;&#65292;&#24403;&#27169;&#22411;&#35780;&#20272;&#26159;&#26377;&#28857;&#21040;&#38750;&#24120;&#26114;&#36149;&#65288;&#20363;&#22914;&#31532;&#20108;&#27425;&#25110;&#26356;&#22810;&#27425;&#65289;&#21644;/&#25110;&#22024;&#26434;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VBMC&#35745;&#31639;&#65306;
&lt;/p&gt;
&lt;p&gt;
PyVBMC is a Python implementation of the Variational Bayesian Monte Carlo (VBMC) algorithm for posterior and model inference for black-box computational models (Acerbi, 2018, 2020). VBMC is an approximate inference method designed for efficient parameter estimation and model assessment when model evaluations are mildly-to-very expensive (e.g., a second or more) and/or noisy. Specifically, VBMC computes:  - a flexible (non-Gaussian) approximate posterior distribution of the model parameters, from which statistics and posterior samples can be easily extracted;  - an approximation of the model evidence or marginal likelihood, a metric used for Bayesian model selection.  PyVBMC can be applied to any computational or statistical model with up to roughly 10-15 continuous parameters, with the only requirement that the user can provide a Python function that computes the target log likelihood of the model, or an approximation thereof (e.g., an estimate of the likelihood obtained via simulation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#30340;GRU&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#20915;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#26368;&#30456;&#20851;&#21644;&#29420;&#31435;&#30340;&#29305;&#24449;&#33021;&#22815;&#26497;&#22823;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09497</link><description>&lt;p&gt;
&#22522;&#20110;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#30340;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#32593;&#32476;&#29992;&#20110;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Gate Recurrent Unit Network based on Hilbert-Schmidt Independence Criterion for State-of-Health Estimation. (arXiv:2303.09497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#30340;GRU&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#20915;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#26368;&#30456;&#20851;&#21644;&#29420;&#31435;&#30340;&#29305;&#24449;&#33021;&#22815;&#26497;&#22823;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#29366;&#24577;&#35780;&#20272;&#26159;&#30830;&#20445;&#30005;&#27744;&#23433;&#20840;&#21487;&#38752;&#36816;&#34892;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30001;&#20110;&#19981;&#21516;&#21608;&#26399;&#20013;&#20986;&#29616;&#25968;&#25454;&#20998;&#24067;&#21644;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#31561;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#20581;&#24247;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#32780;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#36153;&#21147;&#12290;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#33021;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#20013;&#20173;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#65292;&#24433;&#21709;SOH&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65288;GRU-HSIC&#65289;&#30340;&#26032;&#22411;GRU&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#38646;&#25513;&#30721;&#32593;&#32476;&#23558;&#27599;&#20010;&#21608;&#26399;&#27979;&#37327;&#30340;&#30005;&#27744;&#25968;&#25454;&#36716;&#25442;&#20026;&#30456;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#27599;&#20010;&#21608;&#26399;&#21407;&#22987;&#25968;&#25454;&#22823;&#23567;&#30340;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#20174;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#29702;&#35770;&#28436;&#21464;&#32780;&#26469;&#30340;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65288;HSIC&#65289;&#29942;&#39048;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#26368;&#30456;&#20851;&#21644;&#29420;&#31435;&#30340;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#25552;&#21462;&#30340;&#29305;&#24449;&#35757;&#32451;GRU-HSIC&#27169;&#22411;&#20197;&#20934;&#30830;&#20272;&#35745;&#30005;&#27744;&#30340;SOH&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-health (SOH) estimation is a key step in ensuring the safe and reliable operation of batteries. Due to issues such as varying data distribution and sequence length in different cycles, most existing methods require health feature extraction technique, which can be time-consuming and labor-intensive. GRU can well solve this problem due to the simple structure and superior performance, receiving widespread attentions. However, redundant information still exists within the network and impacts the accuracy of SOH estimation. To address this issue, a new GRU network based on Hilbert-Schmidt Independence Criterion (GRU-HSIC) is proposed. First, a zero masking network is used to transform all battery data measured with varying lengths every cycle into sequences of the same length, while still retaining information about the original data size in each cycle. Second, the Hilbert-Schmidt Independence Criterion (HSIC) bottleneck, which evolved from Information Bottleneck (IB) theory, is 
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#21069;&#27839;&#30740;&#31350;&#26041;&#21521;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#37327;&#23376;&#26448;&#26009;&#12289;&#29983;&#29289;&#21270;&#23398;&#21644;&#39640;&#33021;&#29289;&#29702;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#20173;&#26377;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35299;&#20915;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;QML&#26041;&#27861;&#19982;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;QML&#30340;&#37327;&#23376;&#20248;&#21183;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2303.09491</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities in Quantum Machine Learning. (arXiv:2303.09491v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09491
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#21069;&#27839;&#30740;&#31350;&#26041;&#21521;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#37327;&#23376;&#26448;&#26009;&#12289;&#29983;&#29289;&#21270;&#23398;&#21644;&#39640;&#33021;&#29289;&#29702;&#31561;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#20173;&#26377;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35299;&#20915;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;QML&#26041;&#27861;&#19982;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;QML&#30340;&#37327;&#23376;&#20248;&#21183;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#20301;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20855;&#26377;&#21152;&#36895;&#25968;&#25454;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#37327;&#23376;&#25968;&#25454;&#24212;&#29992;&#20110;&#37327;&#23376;&#26448;&#26009;&#12289;&#29983;&#29289;&#21270;&#23398;&#21644;&#39640;&#33021;&#29289;&#29702;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;QML&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#30340;QML&#26041;&#27861;&#19982;&#24212;&#29992;&#65292;&#24182;&#31361;&#20986;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#37325;&#28857;&#20851;&#27880;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#28145;&#24230;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;QML&#30340;&#37327;&#23376;&#20248;&#21183;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the trainability of QML models. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with QML.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SpaceTime&#30340;&#29366;&#24577;&#31354;&#38388;&#26102;&#38388;&#24207;&#21015;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20276;&#38543;&#30697;&#38453;&#30340;&#26032;&#30340;SSM&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#33258;&#22238;&#24402;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#36828;&#26399;&#12290;</title><link>http://arxiv.org/abs/2303.09489</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Effectively Modeling Time Series with Simple Discrete State Spaces. (arXiv:2303.09489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09489
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SpaceTime&#30340;&#29366;&#24577;&#31354;&#38388;&#26102;&#38388;&#24207;&#21015;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20276;&#38543;&#30697;&#38453;&#30340;&#26032;&#30340;SSM&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#33258;&#22238;&#24402;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#36828;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26159;&#19968;&#20010;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#26041;&#27861;&#20855;&#22791;&#20197;&#19979;&#19977;&#20010;&#29305;&#24615;&#65306;&#34920;&#36798;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#39044;&#27979;&#36828;&#26399;&#65292;&#20197;&#21450;&#26377;&#25928;&#35757;&#32451;&#38271;&#24207;&#21015;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#26159;&#26102;&#38388;&#24207;&#21015;&#30340;&#32463;&#20856;&#27169;&#22411;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#23558;SSMs&#19982;&#28145;&#24230;&#23398;&#20064;&#23618;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20808;&#21069;&#26041;&#27861;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;SSM&#34920;&#31034;&#19981;&#33021;&#34920;&#36798;&#33258;&#22238;&#24402;&#30340;&#26102;&#38388;&#24207;&#21015;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SpaceTime&#65292;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#26102;&#38388;&#24207;&#21015;&#26550;&#26500;&#65292;&#25913;&#36827;&#20102;&#25152;&#26377;&#19977;&#20010;&#29305;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#34920;&#29616;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20276;&#38543;&#30697;&#38453;&#30340;&#26032;&#30340;SSM&#21442;&#25968;&#21270;&#26041;&#27861;&#8212;&#8212;&#31163;&#25955;&#26102;&#38388;&#36807;&#31243;&#30340;&#35268;&#33539;&#34920;&#31034;&#8212;&#8212;&#23427;&#20351;&#24471;SpaceTime&#30340;SSM&#23618;&#33021;&#22815;&#23398;&#20064;&#31216;&#24515;&#30340;&#33258;&#22238;&#24402;&#36807;&#31243;&#12290;&#20026;&#20102;&#39044;&#27979;&#36828;&#26399;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#8220;&#38381;&#29615;&#8221;&#21464;&#20307;&#30340;&#20276;&#38543;SSM&#65292;&#20351;&#24471;SpaceTime&#33021;&#22815;&#39044;&#27979;&#35768;&#22810;&#23558;&#26469;&#30340;&#26102;&#38388;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -- a canonical representation for discrete-time processes -- which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a "closed-loop" variation of the companion SSM, which enables SpaceTime to predict many future time
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20351;&#29992;&#22810;&#27169;&#24577;MRI&#36827;&#34892;&#21330;&#20013;&#39044;&#27979;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#22810;&#20010;MRI&#27169;&#24577;&#21644;&#21387;&#32553;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09484</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#32534;&#30721;-&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#20351;&#29992;&#22810;&#27169;&#24577;MRI&#25968;&#25454;&#36827;&#34892;&#21330;&#20013;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Novel Autoencoders-LSTM Model for Stroke Outcome Prediction using Multimodal MRI Data. (arXiv:2303.09484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20351;&#29992;&#22810;&#27169;&#24577;MRI&#36827;&#34892;&#21330;&#20013;&#39044;&#27979;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#22810;&#20010;MRI&#27169;&#24577;&#21644;&#21387;&#32553;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#20154;&#39044;&#21518;&#39044;&#27979;&#23545;&#32570;&#34880;&#24615;&#21330;&#20013;&#30340;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20351;&#29992;&#22810;&#27169;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#36827;&#34892;&#21330;&#20013;&#39044;&#21518;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30001;&#20004;&#20010;&#20018;&#34892;&#32423;&#21035;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;(AEs)&#32452;&#25104;&#65292;&#22312;&#31532;&#19968;&#32423;&#21035;&#30340;&#19981;&#21516;AE&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;MRI&#27169;&#24577;&#23398;&#20064;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#31532;&#20108;&#32423;&#21035;&#30340;AE&#23558;&#21333;&#27169;&#24577;&#29305;&#24449;&#32452;&#21512;&#25104;&#21387;&#32553;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#32473;&#23450;&#30149;&#20154;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#24207;&#21015;&#28982;&#21518;&#30001;LSTM&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;AE2-LSTM&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;MRI&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#21644;&#20307;&#31215;&#24615;&#36136;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;AE2-LSTM&#27169;&#22411;&#36890;&#36807;&#36798;&#21040;&#26368;&#39640;AUC=0.71&#21644;&#26368;&#20302;MAE=0.34&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patient outcome prediction is critical in management of ischemic stroke. In this paper, a novel machine learning model is proposed for stroke outcome prediction using multimodal Magnetic Resonance Imaging (MRI). The proposed model consists of two serial levels of Autoencoders (AEs), where different AEs at level 1 are used for learning unimodal features from different MRI modalities and a AE at level 2 is used to combine the unimodal features into compressed multimodal features. The sequences of multimodal features of a given patient are then used by an LSTM network for predicting outcome score. The proposed AE2-LSTM model is proved to be an effective approach for better addressing the multimodality and volumetric nature of MRI data. Experimental results show that the proposed AE2-LSTM outperforms the existing state-of-the art models by achieving highest AUC=0.71 and lowest MAE=0.34.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;ANCL&#65289;&#65292;&#36890;&#36807;&#23545;&#27969;&#20449;&#24687;&#30340;&#25511;&#21046;&#65292;&#33258;&#28982;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.09483</link><description>&lt;p&gt;
&#36741;&#21161;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;ANCL&#65289;&#65292;&#36890;&#36807;&#23545;&#27969;&#20449;&#24687;&#30340;&#25511;&#21046;&#65292;&#33258;&#28982;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26377;&#21161;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33258;&#28982;&#33021;&#21147;&#30456;&#27604;&#65292;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#27169;&#22411;&#22312;&#34987;&#20248;&#21270;&#20026;&#26032;&#20219;&#21153;&#21518;&#65292;&#22312;&#26087;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#24613;&#21095;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31038;&#21306;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#65288;&#21487;&#22609;&#24615;&#65289;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20197;&#21069;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#31934;&#24230;&#65288;&#31283;&#23450;&#24615;&#65289;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#20294;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#36824;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#20854;&#22522;&#26412;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#36741;&#21161;&#32593;&#32476;&#25345;&#32493;&#23398;&#20064;&#65288;ANCL&#65289;&#65292;&#23427;&#23558;&#19968;&#20010;&#39069;&#22806;&#30340;&#36741;&#21161;&#32593;&#32476;&#24212;&#29992;&#20110;&#20027;&#35201;&#20851;&#27880;&#31283;&#23450;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#20419;&#36827;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#25511;&#21046;&#20027;&#35201;&#32593;&#32476;&#21644;&#36741;&#21161;&#32593;&#32476;&#20043;&#38388;&#20449;&#24687;&#30340;&#27969;&#21160;&#26469;&#33258;&#28982;&#22320;&#25554;&#20540;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ANCL&#22312;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#38544;&#24335;&#22320;&#20248;&#21270;&#20219;&#24847;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#65292;&#20174;&#32780;&#21152;&#36895;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.09478</link><description>&lt;p&gt;
&#31616;&#21333;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#30340;&#20219;&#24847;&#38454;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Arbitrary Order Meta-Learning with Simple Population-Based Evolution. (arXiv:2303.09478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#38544;&#24335;&#22320;&#20248;&#21270;&#20219;&#24847;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#65292;&#20174;&#32780;&#21152;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#20351;&#23398;&#20064;&#31995;&#32479;&#36805;&#36895;&#12289;&#28789;&#27963;&#22320;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#36890;&#24120;&#38656;&#35201;&#23450;&#20041;&#19968;&#32452;&#22806;&#24490;&#29615;&#20803;&#21442;&#25968;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#26356;&#26032;&#19968;&#32452;&#20869;&#37096;&#24490;&#29615;&#21442;&#25968;&#12290;&#22823;&#22810;&#25968;&#20803;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#22797;&#26434;&#30340;&#12289;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#26696;&#26469;&#26356;&#26032;&#36825;&#20123;&#20803;&#21442;&#25968;&#65292;&#20294;&#26631;&#20934;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#24448;&#24448;&#19981;&#36866;&#29992;&#20110;&#26356;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#65292;&#22240;&#20026;&#20803;&#20248;&#21270;&#36807;&#31243;&#21464;&#24471;&#22826;&#22797;&#26434;&#25110;&#19981;&#31283;&#23450;&#12290;&#21463;&#21040;&#30495;&#23454;&#19990;&#30028;&#36827;&#21270;&#20013;&#39640;&#38454;&#20803;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#20351;&#29992;&#31616;&#21333;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#21487;&#20197;&#38544;&#24335;&#22320;&#20248;&#21270;&#20219;&#24847;&#39640;&#38454;&#30340;&#20803;&#21442;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#22522;&#20110;&#32676;&#20307;&#36827;&#21270;&#38544;&#24335;&#22320;&#20248;&#21270;&#20102;&#20219;&#24847;&#38454;&#20803;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning, the notion of learning to learn, enables learning systems to quickly and flexibly solve new tasks. This usually involves defining a set of outer-loop meta-parameters that are then used to update a set of inner-loop parameters. Most meta-learning approaches use complicated and computationally expensive bi-level optimisation schemes to update these meta-parameters. Ideally, systems should perform multiple orders of meta-learning, i.e. to learn to learn to learn and so on, to accelerate their own learning. Unfortunately, standard meta-learning techniques are often inappropriate for these higher-order meta-parameters because the meta-optimisation procedure becomes too complicated or unstable. Inspired by the higher-order meta-learning we observe in real-world evolution, we show that using simple population-based evolution implicitly optimises for arbitrarily-high order meta-parameters. First, we theoretically prove and empirically show that population-based evolution implici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24191;&#20041;&#31209;&#21644;&#39640;&#32500;&#24773;&#20917;&#19979;&#27491;&#21322;&#23450;&#30697;&#38453;&#21435;&#22122;&#38382;&#39064;&#30340;&#26799;&#24230;&#27969;&#65292;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#36830;&#32493;&#30456;&#21464;&#12290;</title><link>http://arxiv.org/abs/2303.09474</link><description>&lt;p&gt;
&#24191;&#20041;&#31209;&#27491;&#21322;&#23450;&#30697;&#38453;&#21435;&#22122;&#30340;&#26799;&#24230;&#27969;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Gradient flow on extensive-rank positive semi-definite matrix denoising. (arXiv:2303.09474v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24191;&#20041;&#31209;&#21644;&#39640;&#32500;&#24773;&#20917;&#19979;&#27491;&#21322;&#23450;&#30697;&#38453;&#21435;&#22122;&#38382;&#39064;&#30340;&#26799;&#24230;&#27969;&#65292;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#36830;&#32493;&#30456;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24191;&#20041;&#31209;&#21644;&#39640;&#32500;&#24773;&#20917;&#19979;&#27491;&#21322;&#23450;&#30697;&#38453;&#21435;&#22122;&#38382;&#39064;&#30340;&#26799;&#24230;&#27969;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#30340;&#32447;&#24615;&#30697;&#38453;&#29702;&#35770;&#25216;&#26415;&#25512;&#23548;&#20986;&#22266;&#23450;&#28857;&#26041;&#31243;&#65292;&#36319;&#36394;&#38382;&#39064;&#30340;&#30697;&#38453;&#22343;&#26041;&#24046;&#30340;&#23436;&#25972;&#26102;&#38388;&#28436;&#21270;&#12290;&#25152;&#24471;&#21040;&#30340;&#22266;&#23450;&#28857;&#26041;&#31243;&#30340;&#39044;&#27979;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#24471;&#21040;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#20030;&#20363;&#31616;&#35201;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#24418;&#24335;&#20027;&#20041;&#30340;&#20960;&#20010;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#25581;&#31034;&#20102;&#24191;&#20041;&#31209;&#21644;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#36830;&#32493;&#30456;&#21464;&#65292;&#36825;&#20123;&#30456;&#21464;&#22312;&#36866;&#24403;&#30340;&#26497;&#38480;&#19979;&#19982;&#20302;&#31209;&#38382;&#39064;&#30340;&#32463;&#20856;&#30456;&#21464;&#26377;&#20851;&#12290;&#35813;&#24418;&#24335;&#20027;&#20041;&#26377;&#27604;&#26412;&#25991;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a new approach to analyze the gradient flow for a positive semi-definite matrix denoising problem in an extensive-rank and high-dimensional regime. We use recent linear pencil techniques of random matrix theory to derive fixed point equations which track the complete time evolution of the matrix-mean-square-error of the problem. The predictions of the resulting fixed point equations are validated by numerical experiments. In this short note we briefly illustrate a few predictions of our formalism by way of examples, and in particular we uncover continuous phase transitions in the extensive-rank and high-dimensional regime, which connect to the classical phase transitions of the low-rank problem in the appropriate limit. The formalism has much wider applicability than shown in this communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615; &#12290;</title><link>http://arxiv.org/abs/2303.09470</link><description>&lt;p&gt;
&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels. (arXiv:2303.09470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615; &#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#29289;&#21697;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24039;&#22937;&#22320;&#20351;&#29992;&#36317;&#31163;&#31867;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20877;&#32467;&#21512;&#25240;&#25187;&#31574;&#30053;&#20197;&#20943;&#23569;&#36317;&#31163;&#25152;&#26377;&#31867;&#20013;&#24515;&#65288;&#21363;&#24322;&#24120;&#20540;&#65289;&#36828;&#30340;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#24819;&#27861;&#65306;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#36317;&#31163;&#21508;&#33258;&#31867;&#20013;&#24515;&#26356;&#36828;&#30340;&#26679;&#26412;&#26356;&#21487;&#33021;&#26159;&#22122;&#22768;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#39046;&#22495;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new approach for addressing the challenge of training machine learning models in the presence of noisy labels. By combining a clever usage of distance to class centroids in the items' latent space with a discounting strategy to reduce the importance of samples far away from all the class centroids (i.e., outliers), our method effectively addresses the issue of noisy labels. Our approach is based on the idea that samples farther away from their respective class centroid in the early stages of training are more likely to be noisy. We demonstrate the effectiveness of our method through extensive experiments on several popular benchmark datasets. Our results show that our approach outperforms the state-of-the-art in this area, achieving significant improvements in classification accuracy when the dataset contains noisy labels.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#22797;&#26434;&#24230;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;Bernoulli&#20998;&#24067;&#26368;&#20339;&#33218;&#35782;&#21035;&#31561;&#20219;&#21153;&#26102;&#26080;&#27861;&#23454;&#29616;&#32479;&#19968;&#26368;&#20339;&#21487;&#36798;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09468</link><description>&lt;p&gt;
&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#30340;&#22797;&#26434;&#24230;&#23384;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Existence of a Complexity in Fixed Budget Bandit Identification. (arXiv:2303.09468v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#22797;&#26434;&#24230;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;Bernoulli&#20998;&#24067;&#26368;&#20339;&#33218;&#35782;&#21035;&#31561;&#20219;&#21153;&#26102;&#26080;&#27861;&#23454;&#29616;&#32479;&#19968;&#26368;&#20339;&#21487;&#36798;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22266;&#23450;&#39044;&#31639;&#36172;&#21338;&#26426;&#26631;&#35782;&#20013;&#65292;&#31639;&#27861;&#25353;&#39034;&#24207;&#35266;&#23519;&#26469;&#33258;&#22810;&#20010;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#30452;&#21040;&#32473;&#23450;&#26368;&#32456;&#26102;&#38388;&#12290;&#28982;&#21518;&#65292;&#23427;&#22238;&#31572;&#20851;&#20110;&#20998;&#24067;&#38598;&#30340;&#26597;&#35810;&#12290;&#19968;&#20010;&#22909;&#30340;&#31639;&#27861;&#23558;&#26377;&#23567;&#30340;&#38169;&#35823;&#27010;&#29575;&#12290;&#34429;&#28982;&#36825;&#20010;&#27010;&#29575;&#38543;&#30528;&#26368;&#32456;&#26102;&#38388;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#19979;&#38477;&#65292;&#20294;&#23545;&#20110;&#22823;&#22810;&#25968;&#26631;&#35782;&#20219;&#21153;&#65292;&#26368;&#20339;&#21487;&#36798;&#29575;&#24182;&#38750;&#31934;&#30830;&#24050;&#30693;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#22266;&#23450;&#39044;&#31639;&#20219;&#21153;&#25509;&#21463;&#22797;&#26434;&#24230;&#65288;&#23450;&#20041;&#20026;&#21333;&#20010;&#31639;&#27861;&#22312;&#25152;&#26377;&#36172;&#21338;&#38382;&#39064;&#20013;&#23454;&#29616;&#30340;&#38169;&#35823;&#27010;&#29575;&#30340;&#19979;&#38480;&#65289;&#65292;&#21017;&#35813;&#22797;&#26434;&#24230;&#30001;&#35813;&#38382;&#39064;&#30340;&#26368;&#20339;&#38750;&#33258;&#36866;&#24212;&#25277;&#26679;&#36807;&#31243;&#30830;&#23450;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20960;&#20010;&#22266;&#23450;&#39044;&#31639;&#35782;&#21035;&#20219;&#21153;&#65292;&#21253;&#25324;&#20855;&#26377;&#20004;&#20010;&#33218;&#30340;&#20271;&#21162;&#21033;&#26368;&#20339;&#33218;&#35782;&#21035;&#65292;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#22797;&#26434;&#24230;&#65306;&#27809;&#26377;&#21333;&#20010;&#31639;&#27861;&#33021;&#22815;&#38543;&#22788;&#23454;&#29616;&#26368;&#20339;&#21487;&#33021;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by a single algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#30340;&#36716;&#24405;&#19978;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#31181;&#27169;&#22411;&#24615;&#33021;&#20248;&#36234;&#65292;&#20351;&#29992;&#26356;&#30456;&#20284;&#30340;&#35821;&#35328;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.09455</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35270;&#35273;&#35821;&#38899;&#34920;&#31034;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Cross-lingual Visual Speech Representations. (arXiv:2303.09455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#30340;&#36716;&#24405;&#19978;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#31181;&#27169;&#22411;&#24615;&#33021;&#20248;&#36234;&#65292;&#20351;&#29992;&#26356;&#30456;&#20284;&#30340;&#35821;&#35328;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#36817;&#24180;&#26469;&#36880;&#28176;&#27969;&#34892;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#24403;&#21069;&#30456;&#20851;&#24037;&#20316;&#20165;&#38480;&#20110;&#21033;&#29992;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;RAVEn&#26694;&#26550;&#65292;&#23545;&#26410;&#26631;&#35760;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#30340;&#36716;&#24405;&#19978;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#20855;&#26377;&#26356;&#22810;&#25968;&#25454;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#31181;&#27169;&#22411;&#65292;&#20294;&#24403;&#25968;&#25454;&#37327;&#22266;&#23450;&#26102;&#65292;&#21333;&#35821;&#31181;&#27169;&#22411;&#24448;&#24448;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65307;&#65288;2&#65289;&#22810;&#35821;&#31181;&#20248;&#20110;&#20165;&#33521;&#35821;&#39044;&#35757;&#32451;&#65307;&#65288;3&#65289;&#20351;&#29992;&#26356;&#30456;&#20284;&#30340;&#35821;&#35328;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#65307;&#65288;4&#65289;&#22312;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#19982;&#22312;&#39044;&#35757;&#32451;&#38598;&#20013;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30456;&#24403;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#21551;&#21457;&#26410;&#26469;&#20851;&#20110;&#38750;&#33521;&#35821;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21407;&#23376;&#32467;&#26500;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#26500;&#24314;&#39044;&#27979;DFT&#20195;&#29702;&#24182;&#20998;&#26512;&#29305;&#24449;&#37325;&#35201;&#24615;&#26469;&#35299;&#20915;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09453</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#20174;&#21407;&#23376;&#32467;&#26500;&#20013;&#21457;&#29616;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Knowledge Discovery from Atomic Structures using Feature Importances. (arXiv:2303.09453v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21407;&#23376;&#32467;&#26500;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#26500;&#24314;&#39044;&#27979;DFT&#20195;&#29702;&#24182;&#20998;&#26512;&#29305;&#24449;&#37325;&#35201;&#24615;&#26469;&#35299;&#20915;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35774;&#35745;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#30340;&#26032;&#22411;&#26448;&#26009;&#65292;&#28145;&#20837;&#29702;&#35299;&#21407;&#23376;&#32467;&#26500;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#38656;&#27714;&#36229;&#20986;&#20102;&#20102;&#35299;&#21407;&#23376;&#25968;&#37327;&#12289;&#31867;&#22411;&#12289;&#21270;&#23398;&#32452;&#25104;&#21644;&#21270;&#23398;&#30456;&#20114;&#20316;&#29992;&#24615;&#36136;&#31561;&#22522;&#30784;&#30693;&#35782;&#12290;&#26356;&#22823;&#30340;&#22270;&#26223;&#21457;&#29983;&#22312;&#37327;&#23376;&#23618;&#38754;&#19978;&#65292;&#21487;&#20197;&#36890;&#36807;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;DFT&#26159;&#19968;&#20010;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#30340;&#36807;&#31243;&#65292;&#20854;&#32467;&#26524;&#24182;&#19981;&#33021;&#25552;&#20379;&#20415;&#20110;&#35299;&#37322;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#35265;&#35299;&#65292;&#36825;&#20123;&#35265;&#35299;&#21017;&#23545;&#26448;&#26009;&#35774;&#35745;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#35299;&#20915;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#21478;&#19968;&#31181;&#26041;&#24335;&#26159;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26500;&#24314;&#24182;&#20998;&#26512;&#39044;&#27979;&#24615;DFT&#20195;&#29702;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20351;&#29992;&#26368;&#36817;&#21457;&#34920;&#30340;&#21487;&#35299;&#37322;&#36317;&#31163;&#22238;&#24402;&#26041;&#27861;&#30340;&#20462;&#25913;&#29256;&#26412;&#25552;&#20986;&#36825;&#26679;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#20351;&#29992;&#20856;&#22411;&#30340;&#37329;&#23646;&#30844;&#28919;&#22242;&#31751;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular-level understanding of the interactions between the constituents of an atomic structure is essential for designing novel materials in various applications. This need goes beyond the basic knowledge of the number and types of atoms, their chemical composition, and the character of the chemical interactions. The bigger picture takes place on the quantum level which can be addressed by using the Density-functional theory (DFT). Use of DFT, however, is a computationally taxing process, and its results do not readily provide easily interpretable insight into the atomic interactions which would be useful information in material design. An alternative way to address atomic interactions is to use an interpretable machine learning approach, where a predictive DFT surrogate is constructed and analyzed. The purpose of this paper is to propose such a procedure using a modification of the recently published interpretable distance-based regression method. Our tests with a representative be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09447</link><description>&lt;p&gt;
&#20351;&#29992;Prompt-Tuning&#30340;&#21407;&#22411;&#36716;&#21521;&#38024;&#23545;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20316;&#20026;&#31867;&#21035;&#23884;&#20837;&#30340;&#19968;&#31181;&#34920;&#31034;&#65292;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#30340;&#20869;&#23384;&#21344;&#29992;&#25110;&#20943;&#36731;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#23548;&#33268;&#30340;&#24615;&#33021;&#24613;&#21095;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65288;CPP&#65289;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#35843;&#25972;&#65292;&#24403;&#22312;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19978;&#36827;&#34892;&#20248;&#21270;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38556;&#30861;&#24182;&#26174;&#30528;&#25552;&#39640;&#21407;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPP&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;CPP&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#23427;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#19979;&#36830;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.09446</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#36755;&#20837;&#25511;&#21046;&#39640;&#32500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#65292;&#21363;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#27492;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#29983;&#25104;&#39640;&#24230;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#25509;&#21475;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29992;&#25143;&#25110;&#25163;&#21160;&#25506;&#32034;&#19981;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25110;&#32773;&#36153;&#21147;&#22320;&#27880;&#37322;&#25968;&#25454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#31232;&#30095;&#12289;&#26131;&#20110;&#20154;&#29702;&#35299;&#30340;&#25511;&#21046;&#31354;&#38388;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#38901;&#24459;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#23454;&#20363;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MICVAE)&#65292;&#23427;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32534;&#30721;&#31232;&#30095;&#30340;&#38901;&#24459;&#29305;&#24449;&#24182;&#36755;&#20986;&#23436;&#25972;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MICVAE&#34920;&#29616;&#20986;&#20102;&#31232;&#30095;&#30340;&#20154;&#22312;&#29615;&#36335;&#25511;&#21046;&#26426;&#21046;&#25152;&#38656;&#30340;&#33391;&#22909;&#21697;&#36136;&#65306;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24615;&#12290;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#23569;&#37327;&#30340;&#36755;&#20837;&#25968;&#20540;(~4)&#65292;MICVAE&#20063;&#33021;&#35753;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32954;&#37096;&#20998;&#21106;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20854;&#39564;&#35777; F1 &#20998;&#25968;&#22312;&#39044;&#27979; CT &#25195;&#25551;&#20013; COVID-19 &#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#26174;&#33879;&#36229;&#36807;&#22522;&#32447;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.09440</link><description>&lt;p&gt;
&#21033;&#29992;&#32954;&#37096;&#20998;&#21106;&#22686;&#24378;CT&#25195;&#25551;&#26816;&#27979;COVID-19&#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Enhanced detection of the presence and severity of COVID-19 from CT scans using lung segmentation. (arXiv:2303.09440v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32954;&#37096;&#20998;&#21106;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20854;&#39564;&#35777; F1 &#20998;&#25968;&#22312;&#39044;&#27979; CT &#25195;&#25551;&#20013; COVID-19 &#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#26174;&#33879;&#36229;&#36807;&#22522;&#32447;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#25104;&#20687;&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#23558;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26356;&#22810;&#27835;&#30103;&#24739;&#32773;&#30340;&#36873;&#25321;&#12290;&#22312;2023&#24180;&#30340;AI-MIA-COV19D &#31454;&#36187;&#20013;&#65292;&#36890;&#36807; CT &#25195;&#25551;&#26816;&#27979; COVID-19 &#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24471;&#21040;&#20102;&#27979;&#35797;&#21644;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;2022&#24180;&#31454;&#36187;&#25552;&#20132;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411; Cov3d &#30340;&#31532;&#20108;&#20010;&#29256;&#26412;&#65292;&#36890;&#36807;&#23545; CT &#25195;&#25551;&#36827;&#34892;&#32954;&#37096;&#20998;&#21106;&#21644;&#35009;&#21098;&#36755;&#20837;&#21040;&#35813;&#21306;&#22495;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#22312;&#39044;&#27979; CT &#25195;&#25551;&#20013; COVID-19 &#23384;&#22312;&#26041;&#38754;&#65292;&#35813;&#27169;&#22411;&#24471;&#21040;&#20102;92.2%&#30340;&#39564;&#35777;&#23439; F1 &#20998;&#25968;&#65292;&#26126;&#26174;&#39640;&#20110;&#22522;&#32447;74%&#12290;&#23427;&#23545;&#20219;&#21153;&#20108;&#30340;&#39564;&#35777;&#38598;&#39044;&#27979; COVID-19 &#30340;&#20005;&#37325;&#31243;&#24230;&#30340;&#23439; F1 &#20998;&#25968;&#20026;67%&#65292;&#39640;&#20110;&#22522;&#32447;38%&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving automated analysis of medical imaging will provide clinicians more options in providing care for patients. The 2023 AI-enabled Medical Image Analysis Workshop and Covid-19 Diagnosis Competition (AI-MIA-COV19D) provides an opportunity to test and refine machine learning methods for detecting the presence and severity of COVID-19 in patients from CT scans. This paper presents version 2 of Cov3d, a deep learning model submitted in the 2022 competition. The model has been improved through a preprocessing step which segments the lungs in the CT scan and crops the input to this region. It results in a validation macro F1 score for predicting the presence of COVID-19 in the CT scans at 92.2% which is significantly above the baseline of 74%. It gives a macro F1 score for predicting the severity of COVID-19 on the validation set for task 2 as 67% which is above the baseline of 38%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#21644;&#32771;&#34385;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#29190;&#21457;&#24615;&#30340;&#26032;&#22411;&#35268;&#33539;&#21270;&#36807;&#31243;&#30340;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09407</link><description>&lt;p&gt;
&#22522;&#20110;&#32771;&#34385;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#29190;&#21457;&#24615;&#30340;CNN&#32929;&#31080;&#20132;&#26131;&#27169;&#22411;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving CNN-base Stock Trading By Considering Data Heterogeneity and Burst. (arXiv:2303.09407v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#21644;&#32771;&#34385;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#29190;&#21457;&#24615;&#30340;&#26032;&#22411;&#35268;&#33539;&#21270;&#36807;&#31243;&#30340;&#26234;&#33021;&#32929;&#31080;&#20132;&#26131;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#26234;&#33021;&#25216;&#26415;&#24212;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#24050;&#26377;&#22810;&#27425;&#23581;&#35797;&#65292;&#26500;&#24314;&#22522;&#20110;&#21382;&#21490;&#32929;&#20215;&#30340;&#33258;&#21160;&#12289;&#26234;&#33021;&#20132;&#26131;&#26694;&#26550;&#12290;&#30001;&#20110;&#37329;&#34701;&#24066;&#22330;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#27874;&#21160;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#20063;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#26234;&#33021;&#20132;&#26131;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;CNN&#20316;&#20026;&#36825;&#31181;&#26694;&#26550;&#30340;&#26680;&#24515;&#21151;&#33021;&#65292;&#22240;&#20026;&#23427;&#33021;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65288;&#21363;&#34892;&#21644;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#35268;&#33539;&#21270;&#36807;&#31243;&#26469;&#20934;&#22791;&#32929;&#31080;&#25968;&#25454;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#26131;&#26694;&#26550;&#19981;&#21516;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#32929;&#31080;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#24322;&#36136;&#24615;&#21644;&#29190;&#21457;&#24615;&#30340;&#65292;&#28982;&#21518;&#20174;&#32479;&#35745;&#30340;&#35282;&#24230;&#39564;&#35777;&#20102;&#32929;&#31080;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#29190;&#21457;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25968;&#25454;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#20197;&#20351;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#24471;&#21040;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been quite a few attempts to apply intelligent techniques to financial trading, i.e., constructing automatic and intelligent trading framework based on historical stock price. Due to the unpredictable, uncertainty and volatile nature of financial market, researchers have also resorted to deep learning to construct the intelligent trading framework. In this paper, we propose to use CNN as the core functionality of such framework, because it is able to learn the spatial dependency (i.e., between rows and columns) of the input data. However, different with existing deep learning-based trading frameworks, we develop novel normalization process to prepare the stock data. In particular, we first empirically observe that the stock data is intrinsically heterogeneous and bursty, and then validate the heterogeneity and burst nature of stock data from a statistical perspective. Next, we design the data normalization method in a way such that the data heterogeneity is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;LSTM-GCN&#65292;&#23427;&#33021;&#22815;&#32467;&#21512;&#20215;&#20540;&#38142;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#20197;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25429;&#33719;&#20215;&#20540;&#38142;&#25968;&#25454;&#20013;&#26410;&#21453;&#26144;&#22312;&#20215;&#26684;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#20132;&#26131;&#32773;&#20248;&#21270;&#20854;&#20132;&#26131;&#31574;&#30053;&#21644;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;</title><link>http://arxiv.org/abs/2303.09406</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#38142;&#25968;&#25454;&#30340;&#26102;&#38388;&#22270;&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stock Price Prediction Using Temporal Graph Model with Value Chain Data. (arXiv:2303.09406v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;LSTM-GCN&#65292;&#23427;&#33021;&#22815;&#32467;&#21512;&#20215;&#20540;&#38142;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#20197;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25429;&#33719;&#20215;&#20540;&#38142;&#25968;&#25454;&#20013;&#26410;&#21453;&#26144;&#22312;&#20215;&#26684;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#20132;&#26131;&#32773;&#20248;&#21270;&#20854;&#20132;&#26131;&#31574;&#30053;&#21644;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#26159;&#37329;&#34701;&#20132;&#26131;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20803;&#32032;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20132;&#26131;&#32773;&#36827;&#34892;&#20080;&#21334;&#21644;&#25345;&#26377;&#32929;&#31080;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32929;&#31080;&#22238;&#25253;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM-GCN&#65289;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32454;&#32990;&#65288;LSTM&#65289;&#12290;GCN&#29992;&#20110;&#25429;&#25417;&#20215;&#20540;&#38142;&#25968;&#25454;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#21644;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;&#32780;LSTM&#21017;&#25429;&#25417;&#32929;&#31080;&#22238;&#25253;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#21160;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#30001;Eurostoxx 600&#21644;S&#65286;P 500&#32452;&#25104;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LSTM-GCN&#27169;&#22411;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#25429;&#33719;&#20215;&#20540;&#38142;&#25968;&#25454;&#20013;&#26410;&#23436;&#20840;&#21453;&#26144;&#22312;&#20215;&#26684;&#25968;&#25454;&#20013;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock price prediction is a crucial element in financial trading as it allows traders to make informed decisions about buying, selling, and holding stocks. Accurate predictions of future stock prices can help traders optimize their trading strategies and maximize their profits. In this paper, we introduce a neural network-based stock return prediction method, the Long Short-Term Memory Graph Convolutional Neural Network (LSTM-GCN) model, which combines the Graph Convolutional Network (GCN) and Long Short-Term Memory (LSTM) Cells. Specifically, the GCN is used to capture complex topological structures and spatial dependence from value chain data, while the LSTM captures temporal dependence and dynamic changes in stock returns data. We evaluated the LSTM-GCN model on two datasets consisting of constituents of Eurostoxx 600 and S&amp;P 500. Our experiments demonstrate that the LSTM-GCN model can capture additional information from value chain data that are not fully reflected in price data, a
&lt;/p&gt;</description></item><item><title>HiT-DVAE&#26159;&#19968;&#31181;&#23618;&#27425;Transformer&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;DVAEs&#22312;&#35821;&#38899;&#39057;&#35889;&#24314;&#27169;&#26041;&#38754;&#12290;&#23427;&#20855;&#26377;&#20004;&#20010;&#28508;&#21464;&#37327;&#27700;&#24179;&#21644;&#31616;&#21333;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#22312;&#20302;&#32423;&#21035;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.09404</link><description>&lt;p&gt;
&#19968;&#31181;&#23618;&#27425;Transformer&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#35821;&#38899;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Speech Modeling with a Hierarchical Transformer Dynamical VAE. (arXiv:2303.09404v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09404
&lt;/p&gt;
&lt;p&gt;
HiT-DVAE&#26159;&#19968;&#31181;&#23618;&#27425;Transformer&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#20248;&#20110;&#20854;&#20182;DVAEs&#22312;&#35821;&#38899;&#39057;&#35889;&#24314;&#27169;&#26041;&#38754;&#12290;&#23427;&#20855;&#26377;&#20004;&#20010;&#28508;&#21464;&#37327;&#27700;&#24179;&#21644;&#31616;&#21333;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#22312;&#20302;&#32423;&#21035;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;DVAEs&#65289;&#26159;&#19968;&#31867;&#28508;&#21464;&#37327;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;VAE&#20197;&#23545;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#24207;&#21015;&#21644;&#30456;&#24212;&#30340;&#28508;&#21521;&#37327;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#25991;&#29486;&#20013;&#30340;&#20960;&#20046;&#25152;&#26377;DVAEs&#20013;&#65292;&#27599;&#20010;&#24207;&#21015;&#20869;&#37096;&#21644;&#20004;&#20010;&#24207;&#21015;&#20043;&#38388;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#37117;&#30001;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Hierarchical Transformer DVAE&#65288;HiT-DVAE&#65289;&#27169;&#25311;&#35821;&#38899;&#20449;&#21495;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#28508;&#21464;&#37327;&#27700;&#24179;&#65288;&#24207;&#21015;&#32423;&#21644;&#24103;&#32423;&#65289;&#30340;DVAE&#65292;&#24182;&#19988;&#20854;&#20013;&#26102;&#38388;&#20381;&#36182;&#24615;&#26159;&#20351;&#29992;Transformer&#26550;&#26500;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HiT-DVAE&#22312;&#35821;&#38899;&#39057;&#35889;&#24314;&#27169;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#33509;&#24178;DVAEs&#65292;&#21516;&#26102;&#36824;&#33021;&#22815;&#23454;&#29616;&#26356;&#31616;&#21333;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#19979;&#28216;&#20302;&#32423;&#21035;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#38899;&#22686;&#24378;&#65289;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical variational autoencoders (DVAEs) are a family of latent-variable deep generative models that extends the VAE to model a sequence of observed data and a corresponding sequence of latent vectors. In almost all the DVAEs of the literature, the temporal dependencies within each sequence and across the two sequences are modeled with recurrent neural networks. In this paper, we propose to model speech signals with the Hierarchical Transformer DVAE (HiT-DVAE), which is a DVAE with two levels of latent variable (sequence-wise and frame-wise) and in which the temporal dependencies are implemented with the Transformer architecture. We show that HiT-DVAE outperforms several other DVAEs for speech spectrogram modeling, while enabling a simpler training procedure, revealing its high potential for downstream low-level speech processing tasks such as speech enhancement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#30340;&#26032;&#21487;&#34892;&#24615;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#24378;&#21046;&#23454;&#26045;&#35813;&#32422;&#26463;&#65292;&#20855;&#26377;&#22312;&#32422;&#26463;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#23454;&#36341;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.09403</link><description>&lt;p&gt;
&#23398;&#20064;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Learning Feasibility Constraints for Control Barrier Functions. (arXiv:2303.09403v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#30340;&#26032;&#21487;&#34892;&#24615;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#24378;&#21046;&#23454;&#26045;&#35813;&#32422;&#26463;&#65292;&#20855;&#26377;&#22312;&#32422;&#26463;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#23454;&#36341;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#21644;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLFs&#65289;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#31283;&#23450;&#20855;&#26377;&#25152;&#38656;&#29366;&#24577;&#21644;&#29366;&#24577;/&#25511;&#21046;&#32422;&#26463;&#30340;&#20223;&#23556;&#25511;&#21046;&#31995;&#32479;&#26102;&#65292;&#20248;&#21270;&#20108;&#27425;&#25104;&#26412;&#21487;&#20197;&#36890;&#36807;&#23558;&#20854;&#20943;&#23569;&#21040;&#19968;&#31995;&#21015;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65288;QPs&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#30830;&#20445;&#36825;&#20123;QPs&#30340;&#21487;&#34892;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38656;&#35201;&#39640;&#38454;CBFs&#30340;&#39640;&#30456;&#23545;&#27425;&#32422;&#26463;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;CBFs&#30340;&#26032;&#21487;&#34892;&#24615;&#32422;&#26463;&#65307;&#28982;&#21518;&#65292;&#36890;&#36807;&#28155;&#21152;&#21040;QPs&#20013;&#30340;&#21478;&#19968;&#20010;&#39640;&#38454;CBF&#26469;&#24378;&#21046;&#25191;&#34892;&#27492;&#32422;&#26463;&#12290;&#32463;&#36807;&#36882;&#24402;&#35757;&#32451;&#31639;&#27861;&#30340;&#19981;&#26029;&#25913;&#36827;&#65292;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#32422;&#26463;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;&#25105;&#20204;&#37325;&#28857;&#38024;&#23545;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#21644;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#20027;&#39550;&#39542;&#30340;&#32422;&#26463;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#23637;&#31034;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown that optimizing quadratic costs while stabilizing affine control systems to desired (sets of) states subject to state and control constraints can be reduced to a sequence of Quadratic Programs (QPs) by using Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs). In this paper, we employ machine learning techniques to ensure the feasibility of these QPs, which is a challenging problem, especially for high relative degree constraints where High Order CBFs (HOCBFs) are required. To this end, we propose a sampling-based learning approach to learn a new feasibility constraint for CBFs; this constraint is then enforced by another HOCBF added to the QPs. The accuracy of the learned feasibility constraint is recursively improved by a recurrent training algorithm. We demonstrate the advantages of the proposed learning approach to constrained optimal control problems with specific focus on a robot control problem and on autonomous driving in an unknown environm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#27604;&#29305;&#24065;&#20215;&#26684;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;3.6&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.09397</link><description>&lt;p&gt;
&#21033;&#29992;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrency Price Prediction using Twitter Sentiment Analysis. (arXiv:2303.09397v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;Twitter&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#27604;&#29305;&#24065;&#20215;&#26684;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21152;&#23494;&#36135;&#24065;&#30340;&#27874;&#21160;&#24615;&#21450;&#22810;&#26679;&#21270;&#30340;&#24847;&#35265;&#65292;&#22312;&#35768;&#22810;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#37117;&#25104;&#20026;&#20102;&#35752;&#35770;&#30340;&#20013;&#24515;&#35805;&#39064;&#12290;Twitter &#36805;&#36895;&#25104;&#20026;&#26032;&#38395;&#26469;&#28304;&#21644;&#27604;&#29305;&#24065;&#35752;&#35770;&#30340;&#23186;&#20171;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26088;&#22312;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#21644;&#25512;&#25991;&#24773;&#24863;&#26469;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21487;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#36716;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25512;&#25991;&#38598;&#30340;&#24773;&#24863;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;&#24773;&#24863;&#20197;&#21450;&#21382;&#21490;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#25968;&#25454;&#12289;&#25512;&#25991;&#25968;&#37327;&#12289;&#29992;&#25143;&#30340;&#36861;&#38543;&#32773;&#25968;&#37327;&#20197;&#21450;&#29992;&#25143;&#26159;&#21542;&#36890;&#36807;&#39564;&#35777;&#26469;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#12290;&#24773;&#24863;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;9.45&#65285;&#65292;&#21453;&#24212;&#20102;&#23454;&#26102;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#24179;&#22343;&#35823;&#24046;&#12290;&#32780;&#20215;&#26684;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#21017;&#20026;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cryptocurrency ecosystem has been the centre of discussion on many social media platforms, following its noted volatility and varied opinions. Twitter is rapidly being utilised as a news source and a medium for bitcoin discussion. Our algorithm seeks to use historical prices and sentiment of tweets to forecast the price of Bitcoin. In this study, we develop an end-to-end model that can forecast the sentiment of a set of tweets (using a Bidirectional Encoder Representations from Transformers - based Neural Network Model) and forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted sentiment and other metrics like historical cryptocurrency price data, tweet volume, a user's following, and whether or not a user is verified. The sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average of real-time data, and test data. The mean absolute percent error for the price prediction was 3.6%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;Auto-TTE&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#24515;&#30005;&#22270;&#29983;&#25104;&#27169;&#22411;&#24182;&#19981;&#21482;&#32771;&#34385;&#21333;&#20010;&#24515;&#30005;&#20449;&#21495;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#22797;&#26434;&#24615;&#12289;&#26356;&#21152;&#30495;&#23454;&#30340;&#29983;&#25104;&#65292;&#22312;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;&#39046;&#22495;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09395</link><description>&lt;p&gt;
&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#30340;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports. (arXiv:2303.09395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;Auto-TTE&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#24515;&#30005;&#22270;&#29983;&#25104;&#27169;&#22411;&#24182;&#19981;&#21482;&#32771;&#34385;&#21333;&#20010;&#24515;&#30005;&#20449;&#21495;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#22797;&#26434;&#24615;&#12289;&#26356;&#21152;&#30495;&#23454;&#30340;&#29983;&#25104;&#65292;&#22312;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;&#39046;&#22495;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#21512;&#25104;&#26159;&#19968;&#31181;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#24515;&#30005;&#20449;&#21495;&#20197;&#20379;&#21307;&#30103;&#20351;&#29992;&#65292;&#26080;&#38656;&#25285;&#24515;&#27880;&#37322;&#25104;&#26412;&#25110;&#20020;&#24202;&#25968;&#25454;&#38544;&#31169;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#24515;&#30005;&#22270;&#29983;&#25104;&#27169;&#22411;&#21482;&#32771;&#34385;&#21333;&#20010;&#24515;&#30005;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#29983;&#25104;&#21333;&#23548;&#32852;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;&#27599;&#20010;&#35786;&#26029;&#20998;&#31867;&#36827;&#34892;&#21333;&#29420;&#30340;&#22521;&#35757;&#12290;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#20998;&#31867;&#26080;&#27861;&#25429;&#25417;&#24515;&#30005;&#22270;&#20043;&#38388;&#30340;&#22797;&#26434;&#24046;&#24322;&#65292;&#36825;&#20123;&#24046;&#24322;&#21462;&#20915;&#20110;&#21508;&#31181;&#29305;&#24449;&#65288;&#20363;&#22914;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#32454;&#33410;&#65292;&#20849;&#23384;&#30340;&#35786;&#26029;&#20998;&#31867;&#31561;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#20219;&#21153;&#65292;&#20854;&#20013;&#20351;&#29992;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#24515;&#30005;&#22270;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;Auto-TTE&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#20020;&#24202;&#25991;&#26412;&#25253;&#21578;&#26465;&#20214;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#39046;&#22495;&#30340;&#20854;&#20182;&#20195;&#34920;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#21040;&#24515;&#30005;&#22270;&#21512;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) synthesis is the area of research focused on generating realistic synthetic ECG signals for medical use without concerns over annotation costs or clinical data privacy restrictions. Traditional ECG generation models consider a single ECG lead and utilize GAN-based generative models. These models can only generate single lead samples and require separate training for each diagnosis class. The diagnosis classes of ECGs are insufficient to capture the intricate differences between ECGs depending on various features (e.g. patient demographic details, co-existing diagnosis classes, etc.). To alleviate these challenges, we present a text-to-ECG task, in which textual inputs are used to produce ECG outputs. Then we propose Auto-TTE, an autoregressive generative model conditioned on clinical text reports to synthesize 12-lead ECGs, for the first time to our knowledge. We compare the performance of our model with other representative models in text-to-speech and text-to-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#26426;&#22312;&#38169;&#35823;&#35268;&#23450;&#30340;&#24773;&#22659;&#19979;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#22312;&#19968;&#23450;&#27700;&#24179;&#20869;&#35823;&#24046;&#21644;&#26368;&#23567;&#27425;&#20248;&#38388;&#38553;&#30456;&#20114;&#21046;&#32422;&#65292;&#20197;&#24120;&#25968;&#35823;&#24046;&#19978;&#38480;&#23454;&#29616;&#38388;&#38553;&#30456;&#20851;&#30340;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.09390</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#26426;&#20013;&#38169;&#35823;&#35268;&#23450;&#19982;&#27425;&#20248;&#38388;&#38553;&#30340;&#30456;&#20114;&#20316;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Interplay Between Misspecification and Sub-optimality Gap in Linear Contextual Bandits. (arXiv:2303.09390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#26426;&#22312;&#38169;&#35823;&#35268;&#23450;&#30340;&#24773;&#22659;&#19979;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#22312;&#19968;&#23450;&#27700;&#24179;&#20869;&#35823;&#24046;&#21644;&#26368;&#23567;&#27425;&#20248;&#38388;&#38553;&#30456;&#20114;&#21046;&#32422;&#65292;&#20197;&#24120;&#25968;&#35823;&#24046;&#19978;&#38480;&#23454;&#29616;&#38388;&#38553;&#30456;&#20851;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#26426;&#22312;&#38169;&#35823;&#35268;&#23450;&#30340;&#24773;&#22659;&#19979;&#65292;&#26399;&#26395;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#20197;&#32447;&#24615;&#20989;&#25968;&#31867;&#26469;&#36924;&#36817;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#36873;&#25321;&#20855;&#26377;&#22823;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#22659;&#21521;&#37327;&#36827;&#34892;&#22312;&#32447;&#22238;&#24402;&#12290;&#24403;&#35823;&#24046;&#35268;&#23450;&#27700;&#24179;&#34987;$\zeta&gt;0$&#25511;&#21046;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#35823;&#24046;&#19978;&#38480;&#19982;&#22909;&#30340;&#25351;&#23450;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#30456;&#21516;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31639;&#27861;&#20063;&#21487;&#20197;&#22312;&#19981;&#30693;&#36947;&#20122;&#20248;&#38388;&#38553;$\Delta$&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#38388;&#38553;&#30456;&#20851;&#30340;&#24120;&#25968;&#35823;&#24046;&#19978;&#38480;&#12290;&#22312;Lattimore et al.&#65288;2020&#65289;&#30340;&#20316;&#21697;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#34920;&#26126;&#20102;&#38169;&#35823;&#35268;&#23450;&#21644;&#27425;&#20248;&#38388;&#38553;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study linear contextual bandits in the misspecified setting, where the expected reward function can be approximated by a linear function class up to a bounded misspecification level $\zeta&gt;0$. We propose an algorithm based on a novel data selection scheme, which only selects the contextual vectors with large uncertainty for online regression. We show that, when the misspecification level $\zeta$ is dominated by $\tilde O (\Delta / \sqrt{d})$ with $\Delta$ being the minimal sub-optimality gap and $d$ being the dimension of the contextual vectors, our algorithm enjoys the same gap-dependent regret bound $\tilde O (d^2/\Delta)$ as in the well-specified setting up to logarithmic factors. In addition, we show that an existing algorithm SupLinUCB (Chu et al., 2011) can also achieve a gap-dependent constant regret bound without the knowledge of sub-optimality gap $\Delta$. Together with a lower bound adapted from Lattimore et al. (2020), our result suggests an interplay between misspecific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010; LLMSecEval &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 150 &#20010;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#23481;&#26131;&#20986;&#29616;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09384</link><description>&lt;p&gt;
LLMSecEval: &#19968;&#20010;&#29992;&#20110;&#23433;&#20840;&#35780;&#20272;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations. (arXiv:2303.09384v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010; LLMSecEval &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 150 &#20010;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#23481;&#26131;&#20986;&#29616;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; Codex &#22312;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#21644;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#20174;&#25968;&#21313;&#20159;&#34892;&#20195;&#30721;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#20844;&#20849; GitHub &#20179;&#24211;&#23398;&#20064;&#35821;&#35328;&#21644;&#32534;&#31243;&#23454;&#36341;&#26469;&#29983;&#25104;&#26469;&#33258;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20195;&#30721;&#29255;&#27573;&#12290;&#23613;&#31649; LLM &#25215;&#35834;&#23454;&#29616;&#36719;&#20214;&#24212;&#29992;&#30340; NL &#39537;&#21160;&#37096;&#32626;&#65292;&#20294;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#35843;&#26597;&#21644;&#35760;&#24405;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LLMSecEval&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547; 150 &#20010; NL &#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;&#36825;&#20123;&#25552;&#31034;&#26159;&#22522;&#20110;MITRE&#30340;&#21069;25&#20010;&#24120;&#35265;&#24369;&#28857;&#21015;&#34920;&#20013;&#23481;&#26131;&#20986;&#29616;&#21508;&#31181;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#29255;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25552;&#31034;&#37117;&#37197;&#26377;&#19968;&#20010;&#23433;&#20840;&#23454;&#29616;&#31034;&#20363;&#65292;&#20197;&#20415;&#19982;&#30001;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language (NL) descriptions by learning languages and programming practices from public GitHub repositories. Although LLMs promise an effortless NL-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present LLMSecEval, a dataset containing 150 NL prompts that can be leveraged for assessing the security performance of such models. Such prompts are NL descriptions of code snippets prone to various security vulnerabilities listed in MITRE's Top 25 Common Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#29992;&#20110;&#35782;&#21035;&#20855;&#26377;&#29983;&#29289;&#23398;&#30456;&#20851;&#24615;&#30340;&#20449;&#24687;&#21464;&#37327;&#12290;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#30340;&#35780;&#20998;&#31639;&#23376;&#21644;&#21487;&#21306;&#20998;&#30340;&#25513;&#30721;&#26469;&#22686;&#24378;&#22270;&#25289;&#26222;&#25289;&#26031;&#25152;&#25429;&#25417;&#30340;&#32467;&#26500;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09381</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21487;&#24494;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Differentiable Unsupervised Feature Selection. (arXiv:2303.09381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#29992;&#20110;&#35782;&#21035;&#20855;&#26377;&#29983;&#29289;&#23398;&#30456;&#20851;&#24615;&#30340;&#20449;&#24687;&#21464;&#37327;&#12290;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#30340;&#35780;&#20998;&#31639;&#23376;&#21644;&#21487;&#21306;&#20998;&#30340;&#25513;&#30721;&#26469;&#22686;&#24378;&#22270;&#25289;&#26222;&#25289;&#26031;&#25152;&#25429;&#25417;&#30340;&#32467;&#26500;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39640;&#36890;&#37327;&#29983;&#29289;&#25968;&#25454;&#26082;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#31185;&#23398;&#26426;&#36935;&#65292;&#21448;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#22312;&#22810;&#27169;&#24577;&#27979;&#37327;&#20013;&#65292;&#27599;&#20010;&#26679;&#26412;&#21516;&#26102;&#34987;&#20004;&#20010;&#25110;&#26356;&#22810;&#32452;&#20256;&#24863;&#22120;&#35266;&#23519;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20004;&#20010;&#27169;&#24577;&#20013;&#30340;&#35768;&#22810;&#35266;&#23519;&#21464;&#37327;&#36890;&#24120;&#37117;&#26159;&#19981;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#19981; carry information about the phenomenon of interest&#12290;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#32806;&#21512;&#30340;&#39640;&#32500;&#27979;&#37327;&#35782;&#21035;&#20449;&#24687;&#21464;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#30340;&#35780;&#20998;&#31639;&#23376;&#12290;&#25105;&#20204;&#23558;&#24471;&#20998;&#19982;&#21487;&#21306;&#20998;&#30340;&#25513;&#30721;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36974;&#34109;&#26080;&#29992;&#29305;&#24449;&#24182;&#22686;&#24378;&#22270;&#25289;&#26222;&#25289;&#26031;&#25152;&#25429;&#25417;&#30340;&#32467;&#26500;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;fMRI&#21644;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#29983;&#29289;&#30456;&#20851;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal high throughput biological data presents a great scientific opportunity and a significant computational challenge. In multi-modal measurements, every sample is observed simultaneously by two or more sets of sensors. In such settings, many observed variables in both modalities are often nuisance and do not carry information about the phenomenon of interest. Here, we propose a multi-modal unsupervised feature selection framework: identifying informative variables based on coupled high-dimensional measurements. Our method is designed to identify features associated with two types of latent low-dimensional structures: (i) shared structures that govern the observations in both modalities and (ii) differential structures that appear in only one modality. To that end, we propose two Laplacian-based scoring operators. We incorporate the scores with differentiable gates that mask nuisance features and enhance the accuracy of the structure captured by the graph Laplacian. The perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09373</link><description>&lt;p&gt;
3D&#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#29992;&#20110;&#24322;&#26500;&#23156;&#20799;&#33041; MRI &#39046;&#22495;&#38388;&#36866;&#24212;&#24615;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#33041; MRI &#22312;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#12289;&#36328;&#22330;&#26223;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#20998;&#21106;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#33945;&#29256;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#26469;&#23545;&#23156;&#20799;&#33041;MRI&#30340;&#19981;&#21516;&#20122;&#30382;&#36136;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#26631;&#35760;&#28304;&#22495;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#34917;&#20805;&#30340;&#20248;&#21183;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#21644;&#22810;&#27169;&#24335;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09367</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned Offline Reinforcement Learning through State Space Partitioning. (arXiv:2303.09367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#34917;&#20805;&#30340;&#20248;&#21183;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#21644;&#22810;&#27169;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20165;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25512;&#26029;&#20986;&#39034;&#24207;&#20915;&#31574;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35201;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#25110;&#32467;&#26524;&#30340;&#30446;&#26631;&#23548;&#21521;&#20915;&#31574;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#21644;&#22810;&#27169;&#24335;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#30340;&#20248;&#21183;&#21152;&#26435;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to infer sequential decision policies using only offline datasets. This is a particularly difficult setup, especially when learning to achieve multiple different goals or outcomes under a given scenario with only sparse rewards. For offline learning of goal-conditioned policies via supervised learning, previous work has shown that an advantage weighted log-likelihood loss guarantees monotonic policy improvement. In this work we argue that, despite its benefits, this approach is still insufficient to fully address the distribution shift and multi-modality problems. The latter is particularly severe in long-horizon tasks where finding a unique and optimal policy that goes from a state to the desired goal is challenging as there may be multiple and potentially conflicting solutions. To tackle these challenges, we propose a complementary advantage-based weighting scheme that introduces an additional source of inductive bias: given a value-based part
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.09366</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#25277;&#21462;&#33539;&#22260;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#27835;&#30103;&#36890;&#24120;&#23545;&#24739;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#12290;&#36829;&#21453;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#65288;MTC&#65289;&#20250;&#23548;&#33268;&#32570;&#20047;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#20197;&#21450;&#19981;&#33391;&#30340;&#20581;&#24247;&#32467;&#26524;&#21644;&#22686;&#21152;&#30340;&#21307;&#30103;&#36153;&#29992;&#12290;&#36825;&#20123;MTC&#22312;&#24739;&#32773;&#25945;&#32946;&#26448;&#26009;&#21644;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#20351;&#29992;&#25351;&#21335;&#65288;DUGs&#65289;&#20013;&#34987;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;&#35745;&#31639;&#19978;&#34920;&#31034;DUGs&#20013;&#30340;MTC&#65292;&#23558;&#26377;&#21161;&#20110;&#36890;&#36807;&#24110;&#21161;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#20998;&#31867;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#27169;&#22411;&#26469;&#35745;&#31639;&#22320;&#34920;&#31034;MTC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19977;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#35745;N = 836&#20010;&#24102;&#26631;&#20934;&#21270;&#30340;MTC&#26631;&#35760;&#30340;DUGs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#65292;&#36328;&#25152;&#26377;&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;0.62&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;ICL&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medications often impose temporal constraints on everyday patient activity. Violations of such medical temporal constraints (MTCs) lead to a lack of treatment adherence, in addition to poor health outcomes and increased healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in both patient education materials and clinical texts. Computationally representing MTCs in DUGs will advance patient-centric healthcare applications by helping to define safe patient activity patterns. We define a novel taxonomy of MTCs found in DUGs and develop a novel context-free grammar (CFG) based model to computationally represent MTCs from unstructured DUGs. Additionally, we release three new datasets with a combined total of N = 836 DUGs labeled with normalized MTCs. We develop an in-context learning (ICL) solution for automatically extracting and normalizing MTCs found in DUGs, achieving an average F1 score of 0.62 across all datasets. Finally, we rigorously investigate ICL model perfor
&lt;/p&gt;</description></item><item><title>&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09354</link><description>&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65306;&#35745;&#31639;&#30149;&#29702;&#23398;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09354
&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#23558;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CompPath&#65289;&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#36716;&#21270;&#20026;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25253;&#21578;&#38590;&#20197;&#37325;&#22797; ML &#32467;&#26524;&#30340;&#22256;&#38590;&#12290;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65288;IDC&#65289;&#26159;&#19968;&#20010;&#20844;&#20849;&#24211;&#65292;&#21253;&#21547; &gt;120 &#20010;&#30284;&#30151;&#22270;&#20687;&#25910;&#38598;&#65292;&#21253;&#25324; &gt;38,000 &#24352;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#65292;&#26088;&#22312;&#19982;&#20113;&#31471; ML &#26381;&#21153;&#19968;&#36215;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102; IDC &#20419;&#36827; CompPath &#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#21147;&#12290; &#26448;&#26009;&#21644;&#26041;&#27861;&#65306;IDC &#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65306;&#25152;&#26377;&#22270;&#20687;&#37117;&#26681;&#25454; DICOM &#26631;&#20934;&#36827;&#34892;&#32534;&#30721;&#65292;&#20855;&#26377;&#25345;&#20037;&#21270;&#26631;&#35782;&#31526;&#12289;&#21487;&#36890;&#36807;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#36827;&#34892;&#21457;&#29616;&#65292;&#24182;&#21487;&#36890;&#36807;&#24320;&#25918;&#24335;&#24037;&#20855;&#35775;&#38382;&#12290;&#20511;&#27492;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312; IDC &#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#38024;&#23545;&#32954;&#30284;&#32452;&#32455;&#20998;&#31867;&#30340;&#19968;&#31181;&#20195;&#34920;&#24615;&#22522;&#20110; ML &#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;/&#25110;&#35780;&#20272;&#12290;&#20026;&#35780;&#20272;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#39564;&#34987;&#22810;&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of &gt;120 cancer image collections, including &gt;38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.09350</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation by learning using privileged information. (arXiv:2303.09350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#21482;&#22312;&#24378;&#20551;&#35774;&#26465;&#20214;&#19979;&#24471;&#20197;&#23454;&#29616;&#65292;&#22914;&#21327;&#21464;&#37327;&#31227;&#20301;&#21644;&#36755;&#20837;&#39046;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#12290;&#21518;&#32773;&#22312;&#39640;&#32500;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#22312;&#38754;&#23545;&#36825;&#31181;&#25361;&#25112;&#26102;&#65292;&#22270;&#20687;&#20998;&#31867;&#20173;&#28982;&#26159;&#31639;&#27861;&#24320;&#21457;&#30340;&#28789;&#24863;&#21644;&#22522;&#20934;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#33719;&#21462;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#30340;&#26377;&#20851;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#25918;&#23485;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#22312;&#23398;&#20064;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20195;&#20215;&#26159;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#21464;&#37327;&#38598;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#23454;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#65292;&#21463;&#21040;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#20943;&#23569;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.09340</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20266;&#24433;&#38477;&#22122;&#30340;&#31232;&#30095;&#35270;&#22270;CT&#22270;&#20687;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39045;&#20869;&#20986;&#34880;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#24555;&#36895;&#19988;&#24120;&#24120;&#38750;&#24120;&#23494;&#38598;&#30340;&#21307;&#30103;&#27835;&#30103;&#12290;&#20026;&#20102;&#35786;&#26029;&#65292;&#36890;&#24120;&#35201;&#36827;&#34892;&#39045;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CCT&#65289;&#25195;&#25551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36752;&#23556;&#24341;&#36215;&#30340;&#22686;&#21152;&#30340;&#20581;&#24247;&#39118;&#38505;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#38477;&#20302;&#36825;&#31181;&#28508;&#22312;&#39118;&#38505;&#30340;&#26368;&#37325;&#35201;&#31574;&#30053;&#26159;&#23613;&#21487;&#33021;&#20445;&#25345;&#36752;&#23556;&#21058;&#37327;&#20302;&#65292;&#24182;&#19982;&#35786;&#26029;&#20219;&#21153;&#19968;&#33268;&#12290; &#31232;&#30095;&#35270;&#22270;CT&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#37319;&#38598;&#30340;&#35270;&#22270;&#24635;&#25968;&#65292;&#20174;&#32780;&#38477;&#20302;&#21058;&#37327;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20294;&#20195;&#20215;&#26159;&#38477;&#20302;&#22270;&#20687;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;U-Net&#26550;&#26500;&#26469;&#20943;&#23569;&#31232;&#30095;&#35270;&#22270;CCT&#30340;&#20266;&#24433;&#65292;&#20174;&#31232;&#30095;&#35270;&#22270;&#20013;&#39044;&#27979;&#23436;&#20840;&#37319;&#26679;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20266;&#24433;&#38477;&#22122;&#21518;&#30340;CCT&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#22270;&#20687;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;ExoplANNET&#65292;&#26088;&#22312;&#35299;&#20915;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#25361;&#25112;&#65292;&#22312;&#23384;&#22312;&#19982;&#26143;&#20307;&#30456;&#20851;&#30340;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#34892;&#26143;&#20449;&#21495;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#32463;&#36807;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09335</link><description>&lt;p&gt;
ExoplANNET: &#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#21644;&#30830;&#35748;&#24452;&#21521;&#36895;&#24230;&#25968;&#25454;&#20013;&#34892;&#26143;&#20449;&#21495;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ExoplANNET: A deep learning algorithm to detect and identify planetary signals in radial velocity data. (arXiv:2303.09335v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;ExoplANNET&#65292;&#26088;&#22312;&#35299;&#20915;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#25361;&#25112;&#65292;&#22312;&#23384;&#22312;&#19982;&#26143;&#20307;&#30456;&#20851;&#30340;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#34892;&#26143;&#20449;&#21495;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#32463;&#36807;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#31995;&#22806;&#34892;&#26143;&#30340;&#26041;&#27861;&#22312;&#20110;&#25506;&#27979;&#30001;&#26410;&#30693;&#19979;&#37096;&#24658;&#26143;&#20276;&#26143;&#24341;&#36215;&#30340;&#24658;&#26143;&#36895;&#24230;&#21464;&#21270;&#12290;&#20202;&#22120;&#35823;&#24046;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#37319;&#26679;&#20197;&#21450;&#28304;&#33258;&#20110;&#24658;&#26143;&#20869;&#22312;&#21464;&#24322;&#30340;&#19981;&#21516;&#22122;&#22768;&#28304;&#21487;&#33021;&#20250;&#38459;&#30861;&#25968;&#25454;&#30340;&#35299;&#37322;&#65292;&#29978;&#33267;&#23548;&#33268;&#34394;&#20551;&#25506;&#27979;&#32467;&#26524;&#12290;&#36817;&#26399;&#65292;&#22312;&#31995;&#22806;&#34892;&#26143;&#39046;&#22495;&#20986;&#29616;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20123;&#32467;&#26524;&#29978;&#33267;&#36229;&#36807;&#20102;&#20256;&#32479;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#22312;&#24452;&#21521;&#36895;&#24230;&#27861;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#19982;&#26143;&#20307;&#30456;&#20851;&#30340;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31995;&#22806;&#34892;&#26143;&#25506;&#27979;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20197;&#20195;&#26367;&#24452;&#21521;&#36895;&#24230;&#27861;&#26816;&#27979;&#21040;&#30340;&#20449;&#21495;&#30340;&#37325;&#35201;&#24615;&#35745;&#31639;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#34892;&#26143;&#36824;&#26159;&#38750;&#34892;&#26143;&#26469;&#28304;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#24050;&#30693;&#26377;&#34892;&#26143;&#31995;&#32479;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26410;&#30693;&#26377;&#34892;&#26143;&#31995;&#32479;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;ExoplANNET&#31639;&#27861;&#23637;&#29616;&#20986;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#24182;&#34920;&#26126;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#31995;&#22806;&#34892;&#26143;&#30340;&#37492;&#23450;&#21644;&#34920;&#24449;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of exoplanets with the radial velocity method consists in detecting variations of the stellar velocity caused by an unseen sub-stellar companion. Instrumental errors, irregular time sampling, and different noise sources originating in the intrinsic variability of the star can hinder the interpretation of the data, and even lead to spurious detections. In recent times, work began to emerge in the field of extrasolar planets that use Machine Learning algorithms, some with results that exceed those obtained with the traditional techniques in the field. We seek to explore the scope of the neural networks in the radial velocity method, in particular for exoplanet detection in the presence of correlated noise of stellar origin. In this work, a neural network is proposed to replace the computation of the significance of the signal detected with the radial velocity method and to classify it as of planetary origin or not. The algorithm is trained using synthetic data of systems wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21508;&#31181;&#35299;&#37322;&#25216;&#26415;&#65292;&#20197;&#31354;&#38388;&#29305;&#24449;&#30340;&#29305;&#24449;&#21464;&#21270;&#26469;&#34920;&#24449;&#27010;&#24565;&#28418;&#31227;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09331</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#27010;&#24565;&#28418;&#31227;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Model Based Explanations of Concept Drift. (arXiv:2303.09331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21508;&#31181;&#35299;&#37322;&#25216;&#26415;&#65292;&#20197;&#31354;&#38388;&#29305;&#24449;&#30340;&#29305;&#24449;&#21464;&#21270;&#26469;&#34920;&#24449;&#27010;&#24565;&#28418;&#31227;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#28418;&#31227;&#26159;&#25351;&#29983;&#25104;&#35266;&#27979;&#25968;&#25454;&#30340;&#20998;&#24067;&#38543;&#26102;&#38388;&#25913;&#21464;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21508;&#31181;&#35299;&#37322;&#25216;&#26415;&#65292;&#20197;&#31354;&#38388;&#29305;&#24449;&#30340;&#29305;&#24449;&#21464;&#21270;&#26469;&#34920;&#24449;&#27010;&#24565;&#28418;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27010;&#24565;&#28418;&#31227;&#30340;&#35299;&#37322;&#31616;&#21270;&#20026;&#37325;&#35201;&#29305;&#24449;&#21450;&#20854;&#31354;&#38388;&#29305;&#24449;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#34920;&#24449;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The notion of concept drift refers to the phenomenon that the distribution generating the observed data changes over time. If drift is present, machine learning models can become inaccurate and need adjustment. While there do exist methods to detect concept drift or to adjust models in the presence of observed drift, the question of explaining drift, i.e., describing the potentially complex and high dimensional change of distribution in a human-understandable fashion, has hardly been considered so far. This problem is of importance since it enables an inspection of the most prominent characteristics of how and where drift manifests itself. Hence, it enables human understanding of the change and it increases acceptance of life-long learning models. In this paper, we present a novel technology characterizing concept drift in terms of the characteristic change of spatial features based on various explanation techniques. To do so, we propose a methodology to reduce the explanation of conce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#35821;&#20041;&#20998;&#21106;&#21644;&#23436;&#20840;&#20108;&#32500;&#21367;&#31215;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26410;&#26469;&#36235;&#21183;&#30340;&#26041;&#27861;</title><link>http://arxiv.org/abs/2303.09323</link><description>&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#65306;&#19968;&#31181;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stock Trend Prediction: A Semantic Segmentation Approach. (arXiv:2303.09323v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#35821;&#20041;&#20998;&#21106;&#21644;&#23436;&#20840;&#20108;&#32500;&#21367;&#31215;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26410;&#26469;&#36235;&#21183;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#37329;&#34701;&#39044;&#27979;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#36235;&#21183;&#39046;&#22495;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#32929;&#24066;&#25968;&#25454;&#20013;&#30340;&#20256;&#32479;&#38382;&#39064;&#65292;&#22914;&#20854;&#26497;&#20854;&#22797;&#26434;&#30340;&#21160;&#24577;&#21644;&#38271;&#26399;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#37319;&#29992;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#26469;&#35828;&#65292;&#23398;&#20064;&#20445;&#25345;&#38271;&#26399;&#20449;&#24687;&#26159;&#22256;&#38590;&#30340;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#29992;&#20110;&#26356;&#22909;&#22320;&#25429;&#25417;&#21160;&#24577;&#24182;&#25552;&#21462;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35821;&#20041;&#20998;&#21106;&#21450;&#20854;&#35774;&#35745;&#33391;&#22909;&#30340;&#20840;&#21367;&#31215;&#32593;&#32476;&#20174;&#26410;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#23494;&#38598;&#20998;&#31867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23436;&#20840;&#20108;&#32500;&#21367;&#31215;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#39044;&#27979;&#38271;&#26399;&#26085;&#24120;&#32929;&#20215;&#21464;&#21160;&#36235;&#21183;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#29983;&#25104;&#21253;&#21547;T&#22825;&#30340;&#27599;&#26085;&#20215;&#26684;&#30340;&#36755;&#20837;&#24103;&#12290;&#30446;&#26631;&#26159;&#36890;&#36807;&#20687;&#32032;&#32423;&#30340;&#20998;&#31867;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Market financial forecasting is a trending area in deep learning. Deep learning models are capable of tackling the classic challenges in stock market data, such as its extremely complicated dynamics as well as long-term temporal correlation. To capture the temporal relationship among these time series, recurrent neural networks are employed. However, it is difficult for recurrent models to learn to keep track of long-term information. Convolutional Neural Networks have been utilized to better capture the dynamics and extract features for both short- and long-term forecasting. However, semantic segmentation and its well-designed fully convolutional networks have never been studied for time-series dense classification. We present a novel approach to predict long-term daily stock price change trends with fully 2D-convolutional encoder-decoders. We generate input frames with daily prices for a time-frame of T days. The aim is to predict future trends by pixel-wise classification of the cur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#31574;&#30053;&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#20998;&#23618;&#20026;k&#25240;&#20132;&#21449;&#39564;&#35777;(CVs)&#65292;&#24182;&#23558;&#39046;&#22495;&#30693;&#35782;&#35299;&#37322;&#25216;&#26415;&#23884;&#20837;&#21040;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;IML&#26694;&#26550;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12289;&#24314;&#31435;&#20449;&#20219;&#65292;&#24182;&#20026;IML&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#25552;&#20379;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.09322</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#35270;&#35282;&#19979;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#23558;&#20998;&#23618;&#21644;&#39046;&#22495;&#30693;&#35782;&#19982;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability from a new lens: Integrating Stratification and Domain knowledge for Biomedical Applications. (arXiv:2303.09322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#31574;&#30053;&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#20998;&#23618;&#20026;k&#25240;&#20132;&#21449;&#39564;&#35777;(CVs)&#65292;&#24182;&#23558;&#39046;&#22495;&#30693;&#35782;&#35299;&#37322;&#25216;&#26415;&#23884;&#20837;&#21040;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;IML&#26694;&#26550;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12289;&#24314;&#31435;&#20449;&#20219;&#65292;&#24182;&#20026;IML&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#25552;&#20379;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#25216;&#26415;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#26085;&#30410;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;COVID-19&#22823;&#27969;&#34892;&#21518;&#25152;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#40657;&#30418;ML&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#33021;&#20250;&#20135;&#29983;&#32570;&#20047;&#20449;&#20219;&#21644;&#37319;&#32435;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#20986;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;ML(IML)&#26041;&#27861;&#65292;&#20294;&#26159;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#31574;&#30053;&#65292;&#29992;&#20110;&#23558;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#20998;&#23618;&#20026;k&#25240;&#20132;&#21449;&#39564;&#35777;(CVs)&#65292;&#24182;&#23558;&#39046;&#22495;&#30693;&#35782;&#35299;&#37322;&#25216;&#26415;&#23884;&#20837;&#21040;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;IML&#26694;&#26550;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12289;&#24314;&#31435;&#20449;&#20219;&#65292;&#24182;&#20026;IML&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#25552;&#20379;&#35299;&#37322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27169;&#22411;&#32467;&#26524;&#65292;&#22914;&#32858;&#21512;&#29305;&#24449;&#26435;&#37325;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#19982;&#26356;&#35814;&#32454;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of machine learning (ML) techniques in the biomedical field has become increasingly important, particularly with the large amounts of data generated by the aftermath of the COVID-19 pandemic. However, due to the complex nature of biomedical datasets and the use of black-box ML models, a lack of trust and adoption by domain experts can arise. In response, interpretable ML (IML) approaches have been developed, but the curse of dimensionality in biomedical datasets can lead to model instability. This paper proposes a novel computational strategy for the stratification of biomedical problem datasets into k-fold cross-validation (CVs) and integrating domain knowledge interpretation techniques embedded into the current state-of-the-art IML frameworks. This approach can improve model stability, establish trust, and provide explanations for outcomes generated by trained IML models. Specifically, the model outcome, such as aggregated feature weight importance, can be linked to further d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20363;&#65292;&#20351;&#29992;&#8220;&#32676;&#20307;&#21453;&#20107;&#23454;&#8221;&#38598;&#20307;&#35299;&#37322;&#31867;&#20284;&#23454;&#20363;&#30340;&#32452;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32676;&#20307;&#21453;&#20107;&#23454;&#31639;&#27861;&#26469;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;&#35299;&#37322;&#65292;&#36866;&#21512;&#20154;&#31867;&#23545;&#36830;&#36143;&#12289;&#24191;&#27867;&#35299;&#37322;&#30340;&#21916;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.09297</link><description>&lt;p&gt;
&#35299;&#37322;&#32676;&#20307;&#23454;&#20363;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#32676;&#20307;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#20351;&#29992;&#26696;&#20363;&#12289;&#31639;&#27861;&#21644;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals. (arXiv:2303.09297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20363;&#65292;&#20351;&#29992;&#8220;&#32676;&#20307;&#21453;&#20107;&#23454;&#8221;&#38598;&#20307;&#35299;&#37322;&#31867;&#20284;&#23454;&#20363;&#30340;&#32452;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32676;&#20307;&#21453;&#20107;&#23454;&#31639;&#27861;&#26469;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;&#35299;&#37322;&#65292;&#36866;&#21512;&#20154;&#31867;&#23545;&#36830;&#36143;&#12289;&#24191;&#27867;&#35299;&#37322;&#30340;&#21916;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#20107;&#21518;&#35299;&#37322;&#24418;&#24335;&#65292;&#22240;&#20026;&#23427;&#20204;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#39046;&#22495;&#12289;&#25552;&#20379;&#20102;&#27861;&#24459;&#21512;&#35268;&#24615;&#65288;&#20363;&#22914;&#31526;&#21512;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65289;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#23545;&#27604;&#24615;&#36136;&#12290;&#34429;&#28982;&#21453;&#20107;&#23454;&#35299;&#37322;&#36890;&#24120;&#29992;&#20110;&#35299;&#37322;&#21333;&#20010;&#39044;&#27979;&#23454;&#20363;&#65292;&#20294;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20363;&#65292;&#21363;&#20351;&#29992;&#8220;&#32676;&#20307;&#21453;&#20107;&#23454;&#8221;&#26469;&#38598;&#20307;&#35299;&#37322;&#31867;&#20284;&#23454;&#20363;&#30340;&#32452;&#65288;&#20363;&#22914;&#31361;&#20986;&#26174;&#31034;&#19968;&#32452;&#24739;&#32773;&#20013;&#30142;&#30149;&#37325;&#22797;&#20986;&#29616;&#30340;&#27169;&#24335;&#65289;&#12290;&#36825;&#20123;&#32676;&#20307;&#21453;&#20107;&#23454;&#28385;&#36275;&#20154;&#20204;&#23545;&#21253;&#21547;&#22810;&#20010;&#20107;&#20214;/&#23454;&#20363;&#30340;&#36830;&#36143;&#12289;&#24191;&#27867;&#35299;&#37322;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32676;&#20307;&#21453;&#20107;&#23454;&#31639;&#27861;&#26469;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;&#35299;&#37322;&#65292;&#36825;&#31181;&#35299;&#37322;&#21448;&#24544;&#23454;&#20110;&#24453;&#35299;&#37322;&#27169;&#22411;&#12290;&#36824;&#20351;&#29992;&#23458;&#35266;&#65288;&#21363;&#20934;&#30830;&#24615;&#65289;&#21644;&#20027;&#35266;&#65288;&#21363;&#20449;&#24515;&#12289;&#35299;&#37322;&#28385;&#24847;&#24230;&#65289;&#35780;&#20272;&#20102;&#35813;&#35299;&#37322;&#31574;&#30053;&#22312;&#22823;&#22411;&#25511;&#21046;&#29992;&#25143;&#30740;&#31350;&#65288;N=207&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are an increasingly popular form of post hoc explanation due to their (i) applicability across problem domains, (ii) proposed legal compliance (e.g., with GDPR), and (iii) reliance on the contrastive nature of human explanation. Although counterfactual explanations are normally used to explain individual predictive-instances, we explore a novel use case in which groups of similar instances are explained in a collective fashion using ``group counterfactuals'' (e.g., to highlight a repeating pattern of illness in a group of patients). These group counterfactuals meet a human preference for coherent, broad explanations covering multiple events/instances. A novel, group-counterfactual algorithm is proposed to generate high-coverage explanations that are faithful to the to-be-explained model. This explanation strategy is also evaluated in a large, controlled user study (N=207), using objective (i.e., accuracy) and subjective (i.e., confidence, explanation satisfa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#20998;&#31867;&#22120;&#23384;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;Class Attribute Inference Attack&#65288;Caia&#65289;&#33021;&#22815;&#20174;&#40657;&#30418;&#35774;&#32622;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#25935;&#24863;&#23646;&#24615;&#65292;&#21253;&#25324;&#20010;&#20154;&#30340;&#21457;&#33394;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#65292;&#36825;&#34920;&#26126;&#22312;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.09289</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#27844;&#38706;&#20854;&#31867;&#21035;&#30340;&#25935;&#24863;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Image Classifiers Leak Sensitive Attributes About Their Classes. (arXiv:2303.09289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#20998;&#31867;&#22120;&#23384;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;Class Attribute Inference Attack&#65288;Caia&#65289;&#33021;&#22815;&#20174;&#40657;&#30418;&#35774;&#32622;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#25935;&#24863;&#23646;&#24615;&#65292;&#21253;&#25324;&#20010;&#20154;&#30340;&#21457;&#33394;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#65292;&#36825;&#34920;&#26126;&#22312;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#26080;&#24847;&#20013;&#36879;&#38706;&#20102;&#26377;&#20851;&#20854;&#31867;&#21035;&#30340;&#25935;&#24863;&#23646;&#24615;&#20449;&#24687;&#65292;&#24341;&#36215;&#20102;&#23545;&#23427;&#20204;&#30340;&#38544;&#31169;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#38544;&#31169;&#27844;&#28431;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;Class Attribute Inference Attack&#65288;Caia&#65289;&#65292;&#21033;&#29992;&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#25512;&#26029;&#20986;&#21333;&#20010;&#31867;&#21035;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#21516;&#26102;&#19982;&#30456;&#20851;&#30340;&#30333;&#30418;&#25915;&#20987;&#30456;&#31454;&#20105;&#12290;&#22312;&#20154;&#33080;&#35782;&#21035;&#39046;&#22495;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;Caia&#33021;&#22815;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#20844;&#24320;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20363;&#22914;&#20010;&#20154;&#30340;&#21457;&#33394;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#22806;&#35980;&#65292;&#36825;&#20123;&#23646;&#24615;&#19981;&#23646;&#20110;&#35757;&#32451;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#27169;&#22411;&#27604;&#26631;&#20934;&#27169;&#22411;&#26356;&#23481;&#26131;&#27844;&#38706;&#38544;&#31169;&#65292;&#34920;&#26126;&#22312;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based image classifiers are powerful tools for computer vision tasks, but they inadvertently reveal sensitive attribute information about their classes, raising concerns about their privacy. To investigate this privacy leakage, we introduce the first Class Attribute Inference Attack (Caia), which leverages recent advances in text-to-image synthesis to infer sensitive attributes of individual classes in a black-box setting, while remaining competitive with related white-box attacks. Our extensive experiments in the face recognition domain show that Caia can accurately infer undisclosed sensitive attributes, such as an individual's hair color, gender and racial appearance, which are not part of the training labels. Interestingly, we demonstrate that adversarial robust models are even more vulnerable to such privacy leakage than standard models, indicating that a trade-off between robustness and privacy exists.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;&#38598;&#21512;&#30340;&#22810;&#26679;&#24615;&#25351;&#26631;&#12289;&#20934;&#30830;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#23454;&#29616;&#35774;&#35745;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#38477;&#20302;&#25925;&#38556;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.09283</link><description>&lt;p&gt;
&#20351;&#29992;&#35774;&#35745;&#22810;&#26679;&#24615;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Resiliency to Natural Image Corruptions in Deep Learning using Design Diversity. (arXiv:2303.09283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;&#38598;&#21512;&#30340;&#22810;&#26679;&#24615;&#25351;&#26631;&#12289;&#20934;&#30830;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#23454;&#29616;&#35774;&#35745;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#38477;&#20302;&#25925;&#38556;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#22120;&#38598;&#21512;&#30340;&#22810;&#26679;&#24615;&#25351;&#26631;&#12289;&#20934;&#30830;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#30340;&#38887;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#24402;&#22240;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#28508;&#21147;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#22522;&#20110;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#24050;&#30693;&#30340;&#20934;&#30830;&#24615;-&#22810;&#26679;&#24615;&#25240;&#34935;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#22522;&#20110;&#35774;&#35745;&#22810;&#26679;&#24615;&#30340;&#20998;&#26512;&#30740;&#31350;&#65292;&#20854;&#26174;&#31034;&#22914;&#26524;&#23454;&#29616;&#35774;&#35745;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#21017;&#21487;&#20197;&#38477;&#20302;&#24120;&#35265;&#25925;&#38556;&#27169;&#24335;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;ResNet50&#29992;&#20316;&#27604;&#36739;&#22522;&#32447;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#21333;&#29420;&#30340;DL&#27169;&#22411;&#32467;&#26500;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#27745;&#26579;&#24341;&#36215;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#20559;&#31227;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#25216;&#26415;&#29420;&#31435;&#35757;&#32451;&#25110;&#35757;&#32451;&#30340;&#20855;&#26377;&#22810;&#26679;&#24615;&#27169;&#22411;&#32467;&#26500;&#30340;&#38598;&#21512;&#65292;&#24182;&#35780;&#20272;&#20102;&#22522;&#20110;&#39044;&#27979;&#21644;&#22522;&#20110;&#24402;&#22240;&#30340;&#22810;&#26679;&#24615;&#19982;&#26368;&#32456;&#38598;&#21512;&#20934;&#30830;&#24615;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#35774;&#35745;&#22810;&#26679;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the relationship between diversity metrics, accuracy, and resiliency to natural image corruptions of Deep Learning (DL) image classifier ensembles. We investigate the potential of an attribution-based diversity metric to improve the known accuracy-diversity trade-off of the typical prediction-based diversity. Our motivation is based on analytical studies of design diversity that have shown that a reduction of common failure modes is possible if diversity of design choices is achieved.  Using ResNet50 as a comparison baseline, we evaluate the resiliency of multiple individual DL model architectures against dataset distribution shifts corresponding to natural image corruptions. We compare ensembles created with diverse model architectures trained either independently or through a Neural Architecture Search technique and evaluate the correlation of prediction-based and attribution-based diversity to the final ensemble accuracy. We evaluate a set of diversity 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09280</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#65306;&#24212;&#29992;&#20110;&#38544;&#34255;&#20960;&#20309;&#32467;&#26500;&#30340;&#38750;&#20405;&#20837;&#24335;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#30005;&#30913;&#12289;&#22768;&#23398;&#25110;&#26426;&#26800;&#36127;&#36733;&#20174;&#34920;&#38754;&#27979;&#37327;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#38750;&#20405;&#20837;&#25104;&#20687;&#25216;&#26415;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#26410;&#30693;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#24418;&#29366;&#12289;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#29289;&#29702;&#35268;&#24459;&#30340;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#36870;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#35768;&#22810;&#20248;&#28857;&#65292;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#38382;&#39064;&#21453;&#28436;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#25299;&#25169;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PINNs&#30340;&#25299;&#25169;&#20248;&#21270;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27809;&#26377;&#24418;&#29366;&#25968;&#37327;&#25110;&#31867;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#20801;&#35768;&#20219;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26448;&#26009;&#23494;&#24230;&#22330;&#26469;&#34920;&#31034;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;Eikonal&#27491;&#21017;&#21270;&#25509;&#36817;&#20108;&#36827;&#21046;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#27979;&#38544;&#21547;&#34394;&#31354;&#21644;&#21253;&#21547;&#29289;&#30340;&#25968;&#37327;&#12289;&#20301;&#32622;&#21644;&#24418;&#29366;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;QuanTraffic&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#24314;&#27169;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29983;&#25104;&#39044;&#27979;&#21306;&#38388;&#24182;&#23450;&#20041;&#20132;&#36890;&#39044;&#27979;&#30340;&#30495;&#23454;&#20540;&#21487;&#33021;&#20986;&#29616;&#30340;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2303.09273</link><description>&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Adaptive Modeling of Uncertainties for Traffic Forecasting. (arXiv:2303.09273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09273
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;QuanTraffic&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#24314;&#27169;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29983;&#25104;&#39044;&#27979;&#21306;&#38388;&#24182;&#23450;&#20041;&#20132;&#36890;&#39044;&#27979;&#30340;&#30495;&#23454;&#20540;&#21487;&#33021;&#20986;&#29616;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#24320;&#21457;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#34987;&#35757;&#32451;&#20026;&#22312;&#24179;&#22343;&#27979;&#35797;&#26696;&#20363;&#19978;&#26368;&#23567;&#21270;&#35823;&#24046;&#24182;&#20135;&#29983;&#21333;&#28857;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;QuanTraffic&#65292;&#19968;&#31181;&#22686;&#24378;&#20219;&#24847;DNN&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#33021;&#21147;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;QuanTraffic&#22312;DNN&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#33258;&#21160;&#23398;&#20064;&#26631;&#20934;&#20998;&#20301;&#20989;&#25968;&#65292;&#20197;&#20135;&#29983;&#21333;&#28857;&#39044;&#27979;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20174;&#32780;&#23450;&#20041;&#20132;&#36890;&#39044;&#27979;&#30340;&#30495;&#23454;&#20540;&#21487;&#33021;&#20986;&#29616;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have emerged as a dominant approach for developing traffic forecasting models. These models are typically trained to minimize error on averaged test cases and produce a single-point prediction, such as a scalar value for traffic speed or travel time. However, single-point predictions fail to account for prediction uncertainty that is critical for many transportation management scenarios, such as determining the best- or worst-case arrival time. We present QuanTraffic, a generic framework to enhance the capability of an arbitrary DNN model for uncertainty modeling. QuanTraffic requires little human involvement and does not change the base DNN architecture during deployment. Instead, it automatically learns a standard quantile function during the DNN model training to produce a prediction interval for the single-point prediction. The prediction interval defines a range where the true value of the traffic prediction is likely to fall. Furthermore, QuanTraffic d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29256;&#26435;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#38024;&#23545;&#36755;&#20837;&#22270;&#20687;&#65292;&#27169;&#22411;&#27700;&#21360;&#21644;&#24402;&#23646;&#32593;&#32476;&#30340;&#24403;&#21069;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;GAN&#33539;&#22260;&#20869;&#22522;&#26412;&#19978;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#20026;&#20445;&#25252;&#35757;&#32451;&#38598;&#24517;&#39035;&#23547;&#25214;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09272</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#29256;&#26435;&#20445;&#25252;&#21644;&#36131;&#20219;&#65306;&#25915;&#20987;&#65292;&#27700;&#21360;&#21644;&#24402;&#23646;
&lt;/p&gt;
&lt;p&gt;
Copyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution. (arXiv:2303.09272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29256;&#26435;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#38024;&#23545;&#36755;&#20837;&#22270;&#20687;&#65292;&#27169;&#22411;&#27700;&#21360;&#21644;&#24402;&#23646;&#32593;&#32476;&#30340;&#24403;&#21069;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;GAN&#33539;&#22260;&#20869;&#22522;&#26412;&#19978;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#20026;&#20445;&#25252;&#35757;&#32451;&#38598;&#24517;&#39035;&#23547;&#25214;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;-GAN&#65289;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#38024;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#65288;&#26377;&#27602;&#22270;&#20687;&#65289;&#21644;&#27169;&#22411;&#65288;&#26377;&#27602;&#27169;&#22411;&#65289;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#65288;&#25110;&#27169;&#22411;&#30340;&#38382;&#36131;&#65289;&#24341;&#21457;&#20102;&#37325;&#22823;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20840;&#38754;&#20102;&#35299;&#24403;&#21069;&#38024;&#23545;&#21508;&#31181;GAN&#26550;&#26500;&#30340;&#29256;&#26435;&#20445;&#25252;&#25514;&#26045;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#24433;&#21709;&#23427;&#20204;&#30340;&#22240;&#32032;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#38024;&#23545;&#36755;&#20837;&#22270;&#20687;&#65292;&#27169;&#22411;&#27700;&#21360;&#21644;&#24402;&#23646;&#32593;&#32476;&#30340;&#24403;&#21069;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;GAN&#33539;&#22260;&#20869;&#22522;&#26412;&#19978;&#20196;&#20154;&#28385;&#24847;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#24517;&#39035;&#23558;&#36827;&#19968;&#27493;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#20445;&#25252;&#35757;&#32451;&#38598;&#19978;&#65292;&#22240;&#20026;&#24403;&#21069;&#30340;&#26041;&#27861;&#26410;&#33021;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#21644;&#30693;&#35782;&#20135;&#26435;&#26469;&#28304;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI (e.g., Generative Adversarial Networks - GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.09271</link><description>&lt;p&gt;
&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles. (arXiv:2303.09271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20851;&#38190;&#31995;&#32479;&#30340;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#33021;&#22815;&#35299;&#37322;&#20026;&#20309;&#27169;&#22411;&#20570;&#20986;&#29305;&#23450;&#39044;&#27979;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#20379;&#30340;&#35299;&#37322;&#24517;&#39035;&#26159;&#21487;&#35777;&#26126;&#30340;&#65292;&#24182;&#19988;&#26368;&#22909;&#19981;&#21253;&#21547;&#20887;&#20313;&#20449;&#24687;&#65292;&#21363;&#26368;&#23567;&#35299;&#37322;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#26159;&#26368;&#23567;&#30340;&#65292;&#32780;&#19988;&#22312;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#20063;&#26159;&#26368;&#23567;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31070;&#35861;&#8221;&#31995;&#32479;&#65292;&#21487;&#20197;&#30830;&#23450;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#65292;&#22312;&#35745;&#31639;&#26368;&#23567;&#35299;&#37322;&#26102;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#26469;&#33258;&#30456;&#20851;&#24037;&#20316;&#30340;&#21483;&#20570;MARCO&#30340;&#31639;&#27861;&#65288;&#23558;&#20854;&#31216;&#20026;m-MARCO&#65289;&#65292;&#30446;&#30340;&#26159;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain why a machine learning model arrives at a particular prediction is crucial when used as decision support by human operators of critical systems. The provided explanations must be provably correct, and preferably without redundant information, called minimal explanations. In this paper, we aim at finding explanations for predictions made by tree ensembles that are not only minimal, but also minimum with respect to a cost function.  To this end, we first present a highly efficient oracle that can determine the correctness of explanations, surpassing the runtime performance of current state-of-the-art alternatives by several orders of magnitude when computing minimal explanations.  Secondly, we adapt an algorithm called MARCO from related works (calling it m-MARCO) for the purpose of computing a single minimum explanation per prediction, and demonstrate an overall speedup factor of two compared to the MARCO algorithm which enumerates all minimal explanations.  Final
&lt;/p&gt;</description></item><item><title>SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09266</link><description>&lt;p&gt;
SmartBERT&#65306;&#29992;&#20110;&#21152;&#36895;BERT&#25512;&#29702;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09266
&lt;/p&gt;
&lt;p&gt;
SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#26679;&#26412;&#22312;&#26089;&#26399;&#36864;&#20986;&#20043;&#21069;&#37117;&#24517;&#39035;&#32463;&#36807;&#25152;&#26377;&#36830;&#32493;&#23618;&#65292;&#36739;&#22797;&#26434;&#30340;&#26679;&#26412;&#36890;&#24120;&#20250;&#32463;&#21382;&#26356;&#22810;&#30340;&#23618;&#65292;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SmartBERT&#30340;Bert&#25512;&#29702;&#30340;&#26032;&#22411;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#30456;&#32467;&#21512;&#30340;&#26426;&#21046;&#65292;&#23427;&#23558;&#36339;&#36807;&#38376;&#21644;&#36864;&#20986;&#31639;&#23376;&#21152;&#20837;&#21040;BERT&#30340;&#27599;&#19968;&#23618;&#20013;&#12290;SmartBERT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#23618;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#25105;&#20204;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#25552;&#39640;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;&#36864;&#20986;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#36339;&#36807;&#38376;&#30340;&#19968;&#33268;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#26435;&#37325;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20843;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#40657;&#33394;&#32032;&#30244;&#20010;&#24615;&#21270;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24555;&#36895;&#29983;&#25104;&#20986;&#20010;&#24615;&#21270;&#30340;&#33402;&#26415;&#27835;&#30103;&#20869;&#23481;&#65292;&#26377;&#25928;&#20943;&#36731;&#24739;&#32773;&#24515;&#29702;&#21387;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09232</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#40657;&#33394;&#32032;&#30244;&#20010;&#24615;&#21270;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Network for Personalized Art Therapy in Melanoma Disease Management. (arXiv:2303.09232v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#40657;&#33394;&#32032;&#30244;&#20010;&#24615;&#21270;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24555;&#36895;&#29983;&#25104;&#20986;&#20010;&#24615;&#21270;&#30340;&#33402;&#26415;&#27835;&#30103;&#20869;&#23481;&#65292;&#26377;&#25928;&#20943;&#36731;&#24739;&#32773;&#24515;&#29702;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#19968;&#31181;&#26368;&#33268;&#21629;&#30340;&#30382;&#32932;&#30284;&#65292;&#24739;&#32773;&#23481;&#26131;&#24739;&#26377;&#24515;&#29702;&#20581;&#24247;&#30142;&#30149;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#30284;&#30151;&#27835;&#30103;&#30340;&#25928;&#26524;&#21644;&#24739;&#32773;&#29992;&#33647;&#35745;&#21010;&#30340;&#36981;&#24490;&#24615;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#24739;&#32773;&#22312;&#25509;&#21463;&#27835;&#30103;&#26102;&#30340;&#24515;&#29702;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;&#24182;&#19981;&#26159;&#20010;&#24615;&#21270;&#21644;&#29420;&#29305;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#22270;&#20687;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#24555;&#36895;&#20174;&#20010;&#20154;&#30382;&#32932;&#38236;&#40657;&#33394;&#32032;&#30244;&#22270;&#20687;&#20013;&#29983;&#25104;&#29420;&#29305;&#30340;&#33402;&#26415;&#21697;&#65292;&#20316;&#20026;&#36741;&#21161;&#40657;&#33394;&#32032;&#30244;&#30149;&#31649;&#29702;&#20013;&#30340;&#33402;&#26415;&#27835;&#30103;&#24037;&#20855;&#12290;&#35270;&#35273;&#33402;&#26415;&#27427;&#36175;&#26159;&#30142;&#30149;&#31649;&#29702;&#20013;&#24120;&#35265;&#30340;&#33402;&#26415;&#27835;&#30103;&#24418;&#24335;&#65292;&#23427;&#21487;&#20197;&#26126;&#26174;&#20943;&#36731;&#24515;&#29702;&#21387;&#21147;&#12290;&#25105;&#20204;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cycle-consistent generative adversarial network&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#36716;&#25442;&#30340;&#32593;&#32476;&#26469;&#29983;&#25104;&#26469;&#33258;&#30382;&#32932;&#38236;&#40657;&#33394;&#32032;&#30244;&#22270;&#20687;&#30340;&#20010;&#24615;&#21270;&#21644;&#29420;&#29305;&#33402;&#26415;&#21697;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#23558;&#40657;&#33394;&#32032;&#30244;&#22270;&#20687;&#36716;&#25442;&#20026;&#29420;&#29305;&#30340;&#33457;&#21321;&#20027;&#39064;&#33402;&#26415;&#21697;
&lt;/p&gt;
&lt;p&gt;
Melanoma is the most lethal type of skin cancer. Patients are vulnerable to mental health illnesses which can reduce the effectiveness of the cancer treatment and the patients adherence to drug plans. It is crucial to preserve the mental health of patients while they are receiving treatment. However, current art therapy approaches are not personal and unique to the patient. We aim to provide a well-trained image style transfer model that can quickly generate unique art from personal dermoscopic melanoma images as an additional tool for art therapy in disease management of melanoma. Visual art appreciation as a common form of art therapy in disease management that measurably reduces the degree of psychological distress. We developed a network based on the cycle-consistent generative adversarial network for style transfer that generates personalized and unique artworks from dermoscopic melanoma images. We developed a model that converts melanoma images into unique flower-themed artworks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#25903;&#25345;&#12290;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#26631;&#31614;&#65292;&#20197;&#21487;&#38752;&#22320;&#20445;&#35777;&#35757;&#32451;&#25439;&#22833;&#25910;&#25947;&#24182;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#36895;&#29575;&#65292;&#23454;&#29616;&#21160;&#24577;&#26631;&#31614;&#22686;&#24378;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#20445;&#35777;&#20102;&#23616;&#37096;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.09216</link><description>&lt;p&gt;
&#21463;&#25511;&#19979;&#38477;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Controlled Descent Training. (arXiv:2303.09216v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#25903;&#25345;&#12290;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#26631;&#31614;&#65292;&#20197;&#21487;&#38752;&#22320;&#20445;&#35777;&#35757;&#32451;&#25439;&#22833;&#25910;&#25947;&#24182;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#36895;&#29575;&#65292;&#23454;&#29616;&#21160;&#24577;&#26631;&#31614;&#22686;&#24378;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#20445;&#35777;&#20102;&#23616;&#37096;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#25903;&#25345;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#26631;&#31614;&#65292;&#20197;&#21487;&#38752;&#22320;&#20445;&#35777;&#35757;&#32451;&#25439;&#22833;&#25910;&#25947;&#24182;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#36895;&#29575;&#12290;&#22312;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#21160;&#24577;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#35757;&#32451;&#25439;&#22833;&#30340;&#25910;&#25947;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20511;&#21161;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#26469;&#25429;&#25417;&#35757;&#32451;&#34892;&#20026;&#65292;&#24182;&#20174;&#31995;&#32479;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#20511;&#37492;&#24037;&#20855;&#26469;&#20998;&#26512;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#35757;&#32451;&#21160;&#24577;&#65288;&#22914;&#31283;&#23450;&#24615;&#12289;&#21487;&#36798;&#24615;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#34394;&#26500;&#30340;&#26631;&#31614;&#20316;&#20026;&#25511;&#21046;&#36755;&#20837;&#21644;&#26368;&#20248;&#29366;&#24577;&#21453;&#39304;&#31574;&#30053;&#26469;&#21160;&#24577;&#25913;&#21464;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26426;&#21046;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24378;&#21046;&#22312;&#26412;&#22320;&#23454;&#29616;$\mathcal{H}_2$&#26368;&#20248;&#21644;&#25910;&#25947;&#30340;&#35757;&#32451;&#34892;&#20026;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#8220;&#21463;&#25511;&#19979;&#38477;&#35757;&#32451;&#8221;&#65288;CDT&#65289;&#20445;&#35777;&#20102;&#23616;&#37096;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a novel and model-based artificial neural network (ANN) training method is developed supported by optimal control theory. The method augments training labels in order to robustly guarantee training loss convergence and improve training convergence rate. Dynamic label augmentation is proposed within the framework of gradient descent training where the convergence of training loss is controlled. First, we capture the training behavior with the help of empirical Neural Tangent Kernels (NTK) and borrow tools from systems and control theory to analyze both the local and global training dynamics (e.g. stability, reachability). Second, we propose to dynamically alter the gradient descent training mechanism via fictitious labels as control inputs and an optimal state feedback policy. In this way, we enforce locally $\mathcal{H}_2$ optimal and convergent training behavior. The novel algorithm, \textit{Controlled Descent Training} (CDT), guarantees local convergence. CDT unleashes 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2303.09184</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#20301;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#31216;&#20026;BBCT&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#21387;&#32553;&#25972;&#20010;Transformer&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#22312;GLUE&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#23454;&#29616;&#23569;&#20110;1&#65285;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;BERT&#12289;GPT-3&#21644;ChatGPT&#31561;&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27969;&#34892;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#35745;&#31639;&#37327;&#12289;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#39640;&#24310;&#36831;&#26159;&#20113;&#35745;&#31639;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BBCT&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#22359;&#20301;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;Transformer&#30340;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;softmax&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#20197;&#39640;&#25928;BERT&#20026;&#26696;&#20363;&#65292;&#20351;&#29992;BBCT&#26041;&#27861;&#36827;&#34892;&#21387;&#32553;&#12290;&#25105;&#20204;&#22312;General Language Understanding Evaluation(GLUE)&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;BBCT&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#23567;&#20110;1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;POMCP&#25191;&#34892;&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#36719;&#25919;&#31574;&#25351;&#23548;&#65292;&#20195;&#26367;&#25163;&#21160;&#23450;&#20041;&#30340;&#31574;&#30053;&#30456;&#20851;&#35268;&#21017;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;&#29615;&#22659;&#29366;&#24577;&#31354;&#38388;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09172</link><description>&lt;p&gt;
POMCP&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#20197;&#23454;&#29616;&#36719;&#25919;&#31574;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Specifications for Soft Policy Guidance in POMCP. (arXiv:2303.09172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09172
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;POMCP&#25191;&#34892;&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#36719;&#25919;&#31574;&#25351;&#23548;&#65292;&#20195;&#26367;&#25163;&#21160;&#23450;&#20041;&#30340;&#31574;&#30053;&#30456;&#20851;&#35268;&#21017;&#65292;&#24182;&#29992;&#20110;&#35299;&#20915;&#29615;&#22659;&#29366;&#24577;&#31354;&#38388;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#65288;POMCP&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#35299;&#20915;&#22120;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#31574;&#30053;&#65292;&#22312;&#26412;&#22320;&#21644;&#22312;&#32447;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#65292;&#20174;&#32780;&#20351;&#24471;&#35268;&#27169;&#19978;&#30340;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;POMCP&#22312;&#31232;&#30095;&#22870;&#21169;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#21363;&#20165;&#22312;&#36798;&#21040;&#26368;&#32456;&#30446;&#26631;&#26102;&#33719;&#24471;&#22870;&#21169;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#29615;&#22659;&#20013;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#23558;&#36923;&#36753;&#35268;&#33539;&#38598;&#25104;&#21040;POMCP&#20013;&#65292;&#20197;&#25351;&#23548;&#25506;&#32034;&#24182;&#28385;&#36275;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#19982;&#31574;&#30053;&#30456;&#20851;&#30340;&#35268;&#21017;&#38656;&#35201;&#30001;&#39046;&#22495;&#19987;&#23478;&#25163;&#21160;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#20174;POMCP&#25191;&#34892;&#30340;&#36319;&#36394;&#20013;&#23398;&#20064;&#36923;&#36753;&#35268;&#33539;&#65292;&#21363;&#30001;&#35268;&#21010;&#22120;&#29983;&#25104;&#30340;&#20449;&#24565;-&#34892;&#20026;&#23545;&#38598;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#33539;&#24335;&#34920;&#31034;&#30340;&#35268;&#21017;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;POMCP&#20013;&#21033;&#29992;&#23427;&#20197;&#23454;&#29616;&#36719;&#25351;&#23548;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for Partially Observable Markov Decision Processes (POMDPs). It allows scaling to large state spaces by computing an approximation of the optimal policy locally and online, using a Monte Carlo Tree Search based strategy. However, POMCP suffers from sparse reward function, namely, rewards achieved only when the final goal is reached, particularly in environments with large state spaces and long horizons. Recently, logic specifications have been integrated into POMCP to guide exploration and to satisfy safety requirements. However, such policy-related rules require manual definition by domain experts, especially in real-world scenarios. In this paper, we use inductive logic programming to learn logic specifications from traces of POMCP executions, i.e., sets of belief-action pairs generated by the planner. Specifically, we learn rules expressed in the paradigm of answer set programming. We then integrate them inside
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25968;&#23398;&#19978;&#28548;&#28165;&#20102;&#24102;&#26377;&#27010;&#24565;&#29942;&#39048;&#32467;&#26500;&#21644;&#22810;&#20219;&#21153;&#32452;&#25104;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#21644;&#33258;&#30001;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09154</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#24565;&#29942;&#39048;&#32467;&#26500;&#21644;&#22810;&#20219;&#21153;&#32452;&#25104;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Generalization Error in Linear Neural Networks with Concept Bottleneck Structure and Multitask Formulation. (arXiv:2303.09154v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25968;&#23398;&#19978;&#28548;&#28165;&#20102;&#24102;&#26377;&#27010;&#24565;&#29942;&#39048;&#32467;&#26500;&#21644;&#22810;&#20219;&#21153;&#32452;&#25104;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#21644;&#33258;&#30001;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#27010;&#24565;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;CBM&#20013;&#65292;&#27010;&#24565;&#34987;&#25554;&#20837;&#21040;&#36755;&#20986;&#23618;&#21644;&#26368;&#21518;&#19968;&#20010;&#20013;&#38388;&#23618;&#20043;&#38388;&#20316;&#20026;&#21487;&#35266;&#23519;&#20540;&#12290;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36755;&#20986;&#30340;&#21407;&#22240;&#65306;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#21040;&#36755;&#20986;&#23618;&#30340;&#27010;&#24565;&#23545;&#24212;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#22312;CBM&#20013;&#29702;&#35299;&#27867;&#21270;&#35823;&#24046;&#34892;&#20026;&#23578;&#19981;&#21487;&#33021;&#65292;&#22240;&#20026;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#22855;&#24322;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#24403;&#27169;&#22411;&#26159;&#22855;&#24322;&#30340;&#26102;&#65292;&#20174;&#21442;&#25968;&#21040;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#19968;&#26144;&#23556;&#19981;&#33021;&#21019;&#24314;&#12290;&#36825;&#31181;&#19981;&#21487;&#35782;&#21035;&#24615;&#20351;&#24471;&#20998;&#26512;&#27867;&#21270;&#24615;&#33021;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#27425;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25968;&#23398;&#19978;&#28548;&#28165;&#20102;CBM&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#35823;&#24046;&#21644;&#33258;&#30001;&#33021;&#65292;&#24403;&#20854;&#26550;&#26500;&#26159;&#19977;&#23618;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#19981;&#20877;&#21482;&#26159;&#19968;&#20010;&#26631;&#31614;&#65292;&#32780;&#26159;&#19968;&#32452;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept bottleneck model (CBM) is a ubiquitous method that can interpret neural networks using concepts. In CBM, concepts are inserted between the output layer and the last intermediate layer as observable values. This helps in understanding the reason behind the outputs generated by the neural networks: the weights corresponding to the concepts from the last hidden layer to the output layer. However, it has not yet been possible to understand the behavior of the generalization error in CBM since a neural network is a singular statistical model in general. When the model is singular, a one to one map from the parameters to probability distributions cannot be created. This non-identifiability makes it difficult to analyze the generalization performance. In this study, we mathematically clarify the Bayesian generalization error and free energy of CBM when its architecture is three-layered linear neural networks. We also consider a multitask problem where the neural network outputs not on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20809;&#32420;&#20013;&#21608;&#26399;&#20449;&#21495;&#30340;&#38750;&#32447;&#24615;&#37325;&#22609;&#65292;&#25506;&#31350;&#20102;&#27491;&#24120;&#21644;&#24322;&#24120;&#20108;&#38454;&#33394;&#25955;&#21306;&#22495;&#65292;&#21487;&#29983;&#25104;&#23450;&#21046;&#33033;&#20914;&#21015;&#25110;&#20986;&#29616;&#26174;&#30528;&#30340;&#26102;&#38388;&#25110;&#39057;&#35889;&#32858;&#28966;&#12290;</title><link>http://arxiv.org/abs/2303.09133</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20809;&#32420;&#20013;&#21608;&#26399;&#20449;&#21495;&#30340;&#38750;&#32447;&#24615;&#37325;&#22609;
&lt;/p&gt;
&lt;p&gt;
Predicting nonlinear reshaping of periodic signals in optical fibre with a neural network. (arXiv:2303.09133v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20809;&#32420;&#20013;&#21608;&#26399;&#20449;&#21495;&#30340;&#38750;&#32447;&#24615;&#37325;&#22609;&#65292;&#25506;&#31350;&#20102;&#27491;&#24120;&#21644;&#24322;&#24120;&#20108;&#38454;&#33394;&#25955;&#21306;&#22495;&#65292;&#21487;&#29983;&#25104;&#23450;&#21046;&#33033;&#20914;&#21015;&#25110;&#20986;&#29616;&#26174;&#30528;&#30340;&#26102;&#38388;&#25110;&#39057;&#35889;&#32858;&#28966;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;&#31616;&#21333;&#27491;&#24358;&#35843;&#21046;&#20449;&#21495;&#22312;&#20809;&#32420;&#38750;&#32447;&#24615;&#20256;&#25773;&#26102;&#37325;&#22609;&#25104;&#20855;&#26377;&#39057;&#22495;&#20013;&#20855;&#26377;&#19968;&#23450;&#32467;&#26500;&#33033;&#20914;&#21015;&#30340;&#26102;&#38388;&#21644;&#39057;&#35889;&#12290;&#30740;&#31350;&#20102;&#20809;&#32420;&#30340;&#27491;&#24120;&#21644;&#24322;&#24120;&#20108;&#38454;&#33394;&#25955;&#21306;&#22495;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#36895;&#24230;&#65292;&#25506;&#32034;&#36755;&#20837;&#21442;&#25968;&#31354;&#38388;&#20197;&#29983;&#25104;&#23450;&#21046;&#33033;&#20914;&#21015;&#25110;&#21457;&#29983;&#26174;&#30528;&#30340;&#26102;&#38388;&#25110;&#39057;&#35889;&#32858;&#28966;&#12290;
&lt;/p&gt;
&lt;p&gt;
We deploy a supervised machine-learning model based on a neural network to predict the temporal and spectral reshaping of a simple sinusoidal modulation into a pulse train having a comb structure in the frequency domain, which occurs upon nonlinear propagation in an optical fibre. Both normal and anomalous second-order dispersion regimes of the fibre are studied, and the speed of the neural network is leveraged to probe the space of input parameters for the generation of custom combs or the occurrence of significant temporal or spectral focusing.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#26512;&#20013;&#22788;&#29702;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32452;&#32455;&#12289;&#39033;&#30446;&#21644;&#27169;&#22359;&#30340;&#33258;&#28982;&#36793;&#30028;&#20998;&#21106;&#26041;&#27861;&#65292;&#21457;&#29616;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#20135;&#29983;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09128</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#20195;&#30721;&#20998;&#26512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09128
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#26512;&#20013;&#22788;&#29702;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32452;&#32455;&#12289;&#39033;&#30446;&#21644;&#27169;&#22359;&#30340;&#33258;&#28982;&#36793;&#30028;&#20998;&#21106;&#26041;&#27861;&#65292;&#21457;&#29616;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#20135;&#29983;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; CodeT5 &#21644; Codex &#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#25512;&#24191;&#21040;&#39046;&#22495;&#22806;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#22522;&#26412;&#24212;&#29992;&#65306;&#20195;&#30721;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#25353;&#29031;&#20854;&#33258;&#28982;&#36793;&#30028;&#65288;&#25353;&#32452;&#32455;&#12289;&#25353;&#39033;&#30446;&#21644;&#25353;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#27169;&#22359;&#65289;&#23558;&#25968;&#25454;&#20998;&#20026;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#36825;&#26679;&#65292;&#22312;&#37096;&#32626;&#26102;&#65292;&#35782;&#21035;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#21464;&#24471;&#31616;&#21333;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#32473;&#36825;&#20004;&#20010;&#27169;&#22411;&#24102;&#26469;&#20998;&#24067;&#20559;&#31227;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22914;&#20309;&#36866;&#24212;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;&#22810;&#20219;&#21153;&#23398;&#20064;&#26412;&#36523;&#26159;&#19968;&#20010;&#21512;&#29702;&#30340;&#22522;&#32447;&#65292;&#20294;&#23558;&#20854;&#19982;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#26816;&#32034;&#30340;&#31034;&#20363;&#30340;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#21487;&#20197;&#23454;&#29616;&#38750;&#24120;&#24378;&#30340;&#24615;&#33021;&#12290;&#20107;&#23454;&#19978;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#20248;&#20110;&#30452;&#25509;&#35843;&#25972;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically study the capacity of two large language models for code CodeT5 and Codex - to generalize to out-of-domain data. In this study, we consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial. We establish that samples from each new domain present both models with a significant challenge of distribution shift. We study how well different established methods can adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#27861;&#21307;&#23398;&#21462;&#35777;&#27604;&#36739;&#26041;&#27861;&#65292;&#21516;&#26102;&#35780;&#20272;&#24182;&#20248;&#21270;&#20102;&#30452;&#25509;&#26041;&#27861;&#21644;&#38388;&#25509;&#26041;&#27861;&#12290;&#38388;&#25509;&#26041;&#27861;&#26356;&#31283;&#20581;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#33258;&#21160;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#12290;</title><link>http://arxiv.org/abs/2303.09126</link><description>&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#21462;&#35777;&#27604;&#36739;&#26041;&#27861;&#35780;&#20272;&#65306;&#24212;&#29992;&#20110;&#25163;&#37096;&#27668;&#21619;&#21462;&#35777;
&lt;/p&gt;
&lt;p&gt;
Evaluation of distance-based approaches for forensic comparison: Application to hand odor evidence. (arXiv:2303.09126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#27861;&#21307;&#23398;&#21462;&#35777;&#27604;&#36739;&#26041;&#27861;&#65292;&#21516;&#26102;&#35780;&#20272;&#24182;&#20248;&#21270;&#20102;&#30452;&#25509;&#26041;&#27861;&#21644;&#38388;&#25509;&#26041;&#27861;&#12290;&#38388;&#25509;&#26041;&#27861;&#26356;&#31283;&#20581;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#33258;&#21160;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#21307;&#23398;&#20013;&#65292;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#30165;&#36857;&#21306;&#20998;&#30456;&#21516;&#26469;&#28304;&#21644;&#19981;&#21516;&#26469;&#28304;&#30340;&#20551;&#35774;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#23427;&#20204;&#33021;&#22815;&#25552;&#20379;&#20284;&#28982;&#27604;&#26469;&#37327;&#21270;&#25903;&#25345;&#20004;&#20010;&#31454;&#20105;&#20551;&#35774;&#30340;&#35777;&#25454;&#20043;&#38388;&#30340;&#30456;&#23545;&#24378;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38598;&#20013;&#20110;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22788;&#29702;&#39640;&#32500;&#24230;&#35777;&#25454;&#30340;&#33021;&#21147;&#38750;&#24120;&#19981;&#21516;&#65292;&#38656;&#35201;&#35780;&#20272;&#21644;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#26041;&#27861;&#21644;&#19968;&#20010;&#38388;&#25509;&#26041;&#27861;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#21069;&#32773;&#22522;&#20110;&#20272;&#35745;&#20004;&#20010;&#31454;&#20105;&#20551;&#35774;&#19979;&#30340;&#30165;&#36857;&#36317;&#31163;&#30340;&#20284;&#28982;&#24615;&#65292;&#21518;&#32773;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#26469;&#21306;&#20998;&#21516;&#19968;&#26469;&#28304;&#21644;&#19981;&#21516;&#26469;&#28304;&#30340;&#36317;&#31163;&#20998;&#24067;&#12290;&#34429;&#28982;&#30452;&#25509;&#26041;&#27861;&#26356;&#28789;&#27963;&#65292;&#20294;&#38388;&#25509;&#26041;&#27861;&#26356;&#31283;&#20581;&#65292;&#24182;&#19988;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30456;&#24403;&#33258;&#28982;&#12290;&#27492;&#22806;&#65292;&#38388;&#25509;&#26041;&#27861;&#36824;&#33021;&#22815;&#23454;&#29616;&#33258;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The issue of distinguishing between the same-source and different-source hypotheses based on various types of traces is a generic problem in forensic science. This problem is often tackled with Bayesian approaches, which are able to provide a likelihood ratio that quantifies the relative strengths of evidence supporting each of the two competing hypotheses. Here, we focus on distance-based approaches, whose robustness and specifically whose capacity to deal with high-dimensional evidence are very different, and need to be evaluated and optimized. A unified framework for direct methods based on estimating the likelihoods of the distance between traces under each of the two competing hypotheses, and indirect methods using logistic regression to discriminate between same-source and different-source distance distributions, is presented. Whilst direct methods are more flexible, indirect methods are more robust and quite natural in machine learning. Moreover, indirect methods also enable the
&lt;/p&gt;</description></item><item><title>SigVIC&#26159;&#19968;&#31181;&#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#25351;&#23548;&#29305;&#24449;&#32553;&#25918;&#21644;&#27604;&#29305;&#20998;&#37197;&#65292;&#36873;&#25321;Top-K&#27973;&#23618;&#29305;&#24449;&#26469;&#31934;&#32454;&#35843;&#25972;&#35299;&#30721;&#29305;&#24449;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09112</link><description>&lt;p&gt;
SigVIC: &#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
SigVIC: Spatial Importance Guided Variable-Rate Image Compression. (arXiv:2303.09112v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09112
&lt;/p&gt;
&lt;p&gt;
SigVIC&#26159;&#19968;&#31181;&#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#25351;&#23548;&#29305;&#24449;&#32553;&#25918;&#21644;&#27604;&#29305;&#20998;&#37197;&#65292;&#36873;&#25321;Top-K&#27973;&#23618;&#29305;&#24449;&#26469;&#31934;&#32454;&#35843;&#25972;&#35299;&#30721;&#29305;&#24449;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#27604;&#26426;&#21046;&#25552;&#39640;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#20026;&#19981;&#21516;&#30340;&#36895;&#29575;-&#22833;&#30495;&#26435;&#34913;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#21464;&#27604;&#29575;&#30340;&#26368;&#24120;&#35265;&#26041;&#27861;&#20043;&#19968;&#26159;&#25353;&#36890;&#36947;&#25110;&#31354;&#38388;&#22343;&#21248;&#32553;&#25918;&#20869;&#37096;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#31354;&#38388;&#37325;&#35201;&#24615;&#30340;&#22810;&#26679;&#24615;&#23545;&#20110;&#22270;&#20687;&#21387;&#32553;&#30340;&#27604;&#29305;&#20998;&#37197;&#26159;&#26377;&#25351;&#23548;&#24847;&#20041;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31354;&#38388;&#37325;&#35201;&#24615;&#25351;&#23548;&#30340;&#21487;&#21464;&#27604;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65288;SigVIC&#65289;&#65292;&#20854;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#31354;&#38388;&#38376;&#25511;&#21333;&#20803;&#65288;SGU&#65289;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#23398;&#20064;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#31354;&#38388;&#32553;&#25918;&#32593;&#32476;&#65288;SSN&#65289;&#20351;&#29992;&#31354;&#38388;&#37325;&#35201;&#24615;&#25513;&#30721;&#26469;&#25351;&#23548;&#29305;&#24449;&#32553;&#25918;&#21644;&#21487;&#21464;&#27604;&#29575;&#30340;&#27604;&#29305;&#20998;&#37197;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#35299;&#30721;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#36873;&#25321;Top-K&#27973;&#23618;&#29305;&#24449;&#36890;&#36807;&#27973;&#23618;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65288;SFFM&#65289;&#26469;&#31934;&#32454;&#22320;&#35843;&#25972;&#35299;&#30721;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;&#26080;&#35770;&#26159;&#21464;&#27604;&#29575;&#36824;&#26159;&#38750;&#21464;&#27604;&#29575;&#65289;&#65292;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable-rate mechanism has improved the flexibility and efficiency of learning-based image compression that trains multiple models for different rate-distortion tradeoffs. One of the most common approaches for variable-rate is to channel-wisely or spatial-uniformly scale the internal features. However, the diversity of spatial importance is instructive for bit allocation of image compression. In this paper, we introduce a Spatial Importance Guided Variable-rate Image Compression (SigVIC), in which a spatial gating unit (SGU) is designed for adaptively learning a spatial importance mask. Then, a spatial scaling network (SSN) takes the spatial importance mask to guide the feature scaling and bit allocation for variable-rate. Moreover, to improve the quality of decoded image, Top-K shallow features are selected to refine the decoded features through a shallow feature fusion module (SFFM). Experiments show that our method outperforms other learning-based methods (whether variable-rate or 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;KNN&#31639;&#27861;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#29305;&#24449;&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09103</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine learning based biomedical image processing for echocardiographic images. (arXiv:2303.09103v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;KNN&#31639;&#27861;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#29305;&#24449;&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#34892;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#23558;&#20854;&#24212;&#29992;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;KNN&#31639;&#27861;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21644;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#20197;&#36827;&#34892;&#20998;&#31867;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#20998;&#31867;&#23545;&#20110;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#22270;&#20687;&#38750;&#24120;&#37325;&#35201;&#65292;KNN&#31639;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#27010;&#24565;&#21270;&#21644;&#35745;&#31639;&#30340;&#31639;&#27861;&#65292;&#22312;&#32467;&#26524;&#26041;&#38754;&#25552;&#20379;&#20102;&#38750;&#24120;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;KNN&#31639;&#27861;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#29992;&#25143;&#21451;&#22909;&#26041;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#29992;&#20110;&#21508;&#31181;&#22270;&#20687;&#22788;&#29702;&#24212;&#29992;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#22238;&#24402;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#29305;&#24449;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#21151;&#22320;&#22312;&#19968;&#32452;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#36807;&#22238;&#24402;&#20998;&#26512;&#27604;&#36739;&#20102;&#35823;&#24046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#31934;&#24230;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of Artificial intelligence and machine learning have prompted researchers to use it in the recent researches. The proposed method uses K-Nearest Neighbor (KNN) algorithm for segmentation of medical images, extracting of image features for analysis by classifying the data based on the neural networks. Classification of the images in medical imaging is very important, KNN is one suitable algorithm which is simple, conceptual and computational, which provides very good accuracy in results. KNN algorithm is a unique user-friendly approach with wide range of applications in machine learning algorithms which are majorly used for the various image processing applications including classification, segmentation and regression issues of the image processing. The proposed system uses gray level co-occurrence matrix features. The trained neural network has been tested successfully on a group of echocardiographic images, errors were compared using regression plot. The results of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09100</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26500;&#24314;&#26377;&#25928;&#25552;&#31034;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#30340;&#25163;&#21160;&#35774;&#35745;&#65292;&#35201;&#20040;&#23558;&#25552;&#31034;&#35843;&#20248;&#20316;&#20026;&#28857;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25551;&#36848;&#31867;&#21035;&#30340;&#22810;&#26679;&#29305;&#24449;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#20174;&#28508;&#22312;&#20998;&#24067;&#20013;&#39318;&#20808;&#37319;&#26679;&#38544;&#21521;&#37327;&#65292;&#28982;&#21518;&#37319;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#26631;&#31614;&#29305;&#23450;&#30340;&#38543;&#26426;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35270;&#35273;&#30693;&#35782;&#19982;&#22270;&#20687;&#30340;&#35821;&#20041;&#35268;&#21017;&#21270;&#65292;&#24182;&#23558;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25552;&#31034;&#35270;&#20026;&#34917;&#19969;&#21644;&#20196;&#29260;&#38598;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#37327;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#31034;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;&#65288;PTBPL&#65289;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#20013;&#21307;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26415;&#21069;&#35780;&#20272;&#24037;&#20855;&#65292;&#32452;&#21512;&#20102;&#26631;&#20934;&#25163;&#26415;&#35780;&#20272;&#12289;&#20013;&#21307;&#20307;&#36136;&#35780;&#20272;&#21644;&#35745;&#21010;&#25163;&#26415;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#33136;&#26894;&#25163;&#26415;&#30340;&#39044;&#21518;&#12290;</title><link>http://arxiv.org/abs/2303.09085</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#33136;&#26894;&#25163;&#26415;&#21069;&#20302;&#32972;&#30171;&#21644;&#22352;&#39592;&#31070;&#32463;&#30171;&#24739;&#32773;&#39044;&#21518;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Preoperative Prognosis Assessment of Lumbar Spinal Surgery for Low Back Pain and Sciatica Patients based on Multimodalities and Multimodal Learning. (arXiv:2303.09085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#20013;&#21307;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26415;&#21069;&#35780;&#20272;&#24037;&#20855;&#65292;&#32452;&#21512;&#20102;&#26631;&#20934;&#25163;&#26415;&#35780;&#20272;&#12289;&#20013;&#21307;&#20307;&#36136;&#35780;&#20272;&#21644;&#35745;&#21010;&#25163;&#26415;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#33136;&#26894;&#25163;&#26415;&#30340;&#39044;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24739;&#32773;&#20986;&#29616;&#20005;&#37325;&#30140;&#30171;&#30151;&#29366;&#26102;&#65292;&#20302;&#32972;&#30171;&#21644;&#22352;&#39592;&#31070;&#32463;&#30171;&#21487;&#33021;&#38656;&#35201;&#25163;&#26415;&#27835;&#30103;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#26377;&#25928;&#30340;&#25514;&#26045;&#26469;&#39044;&#27979;&#25163;&#26415;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#20013;&#21307;&#20803;&#32032;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26415;&#21069;&#35780;&#20272;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#20302;&#32972;&#30171;&#21644;&#22352;&#39592;&#31070;&#32463;&#30171;&#24739;&#32773;&#33136;&#26894;&#25163;&#26415;&#30340;&#39044;&#21518;&#12290;&#25910;&#38598;&#24182;&#23384;&#20648;&#20102;&#26631;&#20934;&#25163;&#26415;&#35780;&#20272;&#12289;&#20013;&#21307;&#20307;&#36136;&#35780;&#20272;&#12289;&#35745;&#21010;&#25163;&#26415;&#26041;&#27861;&#21644;&#20803;&#38899;&#21457;&#38899;&#24405;&#38899;&#31561;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21033;&#29992;&#27169;&#24577;&#32452;&#21512;&#12289;&#22810;&#27169;&#24577;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#35265;&#35299;&#12290;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20063;&#24471;&#21040;&#20102;&#26816;&#26597;&#12290;&#36890;&#36807;&#25307;&#21215;105&#21517;&#24739;&#32773;&#65292;&#25105;&#20204;&#21457;&#29616;&#32467;&#21512;&#26631;&#20934;&#25163;&#26415;&#35780;&#20272;&#12289;&#20307;&#36136;&#35780;&#20272;&#21644;&#35745;&#21010;&#25163;&#26415;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;0.81&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low back pain (LBP) and sciatica may require surgical therapy when they are symptomatic of severe pain. However, there is no effective measures to evaluate the surgical outcomes in advance. This work combined elements of Eastern medicine and machine learning, and developed a preoperative assessment tool to predict the prognosis of lumbar spinal surgery in LBP and sciatica patients. Standard operative assessments, traditional Chinese medicine body constitution assessments, planned surgical approach, and vowel pronunciation recordings were collected and stored in different modalities. Our work provides insights into leveraging modality combinations, multimodals, and fusion strategies. The interpretability of models and correlations between modalities were also inspected. Based on the recruited 105 patients, we found that combining standard operative assessments, body constitution assessments, and planned surgical approach achieved the best performance in 0.81 accuracy. Our approach is ef
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.09079</link><description>&lt;p&gt;
SSL&#28165;&#29702;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning. (arXiv:2303.09079v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#23398;&#20064;&#21644;&#32534;&#30721;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;SSL&#22270;&#20687;&#32534;&#30721;&#22120;&#24182;&#22312;&#20854;&#39030;&#37096;&#35757;&#32451;&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#32780;&#21482;&#38656;&#24456;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;SSL&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#19982;SSL&#32534;&#30721;&#22120;&#30456;&#20851;&#30340;&#23433;&#20840;&#30740;&#31350;&#21644;&#21508;&#31181;&#26408;&#39532;&#25915;&#20987;&#30340;&#21457;&#23637;&#12290;&#22312;SSL&#32534;&#30721;&#22120;&#20013;&#25554;&#20837;&#26408;&#39532;&#25915;&#20987;&#30340;&#21361;&#38505;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#38544;&#34109;&#22320;&#25805;&#20316;&#24182;&#22312;&#21508;&#31181;&#29992;&#25143;&#21644;&#35774;&#22791;&#20043;&#38388;&#24191;&#27867;&#20256;&#25773;&#12290;Trojaned&#32534;&#30721;&#22120;&#20013;&#30340;&#21518;&#38376;&#34892;&#20026;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#34987;&#19979;&#28216;&#20998;&#31867;&#22120;&#24847;&#22806;&#32487;&#25215;&#65292;&#20351;&#26816;&#27979;&#21644;&#32531;&#35299;&#23041;&#32961;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#34429;&#28982;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#24403;&#21069;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a commonly used approach to learning and encoding data representations. By using a pre-trained SSL image encoder and training a downstream classifier on top of it, impressive performance can be achieved on various tasks with very little labeled data. The increasing usage of SSL has led to an uptick in security research related to SSL encoders and the development of various Trojan attacks. The danger posed by Trojan attacks inserted in SSL encoders lies in their ability to operate covertly and spread widely among various users and devices. The presence of backdoor behavior in Trojaned encoders can inadvertently be inherited by downstream classifiers, making it even more difficult to detect and mitigate the threat. Although current Trojan detection methods in supervised learning can potentially safeguard SSL downstream classifiers, identifying and addressing triggers in the SSL encoder before its widespread dissemination is a challenging task. This is be
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23558;&#24037;&#19994;&#29289;&#32852;&#32593;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.09068</link><description>&lt;p&gt;
VFP&#65306;&#32771;&#34385;&#23646;&#24615;&#30456;&#20851;&#24615;&#23558;&#24037;&#19994;&#29289;&#32852;&#32593;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#20197;&#20379;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
VFP: Converting Tabular Data for IIoT into Images Considering Correlations of Attributes for Convolutional Neural Networks. (arXiv:2303.09068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23558;&#24037;&#19994;&#29289;&#32852;&#32593;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#29983;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20256;&#32479;&#22522;&#20110;&#20915;&#31574;&#26641;&#31639;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#24050;&#34987;&#37319;&#29992;&#12290; &#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#30495;&#23454;&#25968;&#23383;&#23646;&#24615;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#34920;&#26684;&#25968;&#25454;&#26102;&#23384;&#22312;&#38480;&#21046;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;DeepInsight&#65292;REFINED&#21644;IGTD&#23558;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#20197;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290; &#20182;&#20204;&#22312;&#22270;&#20687;&#30340;&#26576;&#20123;&#29305;&#23450;&#20301;&#32622;&#25910;&#38598;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#20351;&#36716;&#25442;&#21518;&#30340;&#22270;&#20687;&#30475;&#36215;&#26469;&#20687;&#26159;&#23454;&#38469;&#22270;&#20687;&#12290; &#25910;&#38598;&#31867;&#20284;&#30340;&#29305;&#24449;&#19982;&#20256;&#32479;&#30340;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;ML&#25216;&#26415;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#21024;&#38500;&#19968;&#20123;&#39640;&#24230;&#30456;&#20851;&#30340;&#23646;&#24615;&#20197;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290; &#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#36716;&#25442;&#26041;&#27861;&#22266;&#23450;&#20102;&#22270;&#20687;&#22823;&#23567;&#65292;&#26681;&#25454;&#34920;&#26684;&#25968;&#25454;&#30340;&#23646;&#24615;&#25968;&#37327;&#65292;&#20250;&#36896;&#25104;&#28010;&#36153;&#25110;&#19981;&#36275;&#30340;&#20687;&#32032;&#12290; &#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;Vortex&#29305;&#24449;&#23450;&#20301;&#65288;VFP&#65289;&#12290; VFP&#32771;&#34385;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#24182;&#25918;&#32622;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
For tabular data generated from IIoT devices, traditional machine learning (ML) techniques based on the decision tree algorithm have been employed. However, these methods have limitations in processing tabular data where real number attributes dominate. To address this issue, DeepInsight, REFINED, and IGTD were proposed to convert tabular data into images for utilizing convolutional neural networks (CNNs). They gather similar features in some specific spots of an image to make the converted image look like an actual image. Gathering similar features contrasts with traditional ML techniques for tabular data, which drops some highly correlated attributes to avoid overfitting. Also, previous converting methods fixed the image size, and there are wasted or insufficient pixels according to the number of attributes of tabular data. Therefore, this paper proposes a new converting method, Vortex Feature Positioning (VFP). VFP considers the correlation of features and places similar features fa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#24179;&#28369;&#25903;&#25345;&#21521;&#37327;&#26426;&#38128;&#38142;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;Bernstein&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;BernSVM&#65289;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#27714;&#35299;&#35813;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09066</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#24809;&#32602;&#20271;&#24681;&#26031;&#22374;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Penalized Bernstein Support Vector Machines. (arXiv:2303.09066v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#24179;&#28369;&#25903;&#25345;&#21521;&#37327;&#26426;&#38128;&#38142;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;Bernstein&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;BernSVM&#65289;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#27714;&#35299;&#35813;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#26159;&#19968;&#31181;&#29992;&#20110;&#20108;&#20998;&#31867;&#30340;&#24378;&#22823;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#65292;SVM&#38128;&#38142;&#25439;&#22833;&#20989;&#25968;&#30340;&#19981;&#21487;&#24494;&#24615;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20381;&#36182;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24179;&#28369;&#30340;SVM&#38128;&#38142;&#25439;&#22833;&#20989;&#25968;&#29256;&#26412;&#65292;&#31216;&#20026;Bernstein&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;BernSVM&#65289;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;$p&gt;&gt; n$&#24773;&#20917;&#12290;&#30001;&#20110;BernSVM&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#23646;&#20110;$C^2$&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#24809;&#32602;BernSVM&#35299;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#31532;&#19968;&#20010;&#31639;&#27861;&#22522;&#20110;&#26368;&#22823;&#21270;-&#20027;&#23548;&#65288;MM&#65289;&#21407;&#29702;&#30340;&#22352;&#26631;&#19979;&#38477;&#27861;&#65292;&#31532;&#20108;&#20010;&#31639;&#27861;&#26159;IRLS&#31867;&#22411;&#31639;&#27861;&#65288;&#36845;&#20195;&#37325;&#26032;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#65289;&#12290;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#38181;&#26465;&#20214;&#21644;&#19968;&#20010;&#38480;&#21046;&#24615;&#24378;&#20984;&#24615;&#65292;&#20197;&#24314;&#31435;&#21152;&#26435;Lasso BernSVM&#20272;&#35745;&#22120;&#30340;&#19978;&#30028;&#12290;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#36924;&#36817;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#65292;&#29992;&#20110;&#35843;&#25972;BernSVM&#36229;&#21442;&#25968;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29616;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The support vector machines (SVM) is a powerful classifier used for binary classification to improve the prediction accuracy. However, the non-differentiability of the SVM hinge loss function can lead to computational difficulties in high dimensional settings. To overcome this problem, we rely on Bernstein polynomial and propose a new smoothed version of the SVM hinge loss called the Bernstein support vector machine (BernSVM), which is suitable for the high dimension $p &gt;&gt; n$ regime. As the BernSVM objective loss function is of the class $C^2$, we propose two efficient algorithms for computing the solution of the penalized BernSVM. The first algorithm is based on coordinate descent with maximization-majorization (MM) principle and the second one is IRLS-type algorithm (iterative re-weighted least squares). Under standard assumptions, we derive a cone condition and a restricted strong convexity to establish an upper bound for the weighted Lasso BernSVM estimator. Using a local linear ap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09065</link><description>&lt;p&gt;
&#22522;&#20110;t-SPN&#21644;&#28388;&#27874;&#30340;&#32454;&#32990;&#20998;&#31867;&#30340;&#26368;&#22823;&#38388;&#38548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#27010;&#29575;&#20307;&#31995;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#26641;&#24418;&#27714;&#21644;&#20135;&#21697;&#32593;&#32476;(t-SPN)&#65292;&#29992;&#20110;&#32454;&#32990;&#20998;&#31867;&#12290;&#26500;&#24314;t-SPN&#30340;&#30446;&#30340;&#26159;&#34920;&#31034;&#26410;&#24402;&#19968;&#21270;&#27010;&#29575;&#20316;&#20026;&#26368;&#30456;&#20284;&#30340;&#32454;&#32990;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#26469;&#23398;&#20064;&#26500;&#24314;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#36793;&#32536;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#26368;&#26377;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#12290;&#20026;&#20102;&#22686;&#24378;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;L2&#27491;&#21017;&#21270;&#65288;REG&#65289;&#21644;&#26368;&#22823;&#38388;&#38548;&#65288;MM&#65289;&#26631;&#20934;&#12290;&#20026;&#20102;&#31361;&#20986;&#32454;&#32990;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#30340;&#26377;&#25928;&#24615;&#65306;&#29702;&#24819;&#39640;&#36890;&#28388;&#27874;&#21644;&#25289;&#26222;&#25289;&#26031;&#28388;&#27874;(Log)&#12290;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#26368;&#22823;&#38388;&#38548;&#20934;&#21017;&#19982;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;R-CNN&#65289;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#30058;&#33540;&#26893;&#29289;&#30340;&#21494;&#30149;&#23475;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25972;&#20010;&#30058;&#33540;&#26893;&#29289;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09063</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26893;&#29289;&#30149;&#23475;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Plant Disease Detection using Region-Based Convolutional Neural Network. (arXiv:2303.09063v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;R-CNN&#65289;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#30058;&#33540;&#26893;&#29289;&#30340;&#21494;&#30149;&#23475;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25972;&#20010;&#30058;&#33540;&#26893;&#29289;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#22312;&#23391;&#21152;&#25289;&#22269;&#30340;&#39135;&#21697;&#21644;&#32463;&#27982;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32454;&#33740;&#65292;&#30149;&#27602;&#21644;&#30495;&#33740;&#30149;&#23475;&#38480;&#21046;&#20102;&#20316;&#29289;&#29983;&#20135;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#39044;&#27979;&#30058;&#33540;&#26893;&#29289;&#30340;&#21494;&#30149;&#23475;&#12290;&#36890;&#36807;&#20462;&#25913;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;R-CNN&#65289;&#30340;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#21033;&#29992;&#25972;&#20010;&#30058;&#33540;&#26893;&#29289;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#26816;&#27979;&#26893;&#29289;&#30149;&#23475;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#22235;&#31181;&#24120;&#35265;&#30340;&#30058;&#33540;&#26893;&#29289;&#30149;&#23475;&#26041;&#38754;&#34920;&#29616;&#20986;94.3&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agriculture plays an important role in the food and economy of Bangladesh. The rapid growth of population over the years also has increased the demand for food production. One of the major reasons behind low crop production is numerous bacteria, virus and fungal plant diseases. Early detection of plant diseases and proper usage of pesticides and fertilizers are vital for preventing the diseases and boost the yield. Most of the farmers use generalized pesticides and fertilizers in the entire fields without specifically knowing the condition of the plants. Thus the production cost oftentimes increases, and, not only that, sometimes this becomes detrimental to the yield. Deep Learning models are found to be very effective to automatically detect plant diseases from images of plants, thereby reducing the need for human specialists. This paper aims at building a lightweight deep learning model for predicting leaf disease in tomato plants. By modifying the region-based convolutional neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#22238;&#35793;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#30693;&#35782;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#29992;&#20110;&#20266;&#20195;&#30721;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32570;&#23569;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;-&#20266;&#20195;&#30721;&#24179;&#34892;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09062</link><description>&lt;p&gt;
&#26469;&#33258;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#30340;&#20266;&#20195;&#30721;&#29983;&#25104;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer for Pseudo-code Generation from Low Resource Programming Language. (arXiv:2303.09062v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#27169;&#22411;&#36890;&#36807;&#36845;&#20195;&#22238;&#35793;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#30693;&#35782;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#20013;&#29992;&#20110;&#20266;&#20195;&#30721;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32570;&#23569;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;-&#20266;&#20195;&#30721;&#24179;&#34892;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#36951;&#30041;&#28304;&#20195;&#30721;&#30340;&#20266;&#20195;&#30721;&#25551;&#36848;&#20197;&#23454;&#29616;&#36719;&#20214;&#32500;&#25252;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#33258;&#21160;&#21270;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;C++&#65289;&#30340;&#20266;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#26159;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20195;&#30721;-&#20266;&#20195;&#30721;&#35821;&#26009;&#24211;&#30340;&#21487;&#29992;&#24615;&#12290;&#38024;&#23545;&#22312;&#36951;&#30041;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#20013;&#32534;&#20889;&#20195;&#30721;&#30340;&#20266;&#20195;&#30721;&#27880;&#37322;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#28304;PL&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36890;&#36807;&#20351;&#29992;&#24179;&#34892;&#20195;&#30721;-&#20266;&#20195;&#30721;&#25968;&#25454;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#27169;&#22411;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#31227;&#21040;&#27809;&#26377;PL-&#20266;&#20195;&#30721;&#24179;&#34892;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#30340;&#36951;&#30041;PL&#65288;C&#65289;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#29992;&#20363;&#30340;&#36807;&#28388;&#31574;&#30053;&#30340;&#36845;&#20195;&#22238;&#35793;&#65288;IBT&#65289;&#26041;&#27861;&#65292;&#20197;&#23558;&#32463;&#36807;&#35757;&#32451;&#30340;C++-to-pseudocode&#27169;&#22411;&#35843;&#25972;&#20026;C-to-pseudocode&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of pseudo-code descriptions of legacy source code for software maintenance is a manually intensive task. Recent encoder-decoder language models have shown promise for automating pseudo-code generation for high resource programming languages such as C++, but are heavily reliant on the availability of a large code-pseudocode corpus. Soliciting such pseudocode annotations for codes written in legacy programming languages (PL) is a time consuming and costly affair requiring a thorough understanding of the source PL. In this paper, we focus on transferring the knowledge acquired by the code-to-pseudocode neural model trained on a high resource PL (C++) using parallel code-pseudocode data. We aim to transfer this knowledge to a legacy PL (C) with no PL-pseudocode parallel data for training. To achieve this, we utilize an Iterative Back Translation (IBT) approach with a novel test-cases based filtration strategy, to adapt the trained C++-to-pseudocode model to C-to-pseudocode model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.09051</link><description>&lt;p&gt;
&#25193;&#25955;&#24335;&#23545;&#25239;&#20928;&#21270;&#30340;&#40065;&#26834;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36136;&#30097;&#24403;&#21069;&#23545;&#22522;&#20110;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#30340;&#35780;&#20272;&#26041;&#24335;&#12290;&#25193;&#25955;&#24335;&#20928;&#21270;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#27979;&#35797;&#25968;&#25454;&#28857;&#20013;&#30340;&#23545;&#25239;&#24615;&#24433;&#21709;&#12290;&#30001;&#20110;&#22522;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#35299;&#32806;&#65292;&#35813;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20316;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#20102;&#27979;&#37327;&#20928;&#21270;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#24120;&#37319;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#30333;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#26159;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#22240;&#27492;&#19981;&#30693;&#36947;&#36825;&#20123;&#25915;&#20987;&#26159;&#21542;&#23545;&#25193;&#25955;&#24335;&#20928;&#21270;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#30340;&#23454;&#36341;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#34913;&#37327;&#20928;&#21270;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20928;&#21270;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We question the current evaluation practice on diffusion-based purification methods. Diffusion-based purification methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing attention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robustness of the purification. However, it is unknown whether these attacks are the most effective for the diffusion-based purification since the attacks are often tailored for adversarial training. We analyze the current practices and provide a new guideline for measuring the robustness of purification methods against adversarial attacks. Based on our analysis, we further propose a new purification strategy showing competitive results against the state-of-the-art adversarial training approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;VoIP&#36890;&#20449;&#24179;&#21488;&#36827;&#34892;DNS&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#23558;&#22122;&#22768;&#25233;&#21046;&#21644;VoIP&#29305;&#26377;&#30340;&#22768;&#23398;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#20248;&#20110;&#34892;&#19994;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09048</link><description>&lt;p&gt;
&#22312;VoIP&#24179;&#21488;&#19978;&#25552;&#39640;&#30693;&#35273;&#36136;&#37327;&#12289;&#21487;&#25026;&#24230;&#21644;&#22768;&#23398;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms. (arXiv:2303.09048v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;VoIP&#36890;&#20449;&#24179;&#21488;&#36827;&#34892;DNS&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20854;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#23558;&#22122;&#22768;&#25233;&#21046;&#21644;VoIP&#29305;&#26377;&#30340;&#22768;&#23398;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#20248;&#20110;&#34892;&#19994;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;Deep Noise Suppression (DNS) 2020 Challenge&#27169;&#22411;&#22312;VoIP&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;DNS 2020&#27169;&#22411;&#36866;&#24212;&#20110;VoIP&#36890;&#20449;&#30340;&#29305;&#23450;&#22768;&#23398;&#29305;&#24449;&#65292;&#21253;&#25324;&#22240;&#21387;&#32553;&#12289;&#20256;&#36755;&#21644;&#24179;&#21488;&#29305;&#23450;&#22788;&#29702;&#32780;&#24341;&#36215;&#30340;&#22833;&#30495;&#21644;&#20266;&#24433;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#38899;&#22686;&#24378;&#30340;VoIP-DNS&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#20849;&#21516;&#20248;&#21270;&#38477;&#22122;&#21644;VoIP&#29305;&#23450;&#30340;&#22768;&#23398;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;VoIP&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;VoIP&#24212;&#29992;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#24615;&#33021;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#21033;&#29992;VoIP-DNS&#33021;&#22815;&#25552;&#39640;&#21644;&#23450;&#21046;DNS-2020&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;VoIP&#24179;&#21488;&#19978;&#30340;&#28508;&#21147;&#65292;&#36825;&#39033;&#21457;&#29616;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21161;&#29702;&#31561;&#39046;&#22495;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;IoT&#21644;ML&#25216;&#26415;&#38477;&#20302;&#36873;&#20030;&#25104;&#26412;&#24182;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;E-voting&#31995;&#32479;&#23433;&#20840;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09045</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#32852;&#32593;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#20030;&#31649;&#29702;Web&#21644;&#31227;&#21160;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Web and Mobile Platforms for Managing Elections based on IoT And Machine Learning Algorithms. (arXiv:2303.09045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;IoT&#21644;ML&#25216;&#26415;&#38477;&#20302;&#36873;&#20030;&#25104;&#26412;&#24182;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;E-voting&#31995;&#32479;&#23433;&#20840;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30123;&#24773;&#20005;&#37325;&#24433;&#21709;&#21508;&#22269;&#12290;&#32467;&#26524;&#65292;&#20960;&#20046;&#25152;&#26377;&#22269;&#23478;&#19981;&#24471;&#19981;&#35843;&#25972;&#22312;&#32447;&#25216;&#26415;&#20197;&#32487;&#32493;&#20182;&#20204;&#30340;&#27969;&#31243;&#12290;&#26031;&#37324;&#20848;&#21345;&#27599;&#24180;&#22312;&#36873;&#20030;&#19978;&#33457;&#36153;100&#20159;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#38382;&#39064;&#24182;&#22686;&#21152;&#26102;&#38388;&#25928;&#29575;&#21644;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;IoT&#21644;ML&#25216;&#26415;&#12290;&#22522;&#20110;IoT&#30340;&#25968;&#25454;&#23558;&#35782;&#21035;&#12289;&#27880;&#20876;&#21644;&#29992;&#20110;&#38450;&#27490;&#27450;&#35784;&#65292;&#32780;ML&#31639;&#27861;&#23558;&#25805;&#32437;&#36873;&#20030;&#25968;&#25454;&#24182;&#20135;&#29983;&#33719;&#32988;&#39044;&#27979;&#12289;&#22522;&#20110;&#22825;&#27668;&#30340;&#36873;&#27665;&#20986;&#24109;&#29575;&#21644;&#36873;&#20030;&#26292;&#21147;&#12290;&#25152;&#26377;&#25968;&#25454;&#23558;&#20445;&#23384;&#22312;&#20113;&#35745;&#31639;&#21644;&#26631;&#20934;&#25968;&#25454;&#24211;&#20013;&#20197;&#23384;&#20648;&#21644;&#35775;&#38382;&#25968;&#25454;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;E-voting&#31995;&#32479;&#30340;&#22235;&#20010;&#26041;&#38754;&#12290; E-voting&#30340;&#26368;&#24120;&#35265;&#38382;&#39064;&#26159;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25552;&#20986;&#30340;IoT&#21644;ML&#25216;&#26415;&#26088;&#22312;&#25552;&#39640;E-voting&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#24182;&#22686;&#21152;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The global pandemic situation has severely affected all countries. As a result, almost all countries had to adjust to online technologies to continue their processes. In addition, Sri Lanka is yearly spending ten billion on elections. We have examined a proper way of minimizing the cost of hosting these events online. To solve the existing problems and increase the time potency and cost reduction we have used IoT and ML-based technologies. IoT-based data will identify, register, and be used to secure from fraud, while ML algorithms manipulate the election data and produce winning predictions, weather-based voters attendance, and election violence. All the data will be saved in cloud computing and a standard database to store and access the data. This study mainly focuses on four aspects of an E-voting system. The most frequent problems across the world in E-voting are the security, accuracy, and reliability of the systems. E-government systems must be secured against various cyber-atta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#24310;&#36831;&#23884;&#20837;&#29702;&#35770;&#21644;&#24191;&#20041;&#23884;&#20837;&#29702;&#35770;&#65292;&#20005;&#35880;&#35777;&#26126;&#20102;RC&#26412;&#36136;&#19978;&#26159;&#21407;&#22987;&#36755;&#20837;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#39640;&#32500;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#26102;&#38388;&#24310;&#36831;&#21644;&#32593;&#32476;&#31070;&#32463;&#20803;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#20102;&#27946;&#27867;&#35745;&#31639;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#27604;&#20840;&#23610;&#23544;RC&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09042</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#29702;&#35770;&#22312;&#27946;&#27867;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#21450;&#21033;&#29992;&#26102;&#38388;&#24310;&#36831;&#20943;&#23569;&#27946;&#27867;&#32593;&#32476;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Embedding Theory of Reservoir Computing and Reducing Reservoir Network Using Time Delays. (arXiv:2303.09042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#24310;&#36831;&#23884;&#20837;&#29702;&#35770;&#21644;&#24191;&#20041;&#23884;&#20837;&#29702;&#35770;&#65292;&#20005;&#35880;&#35777;&#26126;&#20102;RC&#26412;&#36136;&#19978;&#26159;&#21407;&#22987;&#36755;&#20837;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#39640;&#32500;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#26102;&#38388;&#24310;&#36831;&#21644;&#32593;&#32476;&#31070;&#32463;&#20803;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#20102;&#27946;&#27867;&#35745;&#31639;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#27604;&#20840;&#23610;&#23544;RC&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27867;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#22312;&#37325;&#26500;&#25110;/&#21644;&#39044;&#27979;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#21151;&#25928;&#21644;&#39640;&#24615;&#33021;&#65292;&#22240;&#27492;&#27491;&#22312;&#29190;&#28856;&#24615;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35302;&#21457;RC&#22914;&#27492;&#26377;&#25928;&#24212;&#29992;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#65292;&#38656;&#35201;&#28145;&#20837;&#32780;&#31995;&#32479;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#32467;&#21512;&#24310;&#36831;&#23884;&#20837;&#29702;&#35770;&#21644;&#24191;&#20041;&#23884;&#20837;&#29702;&#35770;&#65292;&#20005;&#35880;&#35777;&#26126;&#20102;RC&#26412;&#36136;&#19978;&#26159;&#21407;&#22987;&#36755;&#20837;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#39640;&#32500;&#23884;&#20837;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#36825;&#31181;&#23884;&#20837;&#29305;&#24615;&#65292;&#25105;&#20204;&#23558;&#26631;&#20934;RC&#21644;&#26102;&#38388;&#24310;&#36831;RC&#32479;&#19968;&#21040;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#25105;&#20204;&#23545;&#32593;&#32476;&#30340;&#36755;&#20986;&#23618;&#20165;&#24341;&#20837;&#26102;&#38388;&#24310;&#36831;&#65292;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#26102;&#38388;&#24310;&#36831;&#21644;&#32593;&#32476;&#31070;&#32463;&#20803;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#26174;&#30528;&#20943;&#23567;&#20102;&#27946;&#27867;&#35745;&#31639;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#29992;&#20110;&#37325;&#26500;&#21644;&#39044;&#27979;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#19988;&#26356;&#35753;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23454;&#29616;&#20102;&#27604;&#20840;&#23610;&#23544;RC&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing (RC), a particular form of recurrent neural network, is under explosive development due to its exceptional efficacy and high performance in reconstruction or/and prediction of complex physical systems. However, the mechanism triggering such effective applications of RC is still unclear, awaiting deep and systematic exploration. Here, combining the delayed embedding theory with the generalized embedding theory, we rigorously prove that RC is essentially a high dimensional embedding of the original input nonlinear dynamical system. Thus, using this embedding property, we unify into a universal framework the standard RC and the time-delayed RC where we novelly introduce time delays only into the network's output layer, and we further find a trade-off relation between the time delays and the number of neurons in RC. Based on this finding, we significantly reduce the network size of RC for reconstructing and predicting some representative physical systems, and, more surp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#30340;&#28966;&#34385;&#31579;&#26597;&#26694;&#26550;&#65288;MMD-AS&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09041</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#30340;&#28966;&#34385;&#31579;&#26597;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Data-driven Framework for Anxiety Screening. (arXiv:2303.09041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#30340;&#28966;&#34385;&#31579;&#26597;&#26694;&#26550;&#65288;MMD-AS&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28966;&#34385;&#31579;&#26597;&#30340;&#38656;&#27714;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#24448;&#24448;&#36890;&#36807;&#19987;&#19994;&#30340;&#35774;&#22791;&#21450;&#21307;&#29983;&#30340;&#32463;&#39564;&#21644;&#21028;&#26029;&#21147;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#30340;&#26377;&#38480;&#24615;&#65292;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#39640;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#12290;&#32780;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#30001;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#22122;&#38899;&#65292;&#21516;&#26102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#20063;&#23481;&#26131;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#36825;&#20351;&#24471;&#29616;&#26377;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#28966;&#34385;&#31579;&#26597;&#26041;&#27861;&#38590;&#20197;&#24212;&#29992;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#30340;&#28966;&#34385;&#31579;&#26597;&#26694;&#26550;&#65292;&#21363;MMD-AS&#65292;&#24182;&#22312;&#25910;&#38598;&#20102;200&#20010;&#24739;&#32773;&#30340;&#20581;&#24247;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early screening for anxiety and appropriate interventions are essential to reduce the incidence of self-harm and suicide in patients. Due to limited medical resources, traditional methods that overly rely on physician expertise and specialized equipment cannot simultaneously meet the needs for high accuracy and model interpretability. Multimodal data can provide more objective evidence for anxiety screening to improve the accuracy of models. The large amount of noise in multimodal data and the unbalanced nature of the data make the model prone to overfitting. However, it is a non-differentiable problem when high-dimensional and multimodal feature combinations are used as model inputs and incorporated into model training. This causes existing anxiety screening methods based on machine learning and deep learning to be inapplicable. Therefore, we propose a multimodal data-driven anxiety screening framework, namely MMD-AS, and conduct experiments on the collected health data of over 200 se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22870;&#21169;&#26041;&#24046;&#30340;&#20449;&#24687;&#20943;&#23569;&#20102;&#36951;&#25022;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;</title><link>http://arxiv.org/abs/2303.09033</link><description>&lt;p&gt;
&#21482;&#38024;&#23545;&#19981;&#30830;&#23450;&#24615;&#25903;&#20184;&#20195;&#20215;&#65306;&#26041;&#24046;&#33258;&#36866;&#24212;&#27748;&#26222;&#26862;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22870;&#21169;&#26041;&#24046;&#30340;&#20449;&#24687;&#20943;&#23569;&#20102;&#36951;&#25022;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#36172;&#21338;&#31639;&#27861;&#37117;&#20551;&#35774;&#22870;&#21169;&#26041;&#24046;&#25110;&#20854;&#19978;&#30028;&#24050;&#30693;&#12290;&#23613;&#31649;&#26041;&#24046;&#39640;&#20272;&#36890;&#24120;&#26159;&#23433;&#20840;&#30340;&#65292;&#20294;&#23427;&#20250;&#22686;&#21152;&#36951;&#25022;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20302;&#20272;&#30340;&#26041;&#24046;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#36807;&#26089;&#22320;&#36873;&#25321;&#20102;&#27425;&#20248;&#33218;&#32780;&#23548;&#33268;&#30340;&#32447;&#24615;&#36951;&#25022;&#12290;&#36825;&#28608;&#21457;&#20102;&#20851;&#20110;&#26041;&#24046;&#24863;&#30693;&#39057;&#29575;&#31639;&#27861;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#25105;&#20204;&#20026;&#36125;&#21494;&#26031;&#35774;&#32622;&#25171;&#19979;&#22522;&#30784;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24050;&#30693;&#21644;&#26410;&#30693;&#24322;&#36136;&#22870;&#21169;&#26041;&#24046;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#24182;&#20026;&#20004;&#32773;&#24320;&#21457;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#38543;&#30528;&#36739;&#20302;&#22870;&#21169;&#26041;&#24046;&#32780;&#20943;&#23569;&#65292;&#36825;&#20351;&#24471;&#23398;&#20064;&#26356;&#21152;&#23481;&#26131;&#12290;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#30340;&#36793;&#30028;&#25429;&#25417;&#20102;&#20808;&#39564;&#23545;&#23398;&#20064;&#22870;&#21169;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#26159;&#20854;&#31867;&#22411;&#20013;&#30340;&#39318;&#20010;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26041;&#24046;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20063;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most bandit algorithms assume that the reward variance or its upper bound is known. While variance overestimation is usually safe and sound, it increases regret. On the other hand, an underestimated variance may lead to linear regret due to committing early to a suboptimal arm. This motivated prior works on variance-aware frequentist algorithms. We lay foundations for the Bayesian setting. In particular, we study multi-armed bandits with known and \emph{unknown heterogeneous reward variances}, and develop Thompson sampling algorithms for both and bound their Bayes regret. Our regret bounds decrease with lower reward variances, which make learning easier. The bound for unknown reward variances captures the effect of the prior on learning reward variances and is the first of its kind. Our experiments show the superiority of variance-aware Bayesian algorithms and also highlight their robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;(COE)&#30340;&#22522;&#20110;UCT&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2303.09032</link><description>&lt;p&gt;
&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;&#29992;&#20110;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;(COE)&#30340;&#22522;&#20110;UCT&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25506;&#32034;&#23545;&#20110;&#21327;&#20316;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UCT(&#24212;&#29992;&#20110;&#26641;&#30340;&#32622;&#20449;&#19978;&#38480;)&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#40723;&#21169;&#21327;&#21516;&#25506;&#32034;&#12290;&#39640;&#23618;&#27425;&#30340;&#24605;&#36335;&#26159;&#65292;&#20026;&#20102;&#36827;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#25506;&#32034;&#65292;&#22914;&#26524;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#20048;&#35266;&#20272;&#35745;&#25429;&#33719;&#20102;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#32467;&#26500;&#21270;&#20381;&#36182;&#20851;&#31995;&#65292;&#21017;&#26234;&#33021;&#20307;&#23558;&#23454;&#29616;&#21327;&#20316;&#31574;&#30053;&#12290;&#22312;&#25628;&#32034;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#65288;&#21363;&#21160;&#20316;&#65289;&#19978;&#65292;UCT&#20351;&#29992;&#36890;&#36807;&#23545;&#20854;&#29238;&#33410;&#28857;&#30340;&#35775;&#38382;&#35745;&#25968;&#36827;&#34892;&#26465;&#20214;&#25512;&#23548;&#30340;&#22870;&#21169;&#26469;&#25191;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;MARL&#35270;&#20026;&#26641;&#25628;&#32034;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;(COE)&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#26234;&#33021;&#20307;&#25353;&#39034;&#24207;&#25191;&#34892;&#21160;&#20316;&#65292;&#24182;&#23558;&#25628;&#32034;&#26641;&#21516;&#19968;&#28145;&#24230;&#30340;&#33410;&#28857;&#35270;&#20026;&#19968;&#20010;&#21333;&#29420;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#12290;COE&#35745;&#31639;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#22870;&#21169;&#65292;&#20197;&#40723;&#21169;&#26234;&#33021;&#20307;&#36890;&#36807;&#23581;&#35797;&#26032;&#30340;&#34892;&#21160;&#26469;&#25506;&#32034;&#26410;&#30693;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL). In this paper, we propose an exploration method that efficiently encourages cooperative exploration based on the idea of the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees). The high-level intuition is that to perform optimism-based exploration, agents would achieve cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents. At each node (i.e., action) of the search tree, UCT performs optimism-based exploration using a bonus derived by conditioning on the visitation count of its parent node. We provide a perspective to view MARL as tree search iterations and develop a method called Conditionally Optimistic Exploration (COE). We assume agents take actions following a sequential order, and consider nodes at the same depth of the search tree as actions of one individual agent. COE compute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20174;&#20687;&#32032;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.09031</link><description>&lt;p&gt;
&#19968;&#24133;&#22270;&#32988;&#36807;&#21315;&#35328;&#19975;&#35821;&#65306;&#35821;&#35328;&#27169;&#22411;&#20174;&#20687;&#32032;&#20013;&#35268;&#21010;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
A Picture is Worth a Thousand Words: Language Models Plan from Pixels. (arXiv:2303.09031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20174;&#20687;&#32032;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25191;&#34892;&#23454;&#38469;&#29615;&#22659;&#20013;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#25512;&#29702;&#20986;&#35268;&#21010;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#20043;&#21069;&#36890;&#36807;PLM&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#23450;&#35266;&#23519;&#32467;&#26524;&#20197;&#25991;&#26412;&#24418;&#24335;&#21487;&#33719;&#24471;&#65288;&#20363;&#22914;&#30001;&#23383;&#24149;&#27169;&#22411;&#25552;&#20379;&#65289;&#65292;&#35201;&#20040;&#20165;&#20174;&#25351;&#20196;&#20013;&#29702;&#35299;&#35268;&#21010;&#65292;&#25110;&#32773;&#21482;&#26377;&#26377;&#38480;&#26041;&#24335;&#22320;&#25972;&#21512;&#20102;&#26377;&#20851;&#35270;&#35273;&#29615;&#22659;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#39044;&#35757;&#32451;&#30340;&#21487;&#20379;&#24615;&#20989;&#25968;&#65289;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#35266;&#23519;&#32467;&#26524;&#30452;&#25509;&#32534;&#30721;&#20026;PLM&#30340;&#36755;&#20837;&#25552;&#31034;&#65292;PLM&#20063;&#33021;&#22815;&#20934;&#30830;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#22312;ALFWorld&#21644;VirtualHome&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text (e.g., provided by a captioning model), reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM. We show that this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LR4GPM&#30340;&#65288;&#28145;&#24230;&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#20379;&#30340;&#20840;&#23616;&#24615;&#33021;&#24230;&#37327;&#30340;&#22522;&#30784;&#19978;&#20248;&#21270;&#12290;LR4GPM&#22312;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#35757;&#32451;&#31574;&#30053;&#26102;&#20351;&#29992;&#20102;&#20960;&#31181;&#35757;&#32451;&#25216;&#24039;&#65292;&#33021;&#22815;&#36991;&#20813;&#36827;&#34892;&#22870;&#21169;&#24037;&#31243;&#65292;&#20854;&#25928;&#29575;&#39640;&#20110;DAI'2020&#32452;&#32455;&#26368;&#36817;&#20030;&#21150;&#30340;&#19968;&#27425;&#33258;&#21160;&#39550;&#39542;&#31454;&#36187;&#30340;&#33719;&#32988;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.09027</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#22870;&#21169;&#20197;&#20248;&#21270;&#20840;&#23616;&#24615;&#33021;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Rewards to Optimize Global Performance Metrics in Deep Reinforcement Learning. (arXiv:2303.09027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LR4GPM&#30340;&#65288;&#28145;&#24230;&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#38382;&#39064;&#25551;&#36848;&#20013;&#25552;&#20379;&#30340;&#20840;&#23616;&#24615;&#33021;&#24230;&#37327;&#30340;&#22522;&#30784;&#19978;&#20248;&#21270;&#12290;LR4GPM&#22312;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#35757;&#32451;&#31574;&#30053;&#26102;&#20351;&#29992;&#20102;&#20960;&#31181;&#35757;&#32451;&#25216;&#24039;&#65292;&#33021;&#22815;&#36991;&#20813;&#36827;&#34892;&#22870;&#21169;&#24037;&#31243;&#65292;&#20854;&#25928;&#29575;&#39640;&#20110;DAI'2020&#32452;&#32455;&#26368;&#36817;&#20030;&#21150;&#30340;&#19968;&#27425;&#33258;&#21160;&#39550;&#39542;&#31454;&#36187;&#30340;&#33719;&#32988;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21040;&#19968;&#20010;&#26032;&#38382;&#39064;&#19978;&#26102;&#65292;&#22870;&#21169;&#24037;&#31243;&#26159;&#19968;&#20010;&#24517;&#35201;&#20294;&#24120;&#24120;&#21313;&#20998;&#22256;&#38590;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#65292;&#31995;&#32479;&#35774;&#35745;&#24072;&#38656;&#35201;&#38754;&#23545;&#23427;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LR4GPM&#65292;&#19968;&#31181;&#33021;&#22815;&#20248;&#21270;&#20840;&#23616;&#24615;&#33021;&#24230;&#37327;&#30340;&#26032;&#22411;&#65288;&#28145;&#24230;&#65289;RL&#26041;&#27861;&#65292;&#36825;&#20010;&#20840;&#23616;&#24615;&#33021;&#24230;&#37327;&#24212;&#35813;&#20316;&#20026;&#38382;&#39064;&#25551;&#36848;&#30340;&#19968;&#37096;&#20998;&#12290;LR4GPM&#20132;&#26367;&#25191;&#34892;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#23398;&#20064;&#65288;&#21487;&#33021;&#26159;&#21521;&#37327;&#65289;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#29992;&#20110;&#25311;&#21512;&#24615;&#33021;&#24230;&#37327;&#65292;&#65288;2&#65289;&#35757;&#32451;&#31574;&#30053;&#20197;&#20248;&#21270;&#22522;&#20110;&#23398;&#20064;&#22870;&#21169;&#30340;&#36825;&#20010;&#24615;&#33021;&#24230;&#37327;&#30340;&#36817;&#20284;&#20540;&#12290;&#36825;&#26679;&#30340;RL&#35757;&#32451;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#20026;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#37117;&#26159;&#20351;&#29992;&#38750;&#31283;&#24577;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#35757;&#32451;&#25216;&#24039;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;LR4GPM&#30340;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LR4GPM&#32988;&#36807;&#20102;DAI'2020&#32452;&#32455;&#30340;&#26368;&#36817;&#19968;&#27425;&#33258;&#21160;&#39550;&#39542;&#31454;&#36187;&#30340;&#33719;&#32988;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applying reinforcement learning (RL) to a new problem, reward engineering is a necessary, but often difficult and error-prone task a system designer has to face. To avoid this step, we propose LR4GPM, a novel (deep) RL method that can optimize a global performance metric, which is supposed to be available as part of the problem description. LR4GPM alternates between two phases: (1) learning a (possibly vector) reward function used to fit the performance metric, and (2) training a policy to optimize an approximation of this performance metric based on the learned rewards. Such RL training is not straightforward since both the reward function and the policy are trained using non-stationary data. To overcome this issue, we propose several training tricks. We demonstrate the efficiency of LR4GPM on several domains. Notably, LR4GPM outperforms the winner of a recent autonomous driving competition organized at DAI'2020.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#21378;&#26080;&#20154;&#26426;&#33258;&#26816;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#34892;&#21160;&#65292;&#36798;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#20351;&#29992;DQN&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#32771;&#34385;&#20869;&#22806;&#37096;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#26816;&#26597;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09013</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#21378;&#26080;&#20154;&#26426;&#33258;&#26816;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Inspection Method of Unmanned Aerial Vehicles in Power Plants Using Deep Q-Network Reinforcement Learning. (arXiv:2303.09013v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#21378;&#26080;&#20154;&#26426;&#33258;&#26816;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#34892;&#21160;&#65292;&#36798;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#20351;&#29992;DQN&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#32771;&#34385;&#20869;&#22806;&#37096;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#26816;&#26597;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26816;&#26597;&#30005;&#21378;&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#26426;&#22120;&#20154;&#12290;&#26412;&#26041;&#27861;&#22797;&#21046;&#29615;&#22659;&#24182;&#37319;&#29992;&#31616;&#21333;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#35813;&#31574;&#30053;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21457;&#30005;&#34892;&#19994;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#34892;&#21160;&#12290;&#20026;&#35299;&#20915;&#26080;&#20154;&#26426;&#23548;&#33322;&#31561;&#20248;&#21270;&#38382;&#39064;&#65292;Deepmind&#20110;2015&#24180;&#25512;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;Q-learning&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;Deep Q-network&#65288;DQN&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#24403;&#21069;&#31243;&#24207;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;UAV&#33258;&#20027;&#23548;&#33322;&#21644;DQN&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#21378;&#26816;&#26597;&#31995;&#32479;&#12290;&#36825;&#20123;&#35757;&#32451;&#36807;&#31243;&#23545;&#21442;&#32771;&#29366;&#24577;&#35774;&#32622;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20869;&#37096;&#21644;&#22806;&#37096;&#24433;&#21709;&#22240;&#32032;&#65292;&#36825;&#20351;&#20854;&#19982;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the purpose of inspecting power plants, autonomous robots can be built using reinforcement learning techniques. The method replicates the environment and employs a simple reinforcement learning (RL) algorithm. This strategy might be applied in several sectors, including the electricity generation sector. A pre-trained model with perception, planning, and action is suggested by the research. To address optimization problems, such as the Unmanned Aerial Vehicle (UAV) navigation problem, Deep Q-network (DQN), a reinforcement learning-based framework that Deepmind launched in 2015, incorporates both deep learning and Q-learning. To overcome problems with current procedures, the research proposes a power plant inspection system incorporating UAV autonomous navigation and DQN reinforcement learning. These training processes set reward functions with reference to states and consider both internal and external effect factors, which distinguishes them from other reinforcement learning train
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#26469;&#20998;&#26512;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#32454;&#32990;&#32452;&#32676;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#25968;&#25454;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09007</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Flow Cytometry Data Analysis. (arXiv:2303.09007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#26469;&#20998;&#26512;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#32454;&#32990;&#32452;&#32676;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#25968;&#25454;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#32454;&#32990;&#26415;&#20027;&#35201;&#29992;&#20110;&#26816;&#27979;&#32454;&#32990;&#20013;&#29305;&#23450;&#26631;&#35760;&#29289;&#30340;&#34920;&#36798;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26816;&#27979;&#33180;&#34920;&#38754;&#21463;&#20307;&#12289;&#25239;&#21407;&#12289;&#31163;&#23376;&#25110;DNA/RNA &#34920;&#36798;&#36807;&#31243;&#20013;&#12290;&#29616;&#20195;&#30340;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#22120;&#21487;&#20197;&#21516;&#26102;&#24555;&#36895;&#20998;&#26512;&#25968;&#20197;&#19975;&#35745;&#30340;&#32454;&#32990;&#65292;&#24182;&#20174;&#21333;&#20010;&#32454;&#32990;&#20013;&#27979;&#37327;&#22810;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#20998;&#26512;&#26041;&#27861;&#38590;&#20197;&#35299;&#37322;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#65292;&#26469;&#20998;&#26512;&#27969;&#24335;&#32454;&#32990;&#35745;&#25968;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#32454;&#32990;&#32452;&#32676;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#25968;&#25454;&#22797;&#26434;&#20851;&#31995;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow cytometry mainly used for detecting the characteristics of a number of biochemical substances based on the expression of specific markers in cells. It is particularly useful for detecting membrane surface receptors, antigens, ions, or during DNA/RNA expression. Not only can it be employed as a biomedical research tool for recognising distinctive types of cells in mixed populations, but it can also be used as a diagnostic tool for classifying abnormal cell populations connected with disease. Modern flow cytometers can rapidly analyse tens of thousands of cells at the same time while also measuring multiple parameters from a single cell. However, the rapid development of flow cytometers makes it challenging for conventional analysis methods to interpret flow cytometry data. Researchers need to be able to distinguish interesting-looking cell populations manually in multi-dimensional data collected from millions of cells. Thus, it is essential to find a robust approach for analysing f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35299;&#20915;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26102;&#31354;&#32858;&#21512;&#35299;&#20915;&#20102;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#22823;&#12289;&#33410;&#28857;&#29305;&#24449;&#24322;&#26500;&#31561;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#26377;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.08996</link><description>&lt;p&gt;
&#23398;&#20064;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#26102;&#31354;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion Problems. (arXiv:2303.08996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35299;&#20915;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26102;&#31354;&#32858;&#21512;&#35299;&#20915;&#20102;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#22823;&#12289;&#33410;&#28857;&#29305;&#24449;&#24322;&#26500;&#31561;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#26377;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#25237;&#36164;&#35268;&#21010;&#20915;&#31574;&#23545;&#20110;&#30830;&#20445;&#32593;&#32476;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#22312;&#25193;&#23637;&#26399;&#20869;&#28385;&#36275;&#24615;&#33021;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#35745;&#31639;&#36825;&#20123;&#20915;&#31574;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#22312;&#21306;&#22495;&#35268;&#27169;&#30340;&#33021;&#28304;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#22823;&#12289;&#33410;&#28857;&#29305;&#24449;&#24322;&#26500;&#12289;&#25805;&#20316;&#21608;&#26399;&#20247;&#22810;&#31561;&#21407;&#22240;&#65292;&#36825;&#20123;&#38382;&#39064;&#24448;&#24448;&#38590;&#20197;&#35299;&#20915;&#12290;&#20026;&#20102;&#20445;&#25345;&#21487;&#34892;&#24615;&#65292;&#20256;&#32479;&#26041;&#27861;&#20250;&#32858;&#21512;&#32593;&#32476;&#33410;&#28857;&#21644;/&#25110;&#36873;&#25321;&#19968;&#32452;&#20195;&#34920;&#24615;&#26102;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31616;&#21270;&#26410;&#33021;&#25429;&#25417;&#21040;&#20379;&#38656;&#21464;&#21270;&#23545; CEP &#25104;&#26412;&#21644;&#32422;&#26463;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#21040;&#24322;&#26500;&#33410;&#28857;&#30340;&#27867;&#22411; CEP &#30340;&#26102;&#31354;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21033;&#29992;&#22270;&#27744;&#21270;&#26469;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#33410;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#19968;&#20010;&#22810;&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21253;&#25324;&#25104;&#26412;&#21644;&#24615;&#33021;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#30005;&#32593;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#26377;&#22522;&#20934;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#28304;&#22522;&#30784;&#35774;&#26045;&#30340;&#23481;&#37327;&#25193;&#23637;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective investment planning decisions are crucial to ensure cyber-physical infrastructures satisfy performance requirements over an extended time horizon. Computing these decisions often requires solving Capacity Expansion Problems (CEPs). In the context of regional-scale energy systems, these problems are prohibitively expensive to solve due to large network sizes, heterogeneous node characteristics, and a large number of operational periods. To maintain tractability, traditional approaches aggregate network nodes and/or select a set of representative time periods. Often, these reductions do not capture supply-demand variations that crucially impact CEP costs and constraints, leading to suboptimal decisions. Here, we propose a novel graph convolutional autoencoder approach for spatio-temporal aggregation of a generic CEP with heterogeneous nodes (CEPHN). Our architecture leverages graph pooling to identify nodes with similar characteristics and minimizes a multi-objective loss funct
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#27169;&#25311;&#65292;&#21487;&#20197;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;10&#33267;1000&#20493;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#20063;&#36275;&#22815;&#28385;&#36275;&#38656;&#27714;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35268;&#33539;&#21270;NN&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.08994</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#22495;&#27169;&#25311;: &#31934;&#24230;&#65292;&#35745;&#31639;&#25104;&#26412;&#21644;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility. (arXiv:2303.08994v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08994
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#27169;&#25311;&#65292;&#21487;&#20197;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;10&#33267;1000&#20493;&#65292;&#21516;&#26102;&#20934;&#30830;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#20063;&#36275;&#22815;&#28385;&#36275;&#38656;&#27714;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35268;&#33539;&#21270;NN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#27169;&#25311;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#32771;&#34385;&#21040;&#21457;&#30005;&#21644;&#38656;&#27714;&#27169;&#24335;&#19981;&#30830;&#23450;&#24615;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#19981;&#26029;&#35780;&#20272;&#25968;&#21315;&#20010;&#22330;&#26223;&#20197;&#30830;&#20445;&#30005;&#21147;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#21152;&#36895;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;&#36127;&#36733;&#25200;&#21160;&#30340;&#21160;&#24577;&#21709;&#24212;&#12290;&#36890;&#36807;&#23558; PINNs &#39044;&#27979;&#19982;&#20256;&#32479;&#27714;&#35299;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616; PINNs &#21487;&#20197;&#27604;&#20256;&#32479;&#27714;&#35299;&#22120;&#24555; 10 &#21040; 1000 &#20493;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#22823;&#26102;&#38388;&#27493;&#38271;&#20063;&#36275;&#22815;&#20934;&#30830;&#21644;&#25968;&#20540;&#31283;&#23450;&#12290;&#20026;&#20102;&#20419;&#36827;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#22522;&#20110;&#26799;&#24230;&#30340;&#39033;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#35757;&#32451;&#30340;&#26032;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of power system dynamics poses a computationally expensive task. Considering the growing uncertainty of generation and demand patterns, thousands of scenarios need to be continuously assessed to ensure the safety of power systems. Physics-Informed Neural Networks (PINNs) have recently emerged as a promising solution for drastically accelerating computations of non-linear dynamical systems. This work investigates the applicability of these methods for power system dynamics, focusing on the dynamic response to load disturbances. Comparing the prediction of PINNs to the solution of conventional solvers, we find that PINNs can be 10 to 1000 times faster than conventional solvers. At the same time, we find them to be sufficiently accurate and numerically stable even for large time steps. To facilitate a deeper understanding, this paper also presents a new regularisation of Neural Network (NN) training by introducing a gradient-based term in the loss function. The resulting NN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#25216;&#26415;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#26435;&#37325;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#25216;&#26415;&#21435;&#38500;&#19968;&#20123;&#29305;&#23450;&#22855;&#24322;&#20540;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#25311;&#21512;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08986</link><description>&lt;p&gt;
RMT-SVD&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#26435;&#37325;&#21098;&#26525;&#65306;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Weight Pruning with RMT-SVD: Increasing Accuracy and Reducing Overfitting. (arXiv:2303.08986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#25216;&#26415;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#26435;&#37325;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#25216;&#26415;&#21435;&#38500;&#19968;&#20123;&#29305;&#23450;&#22855;&#24322;&#20540;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#25311;&#21512;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;(RMT)&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;RMT&#25216;&#26415;&#26469;&#30830;&#23450;&#22312;&#35757;&#32451;DNN&#36807;&#31243;&#20013;&#24212;&#35813;&#21435;&#38500;&#21738;&#20123;&#21644;&#22810;&#23569;&#22855;&#24322;&#20540;&#65292;&#20197;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;(SVD)&#20943;&#23569;&#36807;&#25311;&#21512;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#21442;&#25968;&#25968;&#37327;&#32780;&#20445;&#25345;&#25110;&#25552;&#39640;DNN&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present some applications of random matrix theory for the training of deep neural networks. Recently, random matrix theory (RMT) has been applied to the overfitting problem in deep learning. Specifically, it has been shown that the spectrum of the weight layers of a deep neural network (DNN) can be studied and understood using techniques from RMT. In this work, these RMT techniques will be used to determine which and how many singular values should be removed from the weight layers of a DNN during training, via singular value decomposition (SVD), so as to reduce overfitting and increase accuracy. We show the results on a simple DNN model trained on MNIST. In general, these techniques may be applied to any fully connected layer of a pretrained DNN to reduce the number of parameters in the layer while preserving and sometimes increasing the accuracy of the DNN.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Logistic LASSO&#22238;&#24402;&#39044;&#27979;&#31890;&#23376;&#21152;&#36895;&#22120;&#20013;&#26029;&#30340;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#12290;&#36890;&#36807;&#36830;&#32493;&#30340;&#35780;&#20272;&#25351;&#26631;&#27979;&#37327;&#20445;&#23384;&#30340;&#26463;&#27969;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.08984</link><description>&lt;p&gt;
&#21033;&#29992;Logistic LASSO&#22238;&#24402;&#39044;&#27979;&#31890;&#23376;&#21152;&#36895;&#22120;&#20013;&#26029;
&lt;/p&gt;
&lt;p&gt;
Forecasting Particle Accelerator Interruptions Using Logistic LASSO Regression. (arXiv:2303.08984v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08984
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Logistic LASSO&#22238;&#24402;&#39044;&#27979;&#31890;&#23376;&#21152;&#36895;&#22120;&#20013;&#26029;&#30340;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#12290;&#36890;&#36807;&#36830;&#32493;&#30340;&#35780;&#20272;&#25351;&#26631;&#27979;&#37327;&#20445;&#23384;&#30340;&#26463;&#27969;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#39044;&#35265;&#30340;&#31890;&#23376;&#21152;&#36895;&#22120;&#20013;&#26029;&#65292;&#20063;&#31216;&#20026;&#32852;&#38145;&#65292;&#23613;&#31649;&#26159;&#24517;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#65292;&#20294;&#20250;&#23548;&#33268;&#31361;&#28982;&#30340;&#25805;&#20316;&#26356;&#25913;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#30340;&#26463;&#27969;&#26102;&#38388;&#20007;&#22833;&#65292;&#29978;&#33267;&#21487;&#33021;&#36896;&#25104;&#35774;&#22791;&#25439;&#22351;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#39044;&#27979;Paul Scherrer Institut&#39640;&#24378;&#24230;&#36136;&#23376;&#21152;&#36895;&#22120;&#22797;&#21512;&#20307;&#20013;&#36825;&#31181;&#20013;&#26029;&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#21046;&#23450;&#20026;&#36923;&#36753;&#22238;&#24402;&#26368;&#23567;&#32477;&#23545;&#20540;&#25910;&#32553;&#21644;&#36873;&#25321;&#31639;&#23376;&#24809;&#32602;&#65292;&#22522;&#20110;&#19968;&#31181;&#32479;&#35745;&#21452;&#26679;&#26412;&#27979;&#35797;&#26469;&#21306;&#20998;&#21152;&#36895;&#22120;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#21644;&#31283;&#23450;&#29366;&#24577;&#12290;&#25509;&#25910;&#32852;&#38145;&#35686;&#25253;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20801;&#35768;&#37319;&#21462;&#23545;&#31574;&#24182;&#20943;&#23569;&#26463;&#27969;&#26102;&#38388;&#30340;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36830;&#32493;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#20219;&#20309;&#26102;&#38388;&#27573;&#20869;&#33410;&#30465;&#30340;&#26463;&#27969;&#26102;&#38388;&#65292;&#20551;&#35774;&#32852;&#38145;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#26463;&#27969;&#30005;&#27969;&#26469;&#35268;&#36991;&#12290;&#26368;&#20339;&#34920;&#29616;&#30340;&#32852;&#38145;-&#31283;&#23450;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unforeseen particle accelerator interruptions, also known as interlocks, lead to abrupt operational changes despite being necessary safety measures. These may result in substantial loss of beam time and perhaps even equipment damage. We propose a simple yet powerful binary classification model aiming to forecast such interruptions, in the case of the High Intensity Proton Accelerator complex at the Paul Scherrer Institut. The model is formulated as logistic regression penalized by least absolute shrinkage and selection operator, based on a statistical two sample test to distinguish between unstable and stable states of the accelerator.  The primary objective for receiving alarms prior to interlocks is to allow for countermeasures and reduce beam time loss. Hence, a continuous evaluation metric is developed to measure the saved beam time in any period, given the assumption that interlocks could be circumvented by reducing the beam current. The best-performing interlock-to-stable classif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08983</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22686;&#24378;&#65306;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08983
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#29992;&#25143;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#31574;&#30053;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;CNN&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#24102;&#26377;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33976;&#39311;&#30740;&#31350;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;ImageDataNet+&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#20197;&#21450;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;CIFAR-100+&#65292;Flowers-102+&#21644;Food-101+&#12290;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12289;&#26356;&#26377;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#20855;&#26377;&#24456;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;&#22312;ImageDataNet+&#19978;&#27979;&#37327;&#30340;Expected Calibration Error&#65288;ECE&#65289;&#20063;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20027;&#21160;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;ASSL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#21644;&#19978;&#32622;&#20449;&#30028;&#31561;&#25216;&#26415;&#26469;&#35299;&#20915;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30830;&#23450;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.08978</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#27599;&#20010;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#21160;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Semi-Supervised Learning by Exploring Per-Sample Uncertainty and Consistency. (arXiv:2303.08978v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08978
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20027;&#21160;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;ASSL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#21644;&#19978;&#32622;&#20449;&#30028;&#31561;&#25216;&#26415;&#26469;&#35299;&#20915;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30830;&#23450;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#20004;&#31181;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#25104;&#26412;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#36890;&#36807;&#26356;&#20302;&#30340;&#25104;&#26412;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20027;&#21160;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;ASSL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20027;&#21160;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#20027;&#21160;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;ASSL&#21644;&#20027;&#21160;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30001;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;ASSL&#27604;&#20027;&#21160;&#23398;&#20064;&#28041;&#21450;&#26356;&#22810;&#30340;&#21160;&#24577;&#27169;&#22411;&#26356;&#26032;&#65292;&#36825;&#23548;&#33268;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#39044;&#27979;&#27010;&#29575;&#23384;&#22312;&#26102;&#38388;&#19981;&#31283;&#23450;&#24615;&#12290;&#36825;&#20351;&#24471;&#22312;ASSL&#20013;&#30830;&#23450;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#21644;&#19978;&#32622;&#20449;&#30028;&#65288;UCB&#65289;&#31561;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24369;&#21644;&#24378;&#25968;&#25454;&#25193;&#20805;&#26469;&#20998;&#26512;&#26631;&#31614;&#22122;&#22768;&#23545;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Learning (AL) and Semi-supervised Learning are two techniques that have been studied to reduce the high cost of deep learning by using a small amount of labeled data and a large amount of unlabeled data. To improve the accuracy of models at a lower cost, we propose a method called Active Semi-supervised Learning (ASSL), which combines AL and SSL. To maximize the synergy between AL and SSL, we focused on the differences between ASSL and AL. ASSL involves more dynamic model updates than AL due to the use of unlabeled data in the training process, resulting in the temporal instability of the predicted probabilities of the unlabeled data. This makes it difficult to determine the true uncertainty of the unlabeled data in ASSL. To address this, we adopted techniques such as exponential moving average (EMA) and upper confidence bound (UCB) used in reinforcement learning. Additionally, we analyzed the effect of label noise on unsupervised learning by using weak and strong augmentation p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Gated Compression&#23618;&#65292;&#21487;&#23558;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36716;&#21270;&#20026;Gated Neural Networks&#65292;&#21487;&#22823;&#24133;&#24230;&#38477;&#20302;&#21151;&#32791;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#21033;&#29992;&#24322;&#26500;&#35745;&#31639;&#26680;&#24515;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21387;&#32553;&#20102;&#27491;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08970</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#30340;&#25345;&#32493;&#27169;&#22411;&#30340;&#38376;&#25511;&#21387;&#32553;&#23618;
&lt;/p&gt;
&lt;p&gt;
Gated Compression Layers for Efficient Always-On Models. (arXiv:2303.08970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Gated Compression&#23618;&#65292;&#21487;&#23558;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36716;&#21270;&#20026;Gated Neural Networks&#65292;&#21487;&#22823;&#24133;&#24230;&#38477;&#20302;&#21151;&#32791;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#21033;&#29992;&#24322;&#26500;&#35745;&#31639;&#26680;&#24515;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21387;&#32553;&#20102;&#27491;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21644;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#22312;&#29306;&#29298;&#31934;&#24230;&#21644;&#22823;&#24133;&#24230;&#21387;&#32553;&#27169;&#22411;&#20197;&#36816;&#34892;&#22312;&#19987;&#29992;&#20302;&#21151;&#32791;&#26680;&#24515;&#12289;&#25110;&#22312;&#26356;&#24378;&#22823;&#30340;&#35745;&#31639;&#26680;&#24515;&#65288;&#22914;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#25110;&#20027;&#24212;&#29992;&#31243;&#24207;&#22788;&#29702;&#22120;&#65289;&#19978;&#36816;&#34892;&#36739;&#22823;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38376;&#25511;&#21387;&#32553;&#23618;&#65292;&#21487;&#20197;&#23558;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36716;&#21270;&#20026;&#38376;&#25511;&#31070;&#32463;&#32593;&#32476;&#12290;&#38376;&#25511;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22810;&#20010;&#29992;&#20110;&#35774;&#22791;&#19978;&#22330;&#26223;&#30340;&#20248;&#28857;&#65292;&#21487;&#22823;&#22823;&#38477;&#20302;&#21151;&#32791;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#21033;&#29992;&#24322;&#26500;&#35745;&#31639;&#26680;&#24515;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#20844;&#20849;&#22270;&#20687;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#32467;&#26524;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#38376;&#25511;&#21387;&#32553;&#23618;&#21487;&#20197;&#26377;&#25928;&#22320;&#38459;&#27490;&#39640;&#36798;96&#65285;&#30340;&#36127;&#26679;&#26412;&#65292;&#21387;&#32553;97&#65285;&#30340;&#27491;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile and embedded machine learning developers frequently have to compromise between two inferior on-device deployment strategies: sacrifice accuracy and aggressively shrink their models to run on dedicated low-power cores; or sacrifice battery by running larger models on more powerful compute cores such as neural processing units or the main application processor. In this paper, we propose a novel Gated Compression layer that can be applied to transform existing neural network architectures into Gated Neural Networks. Gated Neural Networks have multiple properties that excel for on-device use cases that help significantly reduce power, boost accuracy, and take advantage of heterogeneous compute cores. We provide results across five public image and audio datasets that demonstrate the proposed Gated Compression layer effectively stops up to 96% of negative samples, compresses 97% of positive samples, while maintaining or improving model accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;CS-TGN&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#32593;&#32476;&#20013;&#28789;&#27963;&#30340;&#31038;&#32676;&#27169;&#24335;&#21644;&#26102;&#24577;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.08964</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31038;&#32676;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CS-TGN: Community Search via Temporal Graph Neural Networks. (arXiv:2303.08964v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;CS-TGN&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#32593;&#32476;&#20013;&#28789;&#27963;&#30340;&#31038;&#32676;&#27169;&#24335;&#21644;&#26102;&#24577;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19990;&#30028;&#21508;&#31181;&#22797;&#26434;&#32593;&#32476;&#65288;&#22914;&#19975;&#32500;&#32593;&#12289;&#31038;&#20132;&#32593;&#32476;&#12289;&#33041;&#32593;&#32476;&#65289;&#20013;&#65292;&#23547;&#25214;&#23616;&#37096;&#31038;&#32676;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21487;&#20197;&#25903;&#25345;&#20010;&#24615;&#21270;&#30340;&#31038;&#32676;&#21457;&#29616;&#21644;&#39640;&#32423;&#25968;&#25454;&#20998;&#26512;&#65307;&#32593;&#32476;&#30340;&#26102;&#31354;&#28436;&#21270;&#20063;&#20652;&#29983;&#20102;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#20197;&#22312;&#26102;&#38388;&#32593;&#32476;&#20013;&#35782;&#21035;&#23616;&#37096;&#31038;&#32676;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#30340;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;CS-TGN&#65289;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#32593;&#32476;&#20013;&#28789;&#27963;&#30340;&#31038;&#32676;&#27169;&#24335;&#21644;&#26102;&#24577;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26597;&#35810;&#33410;&#28857;&#39537;&#21160;&#31038;&#32676;&#25628;&#32034;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#31038;&#32676;&#30340;&#26102;&#24577;&#21160;&#24577;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#65292;CS-TGN&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching for local communities is an important research challenge that allows for personalized community discovery and supports advanced data analysis in various complex networks, such as the World Wide Web, social networks, and brain networks. The evolution of these networks over time has motivated several recent studies to identify local communities in temporal networks. Given any query nodes, Community Search aims to find a densely connected subgraph containing query nodes. However, existing community search approaches in temporal networks have two main limitations: (1) they adopt pre-defined subgraph patterns to model communities, which cannot find communities that do not conform to these patterns in real-world networks, and (2) they only use the aggregation of disjoint structural information to measure quality, missing the dynamic of connections and temporal properties. In this paper, we propose a query-driven Temporal Graph Convolutional Network (CS-TGN) that can capture flexibl
&lt;/p&gt;</description></item><item><title>NESS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36716;&#23548;&#35774;&#32622;&#19979;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAE&#65289;&#20174;&#38745;&#24577;&#23376;&#22270;&#65288;NESS&#65289;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08958</link><description>&lt;p&gt;
NESS&#65306;&#20174;&#38745;&#24577;&#23376;&#22270;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NESS: Learning Node Embeddings from Static SubGraphs. (arXiv:2303.08958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08958
&lt;/p&gt;
&lt;p&gt;
NESS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36716;&#23548;&#35774;&#32622;&#19979;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAE&#65289;&#20174;&#38745;&#24577;&#23376;&#22270;&#65288;NESS&#65289;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#38142;&#25509;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#36716;&#23548;&#35774;&#32622;&#19979;&#20351;&#29992;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAE&#65289;&#20174;&#38745;&#24577;&#23376;&#22270;&#65288;NESS&#65289;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#20351;&#29992;&#25972;&#20010;&#22270;&#25110;&#38543;&#26426;&#23376;&#22270;&#30340;&#24403;&#21069;&#33258;&#32534;&#30721;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#38745;&#24577;&#23376;&#22270;&#21152;&#19978;GAE&#25913;&#21892;&#20102;&#33410;&#28857;&#34920;&#31034;&#65292;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;NESS&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#20351;&#29992;&#38543;&#26426;&#36793;&#32536;&#25286;&#20998;&#65288;RES&#65289;&#23558;&#35757;&#32451;&#22270;&#21010;&#20998;&#20026;&#23376;&#22270;&#65292;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#65292;2&#65289;&#32858;&#21512;&#20174;&#27599;&#20010;&#23376;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#38388;&#33719;&#24471;&#22270;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NESS&#25913;&#36827;&#20102;&#24191;&#27867;&#30340;&#22270;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#26368;&#26032;&#32467;&#26524;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework for learning Node Embeddings from Static Subgraphs (NESS) using a graph autoencoder (GAE) in a transductive setting. Moreover, we propose a novel approach for contrastive learning in the same setting. We demonstrate that using static subgraphs during training with a GAE improves node representation for link prediction tasks compared to current autoencoding methods using the entire graph or stochastic subgraphs. NESS consists of two steps: 1) Partitioning the training graph into subgraphs using random edge split (RES) during data pre-processing, and 2) Aggregating the node representations learned from each subgraph to obtain a joint representation of the graph at test time. Our experiments show that NESS improves the performance of a wide range of graph encoders and achieves state-of-the-art (SOTA) results for link prediction on multiple benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#21046;&#29305;&#24449;&#24037;&#31243;&#21644;&#24207;&#21015;&#23398;&#20064;&#22120;&#65292;&#25552;&#20986;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#39044;&#27979;&#30828;&#30424;&#21097;&#20313;&#23551;&#21629;&#65292;&#20174;&#32780;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;</title><link>http://arxiv.org/abs/2303.08955</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;&#30828;&#30424;&#23551;&#21629;&#22823;&#35268;&#27169;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large-scale End-of-Life Prediction of Hard Disks in Distributed Datacenters. (arXiv:2303.08955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#21046;&#29305;&#24449;&#24037;&#31243;&#21644;&#24207;&#21015;&#23398;&#20064;&#22120;&#65292;&#25552;&#20986;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#39044;&#27979;&#30828;&#30424;&#21097;&#20313;&#23551;&#21629;&#65292;&#20174;&#32780;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#27599;&#22825;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#23384;&#20648;&#22312;&#20215;&#26684;&#20415;&#23452;&#30340;&#30828;&#30424;&#20013;&#65292;&#29992;&#20110;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#33322;&#22825;&#31561;&#37325;&#35201;&#39046;&#22495;&#12290;&#30828;&#30424;&#30340;&#36807;&#26089;&#25439;&#22351;&#21644;&#25968;&#25454;&#30340;&#20002;&#22833;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#21518;&#26524;&#12290;&#20026;&#20102;&#38477;&#20302;&#25925;&#38556;&#30340;&#39118;&#38505;&#65292;&#20113;&#23384;&#20648;&#25552;&#20379;&#21830;&#25191;&#34892;&#22522;&#20110;&#26465;&#20214;&#30340;&#30417;&#25511;&#24182;&#22312;&#25925;&#38556;&#20043;&#21069;&#26356;&#25442;&#30828;&#30424;&#12290;&#36890;&#36807;&#20272;&#35745;&#30828;&#30424;&#21097;&#20313;&#23551;&#21629;&#65292;&#21487;&#20197;&#39044;&#27979;&#29305;&#23450;&#35774;&#22791;&#30340;&#25925;&#38556;&#26102;&#38388;&#24182;&#22312;&#21512;&#36866;&#30340;&#26102;&#38388;&#20869;&#26367;&#25442;&#30828;&#30424;&#65292;&#30830;&#20445;&#26368;&#22823;&#21033;&#29992;&#29575;&#21516;&#26102;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12290;&#26412;&#25991;&#20351;&#29992;&#23450;&#21046;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#19968;&#22871;&#24207;&#21015;&#23398;&#20064;&#22120;&#23545;&#26497;&#24230;&#20559;&#26012;&#30340;&#20581;&#24247;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#27979;&#20998;&#26512;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;LSTM&#39044;&#27979;&#21097;&#20313;&#23551;&#21629;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LSTM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On a daily basis, data centers process huge volumes of data backed by the proliferation of inexpensive hard disks. Data stored in these disks serve a range of critical functional needs from financial, and healthcare to aerospace. As such, premature disk failure and consequent loss of data can be catastrophic. To mitigate the risk of failures, cloud storage providers perform condition-based monitoring and replace hard disks before they fail. By estimating the remaining useful life of hard disk drives, one can predict the time-to-failure of a particular device and replace it at the right time, ensuring maximum utilization whilst reducing operational costs. In this work, large-scale predictive analyses are performed using severely skewed health statistics data by incorporating customized feature engineering and a suite of sequence learners. Past work suggests using LSTMs as an excellent approach to predicting remaining useful life. To this end, we present an encoder-decoder LSTM model whe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20197;&#23454;&#26102;&#12289;&#40065;&#26834;&#21644;&#20934;&#30830;&#30340;&#26041;&#24335;&#22788;&#29702;&#22825;&#25991;&#20107;&#20214;&#30340;&#24555;&#36895;&#39640;&#25928;&#20998;&#31867;&#31639;&#27861;&#65292;&#19988;&#36890;&#36807;&#28145;&#24230;&#21387;&#32553;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#36866;&#24212;&#26085;&#30410;&#22686;&#38271;&#30340;&#23454;&#26102;&#21450;&#26102;&#22495;&#22825;&#25991;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.08951</link><description>&lt;p&gt;
&#24494;&#22411;&#26102;&#38388;&#24207;&#21015;Transformer&#65306;&#28145;&#24230;&#27169;&#22411;&#21387;&#32553;&#23454;&#29616;&#20302;&#24310;&#36831;&#39640;&#21534;&#21520;&#37327;&#22825;&#25991;&#30636;&#21464;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The Tiny Time-series Transformer: Low-latency High-throughput Classification of Astronomical Transients using Deep Model Compression. (arXiv:2303.08951v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20197;&#23454;&#26102;&#12289;&#40065;&#26834;&#21644;&#20934;&#30830;&#30340;&#26041;&#24335;&#22788;&#29702;&#22825;&#25991;&#20107;&#20214;&#30340;&#24555;&#36895;&#39640;&#25928;&#20998;&#31867;&#31639;&#27861;&#65292;&#19988;&#36890;&#36807;&#28145;&#24230;&#21387;&#32553;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#36866;&#24212;&#26085;&#30410;&#22686;&#38271;&#30340;&#23454;&#26102;&#21450;&#26102;&#22495;&#22825;&#25991;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22825;&#25991;&#23398;&#22823;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#29616;&#20195;&#31185;&#23398;&#22788;&#29702;&#27969;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;18&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#23454;&#29616;&#39640;&#25928;&#30340;&#23454;&#26102;&#20998;&#31867;&#31639;&#27861;&#65292;&#20197;&#36866;&#24212;&#26085;&#30410;&#22686;&#21152;&#30340;&#23454;&#26102;&#21450;&#26102;&#22495;&#22825;&#25991;&#23398;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new golden age in astronomy is upon us, dominated by data. Large astronomical surveys are broadcasting unprecedented rates of information, demanding machine learning as a critical component in modern scientific pipelines to handle the deluge of data. The upcoming Legacy Survey of Space and Time (LSST) of the Vera C. Rubin Observatory will raise the big-data bar for time-domain astronomy, with an expected 10 million alerts per-night, and generating many petabytes of data over the lifetime of the survey. Fast and efficient classification algorithms that can operate in real-time, yet robustly and accurately, are needed for time-critical events where additional resources can be sought for follow-up analyses. In order to handle such data, state-of-the-art deep learning architectures coupled with tools that leverage modern hardware accelerators are essential. We showcase how the use of modern deep compression methods can achieve a $18\times$ reduction in model size, whilst preserving class
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ERM&#31639;&#27861;&#23398;&#20064;&#21487;&#35777;&#26126;&#20855;&#26377;&#22810;&#31181;&#33945;&#29256;&#19979;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#22810;&#40065;&#26834;&#24615;&#23545;&#36148;&#29255;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.08944</link><description>&lt;p&gt;
&#36890;&#36807;ERM&#23454;&#29616;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#21487;&#39564;&#35777;&#65288;&#22810;&#65289;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifiable (Multi)Robustness Against Patch Attacks Using ERM. (arXiv:2303.08944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ERM&#31639;&#27861;&#23398;&#20064;&#21487;&#35777;&#26126;&#20855;&#26377;&#22810;&#31181;&#33945;&#29256;&#19979;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#22810;&#40065;&#26834;&#24615;&#23545;&#36148;&#29255;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36148;&#29255;&#25915;&#20987;&#65292;&#21363;&#22312;&#27979;&#35797;&#26102;&#23545;&#27979;&#35797;&#22270;&#20687;&#36827;&#34892;&#36148;&#29255;&#26893;&#20837;&#65292;&#20197;&#35825;&#23548;&#26377;&#38024;&#23545;&#24615;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#20851;&#27880;&#26368;&#36817;&#38024;&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;Patch-Cleanser&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#35201;&#27714;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#8220;&#20004;&#20010;&#33945;&#29256;&#27491;&#30830;&#24615;&#8221;&#23646;&#24615;&#65292;&#24847;&#21619;&#30528;&#39044;&#27979;&#27169;&#22411;&#22312;&#20219;&#20309;&#26102;&#20505;&#29992;&#20219;&#24847;&#20004;&#20010;&#31354;&#30333;&#33945;&#29256;&#26367;&#25442;&#22270;&#20687;&#37096;&#20998;&#26102;&#37117;&#24212;&#27491;&#30830;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ERM&#65288;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65289;&#31639;&#27861;&#21487;&#20197;&#35777;&#26126;&#22810;&#31181;&#33945;&#29256;&#19979;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#22810;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#20984;&#26494;&#24347;&#21644;&#40065;&#26834;&#20248;&#21270;&#19982;&#22522;&#20110;&#36793;&#30028;&#30340;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#28385;&#36275;&#19968;&#23450;&#22823;&#23567;&#21644;&#20301;&#32622;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#22810;&#40065;&#26834;&#24615;&#23545;&#36148;&#29255;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#36148;&#29255;&#25915;&#20987;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider patch attacks, where at test-time an adversary manipulates a test image with a patch in order to induce a targeted misclassification. We consider a recent defense to patch attacks, Patch-Cleanser (Xiang et al. [2022]). The Patch-Cleanser algorithm requires a prediction model to have a ``two-mask correctness'' property, meaning that the prediction model should correctly classify any image when any two blank masks replace portions of the image. Xiang et al. learn a prediction model to be robust to two-mask operations by augmenting the training set with pairs of masks at random locations of training images and performing empirical risk minimization (ERM) on the augmented dataset.  However, in the non-realizable setting when no predictor is perfectly correct on all two-mask operations on all images, we exhibit an example where ERM fails. To overcome this challenge, we propose a different algorithm that provably learns a predictor robust to all two-mask operations using an ERM orac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25913;&#21892;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#30340;&#35745;&#21010;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21644;&#26356;&#26032;&#20803;&#25968;&#25454;&#65292;&#20135;&#29983;&#26356;&#28789;&#27963;&#30340;&#35789;&#27719;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#20132;&#25442;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#25968;&#25454;&#23545;&#31038;&#21306;&#25152;&#26377;&#25104;&#21592;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.08932</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21152;&#24378;&#25968;&#25454;&#31354;&#38388;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65306;&#19968;&#31181;&#21069;&#30651;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Enhancing Data Space Semantic Interoperability through Machine Learning: a Visionary Perspective. (arXiv:2303.08932v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25913;&#21892;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#30340;&#35745;&#21010;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21644;&#26356;&#26032;&#20803;&#25968;&#25454;&#65292;&#20135;&#29983;&#26356;&#28789;&#27963;&#30340;&#35789;&#27719;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#20132;&#25442;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#25968;&#25454;&#23545;&#31038;&#21306;&#25152;&#26377;&#25104;&#21592;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36825;&#31687;&#35770;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#21892;&#25968;&#25454;&#31354;&#38388;&#20013;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#30340;&#35745;&#21010;&#12290;&#25968;&#25454;&#31354;&#38388;&#65292;&#21363;&#25968;&#25454;&#22312;&#33258;&#25105;&#31649;&#29702;&#30340;&#29615;&#22659;&#20013;&#22312;&#25104;&#21592;&#20043;&#38388;&#20132;&#25442;&#26085;&#30410;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#31649;&#29702;&#20803;&#25968;&#25454;&#21644;&#35789;&#27719;&#30340;&#25163;&#21160;&#23454;&#36341;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#65292;&#19988;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#25105;&#20204;&#30456;&#20449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#12290;&#36825;&#28041;&#21450;&#33258;&#21160;&#29983;&#25104;&#21644;&#26356;&#26032;&#20803;&#25968;&#25454;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#28789;&#27963;&#30340;&#35789;&#27719;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#23376;&#32676;&#20307;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#26415;&#35821;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#31354;&#38388;&#26410;&#26469;&#30340;&#24895;&#26223;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#20132;&#25442;&#30340;&#23616;&#38480;&#24615;&#65292;&#20351;&#25968;&#25454;&#23545;&#31038;&#21306;&#25152;&#26377;&#25104;&#21592;&#26356;&#20855;&#21487;&#35775;&#38382;&#24615;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our vision paper outlines a plan to improve the future of semantic interoperability in data spaces through the application of machine learning. The use of data spaces, where data is exchanged among members in a self-regulated environment, is becoming increasingly popular. However, the current manual practices of managing metadata and vocabularies in these spaces are time-consuming, prone to errors, and may not meet the needs of all stakeholders. By leveraging the power of machine learning, we believe that semantic interoperability in data spaces can be significantly improved. This involves automatically generating and updating metadata, which results in a more flexible vocabulary that can accommodate the diverse terminologies used by different sub-communities. Our vision for the future of data spaces addresses the limitations of conventional data exchange and makes data more accessible and valuable for all members of the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#35782;&#21035;&#37325;&#35201;&#27010;&#24565;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.08928</link><description>&lt;p&gt;
&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#23545;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Applying unsupervised keyphrase methods on concepts extracted from discharge sheets. (arXiv:2303.08928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#35782;&#21035;&#37325;&#35201;&#27010;&#24565;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#29992;&#21508;&#31181;&#31185;&#23398;&#27700;&#24179;&#21644;&#20889;&#20316;&#39118;&#26684;&#32534;&#20889;&#21253;&#21547;&#26377;&#20215;&#20540;&#24739;&#32773;&#20449;&#24687;&#30340;&#20020;&#24202;&#35760;&#24405;&#12290;&#23545;&#20110;&#22788;&#29702;&#24191;&#27867;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65292;&#29702;&#35299;&#20160;&#20040;&#20449;&#24687;&#26159;&#24517;&#35201;&#30340;&#21487;&#33021;&#23545;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26377;&#25152;&#24110;&#21161;&#12290;&#23454;&#20307;&#35782;&#21035;&#21644;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#20934;&#26415;&#35821;&#26159;&#20943;&#23569;&#22788;&#29702;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#27495;&#20041;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23613;&#31649;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#23454;&#20307;&#38142;&#25509;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#23548;&#33268;&#20135;&#29983;&#37325;&#22797;&#21644;&#20302;&#20215;&#20540;&#30340;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#30830;&#23450;&#27599;&#20010;&#20869;&#23481;&#35760;&#24405;&#30340;&#37096;&#20998;&#24182;&#35782;&#21035;&#20851;&#38190;&#27010;&#24565;&#20197;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#21547;&#20041;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20986;&#38498;&#21333;&#20013;&#25552;&#21462;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#26041;&#27861;&#26469;&#35782;&#21035;&#37325;&#35201;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986;&#20851;&#38190;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical notes containing valuable patient information are written by different health care providers with various scientific levels and writing styles. It might be helpful for clinicians and researchers to understand what information is essential when dealing with extensive electronic medical records. Entities recognizing and mapping them to standard terminologies is crucial in reducing ambiguity in processing clinical notes. Although named entity recognition and entity linking are critical steps in clinical natural language processing, they can also result in the production of repetitive and low-value concepts. In other hand, all parts of a clinical text do not share the same importance or content in predicting the patient's condition. As a result, it is necessary to identify the section in which each content is recorded and also to identify key concepts to extract meaning from clinical texts. In this study, these challenges have been addressed by using clinical natural language proc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LRDB&#27169;&#22411;&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#21487;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;DNA&#26679;&#26412;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35835;&#21462;&#26631;&#35782;&#65292;&#24182;&#21487;&#26681;&#25454;&#29992;&#25143;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#20462;&#25913;&#12290;</title><link>http://arxiv.org/abs/2303.08915</link><description>&lt;p&gt;
LRDB&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#27169;&#22411;&#30340;LSTM&#21407;&#22987;&#25968;&#25454;DNA&#30897;&#22522;&#35782;&#21035;&#22120;&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
LRDB: LSTM Raw data DNA Base-caller based on long-short term models in an active learning environment. (arXiv:2303.08915v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LRDB&#27169;&#22411;&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#21487;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;DNA&#26679;&#26412;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35835;&#21462;&#26631;&#35782;&#65292;&#24182;&#21487;&#26681;&#25454;&#29992;&#25143;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;DNA&#23383;&#31526;&#30340;&#31532;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#20351;&#29992;MinION&#35774;&#22791;&#30340;&#36755;&#20986;&#25968;&#25454;&#65292;&#20197;&#30005;&#27969;&#20449;&#21495;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#21508;&#31181;&#23574;&#31471;&#30340;&#30897;&#22522;&#35782;&#21035;&#22120;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#26681;&#25454;&#36755;&#20837;&#26469;&#26816;&#27979;DNA&#23383;&#31526;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20808;&#21069;&#30897;&#22522;&#35782;&#21035;&#22120;&#22312;&#26102;&#38388;&#20851;&#38190;&#24212;&#29992;&#12289;&#38544;&#31169;&#24863;&#30693;&#35774;&#35745;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#31561;&#26041;&#38754;&#30340;&#33509;&#24178;&#19981;&#36275;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LRDB&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#24320;&#28304;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#31169;&#26377;&#24320;&#21457;&#65292;&#22312;&#26412;&#25991;&#20013;&#38024;&#23545;&#30446;&#26631;&#32454;&#33740;&#26679;&#26412;&#20855;&#26377;&#26356;&#22909;&#30340;&#35835;&#21462;&#26631;&#35782;&#65288;&#22686;&#21152;&#20102;0.35%&#65289;&#12290;&#25105;&#20204;&#38480;&#21046;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#33539;&#22260;&#65292;&#24182;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;LRDB&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20027;&#21160;&#20351;&#29992;&#20855;&#26377;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#26032;&#30340;DNA&#26679;&#26412;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#20026;&#32454;&#33740;&#26679;&#26412;&#65289;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#29992;&#25143;&#32422;&#26463;&#26465;&#20214;&#65292;&#21487;&#20197;&#20462;&#25913;LRDB&#65292;&#22240;&#20026;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20351;&#29992;f&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24230;&#25439;&#22833;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The first important step in extracting DNA characters is using the output data of MinION devices in the form of electrical current signals. Various cutting-edge base callers use this data to detect the DNA characters based on the input. In this paper, we discuss several shortcomings of prior base callers in the case of time-critical applications, privacy-aware design, and the problem of catastrophic forgetting. Next, we propose the LRDB model, a lightweight open-source model for private developments with a better read-identity (0.35% increase) for the target bacterial samples in the paper. We have limited the extent of training data and benefited from the transfer learning algorithm to make the active usage of the LRDB viable in critical applications. Henceforth, less training time for adapting to new DNA samples (in our case, Bacterial samples) is needed. Furthermore, LRDB can be modified concerning the user constraints as the results show a negligible accuracy loss in case of using f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.08909</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#26465;&#20214;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning. (arXiv:2303.08909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#36890;&#24120;&#38656;&#35201;&#25214;&#21040;&#24179;&#34913;&#30456;&#20114;&#30683;&#30462;&#30340;&#30446;&#26631;&#30340;&#33391;&#22909;&#24179;&#34913;&#28857;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20204;&#20307;&#29616;&#20102;&#19981;&#21516;&#30340;&#30446;&#26631;&#26435;&#34913;&#27169;&#24335;&#65292;&#24182;&#19988;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#33719;&#24471;&#23427;&#20204;&#20855;&#26377;&#25216;&#26415;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#31574;&#30053;&#32593;&#32476;&#30340;&#35774;&#35745;&#12290;&#22312;&#22522;&#20934;&#29615;&#22659;&#20013;&#30340;&#25968;&#23383;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;MORL&#22522;&#32447;&#30456;&#27604;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision making in the real world often requires finding a good balance of conflicting objectives. In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks. In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete action spaces with no design change of the policy network. Numerical experiments in benchmark environments demonstrate the practicality and efficacy of our approach in comparison to standard MORL baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; kernel &#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#22522;&#24577;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#38656;&#35201;&#22810;&#39033;&#24335;&#36164;&#28304;&#23454;&#29616;&#65292;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08902</link><description>&lt;p&gt;
&#29992; Kernel &#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning ground states of gapped quantum Hamiltonians with Kernel Methods. (arXiv:2303.08902v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; kernel &#26041;&#27861;&#23398;&#20064;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#22522;&#24577;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#65292;&#29702;&#35770;&#19978;&#38656;&#35201;&#22810;&#39033;&#24335;&#36164;&#28304;&#23454;&#29616;&#65292;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#22522;&#24577;&#30340;&#26041;&#27861;&#38656;&#35201;&#35299;&#20915;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992; kernel &#26041;&#27861;&#26469;&#20351;&#20248;&#21270;&#21464;&#24471;&#31616;&#21333;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#26159;&#21151;&#29575;&#27861;&#30340;&#19968;&#31181;&#36817;&#20284;&#23454;&#29616;&#65292;&#20854;&#20013;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#23398;&#20064;&#21151;&#29575;&#36845;&#20195;&#30340;&#19979;&#19968;&#27493;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20551;&#35774;&#30417;&#30563;&#23398;&#20064;&#26159;&#26377;&#25928;&#30340;&#65292;&#37027;&#20040;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#36164;&#28304;&#23454;&#29616;&#23545;&#20219;&#24847;&#20855;&#26377;&#33021;&#38553;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#24615;&#36136;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#20351;&#29992; kernel ridge &#22238;&#24402;&#65292;&#36890;&#36807;&#23545;&#19968;&#32500;&#21644;&#20108;&#32500;&#30340;&#20960;&#20010;&#20856;&#22411;&#30456;&#20114;&#20316;&#29992;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#36827;&#34892;&#22522;&#24577;&#30340;&#23547;&#25214;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#20540;&#27169;&#25311;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network approaches to approximate the ground state of quantum hamiltonians require the numerical solution of a highly nonlinear optimization problem. We introduce a statistical learning approach that makes the optimization trivial by using kernel methods. Our scheme is an approximate realization of the power method, where supervised learning is used to learn the next step of the power iteration. We show that the ground state properties of arbitrary gapped quantum hamiltonians can be reached with polynomial resources under the assumption that the supervised learning is efficient. Using kernel ridge regression, we provide numerical evidence that the learning assumption is verified by applying our scheme to find the ground states of several prototypical interacting many-body quantum systems, both in one and two dimensions, showing the flexibility of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26694;&#26550;&#21644;&#8220;&#20869;&#24490;&#29615;&#8221;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#23553;&#38381;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.08893</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#23553;&#38381;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Multifidelity deep operator network approach to closure for multiscale systems. (arXiv:2303.08893v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26694;&#26550;&#21644;&#8220;&#20869;&#24490;&#29615;&#8221;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#23553;&#38381;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25237;&#24433;&#30340;&#38477;&#38454;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#23569;&#37327;&#24191;&#20041;&#65288;&#25110;&#28508;&#22312;&#65289;&#21464;&#37327;&#26469;&#34920;&#31034;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#20294;&#26159;&#30001;&#20110;&#26410;&#35299;&#26512;&#23610;&#24230;&#21644;&#24050;&#35299;&#26512;&#23610;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65288;&#31216;&#20026;&#23553;&#38381;&#38382;&#39064;&#65289;&#30340;&#19981;&#27491;&#30830;&#22788;&#29702;&#65292;&#23548;&#33268;&#38477;&#38454;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#21644;&#29978;&#33267;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#23558;&#23553;&#38381;&#38382;&#39064;&#35270;&#20026;&#22810;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;DeepONet&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#30340;&#23553;&#38381;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21644;/&#25110;&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#36824;&#37319;&#29992;&#20102;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32806;&#21512;&#25991;&#29486;&#20013;&#26368;&#36817;&#24320;&#21457;&#30340;&#8220;&#20869;&#24490;&#29615;&#8221;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#32456;&#26041;&#27861;&#22312;&#19968;&#32500;&#31896;&#24615;Burgers&#26041;&#31243;&#30340;&#28608;&#27874;&#36755;&#36816;&#21644;&#20108;&#32500;Navier-Stokes&#26041;&#31243;&#30340;&#28457;&#28065;&#21512;&#24182;&#26041;&#38754;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projection-based reduced order models (PROMs) have shown promise in representing the behavior of multiscale systems using a small set of generalized (or latent) variables. Despite their success, PROMs can be susceptible to inaccuracies, even instabilities, due to the improper accounting of the interaction between the resolved and unresolved scales of the multiscale system (known as the closure problem). In the current work, we interpret closure as a multifidelity problem and use a multifidelity deep operator network (DeepONet) framework to address it. In addition, to enhance the stability and/or accuracy of the multifidelity-based closure, we employ the recently developed "in-the-loop" training approach from the literature on coupling physics and machine learning models. The resulting approach is tested on shock advection for the one-dimensional viscous Burgers equation and vortex merging for the two-dimensional Navier-Stokes equations. The numerical experiments show significant improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;PIML&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#31995;&#32479;&#30340;&#21453;&#39304;&#32447;&#24615;&#21270;&#65292;&#36890;&#36807;&#26497;&#28857;&#25918;&#32622;&#26469;&#30830;&#20445;&#31283;&#23450;&#24615;&#65292;&#22312;&#38750;&#32447;&#24615;&#36716;&#25442;&#35268;&#24459;&#20013;&#23384;&#22312;&#38497;&#23789;&#30340;&#26799;&#24230;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.08884</link><description>&lt;p&gt;
&#29289;&#29702;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#21453;&#39304;&#32447;&#24615;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Discrete-Time Nonlinear Feedback Linearization via Physics-Informed Machine Learning. (arXiv:2303.08884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;PIML&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#31995;&#32479;&#30340;&#21453;&#39304;&#32447;&#24615;&#21270;&#65292;&#36890;&#36807;&#26497;&#28857;&#25918;&#32622;&#26469;&#30830;&#20445;&#31283;&#23450;&#24615;&#65292;&#22312;&#38750;&#32447;&#24615;&#36716;&#25442;&#35268;&#24459;&#20013;&#23384;&#22312;&#38497;&#23789;&#30340;&#26799;&#24230;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;PIML&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#31995;&#32479;&#30340;&#21453;&#39304;&#32447;&#24615;&#21270;&#12290;PIML&#20197;&#19968;&#27493;&#25214;&#21040;&#38750;&#32447;&#24615;&#36716;&#25442;&#35268;&#24459;&#65292;&#20174;&#32780;&#36890;&#36807;&#26497;&#28857;&#25918;&#32622;&#26469;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#38750;&#32447;&#24615;&#36716;&#25442;&#35268;&#24459;&#20013;&#23384;&#22312;&#38497;&#23789;&#30340;&#26799;&#24230;&#26102;&#20419;&#36827;&#25910;&#25947;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#35757;&#32451;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#20010;&#22522;&#20934;&#38750;&#32447;&#24615;&#31163;&#25955;&#26144;&#23556;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;PIML&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21453;&#39304;&#32447;&#24615;&#21270;&#36716;&#25442;&#35268;&#24459;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#25512;&#23548;&#24471;&#20986;&#65307;&#30001;&#20110;&#23384;&#22312;&#22855;&#28857;&#65292;&#22312;&#24863;&#20852;&#36259;&#30340;&#22495;&#20013;&#23384;&#22312;&#38497;&#23789;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;PIML&#26041;&#27861;&#22312;&#25968;&#20540;&#36924;&#36817;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#23454;&#29616;&#65292;&#21518;&#32773;&#28041;&#21450;&#26500;&#36896;&#24182;&#35299;&#20915;&#19968;&#22871;&#21516;&#35843;&#26041;&#31243;&#30340;&#31995;&#25968;&#30340;&#24130;&#32423;&#25968;&#23637;&#24320;&#20197;&#21450;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a physics-informed machine learning (PIML) scheme for the feedback linearization of nonlinear discrete-time dynamical systems. The PIML finds the nonlinear transformation law, thus ensuring stability via pole placement, in one step. In order to facilitate convergence in the presence of steep gradients in the nonlinear transformation law, we address a greedy-wise training procedure. We assess the performance of the proposed PIML approach via a benchmark nonlinear discrete map for which the feedback linearization transformation law can be derived analytically; the example is characterized by steep gradients, due to the presence of singularities, in the domain of interest. We show that the proposed PIML outperforms, in terms of numerical approximation accuracy, the traditional numerical implementation, which involves the construction--and the solution in terms of the coefficients of a power-series expansion--of a system of homological equations as well as the implementation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#26500;&#24314;&#21152;&#26435;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#24403;&#21069;&#21516;&#31867;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2303.08874</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Bayesian Quadrature for Neural Ensemble Search. (arXiv:2303.08874v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#26500;&#24314;&#21152;&#26435;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#24403;&#21069;&#21516;&#31867;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#26500;&#24314;&#22343;&#31561;&#21152;&#26435;&#30340;&#38598;&#25104;&#65292;&#36825;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#36739;&#24369;&#26550;&#26500;&#30340;&#22833;&#25928;&#27169;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#38598;&#25104;&#35270;&#20026;&#36817;&#20284;&#36793;&#32536;&#21270;&#26550;&#26500;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#24037;&#20855;&#26500;&#24314;&#38598;&#25104;&#26041;&#27861;&#8212;&#8212;&#36825;&#20123;&#24037;&#20855;&#38750;&#24120;&#36866;&#21512;&#25506;&#32034;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38598;&#25104;&#30001;&#20307;&#29616;&#20854;&#24615;&#33021;&#30340;&#26550;&#26500;&#21152;&#26435;&#26435;&#37325;&#32452;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#8212;&#8212;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#8212;&#8212;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#65292;&#24182;&#36890;&#36807;&#21066;&#20943;&#30740;&#31350;&#39564;&#35777;&#20854;&#21508;&#25104;&#20998;&#30340;&#29420;&#31435;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;OpenMP&#25193;&#23637;&#65292;&#20351;&#29992;&#29983;&#20135;&#32773;-&#28040;&#36153;&#32773;&#27169;&#24335;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#24555;&#30340;&#20195;&#30721;&#21464;&#20307;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#29992;&#25143;&#22312;&#24322;&#26500;&#20307;&#31995;&#32467;&#26500;&#19978;&#32534;&#31243;&#33258;&#36866;&#24212;&#24212;&#29992;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.08873</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;OpenMP&#23454;&#29616;&#36328;&#24322;&#26500;&#31995;&#32479;&#30340;&#21487;&#31227;&#26893;&#24615;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Driven Adaptive OpenMP For Portable Performance on Heterogeneous Systems. (arXiv:2303.08873v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;OpenMP&#25193;&#23637;&#65292;&#20351;&#29992;&#29983;&#20135;&#32773;-&#28040;&#36153;&#32773;&#27169;&#24335;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#24555;&#30340;&#20195;&#30721;&#21464;&#20307;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#29992;&#25143;&#22312;&#24322;&#26500;&#20307;&#31995;&#32467;&#26500;&#19978;&#32534;&#31243;&#33258;&#36866;&#24212;&#24212;&#29992;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#26550;&#26500;&#24050;&#25104;&#20026;&#26500;&#24314;&#39640;&#24615;&#33021;&#35745;&#31639;&#31995;&#32479;&#30340;&#20027;&#27969;&#35774;&#35745;&#36873;&#25321;&#12290;&#20294;&#26159;&#65292;&#24322;&#26500;&#26550;&#26500;&#23545;&#20110;&#23454;&#29616;&#21487;&#25191;&#34892;&#24615;&#30340;&#24615;&#33021;&#21487;&#31227;&#26893;&#24615;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23558;&#31243;&#24207;&#35843;&#25972;&#21040;&#26032;&#24322;&#26500;&#24179;&#21488;&#19978;&#26159;&#32791;&#26102;&#36153;&#21147;&#30340;&#65292;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#25163;&#21160;&#25506;&#32034;&#22823;&#37327;&#25191;&#34892;&#21442;&#25968;&#31354;&#38388;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;OpenMP&#25193;&#23637;&#65292;&#20197;&#23454;&#29616;&#33258;&#20027;&#30340;&#12289;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19968;&#32452;&#26032;&#39062;&#30340;&#35821;&#35328;&#32467;&#26500;&#12289;&#32534;&#35793;&#22120;&#36716;&#25442;&#21644;&#36816;&#34892;&#26102;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#20135;&#32773;&#28040;&#36153;&#32773;&#27169;&#24335;&#26469;&#28789;&#27963;&#22320;&#23450;&#20041;&#22810;&#20010;&#19981;&#21516;&#21464;&#20307;&#30340;OpenMP&#20195;&#30721;&#21306;&#22495;&#20197;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;&#36825;&#20123;&#21306;&#22495;&#22312;&#36816;&#34892;&#26102;&#36879;&#26126;&#22320;&#36827;&#34892;&#20998;&#26512;&#65292;&#33258;&#20027;&#23398;&#20064;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#24555;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#29992;&#25143;&#22312;&#24322;&#26500;&#20307;&#31995;&#32467;&#26500;&#19978;&#32534;&#31243;&#33258;&#36866;&#24212;&#24212;&#29992;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneity has become a mainstream architecture design choice for building High Performance Computing systems. However, heterogeneity poses significant challenges for achieving performance portability of execution. Adapting a program to a new heterogeneous platform is laborious and requires developers to manually explore a vast space of execution parameters. To address those challenges, this paper proposes new extensions to OpenMP for autonomous, machine learning-driven adaptation.  Our solution includes a set of novel language constructs, compiler transformations, and runtime support. We propose a producer-consumer pattern to flexibly define multiple, different variants of OpenMP code regions to enable adaptation. Those regions are transparently profiled at runtime to autonomously learn optimizing machine learning models that dynamically select the fastest variant. Our approach significantly reduces users' efforts of programming adaptive applications on heterogeneous architectures 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;EvalAttAI&#65292;&#26088;&#22312;&#21516;&#26102;&#32771;&#34385;&#24402;&#22240;&#26144;&#23556;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#31283;&#20581;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#30340;&#24615;&#33021;&#24182;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08866</link><description>&lt;p&gt;
EvalAttAI&#65306;&#19968;&#31181;&#32508;&#21512;&#35780;&#20272;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#20013;&#30340;&#24402;&#22240;&#26144;&#23556;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models. (arXiv:2303.08866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;EvalAttAI&#65292;&#26088;&#22312;&#21516;&#26102;&#32771;&#34385;&#24402;&#22240;&#26144;&#23556;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#31283;&#20581;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#30340;&#24615;&#33021;&#24182;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#25193;&#24352;&#65292;&#24050;&#32463;&#20135;&#29983;&#20102;&#35768;&#22810;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40657;&#30418;&#30340;&#26041;&#27861;&#12290;&#24402;&#22240;&#26144;&#23556;&#36890;&#24120;&#29992;&#20110;&#31361;&#20986;&#26174;&#31034;&#24433;&#21709;&#27169;&#22411;&#20570;&#20986;&#29305;&#23450;&#20915;&#31574;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#37096;&#20998;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#33258;&#28982;&#22122;&#22768;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#20063;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#37325;&#28857;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#26356;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#25506;&#32034;&#22312;&#21307;&#23398;&#25104;&#20687;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#24050;&#32463;&#38519;&#20837;&#20102;&#20725;&#23616;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#24402;&#22240;&#26144;&#23556;&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#24182;&#27809;&#26377;&#20849;&#35782;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#24182;&#30830;&#23450;&#26368;&#22909;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;&#33258;&#28982;&#21644;&#21307;&#23398;&#25104;&#20687;&#65289;&#21644;&#21508;&#31181;&#24402;&#22240;&#26041;&#27861;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20004;&#31181;&#27969;&#34892;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21024;&#38500;&#21644;&#25554;&#20837;&#31283;&#20581;&#24615;&#65292;&#19981;&#36275;&#20197;&#35780;&#20272;&#40065;&#26834;&#27169;&#22411;&#20013;&#30340;&#24402;&#22240;&#26144;&#23556;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;EvalAttAI&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#32771;&#34385;&#24402;&#22240;&#26144;&#23556;&#30340;&#31283;&#20581;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;EvalAttAI&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#24402;&#22240;&#26144;&#23556;&#30340;&#24615;&#33021;&#65292;&#24182;&#36873;&#25321;&#36866;&#21512;&#20854;&#29305;&#23450;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expansion of explainable artificial intelligence as a field of research has generated numerous methods of visualizing and understanding the black box of a machine learning model. Attribution maps are generally used to highlight the parts of the input image that influence the model to make a specific decision. On the other hand, the robustness of machine learning models to natural noise and adversarial attacks is also being actively explored. This paper focuses on evaluating methods of attribution mapping to find whether robust neural networks are more explainable. We explore this problem within the application of classification for medical imaging. Explainability research is at an impasse. There are many methods of attribution mapping, but no current consensus on how to evaluate them and determine the ones that are the best. Our experiments on multiple datasets (natural and medical imaging) and various attribution methods reveal that two popular evaluation metrics, Deletion and Ins
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#22270;&#20687;&#33267;&#22270;&#20687;&#21644;&#31867;&#21035;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#32452;&#21512;&#65292;&#29992;&#20110;&#26377;&#20803;&#25968;&#25454;&#26631;&#31614;&#30340;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#21487;&#20197;&#25552;&#21319;&#22270;&#20687;&#37325;&#24314;&#30340;&#20869;&#23481;&#65292;&#20248;&#20110;&#26410;&#24341;&#23548;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08863</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#25193;&#25955;&#65306;&#36890;&#36807;&#31867;&#21035;&#26631;&#31614;&#20174;&#20142;&#22330;&#22270;&#20687;&#20013;&#32454;&#32990;&#32472;&#30011;
&lt;/p&gt;
&lt;p&gt;
Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels. (arXiv:2303.08863v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#22270;&#20687;&#33267;&#22270;&#20687;&#21644;&#31867;&#21035;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#32452;&#21512;&#65292;&#29992;&#20110;&#26377;&#20803;&#25968;&#25454;&#26631;&#31614;&#30340;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#21487;&#20197;&#25552;&#21319;&#22270;&#20687;&#37325;&#24314;&#30340;&#20869;&#23481;&#65292;&#20248;&#20110;&#26410;&#24341;&#23548;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#65292;&#24120;&#24120;&#20986;&#29616;&#24102;&#26377;&#33258;&#30001;&#25110;&#24265;&#20215;&#20803;&#25968;&#25454;&#24418;&#24335;&#30340;&#31867;&#21035;&#26631;&#31614;&#30340;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#24341;&#23548;&#25110;&#39118;&#26684;&#36716;&#31227;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#26080;&#27861;&#36716;&#21270;&#20026;&#25552;&#20379;&#31163;&#25955;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#23558;&#22270;&#20687;&#33267;&#22270;&#20687;&#21644;&#31867;&#21035;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#32452;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#26174;&#24494;&#38236;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#21644;&#27809;&#26377;&#20803;&#25968;&#25454;&#26631;&#31614;&#12290;&#36890;&#36807;&#25506;&#32034;&#20855;&#26377;&#30456;&#20851;&#26631;&#31614;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#21035;&#24341;&#23548;&#30340;&#22270;&#20687;&#33267;&#22270;&#20687;&#25193;&#25955;&#21487;&#20197;&#25552;&#39640;&#37325;&#24314;&#22270;&#20687;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#26377;&#29992;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#26410;&#24341;&#23548;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-image reconstruction problems with free or inexpensive metadata in the form of class labels appear often in biological and medical image domains. Existing text-guided or style-transfer image-to-image approaches do not translate to datasets where additional information is provided as discrete classes. We introduce and implement a model which combines image-to-image and class-guided denoising diffusion probabilistic models. We train our model on a real-world dataset of microscopy images used for drug discovery, with and without incorporating metadata labels. By exploring the properties of image-to-image diffusion with relevant labels, we show that class-guided image-to-image diffusion can improve the meaningful content of the reconstructed images and outperform the unguided model in useful downstream tasks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21033;&#29992;&#31995;&#32479;&#32467;&#26500;&#20449;&#24687;&#20943;&#23569;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2303.08856</link><description>&lt;p&gt;
&#20851;&#20110;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#35268;&#21010;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
On the Benefits of Leveraging Structural Information in Planning Over the Learned Model. (arXiv:2303.08856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08856
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21033;&#29992;&#31995;&#32479;&#32467;&#26500;&#20449;&#24687;&#20943;&#23569;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23558;&#23398;&#20064;&#21644;&#35268;&#21010;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#26174;&#30528;&#30340;&#25104;&#26412;&#65288;&#26679;&#26412;&#22797;&#26434;&#24615;&#65289;&#65292;&#22240;&#20026;&#38656;&#35201;&#20026;&#27599;&#20010;&#29366;&#24577;-&#21160;&#20316;&#23545;&#33719;&#21462;&#36275;&#22815;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31995;&#32479;&#32467;&#26500;&#20449;&#24687;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22909;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#26159;&#19968;&#20123;&#32467;&#26500;&#21270;&#21442;&#25968;&#30340;&#24050;&#30693;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#21442;&#25968;&#30340;&#20540;&#26368;&#21021;&#26159;&#26410;&#30693;&#30340;&#12290;&#28982;&#21518;&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#20272;&#35745;&#36825;&#20123;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;Q&#20540;&#20272;&#35745;&#21644;&#26368;&#20248;Q&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#35813;&#24046;&#24322;&#26159;&#26679;&#26412;&#25968;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20123;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based Reinforcement Learning (RL) integrates learning and planning and has received increasing attention in recent years. However, learning the model can incur a significant cost (in terms of sample complexity), due to the need to obtain a sufficient number of samples for each state-action pair. In this paper, we investigate the benefits of leveraging structural information about the system in terms of reducing sample complexity. Specifically, we consider the setting where the transition probability matrix is a known function of a number of structural parameters, whose values are initially unknown. We then consider the problem of estimating those parameters based on the interactions with the environment. We characterize the difference between the Q estimates and the optimal Q value as a function of the number of samples. Our analysis shows that there can be a significant saving in sample complexity by leveraging structural information about the model. We illustrate the findings b
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;&#35762;&#36848;&#20102;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#20256;&#24863;&#25968;&#25454;&#20013;&#24322;&#24120;&#27169;&#24335;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.08823</link><description>&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Wireless Sensor Networks anomaly detection using Machine Learning: A Survey. (arXiv:2303.08823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;&#35762;&#36848;&#20102;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#35299;&#20915;&#20256;&#24863;&#25968;&#25454;&#20013;&#24322;&#24120;&#27169;&#24335;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#20844;&#27665;/&#20891;&#20107;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#65292;&#22914;&#24037;&#19994;&#36807;&#31243;&#25511;&#21046;&#12289;&#24314;&#31569;&#32467;&#26500;&#24378;&#24230;&#30417;&#27979;&#12289;&#29615;&#22659;&#30417;&#27979;&#12289;&#36793;&#22659;&#20837;&#20405;&#12289;&#29289;&#32852;&#32593;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#12290;&#28982;&#32780;&#65292;WSN&#20135;&#29983;&#30340;&#20256;&#24863;&#25968;&#25454;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35786;&#26029;&#24322;&#24120;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#35782;&#21035;&#20256;&#24863;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#27010;&#36848;&#20102;ML&#25216;&#26415;&#22312;WSN&#39046;&#22495;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;WSN&#30340;&#29305;&#24449;&#21644;WSN&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#21508;&#31181;&#24212;&#29992;&#20110;WSN&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#30417;&#30563;&#65292;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31561;ML&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;ML&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless Sensor Networks (WSNs) have become increasingly valuable in various civil/military applications like industrial process control, civil engineering applications such as buildings structural strength monitoring, environmental monitoring, border intrusion, IoT (Internet of Things), and healthcare. However, the sensed data generated by WSNs is often noisy and unreliable, making it a challenge to detect and diagnose anomalies. Machine learning (ML) techniques have been widely used to address this problem by detecting and identifying unusual patterns in the sensed data. This survey paper provides an overview of the state of the art applications of ML techniques for data anomaly detection in WSN domains. We first introduce the characteristics of WSNs and the challenges of anomaly detection in WSNs. Then, we review various ML techniques such as supervised, unsupervised, and semi-supervised learning that have been applied to WSN data anomaly detection. We also compare different ML-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.08806</link><description>&lt;p&gt;
&#29702;&#35299;&#20107;&#21518;&#35299;&#37322;&#22120;&#65306;&#20197;Anchors&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#39640;&#24230;&#35201;&#27714;&#20294;&#38590;&#20197;&#23454;&#29616;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#20010;&#20307;&#39044;&#27979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26412;&#22320;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#35299;&#37322;&#30340;&#36807;&#31243;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#21487;&#33021;&#19982;&#35201;&#35299;&#37322;&#30340;&#39044;&#27979;&#19968;&#26679;&#31070;&#31192;&#12290;&#27492;&#22806;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#32463;&#24120;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#31616;&#21333;&#27169;&#22411;&#19978;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#23545;Anchors&#65288;Ribeiro&#31561;&#20154;&#65292;2018&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65306;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#24378;&#35843;&#19968;&#23567;&#32452;&#21333;&#35789;&#20197;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;&#26041;&#27861;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#65292;&#24182;&#20030;&#20986;&#23454;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24402;&#32435;&#12289;&#28436;&#32462;&#21644;&#35825;&#23548;&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08769</link><description>&lt;p&gt;
&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;&#30340;&#26041;&#27861;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models With the Socratic Method. (arXiv:2303.08769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;&#26041;&#27861;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#65292;&#24182;&#20030;&#20986;&#23454;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24402;&#32435;&#12289;&#28436;&#32462;&#21644;&#35825;&#23548;&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;&#26041;&#27861;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;GPT-3&#65289;&#20132;&#20114;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#21487;&#20197;&#20135;&#29983;&#31934;&#30830;&#31572;&#26696;&#21644;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#25552;&#39640;&#21019;&#24847;&#20889;&#20316;&#26041;&#38754;&#20419;&#36827;&#21019;&#36896;&#21147;&#21644;&#24819;&#35937;&#21147;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#24212;&#29992;&#23450;&#20041;&#12289;elenchus&#12289;&#36777;&#35777;&#27861;&#12289;&#27597;&#20307;&#12289;&#27010;&#25324;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#31561;&#25216;&#26415;&#26469;&#32534;&#20889;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#31034;&#20363;&#65292;&#23637;&#31034;&#23427;&#20204;&#22312;&#24402;&#32435;&#12289;&#28436;&#32462;&#21644;&#35825;&#23548;&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. We examine various methods and identify those that yield precise answers and justifications while simultaneously fostering creativity and imagination to enhance creative writing. Specifically, we discuss how techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning can be applied in engineering prompt templates, and provide practical examples that demonstrate their effectiveness in performing inductive, deductive, and abductive reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#24102;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#30740;&#31350;&#20102;DEPINN&#22312;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#21644;&#25552;&#39640;&#20808;&#39564;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#27492;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.08455</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem. (arXiv:2303.08455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#24102;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#30740;&#31350;&#20102;DEPINN&#22312;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#21644;&#25552;&#39640;&#20808;&#39564;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#27492;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24037;&#31243;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#25506;&#27979;&#22120;&#33719;&#24471;&#30340;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26377;&#22122;&#22768;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#20808;&#39564;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#22122;&#22768;&#26102;&#65292;&#24050;&#32463;&#25552;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;DEPINN&#65289;&#22312;&#35745;&#31639;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#22122;&#22768;&#20808;&#39564;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#20005;&#26684;&#30340;&#25968;&#23398;&#35777;&#26126;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#26412;&#25991;&#22312;&#20004;&#20010;&#20856;&#22411;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#26816;&#39564;&#20102;DEPINN&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#30830;&#35748;&#20102;&#25913;&#36827;&#30340;DEPINN&#22312;&#26680;&#21453;&#24212;&#22534;&#29289;&#29702;&#23454;&#38469;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical engineering experiments, the data obtained through detectors are inevitably noisy. For the already proposed data-enabled physics-informed neural network (DEPINN) \citep{DEPINN}, we investigate the performance of DEPINN in calculating the neutron diffusion eigenvalue problem from several perspectives when the prior data contain different scales of noise. Further, in order to reduce the effect of noise and improve the utilization of the noisy prior data, we propose innovative interval loss functions and give some rigorous mathematical proofs. The robustness of DEPINN is examined on two typical benchmark problems through a large number of numerical results, and the effectiveness of the proposed interval loss function is demonstrated by comparison. This paper confirms the feasibility of the improved DEPINN for practical engineering applications in nuclear reactor physics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.08157</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#22270;&#36807;&#28388;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Surrogates of Fair Graph Filtering. (arXiv:2303.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08157
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36793;&#20256;&#25773;&#23558;&#20808;&#21069;&#30340;&#33410;&#28857;&#20540;&#36716;&#25442;&#20026;&#21518;&#26469;&#30340;&#20998;&#25968;&#30340;&#22270;&#28388;&#27874;&#22120;&#36890;&#24120;&#25903;&#25345;&#24433;&#21709;&#20154;&#31867;&#30340;&#22270;&#25366;&#25496;&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#33616;&#21644;&#25490;&#21517;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#28385;&#36275;&#33410;&#28857;&#32452;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#20351;&#23427;&#20204;&#20844;&#24179;&#65288;&#20363;&#22914;&#65292;&#25353;&#20854;&#20195;&#34920;&#24615;&#23558;&#20998;&#25968;&#36136;&#37327;&#22312;&#24615;&#21035;&#20043;&#38388;&#22343;&#34913;&#20998;&#37197;&#65289;&#12290;&#20026;&#20102;&#22312;&#26368;&#23567;&#31243;&#24230;&#22320;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#29992;&#20110;&#21518;&#39564;&#30446;&#26631;&#12290;&#36825;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#65292;&#31867;&#20284;&#20110;&#36807;&#28388;&#22120;&#65292;&#20294;&#20063;&#22312;&#26412;&#22320;&#20248;&#21270;&#21253;&#25324;&#20844;&#24179;&#24863;&#30693;&#22312;&#20869;&#30340;&#22823;&#31867;&#30446;&#26631;&#12290;&#22312;&#19968;&#32452;8&#20010;&#36807;&#28388;&#22120;&#21644;5&#20010;&#22270;&#24418;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#34920;&#29616;&#24471;&#19981;&#20122;&#20110;&#26367;&#20195;&#21697;&#65292;&#21516;&#26102;&#20445;&#30041;&#22522;&#20110;&#20998;&#25968;&#30340;&#31038;&#21306;&#25104;&#21592;&#25512;&#33616;&#30340;AUC&#24182;&#22312;&#20256;&#25773;&#20808;&#21069;&#33410;&#25293;&#26102;&#21019;&#24314;&#26368;&#23567;&#23454;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph filters that transform prior node values to posterior scores via edge propagation often support graph mining tasks affecting humans, such as recommendation and ranking. Thus, it is important to make them fair in terms of satisfying statistical parity constraints between groups of nodes (e.g., distribute score mass between genders proportionally to their representation). To achieve this while minimally perturbing the original posteriors, we introduce a filter-aware universal approximation framework for posterior objectives. This defines appropriate graph neural networks trained at runtime to be similar to filters but also locally optimize a large class of objectives, including fairness-aware ones. Experiments on a collection of 8 filters and 5 graphs show that our approach performs equally well or better than alternatives in meeting parity constraints while preserving the AUC of score-based community member recommendation and creating minimal utility loss in prior diffusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2303.08112</link><description>&lt;p&gt;
&#29992;&#35843;&#35856;&#36879;&#38236;&#20174;Transformer&#20013;&#33719;&#21462;&#28508;&#22312;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36845;&#20195;&#25512;&#29702;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;transformers&#27169;&#22411;&#65292;&#26088;&#22312;&#20102;&#35299;&#27169;&#22411;&#39044;&#27979;&#26159;&#22914;&#20309;&#36880;&#23618;&#36827;&#34892;&#31934;&#21270;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#22359;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#20351;&#24471;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#26159;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21069;&#32773;&#32473;&#20986;&#20102;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#20294;&#24120;&#24120;&#26131;&#30862;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20855;&#26377;&#22810;&#36798;20B&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#34920;&#26126;&#20854;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#26174;&#31034;&#65292;&#35843;&#35856;&#36879;&#38236;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/AlignmentResearch/tuned-lens &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;UNET&#27169;&#22411;&#32467;&#21512;&#20174;COVID-19&#24739;&#32773;&#20013;&#25552;&#21462;&#24863;&#26579;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#23558;&#24863;&#26579;&#30340;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;COVID-19&#37325;&#30151;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2303.07130</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;COVID-19&#37325;&#30151;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing COVID-19 Severity Analysis through Ensemble Methods. (arXiv:2303.07130v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;UNET&#27169;&#22411;&#32467;&#21512;&#20174;COVID-19&#24739;&#32773;&#20013;&#25552;&#21462;&#24863;&#26579;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#23558;&#24863;&#26579;&#30340;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;COVID-19&#37325;&#30151;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(CT)&#25552;&#20379;&#20102;&#32954;&#37096;&#30340;&#35814;&#32454;&#22270;&#20687;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#35266;&#23519;COVID-19&#25152;&#36896;&#25104;&#30340;&#25439;&#20260;&#31243;&#24230;&#12290;&#22522;&#20110;CT&#30340;&#32954;&#37096;&#21463;&#32047;&#31243;&#24230;&#35780;&#20998;(CTSS)&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;CT&#25195;&#25551;&#20013;&#35266;&#23519;&#21040;&#30340;&#32954;&#37096;&#21463;&#32047;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;UNET&#27169;&#22411;&#32467;&#21512;&#20174;COVID-19&#24739;&#32773;&#20013;&#25552;&#21462;&#24863;&#26579;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#65306;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#12289;&#26497;&#24230;&#38543;&#26426;&#21270;&#26641;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#23558;&#24863;&#26579;&#30340;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#35813;&#31995;&#32479;&#22312;AI-Enabled&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30740;&#35752;&#20250;&#21644;COVID-19&#35786;&#26029;&#22823;&#36187;(AI-MIA-COV19D)&#30340;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;64\%&#30340;&#23439;F1&#24471;&#20998;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31934;&#30830;COVID-19&#35786;&#26029;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computed Tomography (CT) scans provide a detailed image of the lungs, allowing clinicians to observe the extent of damage caused by COVID-19. The CT severity score (CTSS) based scoring method is used to identify the extent of lung involvement observed on a CT scan. This paper presents a domain knowledge-based pipeline for extracting regions of infection in COVID-19 patients using a combination of image-processing algorithms and a pre-trained UNET model. The severity of the infection is then classified into different categories using an ensemble of three machine-learning models: Extreme Gradient Boosting, Extremely Randomized Trees, and Support Vector Machine. The proposed system was evaluated on a validation dataset in the AI-Enabled Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D) and achieved a macro F1 score of 64\%. These results demonstrate the potential of combining domain knowledge with machine learning techniques for accurate COVID-19 diagnosis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>ODIN&#37319;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#29983;&#25104;&#25353;&#38656;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#32422;&#26463;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23398;&#20064;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.06832</link><description>&lt;p&gt;
ODIN&#65306;&#24212;&#23545;&#25968;&#25454;&#38145;&#23450;&#30340;&#25353;&#38656;&#25968;&#25454;&#21046;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in. (arXiv:2303.06832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06832
&lt;/p&gt;
&lt;p&gt;
ODIN&#37319;&#29992;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#29983;&#25104;&#25353;&#38656;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#32422;&#26463;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23398;&#20064;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#32422;&#26463;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#21463;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#36739;&#22823;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;ODIN&#35797;&#22270;&#36890;&#36807;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#29983;&#25104;&#25353;&#38656;&#25968;&#25454;&#38598;&#26469;&#20943;&#36731;&#25968;&#25454;&#38598;&#32422;&#26463;&#12290;ODIN&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#25552;&#31034;&#29983;&#25104;&#22120;&#12289;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21644;&#22270;&#20687;&#21518;&#22788;&#29702;&#22120;&#12290;&#20026;&#20102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#21644;&#22270;&#20687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#65289;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;Stable Diffusion&#65289;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;ODIN&#36827;&#34892;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#35780;&#20272;&#20197;&#23637;&#31034;&#20854;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#21518;&#22788;&#29702;&#23454;&#39564;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;ODIN&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23398;&#20064;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
ODIN is an innovative approach that addresses the problem of dataset constraints by integrating generative AI models. Traditional zero-shot learning methods are constrained by the training dataset. To fundamentally overcome this limitation, ODIN attempts to mitigate the dataset constraints by generating on-demand datasets based on user requirements. ODIN consists of three main modules: a prompt generator, a text-to-image generator, and an image post-processor. To generate high-quality prompts and images, we adopted a large language model (e.g., ChatGPT), and a text-to-image diffusion model (e.g., Stable Diffusion), respectively. We evaluated ODIN on various datasets in terms of model accuracy and data diversity to demonstrate its potential, and conducted post-experiments for further investigation. Overall, ODIN is a feasible approach that enables Al to learn unseen knowledge beyond the training dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.06438</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#28304;&#20998;&#31163;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65306;&#20849;&#20449;&#36947;OFDM&#20449;&#21495;&#30340;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals. (arXiv:2303.06438v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the single-channel source separation problem involving OFDM signals and evaluates the efficacy of using audio-oriented neural architectures in separating co-channel OFDM waveforms. Critical domain-informed modifications to the network parameterization are proposed based on insights from OFDM structures.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36825;&#31181;&#20449;&#21495;&#22312;&#35768;&#22810;&#29616;&#20195;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#21333;&#22768;&#36947;&#28304;&#20998;&#31163;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#21162;&#21147;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#22120;&#65288;&#20316;&#20026;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#65289;&#12290;&#36890;&#36807;&#22522;&#20110;OFDM&#28304;&#27169;&#22411;&#30340;&#21407;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#24182;&#36136;&#30097;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22522;&#20110;&#36890;&#20449;&#27874;&#24418;&#30456;&#20851;&#29305;&#24449;&#20998;&#31163;&#20449;&#21495;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#37197;&#32622;&#20013;&#65292;&#21363;&#20351;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#22522;&#20110;OFDM&#32467;&#26500;&#30340;&#27934;&#23519;&#65292;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess -- and question -- the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed modifications to the network parameterization, based on insights from OFDM structures, that can co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2303.05735</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#24418;&#30340;&#30828;&#20214;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#28210;&#26579;&#21644;&#21453;&#28210;&#26579;&#31639;&#27861;&#24050;&#34987;&#31070;&#32463;&#34920;&#31034;&#65288;NR&#65289;&#25152;&#21462;&#20195;&#12290;NR&#26368;&#36817;&#34987;&#29992;&#20110;&#23398;&#20064;&#22330;&#26223;&#30340;&#20960;&#20309;&#21644;&#26448;&#36136;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#21512;&#25104;&#30495;&#23454;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#25215;&#35834;&#29992;&#21487;&#20280;&#32553;&#30340;&#36136;&#37327;&#21644;&#21487;&#39044;&#27979;&#30340;&#24615;&#33021;&#26367;&#25442;&#20256;&#32479;&#30340;&#28210;&#26579;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#38382;&#39064;&#65306;&#31070;&#32463;&#22270;&#24418;&#65288;NG&#65289;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#20195;&#34920;&#24615;&#30340;NG&#24212;&#29992;&#31243;&#24207;&#65292;&#21457;&#29616;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#24403;&#21069;&#30340;GPU&#19978;&#20197;60FPS&#28210;&#26579;4K&#20998;&#36776;&#29575;&#65292;&#21017;&#25152;&#38656;&#24615;&#33021;&#19982;&#24403;&#21069;GPU&#30340;&#23454;&#38469;&#24615;&#33021;&#23384;&#22312;1.5&#20493;&#33267;55&#20493;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#25152;&#38656;&#24615;&#33021;&#19982;&#25152;&#38656;&#31995;&#32479;&#21151;&#29575;&#20043;&#38388;&#23384;&#22312;&#26356;&#22823;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#65292;&#23545;&#20110;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#12289;&#22810;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#21644;&#20302;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#65292;&#23427;&#20204;&#21344;&#24212;&#29992;&#31243;&#24207;&#26102;&#38388;&#30340;72&#65285;&#12289;60&#65285;&#21644;59&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#24418;&#24335;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#23450;&#29702;&#30340;&#35777;&#26126;&#65292;&#32780;&#19981;&#26159;&#19968;&#27493;&#27493;&#36827;&#34892;&#65292;&#20174;&#32780;&#25552;&#39640;&#35777;&#26126;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.04910</link><description>&lt;p&gt;
Baldur&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#35777;&#26126;&#29983;&#25104;&#21644;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Baldur: Whole-Proof Generation and Repair with Large Language Models. (arXiv:2303.04910v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#24418;&#24335;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#23450;&#29702;&#30340;&#35777;&#26126;&#65292;&#32780;&#19981;&#26159;&#19968;&#27493;&#27493;&#36827;&#34892;&#65292;&#20174;&#32780;&#25552;&#39640;&#35777;&#26126;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#24335;&#39564;&#35777;&#36719;&#20214;&#23646;&#24615;&#26159;&#19968;&#39033;&#38750;&#24120;&#21487;&#21462;&#20294;&#21364;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#24037;&#20316;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20351;&#29992;&#35777;&#26126;&#21161;&#25163;&#65288;&#22914;Coq&#21644;Isabelle/HOL&#65289;&#33258;&#21160;&#21270;&#24418;&#24335;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#19968;&#27425;&#39044;&#27979;&#19968;&#20010;&#35777;&#26126;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992;&#35813;&#27169;&#22411;&#22312;&#21487;&#33021;&#30340;&#35777;&#26126;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#24418;&#24335;&#39564;&#35777;&#65306;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#21644;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#35777;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#23450;&#29702;&#30340;&#35777;&#26126;&#65292;&#32780;&#19981;&#26159;&#19968;&#27493;&#27493;&#36827;&#34892;&#12290;&#25105;&#20204;&#23558;&#27492;&#35777;&#26126;&#29983;&#25104;&#27169;&#22411;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;&#20462;&#22797;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#20462;&#22797;&#29983;&#25104;&#30340;&#35777;&#26126;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#35777;&#26126;&#33021;&#21147;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#26412;&#25991;&#39318;&#27425;&#34920;&#26126;&#65306;&#65288;1&#65289;&#20351;&#29992;transformers&#36827;&#34892;&#20840;&#35777;&#26126;&#29983;&#25104;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#19982;&#19981;&#38656;&#35201;&#26114;&#36149;&#25628;&#32034;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#25216;&#26415;&#19968;&#26679;&#26377;&#25928;&#12290; &#65288;2&#65289;&#32473;&#25152;&#23398;&#27169;&#22411;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#20043;&#21069;&#22833;&#36133;&#30340;&#35777;&#26126;&#23581;&#35797;&#65292;&#21487;&#20197;&#25552;&#39640;&#35777;&#26126;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formally verifying software properties is a highly desirable but labor-intensive task. Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time, and using that model to search through the space of possible proofs. This paper introduces a new method to automate formal verification: We use large language models, trained on natural language text and code and fine-tuned on proofs, to generate whole proofs for theorems at once, rather than one step at a time. We combine this proof generation model with a fine-tuned repair model to repair generated proofs, further increasing proving power. As its main contributions, this paper demonstrates for the first time that: (1) Whole-proof generation using transformers is possible and is as effective as search-based techniques without requiring costly search. (2) Giving the learned model additional context, such as a prior faile
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.03770</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation. (arXiv:2303.03770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#20551;&#23450;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21516;&#26102;&#21487;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#23427;&#26159;UDA&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;SF-UDA&#35774;&#32622;&#65292;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#23545;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#20998;&#31867;&#25439;&#22833;&#22522;&#20110;&#20272;&#35745;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#37325;&#26032;&#21152;&#26435;&#65292;&#20197;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#38598;&#30456;&#37051;&#26679;&#26412;&#30340;&#30693;&#35782;&#26469;&#36880;&#27493;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#26469;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#30340;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#26679;&#26412;&#23545;&#25490;&#38500;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#21644;&#25490;&#38500;&#30001;&#20849;&#20139;&#30456;&#21516;&#29305;&#24449;&#30340;&#26679;&#26412;&#26500;&#25104;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Unsupervised Domain Adaptation (UDA) methods assume the availability of both source and target data during the adaptation. In this work, we investigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweighting strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Furthermore, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is proposed to identify and exclude negative pairs made of samples shari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01903</link><description>&lt;p&gt;
&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#36229;&#20986;&#22270;&#20687;&#33539;&#22260;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#20174;&#26174;&#24335;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#26816;&#32034;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#36825;&#32463;&#24120;&#20250;&#24341;&#20837;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;GPT-3&#65289;&#20316;&#20026;&#38544;&#21547;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#22238;&#31572;&#25152;&#38656;&#30340;&#24517;&#35201;&#30693;&#35782;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#36824;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;GPT-3&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#36755;&#20837;&#20449;&#24687;&#20173;&#28982;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prophet&#8212;&#8212;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22238;&#31572;&#21551;&#21457;&#24335;&#26041;&#24335;&#65292;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29305;&#23450;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#31572;&#26696;&#21551;&#21457;&#24335;&#65306;&#31572;&#26696;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#32570;&#34880;&#24615;&#21330;&#20013;&#30149;&#28790;&#20998;&#21106;&#30340;&#33258;&#30417;&#30563;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#21442;&#25968;&#22270;&#29983;&#25104;&#39068;&#33394;&#32534;&#30721;&#30340;&#21442;&#25968;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.01332</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23569;&#26679;&#26412;&#23398;&#20064;&#29992;&#20110;&#32570;&#34880;&#24615;&#21330;&#20013;&#30149;&#28790;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Few-Shot Learning for Ischemic Stroke Lesion Segmentation. (arXiv:2303.01332v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#32570;&#34880;&#24615;&#21330;&#20013;&#30149;&#28790;&#20998;&#21106;&#30340;&#33258;&#30417;&#30563;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#21442;&#25968;&#22270;&#29983;&#25104;&#39068;&#33394;&#32534;&#30721;&#30340;&#21442;&#25968;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#30340;&#32570;&#34880;&#24615;&#30149;&#28790;&#20998;&#21106;&#22312;&#25913;&#21892;&#35786;&#26029;&#21644;&#27835;&#30103;&#32570;&#34880;&#24615;&#21330;&#20013;&#31561;&#39640;&#27515;&#20129;&#29575;&#30142;&#30149;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#38656;&#35201;&#22312;&#35757;&#32451;&#26399;&#38388;&#22823;&#37327;&#30340;&#26631;&#27880;&#21306;&#22495;&#65292;&#36825;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#21487;&#33021;&#20250;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#23569;&#26679;&#26412;&#20998;&#21106;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#26631;&#27880;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#21442;&#25968;&#22270;&#65292;&#29983;&#25104;&#39068;&#33394;&#32534;&#30721;&#30340;&#21442;&#25968;&#22270;&#65292;&#22312;&#33258;&#30417;&#30563;&#30340;&#35757;&#32451;&#26426;&#21046;&#20013;&#36827;&#34892;&#35843;&#25972;&#65292;&#20351;&#20854;&#38024;&#23545;&#32570;&#34880;&#24615;&#21330;&#20013;&#30149;&#28790;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26426;&#21046;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#26377;&#20102;&#21333;&#20010;&#26631;&#35760;&#30340;&#24739;&#32773;&#25195;&#25551;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise ischemic lesion segmentation plays an essential role in improving diagnosis and treatment planning for ischemic stroke, one of the prevalent diseases with the highest mortality rate. While numerous deep neural network approaches have recently been proposed to tackle this problem, these methods require large amounts of annotated regions during training, which can be impractical in the medical domain where annotated data is scarce. As a remedy, we present a prototypical few-shot segmentation approach for ischemic lesion segmentation using only one annotated sample during training. The proposed approach leverages a novel self-supervised training mechanism that is tailored to the task of ischemic stroke lesion segmentation by exploiting color-coded parametric maps generated from Computed Tomography Perfusion scans. We illustrate the benefits of our proposed training mechanism, leading to considerable improvements in performance in the few-shot setting. Given a single annotated pati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00085</link><description>&lt;p&gt;
AR3n: &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AR3n&#65288;&#21457;&#38899;&#20026;Aaron&#65289;&#65292;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#65292;&#21487;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#20070;&#20889;&#24247;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#36866;&#24212;&#24615;&#36741;&#21161;&#12290;&#19982;&#20197;&#24448;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#24739;&#32773;&#29305;&#23450;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#25110;&#29289;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#26469;&#20351;AR3n&#25512;&#24191;&#21040;&#22810;&#20010;&#21463;&#35797;&#32773;&#12290;&#35813;&#31995;&#32479;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#22522;&#20110;&#34987;&#35797;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#36890;&#36807;&#19968;&#32452;&#20223;&#30495;&#23454;&#39564;&#21644;&#20154;&#20307;&#21463;&#35797;&#23454;&#39564;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19982;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#25511;&#21046;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#20004;&#31181;&#25511;&#21046;&#22120;&#30340;&#36741;&#21161;&#26426;&#21046;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#25163;&#21183;&#24182;&#21487;&#35299;&#37322;&#29983;&#25104;&#25163;&#21183;&#36716;&#24405;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#12290;&#36890;&#36807;&#26816;&#26597;&#24037;&#20855;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#20132;&#28857;&#65292;&#20351;&#29992;&#20998;&#21106;&#33945;&#29256;&#26816;&#27979;&#25163;&#26415;&#32972;&#26223;&#12290;&#26412;&#26041;&#27861;&#21487;&#20197;&#39640;&#24230;&#19968;&#33268;&#22320;&#33258;&#21160;&#26816;&#27979;&#37325;&#35201;&#30340;&#25163;&#26415;&#29366;&#24577;&#65292;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.14237</link><description>&lt;p&gt;
&#23454;&#29616;&#25163;&#26415;&#32972;&#26223;&#25512;&#29702;&#19982;&#25163;&#21183;&#36716;&#25442;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Surgical Context Inference and Translation to Gestures. (arXiv:2302.14237v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#25163;&#21183;&#24182;&#21487;&#35299;&#37322;&#29983;&#25104;&#25163;&#21183;&#36716;&#24405;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#12290;&#36890;&#36807;&#26816;&#26597;&#24037;&#20855;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#20132;&#28857;&#65292;&#20351;&#29992;&#20998;&#21106;&#33945;&#29256;&#26816;&#27979;&#25163;&#26415;&#32972;&#26223;&#12290;&#26412;&#26041;&#27861;&#21487;&#20197;&#39640;&#24230;&#19968;&#33268;&#22320;&#33258;&#21160;&#26816;&#27979;&#37325;&#35201;&#30340;&#25163;&#26415;&#29366;&#24577;&#65292;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#20013;&#25163;&#21183;&#30340;&#25163;&#21160;&#26631;&#27880;&#24037;&#20316;&#32321;&#29712;&#19988;&#23481;&#26131;&#20986;&#38169;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#22521;&#35757;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#25163;&#21183;&#24182;&#21487;&#35299;&#37322;&#29983;&#25104;&#25163;&#21183;&#36716;&#24405;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#12290;&#36890;&#36807;&#26816;&#26597;&#24037;&#20855;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#20132;&#28857;&#65292;&#20351;&#29992;&#20998;&#21106;&#33945;&#29256;&#26816;&#27979;&#25163;&#26415;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22522;&#20110;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#26426;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#38271;&#26399;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#23558;&#32972;&#26223;&#26631;&#31614;&#36716;&#25442;&#20026;&#25163;&#21183;&#36716;&#24405;&#12290;&#36890;&#36807;&#23558;&#32467;&#26524;&#19982;JIGSAWS&#25968;&#25454;&#38598;&#20013;&#30340;&#22320;&#38754;&#30495;&#23454;&#20998;&#21106;&#33945;&#29256;&#12289;&#20849;&#35782;&#32972;&#26223;&#26631;&#31614;&#21644;&#25163;&#21183;&#26631;&#31614;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#38454;&#27573;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20998;&#21106;&#27169;&#22411;&#22312;&#32541;&#21512;&#26102;&#35782;&#21035;&#38024;&#21644;&#32447;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#26032;&#27700;&#24179;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#39640;&#24230;&#19968;&#33268;&#22320;&#33258;&#21160;&#26816;&#27979;&#37325;&#35201;&#30340;&#25163;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Machine (FSM) and data-driven Long Short Term Memory (LSTM) models. We evaluate the performance of each stage of our method by comparing the results with the ground truth segmentation masks, the consensus context labels, and the gesture labels in the JIGSAWS dataset. Our results show that our segmentation models achieve state-of-the-art performance in recognizing needle and thread in Suturing and we can automatically detect important surgical states with high agreement with crowd-sourced labels (e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#20849;&#20139;DETR&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#30340;&#21069;&#26223;-&#32972;&#26223;&#33945;&#29256;&#36827;&#34892;&#26435;&#37325;/&#20540;&#23398;&#20064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;DETR&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11208</link><description>&lt;p&gt;
KS-DETR: &#30693;&#35782;&#20849;&#20139;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#29992;&#20110;&#26816;&#27979;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer. (arXiv:2302.11208v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#20849;&#20139;DETR&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#30340;&#21069;&#26223;-&#32972;&#26223;&#33945;&#29256;&#36827;&#34892;&#26435;&#37325;/&#20540;&#23398;&#20064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;DETR&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#21147;&#65288;scaled dot-product attention&#65289;&#20351;&#29992;&#26631;&#20934;&#21270;&#21518;&#30340;&#28857;&#31215;&#23545;&#26597;&#35810;&#21644;&#38190;&#36827;&#34892;softmax&#35745;&#31639;&#26469;&#35745;&#31639;&#26435;&#37325;&#65292;&#28982;&#21518;&#20056;&#20197;&#26435;&#37325;&#21644;&#20540;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#25913;&#36827;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#20197;&#25552;&#39640;DETR&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#65306;&#20351;&#29992;&#30495;&#23454;&#30340;&#21069;&#26223;-&#32972;&#26223;&#33945;&#29256;&#65288;GT Fg-Bg Mask&#65289;&#20316;&#20026;&#39069;&#22806;&#30340;&#25552;&#31034;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#26435;&#37325;/&#20540;&#65307;&#36890;&#36807;&#26356;&#22909;&#30340;&#26435;&#37325;/&#20540;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#20540;/&#26435;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#37325;&#27880;&#24847;&#27169;&#22359;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#27880;&#24847;&#21147;&#26159;&#32431;&#31929;&#30340;&#32553;&#25918;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#31532;&#20108;/&#19977;&#20010;&#27880;&#24847;&#21147;&#36890;&#36807;GT Fg-Bg Mask&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26435;&#37325;/&#20540;&#65292;&#24182;&#19982;&#31532;&#19968;&#20010;&#27880;&#24847;&#21147;&#20849;&#20139;&#20540;/&#26435;&#37325;&#20197;&#25552;&#39640;&#20540;/&#26435;&#37325;&#30340;&#36136;&#37327;&#12290;&#25512;&#26029;&#26102;&#31227;&#38500;&#31532;&#20108;&#21644;&#31532;&#19977;&#20010;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#31216;&#35813;&#26041;&#27861;&#20026;&#30693;&#35782;&#20849;&#20139;DETR&#65288;KS-DETR&#65289;&#65292;&#23427;&#26159;&#23545;DETR&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaled dot-product attention applies a softmax function on the scaled dot-product of queries and keys to calculate weights and then multiplies the weights and values. In this work, we study how to improve the learning of scaled dot-product attention to improve the accuracy of DETR. Our method is based on the following observations: using ground truth foreground-background mask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables learning much better weights/values; with better weights/values, better values/weights can be learned. We propose a triple-attention module in which the first attention is a plain scaled dot-product attention, the second/third attention generates high-quality weights/values (with the assistance of GT Fg-Bg Mask) and shares the values/weights with the first attention to improve the quality of values/weights. The second and third attentions are removed during inference. We call our method knowledge-sharing DETR (KS-DETR), which is an extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; QIKT &#30340;&#22522;&#20110;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20197;&#38382;&#39064;&#20026;&#20013;&#24515;&#30340;&#35748;&#30693;&#34920;&#31034;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#21516;&#36136;&#21270;&#38382;&#39064;&#30340;&#20551;&#35774;&#23545;&#23454;&#38469;&#24773;&#20917;&#19981;&#20934;&#30830;&#20197;&#21450;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06885</link><description>&lt;p&gt;
&#37319;&#29992;&#20197;&#38382;&#39064;&#20026;&#20013;&#24515;&#30340;&#35748;&#30693;&#34920;&#31034;&#25913;&#36827;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations. (arXiv:2302.06885v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; QIKT &#30340;&#22522;&#20110;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20197;&#38382;&#39064;&#20026;&#20013;&#24515;&#30340;&#35748;&#30693;&#34920;&#31034;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#21516;&#36136;&#21270;&#38382;&#39064;&#30340;&#20551;&#35774;&#23545;&#23454;&#38469;&#24773;&#20917;&#19981;&#20934;&#30830;&#20197;&#21450;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#26159;&#39044;&#27979;&#23398;&#29983;&#26410;&#26469;&#34920;&#29616;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#35266;&#23519;&#20182;&#20204;&#30340;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22823;&#37096;&#20998;&#37117;&#20381;&#36182;&#20110;&#8220;&#21516;&#36136;&#21270;&#38382;&#39064;&#8221;&#30340;&#20551;&#35774;&#65292;&#21363;&#22914;&#26524;&#38382;&#39064;&#20855;&#26377;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#38598;&#65292;&#21017;&#23427;&#20204;&#30340;&#36129;&#29486;&#26159;&#31561;&#20215;&#30340;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#38469;&#25945;&#32946;&#22330;&#26223;&#20013;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#27492;&#22806;&#65292;&#20174;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#20013;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QIKT&#30340;&#22522;&#20110;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#26469;&#24212;&#23545;&#20197;&#19978;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;QIKT&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#38382;&#39064;&#30340;&#35748;&#30693;&#34920;&#31034;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#20174;&#38382;&#39064;-&#31572;&#26696;&#20132;&#20114;&#20013;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#25935;&#24863;&#30340;&#35748;&#30693;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing (KT) is a crucial technique to predict students' future performance by observing their historical learning processes. Due to the powerful representation ability of deep neural networks, remarkable progress has been made by using deep learning techniques to solve the KT problem. The majority of existing approaches rely on the \emph{homogeneous question} assumption that questions have equivalent contributions if they share the same set of knowledge components. Unfortunately, this assumption is inaccurate in real-world educational scenarios. Furthermore, it is very challenging to interpret the prediction results from the existing deep learning based KT models. Therefore, in this paper, we present QIKT, a question-centric interpretable KT model to address the above challenges. The proposed QIKT approach explicitly models students' knowledge state variations at a fine-grained level with question-sensitive cognitive representations that are jointly learned from a question-c
&lt;/p&gt;</description></item><item><title>OpenHLS&#26159;&#19968;&#20010;&#22522;&#20110;&#39640;&#32423;&#32508;&#21512;&#25216;&#26415;&#30340;&#24320;&#28304;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#36817;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#20302;&#32423;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23454;&#39564;&#31185;&#23398;&#39046;&#22495;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#20013;&#20302;&#24310;&#36831;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06751</link><description>&lt;p&gt;
OpenHLS&#65306;&#36866;&#29992;&#20110;&#23454;&#39564;&#31185;&#23398;&#30340;&#20302;&#24310;&#36831;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39640;&#32423;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for Experimental Science. (arXiv:2302.06751v3 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06751
&lt;/p&gt;
&lt;p&gt;
OpenHLS&#26159;&#19968;&#20010;&#22522;&#20110;&#39640;&#32423;&#32508;&#21512;&#25216;&#26415;&#30340;&#24320;&#28304;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#36817;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#20302;&#32423;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23454;&#39564;&#31185;&#23398;&#39046;&#22495;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#20013;&#20302;&#24310;&#36831;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#39564;&#39537;&#21160;&#30340;&#31185;&#23398;&#39046;&#22495;&#65292;&#20363;&#22914;&#39640;&#33021;&#29289;&#29702;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#65292;&#39640;&#25968;&#25454;&#29575;&#23454;&#39564;&#23545;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#26045;&#21152;&#30828;&#24615;&#32422;&#26463;&#65306;&#25910;&#38598;&#30340;&#25968;&#25454;&#24517;&#39035;&#26080;&#24046;&#21035;&#22320;&#23384;&#20648;&#20197;&#36827;&#34892;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#38656;&#35201;&#22823;&#23481;&#37327;&#23384;&#20648;&#65292;&#25110;&#32773;&#22312;&#23454;&#26102;&#20934;&#30830;&#36807;&#28388;&#26102;&#65292;&#20174;&#32780;&#38656;&#35201;&#20302;&#24310;&#36831;&#22788;&#29702;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20854;&#20182;&#36807;&#28388;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#35774;&#35745;&#21644;&#37096;&#32626;&#22256;&#38590;&#65292;&#23578;&#26410;&#24191;&#27867;&#24212;&#29992;&#20110;&#27492;&#31867;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#32534;&#35793;&#22120;&#26694;&#26550;OpenHLS&#65292;&#22522;&#20110;&#39640;&#32423;&#32508;&#21512;&#25216;&#26415;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#31561;&#36817;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#20302;&#32423;&#34920;&#31034;&#65292;&#20854;&#20013;&#27809;&#26377;&#20219;&#20309;&#19987;&#26377;&#20381;&#36182;&#39033;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24037;&#20316;&#36127;&#36733;&#19978;&#35780;&#20272;OpenHLS&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26696;&#20363;&#30740;&#31350;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many experiment-driven scientific domains, such as high-energy physics, material science, and cosmology, high data rate experiments impose hard constraints on data acquisition systems: collected data must either be indiscriminately stored for post-processing and analysis, thereby necessitating large storage capacity, or accurately filtered in real-time, thereby necessitating low-latency processing. Deep neural networks, effective in other filtering tasks, have not been widely employed in such data acquisition systems, due to design and deployment difficulties. We present an open source, lightweight, compiler framework, without any proprietary dependencies, OpenHLS, based on high-level synthesis techniques, for translating high-level representations of deep neural networks to low-level representations, suitable for deployment to near-sensor devices such as field-programmable gate arrays. We evaluate OpenHLS on various workloads and present a case-study implementation of a deep neural
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;DBS&#27835;&#30103;&#24085;&#37329;&#26862;&#27663;&#30151;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#38381;&#29615;&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#25511;&#21046;&#22120;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#27835;&#30103;&#24133;&#24230;&#65292;&#20943;&#23569;&#33021;&#37327;&#20351;&#29992;&#65292;&#24182;&#26816;&#27979;&#20854;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02477</link><description>&lt;p&gt;
&#38024;&#23545;&#24085;&#37329;&#26862;&#30149;&#27835;&#30103;&#30340;&#38381;&#29615;&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#25511;&#21046;&#22120;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment. (arXiv:2302.02477v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;DBS&#27835;&#30103;&#24085;&#37329;&#26862;&#27663;&#30151;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#38381;&#29615;&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#25511;&#21046;&#22120;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#27835;&#30103;&#24133;&#24230;&#65292;&#20943;&#23569;&#33021;&#37327;&#20351;&#29992;&#65292;&#24182;&#26816;&#27979;&#20854;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#33041;&#30005;&#21050;&#28608;&#65288;DBS&#65289;&#36890;&#36807;&#21521;&#33041;&#30340;&#22522;&#24213;&#31070;&#32463;&#33410;&#21306;&#22495;&#20256;&#36882;&#30005;&#33033;&#20914;&#65292;&#26174;&#31034;&#20986;&#27835;&#30103;&#24085;&#37329;&#26862;&#27663;&#30151;&#65288;PD&#65289;&#24341;&#36215;&#30340;&#36816;&#21160;&#30151;&#29366;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24471;&#21040;&#32654;&#22269;&#39135;&#21697;&#21644;&#33647;&#29289;&#31649;&#29702;&#23616;&#65288;FDA&#65289;&#25209;&#20934;&#30340;DBS&#20165;&#33021;&#20197;&#22266;&#23450;&#24133;&#24230;&#25552;&#20379;&#25345;&#32493;DBS&#65288;cDBS&#65289;&#21050;&#28608;&#65307;&#36825;&#31181;&#33021;&#37327;&#20302;&#25928;&#30340;&#25805;&#20316;&#38477;&#20302;&#20102;&#35774;&#22791;&#30340;&#30005;&#27744;&#23551;&#21629;&#65292;&#19981;&#33021;&#26681;&#25454;&#27963;&#21160;&#24773;&#20917;&#21160;&#24577;&#35843;&#25972;&#27835;&#30103;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#26174;&#33879;&#30340;&#21103;&#20316;&#29992;&#65288;&#22914;&#27493;&#24577;&#38556;&#30861;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#21033;&#29992;&#36807;&#21435;&#30340;&#20020;&#24202;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;RL&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23454;&#26102;&#35843;&#25972;&#21050;&#28608;&#24133;&#24230;&#30340;&#21516;&#26102;&#65292;&#20197;&#20943;&#23569;&#33021;&#37327;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;cDBS&#30456;&#21516;&#27700;&#24179;&#30340;&#27835;&#30103;&#25928;&#21147;&#65288;&#21363;&#25511;&#21046;&#65289;&#12290;&#27492;&#22806;&#65292;&#20020;&#24202;&#21327;&#35758;&#35201;&#27714;&#22312;&#23558;&#36825;&#31181;RL&#25511;&#21046;&#22120;&#37096;&#32626;&#21040;&#24739;&#32773;&#20043;&#21069;&#65292;&#38656;&#35777;&#26126;&#20854;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#25311;&#25216;&#26415;&#30340;&#23433;&#20840;&#26816;&#27979;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep brain stimulation (DBS) has shown great promise toward treating motor symptoms caused by Parkinson's disease (PD), by delivering electrical pulses to the Basal Ganglia (BG) region of the brain. However, DBS devices approved by the U.S. Food and Drug Administration (FDA) can only deliver continuous DBS (cDBS) stimuli at a fixed amplitude; this energy inefficient operation reduces battery lifetime of the device, cannot adapt treatment dynamically for activity, and may cause significant side-effects (e.g., gait impairment). In this work, we introduce an offline reinforcement learning (RL) framework, allowing the use of past clinical data to train an RL policy to adjust the stimulation amplitude in real time, with the goal of reducing energy use while maintaining the same level of treatment (i.e., control) efficacy as cDBS. Moreover, clinical protocols require the safety and performance of such RL controllers to be demonstrated ahead of deployments in patients. Thus, we also introduce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#30340;&#39592;&#24178;&#65292;&#20849;&#20139;&#23884;&#20837;&#31867;&#21521;&#37327;&#26469;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#24615;&#33021;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#38544;&#31169;&#20445;&#25252;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2301.11705</link><description>&lt;p&gt;
FedPH: &#38544;&#31169;&#22686;&#24378;&#22411;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedPH: Privacy-enhanced Heterogeneous Federated Learning. (arXiv:2301.11705v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#30340;&#39592;&#24178;&#65292;&#20849;&#20139;&#23884;&#20837;&#31867;&#21521;&#37327;&#26469;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#24615;&#33021;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#38544;&#31169;&#20445;&#25252;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#65292;&#36890;&#36807;&#20132;&#25442;&#21442;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35745;&#31639;&#36164;&#28304;&#24046;&#24322;&#20351;&#24471;&#30456;&#20851;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#30340;&#39592;&#24178;&#65292;&#23436;&#20840;&#36830;&#25509;&#30340;&#23618;&#26500;&#25104;&#22836;&#37096;&#12290;&#39592;&#24178;&#25552;&#21462;&#22836;&#37096;&#29305;&#24449;&#65292;&#31867;&#30340;&#23884;&#20837;&#21521;&#37327;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#65292;&#20197;&#25913;&#21892;&#22836;&#37096;&#24182;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20849;&#20139;&#31867;&#30340;&#23884;&#20837;&#21521;&#37327;&#32780;&#19981;&#26159;&#26799;&#24230;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#31169;&#26377;&#25968;&#25454;&#65292;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#26356;&#21152;&#26377;&#25928;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21521;&#31867;&#30340;&#23884;&#20837;&#21521;&#37327;&#28155;&#21152;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is a distributed machine-learning environment that allows clients to learn collaboratively without sharing private data. This is accomplished by exchanging parameters. However, the differences in data distributions and computing resources among clients make related studies difficult. To address these heterogeneous problems, we propose a novel Federated Learning method. Our method utilizes a pre-trained model as the backbone of the local model, with fully connected layers comprising the head. The backbone extracts features for the head, and the embedding vector of classes is shared between clients to improve the head and enhance the performance of the local model. By sharing the embedding vector of classes instead of gradient-based parameters, clients can better adapt to private data, and communication between the server and clients is more effective. To protect privacy, we propose a privacy-preserving hybrid method that adds noise to the embedding vector of classes. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#28457;&#28065;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#20174;&#21333;&#20010;&#35270;&#39057;&#20013;&#25512;&#26029;&#21644;&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20302;&#32500;&#30340;&#29289;&#29702;&#32422;&#26463;&#27969;&#29305;&#24449;&#34920;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11494</link><description>&lt;p&gt;
&#23398;&#20064;&#28457;&#28065;&#21160;&#21147;&#23398;&#29992;&#20110;&#27969;&#20307;&#25512;&#29702;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Vortex Dynamics for Fluid Inference and Prediction. (arXiv:2301.11494v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11494
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#28457;&#28065;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#20174;&#21333;&#20010;&#35270;&#39057;&#20013;&#25512;&#26029;&#21644;&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20302;&#32500;&#30340;&#29289;&#29702;&#32422;&#26463;&#27969;&#29305;&#24449;&#34920;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#28457;&#28065;&#31890;&#23376;&#65288;DVP&#65289;&#26041;&#27861;&#65292;&#20174;&#21333;&#20010;&#35270;&#39057;&#20013;&#25512;&#26029;&#21644;&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#20854;&#26680;&#24515;&#26159;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#29992;&#20110;&#21253;&#21547;&#25903;&#25745;&#21487;&#35266;&#27979;&#30340;&#27431;&#25289;&#27969;&#29616;&#35937;&#30340;&#38544;&#34109;&#30340;&#25289;&#26684;&#26391;&#26085;&#28457;&#28065;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#21487;&#24494;&#28457;&#28065;&#31890;&#23376;&#19982;&#21487;&#23398;&#20064;&#30340;&#28457;&#28065;&#21040;&#36895;&#24230;&#21160;&#21147;&#23398;&#26144;&#23556;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#29289;&#29702;&#32422;&#26463;&#30340;&#22797;&#26434;&#27969;&#29305;&#24449;&#12289;&#20302;&#32500;&#31354;&#38388;&#12290;&#35813;&#34920;&#31034;&#26377;&#21161;&#20110;&#23398;&#20064;&#38024;&#23545;&#36755;&#20837;&#35270;&#39057;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#65292;&#21487;&#25552;&#20379;&#31283;&#20581;&#30340;&#12289;&#38271;&#26399;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20215;&#20540;&#22312;&#20110;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#20351;&#24471;&#21487;&#20197;&#32431;&#31929;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#26469;&#25512;&#26029;&#38544;&#21547;&#30340;&#29289;&#29702;&#37327;&#65288;&#20363;&#22914;&#36895;&#24230;&#22330;&#65289;&#65307;&#20854;&#27425;&#65292;&#23427;&#36824;&#25903;&#25345;&#26410;&#26469;&#39044;&#27979;&#65292;&#26500;&#36896;&#36755;&#20837;&#35270;&#39057;&#30340;&#24207;&#21015;&#20197;&#21450;&#20854;&#26410;&#26469;&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#35270;&#39057;&#19978;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19968;&#31995;&#21015;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20986;&#20248;&#36234;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24615;&#33021;&#65292;&#20197;&#21450;&#23545;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel differentiable vortex particle (DVP) method to infer and predict fluid dynamics from a single video. Lying at its core is a particle-based latent space to encapsulate the hidden, Lagrangian vortical evolution underpinning the observable, Eulerian flow phenomena. Our differentiable vortex particles are coupled with a learnable, vortex-to-velocity dynamics mapping to effectively capture the complex flow features in a physically-constrained, low-dimensional space. This representation facilitates the learning of a fluid simulator tailored to the input video that can deliver robust, long-term future predictions. The value of our method is twofold: first, our learned simulator enables the inference of hidden physics quantities (e.g., velocity field) purely from visual observation; secondly, it also supports future prediction, constructing the input video's sequel along with its future dynamics evolution. We compare our method with a range of existing methods on both synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#65288;MIHO&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;MIHO&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#65292;&#21442;&#25968;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36710;&#36742;&#22312;&#22330;&#22320;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#21644;&#31283;&#23450;&#36991;&#38556;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.01470</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#23454;&#29616;&#33258;&#20027;&#36187;&#36710;&#31995;&#32479;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model Parameter Identification via a Hyperparameter Optimization Scheme for Autonomous Racing Systems. (arXiv:2301.01470v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#65288;MIHO&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;MIHO&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#65292;&#21442;&#25968;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36710;&#36742;&#22312;&#22330;&#22320;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#21644;&#31283;&#23450;&#36991;&#38556;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#65288;MIHO&#65289;&#23454;&#29616;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#26469;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#35782;&#21035;&#21160;&#24577;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;MIHO&#36827;&#34892;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;&#20248;&#21270;&#21518;&#30340;&#21442;&#25968;&#34701;&#20837;&#25105;&#20204;&#24179;&#21488;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#35774;&#35745;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MIHO&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;MIHO&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22330;&#22320;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#27169;&#22411;&#30340;&#31995;&#32479;&#20855;&#26377;&#31283;&#23450;&#30340;&#36991;&#38556;&#24615;&#33021;&#21644;&#39640;&#36895;&#34892;&#39542;&#24615;&#33021;&#65292;&#22312;&#21360;&#31532;&#23433;&#32435;&#27874;&#21033;&#26031;&#36710;&#36895;&#20844;&#22253;&#21644;&#25289;&#26031;&#32500;&#21152;&#26031;&#36710;&#36895;&#20844;&#22253;&#30340;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this letter, we propose a model parameter identification method via a hyperparameter optimization scheme (MIHO). Our method adopts an efficient explore-exploit strategy to identify the parameters of dynamic models in a data-driven optimization manner. We utilize MIHO for model parameter identification of the AV-21, a full-scaled autonomous race vehicle. We then incorporate the optimized parameters for the design of model-based planning and control systems of our platform. In experiments, MIHO exhibits more than 13 times faster convergence than traditional parameter identification methods. Furthermore, the parametric models learned via MIHO demonstrate good fitness to the given datasets and show generalization ability in unseen dynamic scenarios. We further conduct extensive field tests to validate our model-based system, demonstrating stable obstacle avoidance and high-speed driving up to 217 km/h at the Indianapolis Motor Speedway and Las Vegas Motor Speedway. The source code for M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Transkribus&#24179;&#21488;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36341;&#32463;&#39564;&#65292;&#21253;&#25324;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#30340;&#21512;&#20316;&#24615;&#36136;&#21644;&#25968;&#25454;&#20998;&#20139;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2212.11146</link><description>&lt;p&gt;
HTR&#27169;&#22411;&#35757;&#32451;&#30340;&#25361;&#25112;&#65306;&#12298;&#25968;&#23383;&#26102;&#20195;&#30340;&#26723;&#26696;&#20445;&#25252;&#35745;&#21010;&#12299;&#39033;&#30446;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique. (arXiv:2212.11146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Transkribus&#24179;&#21488;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36341;&#32463;&#39564;&#65292;&#21253;&#25324;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#30340;&#21512;&#20316;&#24615;&#36136;&#21644;&#25968;&#25454;&#20998;&#20139;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#20889;&#35782;&#21035;&#25216;&#26415;&#30340;&#20986;&#29616;&#20026;&#25991;&#21270;&#36951;&#20135;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#29616;&#22312;&#38656;&#35201;&#21453;&#24605;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#30340;&#32463;&#39564;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#33258;2018&#24180;&#20197;&#26469;&#20351;&#29992;Transkribus&#24179;&#21488;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#36716;&#24405;17&#19990;&#32426;&#30340;&#27861;&#35821;&#25163;&#20889;&#25991;&#26412;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;HTR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23558;&#25152;&#26377;&#36825;&#20123;&#20803;&#32032;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#65288;&#22914;Transkribus&#65289;&#30340;&#21512;&#20316;&#24615;&#36136;&#20197;&#21450;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#20998;&#20139;&#20854;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The arrival of handwriting recognition technologies offers new possibilities for research in heritage studies. However, it is now necessary to reflect on the experiences and the practices developed by research teams. Our use of the Transkribus platform since 2018 has led us to search for the most significant ways to improve the performance of our handwritten text recognition (HTR) models which are made to transcribe French handwriting dating from the 17th century. This article therefore reports on the impacts of creating transcribing protocols, using the language model at full scale and determining the best way to use base models in order to help increase the performance of HTR models. Combining all of these elements can indeed increase the performance of a single model by more than 20% (reaching a Character Error Rate below 5%). This article also discusses some challenges regarding the collaborative nature of HTR platforms such as Transkribus and the way researchers can share their da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#24182;&#32467;&#21512;&#32676;&#20247;&#22806;&#21253;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#35757;&#32451;&#19968;&#20010;&#26144;&#23556;&#35821;&#20041;&#30456;&#20284;&#24615;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24357;&#21512;&#20154;&#31867;&#30452;&#35273;&#21644;&#20844;&#24179;&#20998;&#31867;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10154</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#31867;&#23548;&#21521;&#30340;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#24182;&#32467;&#21512;&#32676;&#20247;&#22806;&#21253;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#35757;&#32451;&#19968;&#20010;&#26144;&#23556;&#35821;&#20041;&#30456;&#20284;&#24615;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24357;&#21512;&#20154;&#31867;&#30452;&#35273;&#21644;&#20844;&#24179;&#20998;&#31867;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#31616;&#21382;&#31579;&#36873;&#21644;&#20869;&#23481;&#23457;&#26680;&#31561;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#24517;&#39035;&#26159;&#20844;&#24179;&#30340;&#65292;&#24182;&#36890;&#36807;&#23545;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#30340;&#25200;&#21160;&#19981;&#21464;&#26469;&#36991;&#20813;&#27495;&#35270;&#24615;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23545;&#36825;&#20123;&#25200;&#21160;&#30340;&#30452;&#35273;&#19982;&#25429;&#25417;&#23427;&#20204;&#30340;&#24418;&#24335;&#30456;&#20284;&#24230;&#35268;&#33539;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#30828;&#32534;&#30721;&#21333;&#35789;&#26367;&#25442;&#65292;&#23548;&#33268;&#35268;&#33539;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#25110;&#32773;&#26080;&#27861;&#20805;&#20998;&#22320;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#19968;&#33268;&#65288;&#20363;&#22914;&#65292;&#22312;&#19981;&#23545;&#31216;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#19979;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#21457;&#29616;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#30452;&#35273;&#20844;&#24179;&#35268;&#33539;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26080;&#30417;&#30563;&#24335;&#36716;&#25442;&#21644;GPT-3&#30340;&#38646;-shot&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#19978;&#31867;&#20284;&#20294;&#22312;&#25935;&#24863;&#23646;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#34920;&#36798;&#20016;&#23500;&#30340;&#20505;&#36873;&#21477;&#23376;&#23545;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#32676;&#20247;&#22806;&#21253;&#33719;&#24471;&#36825;&#20123;&#20505;&#36873;&#20154;&#30340;&#25104;&#23545;&#20154;&#31867;&#21028;&#26029;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#23558;&#35821;&#20041;&#30456;&#20284;&#24615;&#26144;&#23556;&#21040;&#32479;&#35745;&#20195;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#29616;&#33021;&#21147;&#12289;&#19982;&#20154;&#31867;&#30452;&#35273;&#30340;&#19968;&#33268;&#24615;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2212.08189</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#22312;&#32447;&#30830;&#23450;&#24615;&#36864;&#28779;&#65306;&#19968;&#31181;&#20998;&#23618;&#21644;&#28176;&#36827;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture. (arXiv:2212.08189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#36880;&#27493;&#36924;&#36817;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20998;&#23618;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#20915;&#31574;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#21487;&#33021;&#30340;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#31354;&#38388;&#30340;&#28176;&#36827;&#20998;&#21306;&#12290;&#26368;&#20248;&#20998;&#21306;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#20248;&#21270;&#23376;&#38382;&#39064;&#36880;&#27493;&#36924;&#36817;&#65292;&#29983;&#25104;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#30340;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#23545;&#27599;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#20998;&#21306;&#30340;&#27599;&#20010;&#23376;&#38598;&#20013;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#12290;&#36825;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#21487;&#35299;&#37322;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36880;&#27493;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical learning algorithms that gradually approximate a solution to a data-driven optimization problem are essential to decision-making systems, especially under limitations on time and computational resources. In this study, we introduce a general-purpose hierarchical learning architecture that is based on the progressive partitioning of a possibly multi-resolution data space. The optimal partition is gradually approximated by solving a sequence of optimization sub-problems that yield a sequence of partitions with increasing number of subsets. We show that the solution of each optimization problem can be estimated online using gradient-free stochastic approximation updates. As a consequence, a function approximation problem can be defined within each subset of the partition and solved using the theory of two-timescale stochastic approximation algorithms. This simulates an annealing process and defines a robust and interpretable heuristic method to gradually increase the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29615;&#38754;&#22352;&#26631;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22278;&#20540;&#22270;&#24418;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#27010;&#24565;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#36807;&#31243;&#65292;&#29992;&#20110;&#26500;&#24314;&#26368;&#23567;&#33021;&#37327;&#30340;&#29615;&#38754;&#20540;&#22270;&#12290;</title><link>http://arxiv.org/abs/2212.07201</link><description>&lt;p&gt;
&#29615;&#38754;&#22352;&#26631;&#65306;&#26684;&#28857;&#32422;&#21270;&#23454;&#29616;&#24490;&#29615;&#22352;&#26631;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction. (arXiv:2212.07201v2 [cs.CG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29615;&#38754;&#22352;&#26631;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22278;&#20540;&#22270;&#24418;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#27010;&#24565;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#36807;&#31243;&#65292;&#29992;&#20110;&#26500;&#24314;&#26368;&#23567;&#33021;&#37327;&#30340;&#29615;&#38754;&#20540;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#29615;&#38754;&#22352;&#26631;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#24212;&#29992;&#20110;&#22810;&#20010;&#19978;&#21516;&#35843;&#31867;&#26102;&#65292;&#21363;&#20351;&#25152;&#36873;&#23450;&#30340;&#19978;&#21516;&#35843;&#31867;&#26159;&#32447;&#24615;&#26080;&#20851;&#30340;&#65292;&#36755;&#20986;&#30340;&#29615;&#38754;&#20540;&#22270;&#20063;&#21487;&#33021;&#20250;&#34987;&#20960;&#20309;&#30456;&#20851;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#19978;&#21516;&#35843;&#31867;&#30340;&#25972;&#25968;&#32447;&#24615;&#32452;&#21512;&#21487;&#20197;&#33719;&#24471;&#36739;&#19981;&#30456;&#20851;&#30340;&#26144;&#23556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22278;&#20540;&#22270;&#24418;&#30340;&#20960;&#20309;&#30456;&#20851;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#22312;&#40654;&#26364;&#27969;&#24418;&#24773;&#20917;&#19979;&#23545;&#24212;&#20110;&#29380;&#21033;&#20811;&#38647;&#24418;&#24335;&#65292;&#36825;&#26159;&#20174;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#23548;&#20986;&#30340;&#21452;&#32447;&#24615;&#24418;&#24335;&#12290;&#20316;&#32773;&#25551;&#36848;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#36807;&#31243;&#65292;&#29992;&#20110;&#26500;&#24314;&#26368;&#23567;&#33021;&#37327;&#30340;&#29615;&#38754;&#20540;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The circular coordinates algorithm of de Silva, Morozov, and Vejdemo-Johansson takes as input a dataset together with a cohomology class representing a $1$-dimensional hole in the data; the output is a map from the data into the circle that captures this hole, and that is of minimum energy in a suitable sense. However, when applied to several cohomology classes, the output circle-valued maps can be "geometrically correlated" even if the chosen cohomology classes are linearly independent. It is shown in the original work that less correlated maps can be obtained with suitable integer linear combinations of the cohomology classes, with the linear combinations being chosen by inspection. In this paper, we identify a formal notion of geometric correlation between circle-valued maps which, in the Riemannian manifold case, corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet energy. We describe a systematic procedure for constructing low energy torus-valued maps on d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VideoCoCa&#30340;&#22522;&#20110;&#23545;&#27604;&#24335;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#25991;&#26412;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#39069;&#22806;&#35757;&#32451;&#19979;&#65292;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#26816;&#32034;&#32467;&#26524;&#65292;&#24182;&#22312;&#36731;&#24494;&#24494;&#35843;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.04979</link><description>&lt;p&gt;
VideoCoCa: &#20174;&#23545;&#27604;&#24335;&#23383;&#24149;&#29983;&#25104;&#22120;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#22495;&#35270;&#39057;&#25991;&#26412;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners. (arXiv:2212.04979v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VideoCoCa&#30340;&#22522;&#20110;&#23545;&#27604;&#24335;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#25991;&#26412;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#39069;&#22806;&#35757;&#32451;&#19979;&#65292;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#35270;&#39057;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#26816;&#32034;&#32467;&#26524;&#65292;&#24182;&#22312;&#36731;&#24494;&#24494;&#35843;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#24314;&#31435;&#19968;&#20010;&#22522;&#30784;&#30340;&#35270;&#39057;-&#25991;&#26412;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VideoCoCa&#65292;&#23427;&#26368;&#22823;&#38480;&#24230;&#22320;&#37325;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#24335;&#23383;&#24149;&#29983;&#25104;&#22120;&#65288;CoCa&#65289;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#30340;&#39069;&#22806;&#35757;&#32451;&#26469;&#36866;&#24212;&#35270;&#39057;-&#25991;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20197;&#24448;&#30340;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24103;&#38388;&#34701;&#21512;&#27169;&#22359;&#26469;&#36866;&#24212;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#65292;&#32780;CoCa&#20013;&#30340;&#29983;&#25104;&#24335;&#27880;&#24847;&#21147;&#27744;&#21270;&#21644;&#23545;&#27604;&#24335;&#27880;&#24847;&#21147;&#27744;&#21270;&#23618;&#21487;&#20197;&#31435;&#21363;&#36866;&#24212;&#25153;&#24179;&#21270;&#30340;&#24103;&#23884;&#20837;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#20998;&#31867;&#21644;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;VideoCoCa&#20043;&#19978;&#36827;&#34892;&#36731;&#24494;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#35270;&#39057;&#38382;&#31572;&#21644;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore an efficient approach to establish a foundational video-text model. We present VideoCoCa that maximally reuses a pretrained image-text contrastive captioner (CoCa) model and adapt it to video-text tasks with minimal extra training. While previous works adapt image-text models with various cross-frame fusion modules, we find that the generative attentional pooling and contrastive attentional pooling layers in CoCa are instantly adaptable to flattened frame embeddings, yielding state-of-the-art results on zero-shot video classification and zero-shot text-to-video retrieval. Furthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve strong results on video question-answering and video captioning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;VLPCook&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25216;&#26415;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#23454;&#29616;&#22522;&#20110;&#32467;&#26500;&#21270;&#25991;&#26412;&#30340;&#35745;&#31639;&#26009;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#39135;&#29289;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36229;&#36234;&#24403;&#21069;SoTA&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04267</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#32467;&#26500;&#21270;&#35821;&#35328;&#39044;&#35757;&#32451;&#29992;&#20110;&#36328;&#27169;&#24577;&#39135;&#29289;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval. (arXiv:2212.04267v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;VLPCook&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25216;&#26415;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#23454;&#29616;&#22522;&#20110;&#32467;&#26500;&#21270;&#25991;&#26412;&#30340;&#35745;&#31639;&#26009;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#39135;&#29289;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36229;&#36234;&#24403;&#21069;SoTA&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#21644;&#22522;&#30784;&#27169;&#22411;&#19968;&#30452;&#26159;&#22312;&#36890;&#29992;&#22522;&#20934;&#19978;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#30340;&#24120;&#35268;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#27604;&#22914;&#28041;&#21450;&#26356;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#30340;&#28921;&#39274;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#25216;&#26415;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#22522;&#20110;&#32467;&#26500;&#21270;&#25991;&#26412;&#30340;&#35745;&#31639;&#26009;&#29702;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#34987;&#31216;&#20026;VLPCook&#65292;&#39318;&#20808;&#23558;&#29616;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36716;&#25442;&#20026;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#23545;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#36866;&#24212;&#20110;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;VLP&#30446;&#26631;&#39044;&#35757;&#32451;&#25105;&#20204;&#30340;VLPCook&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#35745;&#31639;&#28921;&#39274;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#20016;&#23500;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#25552;&#20379;&#23616;&#37096;&#21644;&#20840;&#23616;&#25991;&#26412;&#19978;&#19979;&#25991;&#12290;VLPCook&#22312;&#36328;&#27169;&#24577;&#39135;&#29289;&#26816;&#32034;&#20219;&#21153;&#19978;&#36890;&#36807;&#26174;&#33879;&#30340;&#36739;&#39640;&#27604;&#20363;&#65288;+3.3 Recall@1&#32477;&#23545;&#25913;&#21892;&#65289;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;SoTA&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Pretraining (VLP) and Foundation models have been the go-to recipe for achieving SoTA performance on general benchmarks. However, leveraging these powerful techniques for more complex vision-language tasks, such as cooking applications, with more structured input data, is still little investigated. In this work, we propose to leverage these techniques for structured-text based computational cuisine tasks. Our strategy, dubbed VLPCook, first transforms existing image-text pairs to image and structured-text pairs. This allows to pretrain our VLPCook model using VLP objectives adapted to the strutured data of the resulting datasets, then finetuning it on downstream computational cooking tasks. During finetuning, we also enrich the visual encoder, leveraging pretrained foundation models (e.g. CLIP) to provide local and global textual context. VLPCook outperforms current SoTA by a significant margin (+3.3 Recall@1 absolute improvement) on the task of Cross-Modal Food Retriev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#23413;&#21270;&#30340;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#25104;&#36739;&#23567;&#30340;&#23376;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#24182;&#26080;&#32541;&#22320;&#32452;&#35013;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#39640;&#25928;&#12289;&#26377;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2212.04129</link><description>&lt;p&gt;
&#28145;&#24230;&#23413;&#21270;: &#20998;&#32780;&#27835;&#20043;&#22320;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Incubation: Training Large Models by Divide-and-Conquering. (arXiv:2212.04129v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#23413;&#21270;&#30340;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#25104;&#36739;&#23567;&#30340;&#23376;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#24182;&#26080;&#32541;&#22320;&#32452;&#35013;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#39640;&#25928;&#12289;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#12289;&#32531;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#36807;&#24230;&#25311;&#21512;&#31561;&#38382;&#39064;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#23413;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#25104;&#36739;&#23567;&#30340;&#23376;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#24182;&#26080;&#32541;&#22320;&#32452;&#35013;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#39640;&#25928;&#12289;&#26377;&#25928;&#35757;&#32451;&#12290;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#20445;&#29420;&#31435;&#35757;&#32451;&#30340;&#23376;&#27169;&#22359;&#30340;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#23616;&#30340;&#20849;&#20139;&#20803;&#27169;&#22411;&#65292;&#23427;&#34987;&#29992;&#26469;&#38544;&#24335;&#22320;&#23558;&#25152;&#26377;&#27169;&#22359;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#24182;&#19988;&#21487;&#20197;&#35774;&#35745;&#20026;&#19968;&#20010;&#20855;&#26377;&#21487;&#24573;&#30053;&#35745;&#31639;&#24320;&#38144;&#30340;&#26497;&#23567;&#32593;&#32476;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#23413;&#21270;&#31639;&#27861;&#65292;&#23427;&#35757;&#32451;&#27599;&#20010;&#23376;&#27169;&#22359;&#26469;&#26367;&#25442;&#20803;&#27169;&#22411;&#30340;&#30456;&#24212;&#37096;&#20998;&#24182;&#23436;&#25104;&#32473;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a remarkable success of large deep learning models. However, training these models is challenging due to high computational costs, painfully slow convergence, and overfitting issues. In this paper, we present Deep Incubation, a novel approach that enables the efficient and effective training of large models by dividing them into smaller sub-modules that can be trained separately and assembled seamlessly. A key challenge for implementing this idea is to ensure the compatibility of the independently trained sub-modules. To address this issue, we first introduce a global, shared meta model, which is leveraged to implicitly link all the modules together, and can be designed as an extremely small network with negligible computational overhead. Then we propose a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and accomplish a given learning task. Despite the simplicity, our approach effectively enc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2212.02710</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#35937;&#35782;&#21035;&#65306;&#38754;&#21521;&#23545;&#35937;&#27010;&#24565;&#23398;&#20064;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Beyond Object Recognition: A New Benchmark towards Object Concept Learning. (arXiv:2212.02710v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#35937;&#23646;&#24615;&#12289;&#20316;&#29992;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#20197;&#25903;&#25345; OCL&#65292;&#25552;&#20986;&#20102; Object Concept Reasoning Network (OCRN) &#20316;&#20026;&#22522;&#32447;&#65292;&#25552;&#21319;&#20102;&#23545;&#35937;&#35748;&#30693;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23545;&#35937;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#20307;&#39564;&#30340;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#26426;&#22120;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#26356;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#23545;&#35937;&#20855;&#26377;&#21738;&#20123;&#23646;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#23545;&#35937;&#20570;&#20160;&#20040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; Object Concept Learning (OCL) &#20219;&#21153;&#65292;&#20197;&#25512;&#21160;&#23545;&#35937;&#29702;&#35299;&#30340;&#21457;&#23637;&#12290;&#23427;&#35201;&#27714;&#26426;&#22120;&#25512;&#29702;&#20986;&#23545;&#35937;&#30340;&#20316;&#29992;&#65292;&#24182;&#21516;&#26102;&#32473;&#20986;&#21407;&#22240;&#65306;&#26159;&#21738;&#20123;&#23646;&#24615;&#20351;&#24471;&#19968;&#20010;&#23545;&#35937;&#20855;&#26377;&#36825;&#20123;&#20316;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345; OCL&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23494;&#38598;&#27880;&#37322;&#30340;&#30693;&#35782;&#24211;&#65292;&#21253;&#25324;&#19977;&#20010;&#23618;&#27425;&#30340;&#23545;&#35937;&#27010;&#24565;&#65288;&#31867;&#21035;&#12289;&#23646;&#24615;&#12289;&#20316;&#29992;&#65289;&#65292;&#20197;&#21450;&#19977;&#20010;&#23618;&#27425;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512; OCL &#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#65306;Object Concept Reasoning Network (OCRN)&#12290;&#23427;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#21644;&#27010;&#24565;&#23454;&#20363;&#21270;&#26469;&#25512;&#26029;&#19977;&#20010;&#23618;&#27425;&#65292;&#36981;&#24490;&#23427;&#20204;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#31574;&#30053;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31574;&#30053;&#28151;&#21512;&#65292;&#24041;&#22266;&#24213;&#23618;&#40654;&#26364;&#36816;&#21160;&#31574;&#30053;&#30340;&#27604;&#20363;&#65292;&#26377;&#25928;&#35843;&#25972;&#40654;&#26364;&#30697;&#38453;&#65292;&#20915;&#23450;&#19987;&#23478;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#20248;&#20808;&#32423;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#20219;&#21153;&#25104;&#21151;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#36229;&#36807;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2212.01938</link><description>&lt;p&gt;
&#20998;&#32423;&#31574;&#30053;&#28151;&#21512;&#20316;&#20026;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Policy Blending As Optimal Transport. (arXiv:2212.01938v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#31574;&#30053;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31574;&#30053;&#28151;&#21512;&#65292;&#24041;&#22266;&#24213;&#23618;&#40654;&#26364;&#36816;&#21160;&#31574;&#30053;&#30340;&#27604;&#20363;&#65292;&#26377;&#25928;&#35843;&#25972;&#40654;&#26364;&#30697;&#38453;&#65292;&#20915;&#23450;&#19987;&#23478;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#20248;&#20808;&#32423;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#20219;&#21153;&#25104;&#21151;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#36229;&#36807;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#32423;&#31574;&#30053;&#28151;&#21512;&#20316;&#20026;&#26368;&#20248;&#36755;&#36816;&#65288;HiPBOT&#65289;&#12290;&#36825;&#31181;&#20998;&#23618;&#26694;&#26550;&#36890;&#36807;&#23545;&#19987;&#23478;&#31574;&#30053;&#30340;&#20302;&#32423;&#21453;&#24212;&#36866;&#24212;&#26435;&#37325;&#65292;&#22312;&#19987;&#23478;&#31574;&#30053;&#21644;&#20195;&#29702;&#20154;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#21069;&#30651;&#35268;&#21010;&#23618;&#12290;&#25105;&#20204;&#30340;&#39640;&#23618;&#35268;&#21010;&#32773;&#36890;&#36807;&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31574;&#30053;&#28151;&#21512;&#65292;&#24041;&#22266;&#24213;&#23618;&#40654;&#26364;&#36816;&#21160;&#31574;&#30053;&#30340;&#27604;&#20363;&#65292;&#26377;&#25928;&#35843;&#25972;&#23427;&#20204;&#30340;&#40654;&#26364;&#30697;&#38453;&#65292;&#24182;&#22312;&#19987;&#23478;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#20915;&#23450;&#20248;&#20808;&#32423;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#20219;&#21153;&#25104;&#21151;&#12290;&#25105;&#20204;&#22312;&#20174;&#20302;&#32500;&#23548;&#33322;&#21040;&#39640;&#32500;&#20840;&#36523;&#25511;&#21046;&#30340;&#19968;&#31995;&#21015;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;HiPBOT&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25191;&#34892;&#27010;&#29575;&#25512;&#26029;&#25110;&#23450;&#20041;&#19987;&#23478;&#26641;&#32467;&#26500;&#30340;&#29616;&#26377;&#22522;&#32447;&#65292;&#20026;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#30340;&#26032;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present hierarchical policy blending as optimal transport (HiPBOT). This hierarchical framework adapts the weights of low-level reactive expert policies, adding a look-ahead planning layer on the parameter space of a product of expert policies and agents. Our high-level planner realizes a policy blending via unbalanced optimal transport, consolidating the scaling of underlying Riemannian motion policies, effectively adjusting their Riemannian matrix, and deciding over the priorities between experts and agents, guaranteeing safety and task success. Our experimental results in a range of application scenarios from low-dimensional navigation to high-dimensional whole-body control showcase the efficacy and efficiency of HiPBOT, which outperforms state-of-the-art baselines that either perform probabilistic inference or define a tree structure of experts, paving the way for new applications of optimal transport to robot control. More material at https://sites.google.com/view/hipobot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22495;&#27867;&#21270;&#30340;&#30452;&#25509;&#24433;&#21709;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#24433;&#21709;&#27010;&#24565;&#35299;&#20915;&#20102;&#30456;&#20851;&#24615;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#19982;&#29616;&#26377;&#22495;&#27867;&#21270;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14594</link><description>&lt;p&gt;
&#38024;&#23545;&#22495;&#27867;&#21270;&#30340;&#30452;&#25509;&#24433;&#21709;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct-Effect Risk Minimization for Domain Generalization. (arXiv:2211.14594v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22495;&#27867;&#21270;&#30340;&#30452;&#25509;&#24433;&#21709;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#24433;&#21709;&#27010;&#24565;&#35299;&#20915;&#20102;&#30456;&#20851;&#24615;&#36716;&#31227;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#19982;&#29616;&#26377;&#22495;&#27867;&#21270;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22495;&#22806;&#25512; generalization &#20013;&#23646;&#24615;&#34394;&#20551;&#30456;&#20851;&#24615;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#22495;&#38388;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#21363;&#30456;&#20851;&#24615;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#36896;&#25104;&#20102;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#24433;&#21709;&#27010;&#24565;&#26469;&#35299;&#20915;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#33021;&#22815;&#23398;&#20064;&#21040;&#30452;&#25509;&#24433;&#21709;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#20351;&#29992;&#34920;&#31034;&#21644;&#31867;&#26631;&#31614;&#39044;&#27979;&#22495;&#26631;&#31614;&#30340;&#38169;&#35823;&#26469;&#23398;&#20064;&#38388;&#25509;&#24433;&#21709;&#34920;&#31034;&#65307;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#21644;&#39564;&#35777;&#38454;&#27573;&#20013;&#20855;&#26377;&#30456;&#20284;&#38388;&#25509;&#24433;&#21709;&#34920;&#31034;&#20294;&#26631;&#31614;&#19981;&#21516;&#30340;&#25968;&#25454;&#30456;&#21305;&#37197;&#26469;&#28040;&#38500;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#23398;&#20064;&#21040;&#30340;&#38388;&#25509;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#20854;&#21487;&#19982;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#31639;&#27861;&#30456;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be com
&lt;/p&gt;</description></item><item><title>uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12872</link><description>&lt;p&gt;
{\mu}Split: &#26174;&#24494;&#38236;&#25968;&#25454;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
{\mu}Split: efficient image decomposition for microscopy data. (arXiv:2211.12872v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12872
&lt;/p&gt;
&lt;p&gt;
uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; uSplit&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#35757;&#32451;&#22270;&#20687;&#20998;&#35299;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#24120;&#35268;&#30340;&#28145;&#24230;&#32467;&#26500;&#20307;&#31995;&#32467;&#26500;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#22823;&#22270;&#20687;&#22359;&#20250;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#20351;&#20869;&#23384;&#28040;&#32791;&#25104;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65288;LC&#65289;&#65292;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#24378;&#22823;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;LC&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#22987;&#32456;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;LC&#19982;U-Nets&#12289;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;VAEs&#38598;&#25104;&#65292;&#20026;&#27492;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;ELBO loss&#12290;&#27492;&#22806;&#65292;LC&#20351;&#24471;&#35757;&#32451;&#27604;&#21407;&#26412;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#20943;&#23569;&#20351;&#29992;&#20998;&#21106;VAE&#39044;&#27979;&#26102;&#19981;&#21487;&#36991;&#20813;&#30340;&#24179;&#38138;&#20266;&#24433;&#12290;&#25105;&#20204;&#23558;uSplit&#24212;&#29992;&#20110;&#20116;&#20010;&#20998;&#35299;&#20219;&#21153;&#65292;&#19968;&#20010;&#26159;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21478;&#22806;&#22235;&#20010;&#26469;&#33258;&#23454;&#38469;&#26174;&#24494;&#38236;&#25968;&#25454;&#12290;LC&#23454;&#29616;&#20102;SOTA&#30340;&#32467;&#26524;&#65288;&#24179;&#22343;im&#65289;
&lt;/p&gt;
&lt;p&gt;
We present uSplit, a dedicated approach for trained image decomposition in the context of fluorescence microscopy images. We find that best results using regular deep architectures are achieved when large image patches are used during training, making memory consumption the limiting factor to further improving performance. We therefore introduce lateral contextualization (LC), a memory efficient way to train powerful networks and show that LC leads to consistent and significant improvements on the task at hand. We integrate LC with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a modified ELBO loss. Additionally, LC enables training deeper hierarchical models than otherwise possible and, interestingly, helps to reduce tiling artefacts that are inherently impossible to avoid when using tiled VAE predictions. We apply uSplit to five decomposition tasks, one on a synthetic dataset, four others derived from real microscopy data. LC achieves SOTA results (average im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36339;&#36291;&#20132;&#20114;&#33539;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#30446;&#26631;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#37096;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#22810;&#36339;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;HopGNN&#26694;&#26550;&#23454;&#29616;&#36339;&#36291;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2211.11761</link><description>&lt;p&gt;
&#20174;&#33410;&#28857;&#20132;&#20114;&#21040;&#36339;&#36291;&#20132;&#20114;&#65306;&#26032;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm. (arXiv:2211.11761v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36339;&#36291;&#20132;&#20114;&#33539;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#30446;&#26631;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#37096;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#22810;&#36339;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;HopGNN&#26694;&#26550;&#23454;&#29616;&#36339;&#36291;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36981;&#24490;&#36845;&#20195;&#22320;&#22312;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#20449;&#24687;&#20132;&#20114;&#30340;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33410;&#28857;&#20132;&#20114;&#33539;&#24335;&#20173;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#65292;&#21363;&#22312;&#24555;&#36895;&#25193;&#23637;&#30340;&#37051;&#23621;&#20043;&#38388;&#36827;&#34892;&#33410;&#28857;&#20132;&#20114;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#65307;&#20197;&#21450;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#33410;&#28857;&#30340;&#21028;&#21035;&#33021;&#21147;&#21463;&#38480;&#65292;&#37325;&#22797;&#33410;&#28857;&#20132;&#20114;&#21518;&#19981;&#21516;&#31867;&#21035;&#33410;&#28857;&#30340;&#34920;&#31034;&#23558;&#20250;&#25910;&#25947;&#21040;&#26080;&#27861;&#21306;&#20998;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36339;&#36291;&#20132;&#20114;&#33539;&#24335;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#33410;&#28857;&#20043;&#38388;&#30340;&#20132;&#20114;&#30446;&#26631;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#37096;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#22810;&#36339;&#29305;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340; HopGNN &#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#21033;&#29992;&#29616;&#26377;&#30340;GNN&#23454;&#29616;&#36339;&#36291;&#20132;&#24448;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Graph Neural Networks (GNNs) follow the message-passing mechanism that conducts information interaction among nodes iteratively. While considerable progress has been made, such node interaction paradigms still have the following limitation. First, the scalability limitation precludes the broad application of GNNs in large-scale industrial settings since the node interaction among rapidly expanding neighbors incurs high computation and memory costs. Second, the over-smoothing problem restricts the discrimination ability of nodes, i.e., node representations of different classes will converge to indistinguishable after repeated node interactions. In this work, we propose a novel hop interaction paradigm to address these limitations simultaneously. The core idea is to convert the interaction target among nodes to pre-processed multi-hop features inside each node. We design a simple yet effective HopGNN framework that can easily utilize existing GNNs to achieve hop interaction. Fur
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11727</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#21270;&#20998;&#31867;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;:&#19968;&#20010;&#22522;&#32447;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Parametric Classification for Generalized Category Discovery: A Baseline Study. (arXiv:2211.11727v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#26088;&#22312;&#21033;&#29992;&#20174;&#26631;&#27880;&#26679;&#26412;&#20013;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#22312;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#26032;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#23481;&#26131;&#23545;&#24050;&#30693;&#31867;&#21035;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#21322;&#30417;&#30563;k&#22343;&#20540;&#24418;&#25104;&#30340;&#38750;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#30340;&#22833;&#36133;&#24773;&#20917;&#65292;&#39564;&#35777;&#20102;&#24403;&#26377;&#39640;&#36136;&#37327;&#30340;&#30417;&#30563;&#21487;&#29992;&#26102;&#20808;&#21069;&#30340;&#35774;&#35745;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#19981;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#20004;&#31181;&#39044;&#27979;&#20559;&#24046;&#65306;&#20998;&#31867;&#22120;&#26356;&#20542;&#21521;&#20110;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#22312;&#24050;&#30693;&#21644;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#20135;&#29983;&#19968;&#20010;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#21270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#21463;&#30410;&#20110;&#29109;&#27491;&#21017;&#21270;&#65292;&#22312;&#22810;&#20010;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#23545;&#26410;&#30693;&#31867;&#21035;&#25968;&#37327;&#20855;&#26377;&#24378;&#22823;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#25104;&#26524;&#33021;&#20026;&#26410;&#26469;&#26356;&#26377;&#25928;&#30340;GCD&#26041;&#27861;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Category Discovery (GCD) aims to discover novel categories in unlabelled datasets using knowledge learned from labelled samples. Previous studies argued that parametric classifiers are prone to overfitting to seen categories, and endorsed using a non-parametric classifier formed with semi-supervised k-means. However, in this study, we investigate the failure of parametric classifiers, verify the effectiveness of previous design choices when high-quality supervision is available, and identify unreliable pseudo-labels as a key problem. We demonstrate that two prediction biases exist: the classifier tends to predict seen classes more often, and produces an imbalanced distribution across seen and novel categories. Based on these findings, we propose a simple yet effective parametric classification method that benefits from entropy regularisation, achieves state-of-the-art performance on multiple GCD benchmarks and shows strong robustness to unknown class numbers. We hope the in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2211.02408</link><description>&lt;p&gt;
&#25554;&#20837;&#21518;&#38376;&#20803;&#32032;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. (arXiv:2211.02408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23384;&#22312;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#21521;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25554;&#20837;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#36827;&#25552;&#31034;&#20013;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#25216;&#26415;&#22312;&#30740;&#31350;&#32773;&#21644;&#20844;&#20247;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20381;&#36182;&#20110;&#22806;&#37096;&#26469;&#28304;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#29992;&#25143;&#30456;&#20449;&#26816;&#32034;&#21040;&#30340;&#27169;&#22411;&#20250;&#20687;&#25215;&#35834;&#30340;&#37027;&#26679;&#36816;&#34892;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#21521;&#38376;&#25915;&#20987;&#25991;&#26412;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#31713;&#25913;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21482;&#26159;&#36731;&#24494;&#22320;&#25913;&#21464;&#20102;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;&#23545;&#20110;&#24102;&#26377;&#24178;&#20928;&#25552;&#31034;&#30340;&#22270;&#20687;&#29983;&#25104;&#27809;&#26377;&#21487;&#30097;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#19968;&#20010;&#21333;&#19968;&#23383;&#31526;&#35302;&#21457;&#22120;&#25554;&#20837;&#25552;&#31034;&#20013;&#65292;&#20363;&#22914;&#19968;&#20010;&#38750;&#25289;&#19969;&#23383;&#31526;&#25110;&#34920;&#24773;&#31526;&#21495;&#65292;&#25915;&#20987;&#32773;&#23601;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#23646;&#24615;&#30340;&#22270;&#20687;&#25110;&#36981;&#24490;&#38544;&#34255;&#30340;&#12289;&#28508;&#22312;&#30340;&#24694;&#24847;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;Stable Diffusion&#21644;highligh&#19978;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highligh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21644;&#21028;&#21035;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35821;&#38899;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#38477;&#22122;&#21644;&#24102;&#23485;&#25193;&#23637;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.02397</link><description>&lt;p&gt;
&#27604;&#36739;&#25193;&#25955;&#29983;&#25104;&#26041;&#27861;&#21644;&#21028;&#21035;&#26041;&#27861;&#22312;&#35821;&#38899;&#22797;&#21407;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Analysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration. (arXiv:2211.02397v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21644;&#21028;&#21035;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35821;&#38899;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22312;&#35821;&#38899;&#38477;&#22122;&#21644;&#24102;&#23485;&#25193;&#23637;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#21644;&#21028;&#21035;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35821;&#38899;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#38899;&#38477;&#22122;&#21644;&#24102;&#23485;&#25193;&#23637;&#20219;&#21153;&#20013;&#65292;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#21028;&#21035;&#26041;&#27861;&#65292;&#32780;&#22312;&#21435;&#28151;&#21709;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have had a high impact on the computer vision and speech processing communities these past years. Besides data generation tasks, they have also been employed for data restoration tasks like speech enhancement and dereverberation. While discriminative models have traditionally been argued to be more powerful e.g. for speech enhancement, generative diffusion approaches have recently been shown to narrow this performance gap considerably. In this paper, we systematically compare the performance of generative diffusion models and discriminative approaches on different speech restoration tasks. For this, we extend our prior contributions on diffusion-based speech enhancement in the complex time-frequency domain to the task of bandwith extension. We then compare it to a discriminatively trained neural network with the same network architecture on three restoration tasks, namely speech denoising, dereverberation and bandwidth extension. We observe that the ge
&lt;/p&gt;</description></item><item><title>MPCFormer&#20351;&#29992;MPC&#21644;KD&#25216;&#26415;&#23454;&#29616;&#31169;&#23494;&#30340;Transformer&#27169;&#22411;&#25512;&#29702;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.01452</link><description>&lt;p&gt;
MPCFormer: &#22522;&#20110;MPC&#30340;&#24555;&#36895;&#12289;&#39640;&#24615;&#33021;&#21644;&#31169;&#23494;&#30340;Transformer&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MPCFormer: fast, performant and private Transformer inference with MPC. (arXiv:2211.01452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01452
&lt;/p&gt;
&lt;p&gt;
MPCFormer&#20351;&#29992;MPC&#21644;KD&#25216;&#26415;&#23454;&#29616;&#31169;&#23494;&#30340;Transformer&#27169;&#22411;&#25512;&#29702;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#31169;&#23494;&#30340;&#25512;&#29702;&#23545;&#20110;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#20113;&#25512;&#29702;&#26381;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31169;&#23494;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#22686;&#21152;&#25512;&#29702;&#24310;&#36831;60&#20493;&#20197;&#19978;&#65292;&#25110;&#26174;&#33879;&#25439;&#23475;&#25512;&#29702;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;MPCFORMER&#26694;&#26550;&#20316;&#20026;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;MPC&#65289;&#21644;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPCFORMER&#22312;MPC&#35774;&#32622;&#20013;&#26174;&#33879;&#21152;&#36895;&#20102;Transformer&#25512;&#29702;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#36755;&#20837;&#27169;&#22411;&#30340;ML&#24615;&#33021;&#12290;&#22312;IMDb&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;BERTBASE&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36895;&#24230;&#24555;5.3&#20493;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23427;&#20197;2.2&#20493;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;97%&#30340;BERTBASE&#24615;&#33021;&#12290;MPCFORMER&#23545;&#19981;&#21516;&#30340;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#26435;&#37325;&#65292;&#22914;ROBERTABASE&#21644;&#26356;&#22823;&#30340;&#27169;&#22411;&#65292;&#22914;BERTLarge&#20063;&#20445;&#25345;&#26377;&#25928;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/MccRee177/MPCFormer&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60x or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to BERTBASE, while being 5.3x faster. On the GLUE benchmark, it achieves 97% performance of BERTBASE with a 2.2x speedup. MPCFORMER remains effective with different trained Transformer weights such as ROBERTABASE and larger models including BERTLarge. Code is available at https://github.com/MccRee177/MPCFormer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644; k-means &#32858;&#31867;&#31639;&#27861;&#26469;&#33258;&#21160;&#32858;&#21512;&#22810;&#36793;&#24418;&#32593;&#26684;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#27604;&#26631;&#20934;&#31639;&#27861; METIS &#26356;&#26377;&#25928;&#29575;&#24182;&#33021;&#26174;&#33879;&#25552;&#39640;&#27714;&#35299;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.17457</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32858;&#21512;&#22810;&#36793;&#24418;&#32593;&#26684;&#24182;&#24212;&#29992;&#20110;&#22810;&#37325;&#32593;&#26684;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Agglomeration of Polygonal Grids using Graph Neural Networks with applications to Multigrid solvers. (arXiv:2210.17457v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644; k-means &#32858;&#31867;&#31639;&#27861;&#26469;&#33258;&#21160;&#32858;&#21512;&#22810;&#36793;&#24418;&#32593;&#26684;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#27604;&#26631;&#20934;&#31639;&#27861; METIS &#26356;&#26377;&#25928;&#29575;&#24182;&#33021;&#26174;&#33879;&#25552;&#39640;&#27714;&#35299;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#31574;&#30053;&#22312;&#33258;&#36866;&#24212;&#32454;&#21270;&#31639;&#27861;&#21644;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#22810;&#32423;&#20195;&#25968;&#27714;&#35299;&#22120;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#33258;&#21160;&#25191;&#34892;&#22810;&#36793;&#24418;&#32593;&#26684;&#32858;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#33258;&#28982;&#22320;&#21033;&#29992;&#32593;&#26684;&#30340;&#20960;&#20309;&#20449;&#24687;&#26469;&#20445;&#25345;&#32593;&#26684;&#36136;&#37327;&#65292;&#25552;&#39640;&#25968;&#20540;&#26041;&#27861;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#24635;&#20307;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20855;&#20307;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;k-means&#32858;&#31867;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26469;&#21010;&#20998;&#35745;&#31639;&#32593;&#26684;&#30340;&#36830;&#25509;&#22270;&#65292;&#24182;&#19988;GNNs&#20855;&#26377;&#39640;&#36895;&#22312;&#32447;&#25512;&#29702;&#21644;&#22788;&#29702;&#32593;&#26684;&#32467;&#26500;&#21644;&#20960;&#20309;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#26415;&#19982;&#29992;&#20110;&#22270;&#20998;&#21106;&#30340;&#26631;&#20934;&#31639;&#27861;METIS&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#20248;&#20110;METIS&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#27714;&#35299;&#22120;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agglomeration-based strategies are important both within adaptive refinement algorithms and to construct scalable multilevel algebraic solvers. In order to automatically perform agglomeration of polygonal grids, we propose the use of Machine Learning (ML) strategies, that can naturally exploit geometrical information about the mesh in order to preserve the grid quality, enhancing performance of numerical methods and reducing the overall computational cost. In particular, we employ the k-means clustering algorithm and Graph Neural Networks (GNNs) to partition the connectivity graph of a computational mesh. Moreover, GNNs have high online inference speed and the advantage to process naturally and simultaneously both the graph structure of mesh and the geometrical information, such as the areas of the elements or their barycentric coordinates. These techniques are compared with METIS, a standard algorithm for graph partitioning, which is meant to process only the graph information of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#29289;&#20307;&#22312;&#22270;&#20687;&#20013;&#30340;&#20301;&#32622;&#65292;&#24182;&#23558;&#35270;&#35273;&#30456;&#20851;&#37096;&#20998;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#20462;&#25913;&#65292;&#20351;&#27169;&#22411;&#29420;&#29305;&#22320;&#23398;&#20064;&#20102;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#25351;&#26631;&#34913;&#37327;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.09996</link><description>&lt;p&gt;
&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24863;&#30693;&#20998;&#32452;
&lt;/p&gt;
&lt;p&gt;
Perceptual Grouping in Contrastive Vision-Language Models. (arXiv:2210.09996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#29289;&#20307;&#22312;&#22270;&#20687;&#20013;&#30340;&#20301;&#32622;&#65292;&#24182;&#23558;&#35270;&#35273;&#30456;&#20851;&#37096;&#20998;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#20462;&#25913;&#65292;&#20351;&#27169;&#22411;&#29420;&#29305;&#22320;&#23398;&#20064;&#20102;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#25351;&#26631;&#34913;&#37327;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#38646;&#26679;&#26412;&#22270;&#20687;&#35782;&#21035;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20102;&#39640;&#24230;&#35821;&#20041;&#20449;&#24687;&#30340;&#36890;&#29992;&#35270;&#35273;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30701;&#35821;&#36827;&#34892;&#20219;&#24847;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#22270;&#20687;&#19981;&#20165;&#20165;&#26159;&#29702;&#35299;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#20869;&#23481;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20102;&#35299;&#36825;&#20123;&#20869;&#23481;&#25152;&#22312;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29702;&#35299;&#29289;&#20307;&#22312;&#22270;&#20687;&#20013;&#30340;&#20301;&#32622;&#65292;&#24182;&#23558;&#35270;&#35273;&#30456;&#20851;&#37096;&#20998;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#21644;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#30340;&#29616;&#20195;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#23450;&#20301;&#20449;&#24687;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#23567;&#30340;&#20462;&#25913;&#65292;&#20351;&#27169;&#22411;&#29420;&#29305;&#22320;&#23398;&#20064;&#20102;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#22270;&#20687;&#35782;&#21035;&#12289;&#26080;&#30417;&#30563;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20197;&#21450;&#40065;&#26834;&#30340;&#30446;&#26631;&#23450;&#20301;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robu
&lt;/p&gt;</description></item><item><title>&#35299;&#30721;&#31639;&#27861;&#30340;&#36873;&#25321;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#20219;&#21153;&#25928;&#29992;&#30340;&#21305;&#37197;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#31867;&#19981;&#21305;&#37197;&#32531;&#35299;&#31574;&#30053;&#65288;MMS&#65289;&#30340;&#35270;&#35282;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#30721;&#31639;&#27861;&#30340;&#36890;&#29992;&#24615;</title><link>http://arxiv.org/abs/2210.07228</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#20316;&#20026;&#20284;&#28982;&#24230;-&#25928;&#29992;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Language Model Decoding as Likelihood-Utility Alignment. (arXiv:2210.07228v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07228
&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#31639;&#27861;&#30340;&#36873;&#25321;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#20219;&#21153;&#25928;&#29992;&#30340;&#21305;&#37197;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#31867;&#19981;&#21305;&#37197;&#32531;&#35299;&#31574;&#30053;&#65288;MMS&#65289;&#30340;&#35270;&#35282;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#30721;&#31639;&#27861;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#35821;&#35328;&#29983;&#25104;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#35299;&#30721;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24212;&#35813;&#25351;&#23548;&#36873;&#25321;&#35299;&#30721;&#31639;&#27861;&#30340;&#19968;&#33324;&#21407;&#21017;&#20173;&#19981;&#28165;&#26970;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20165;&#22312;&#29421;&#31364;&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#35299;&#30721;&#31639;&#27861;&#65292;&#20182;&#20204;&#30340;&#21457;&#29616;&#19981;&#33021;&#25512;&#24191;&#21040;&#36328;&#20219;&#21153;&#12290;&#25105;&#20204;&#35748;&#20026;&#27169;&#22411;&#30340;&#20284;&#28982;&#21644;&#20219;&#21153;&#29305;&#23450;&#25928;&#29992;&#30340;&#19981;&#21305;&#37197;&#26159;&#29702;&#35299;&#35299;&#30721;&#31639;&#27861;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#32467;&#26500;&#21270;&#35752;&#35770;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#19981;&#21305;&#37197;&#32531;&#35299;&#31574;&#30053;&#65288;MMS&#65289;&#30340;&#20998;&#31867;&#27861;&#65292;&#25552;&#20379;&#35299;&#30721;&#20316;&#20026;&#23545;&#40784;&#24037;&#20855;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#36825;&#20010;MMS&#20998;&#31867;&#27861;&#26681;&#25454;&#35299;&#30721;&#31639;&#27861;&#23545;&#20284;&#28982;&#24230;-&#25928;&#29992;&#19981;&#21305;&#37197;&#30340;&#38544;&#21547;&#20551;&#35774;&#23545;&#20854;&#36827;&#34892;&#20998;&#32452;&#65292;&#20135;&#29983;&#20851;&#20110;&#23427;&#20204;&#36328;&#20219;&#21153;&#36866;&#29992;&#24615;&#30340;&#19968;&#33324;&#24615;&#22768;&#26126;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20998;&#26512;&#36328;&#22810;&#20010;&#20219;&#21153;&#30340;&#39044;&#27979;&#30340;&#20284;&#28982;&#24230;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
A critical component of a successful language generation pipeline is the decoding algorithm. However, the general principles that should guide the choice of a decoding algorithm remain unclear. Previous works only compare decoding algorithms in narrow scenarios, and their findings do not generalize across tasks. We argue that the misalignment between the model's likelihood and the task-specific notion of utility is the key factor to understanding the effectiveness of decoding algorithms. To structure the discussion, we introduce a taxonomy of misalignment mitigation strategies (MMSs), providing a unifying view of decoding as a tool for alignment. The MMS taxonomy groups decoding algorithms based on their implicit assumptions about likelihood--utility misalignment, yielding general statements about their applicability across tasks. Specifically, by analyzing the correlation between the likelihood and the utility of predictions across a diverse set of tasks, we provide empirical evidence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#21644;&#31232;&#30095;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#23569;&#37327;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#31526;&#21495;&#31616;&#27905;&#30340;&#24320;&#25918;&#24335;PDE&#65292;&#24182;&#36890;&#36807;&#20248;&#31168;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26356;&#26032;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2210.02181</link><description>&lt;p&gt;
DISCOVER&#65306;&#36890;&#36807;&#22686;&#24378;&#30340;&#24378;&#21270;&#23398;&#20064;&#28145;&#24230;&#35782;&#21035;&#31526;&#21495;&#31616;&#27905;&#30340;&#24320;&#25918;&#24335;PDE
&lt;/p&gt;
&lt;p&gt;
DISCOVER: Deep identification of symbolically concise open-form PDEs via enhanced reinforcement-learning. (arXiv:2210.02181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#21644;&#31232;&#30095;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#23569;&#37327;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#31526;&#21495;&#31616;&#27905;&#30340;&#24320;&#25918;&#24335;PDE&#65292;&#24182;&#36890;&#36807;&#20248;&#31168;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26356;&#26032;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#33258;&#28982;&#31995;&#32479;&#30340;&#24037;&#20316;&#26426;&#21046;&#24448;&#24448;&#26381;&#20174;&#31616;&#27905;&#28145;&#21051;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12290;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#25366;&#25496;&#26041;&#31243;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;PDE&#21457;&#29616;&#65292;&#23427;&#25581;&#31034;&#20102;&#19968;&#33268;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#24182;&#20419;&#36827;&#20102;&#25105;&#20204;&#19982;&#33258;&#28982;&#30028;&#30340;&#36866;&#24212;&#24615;&#20114;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#23569;&#37327;&#20808;&#21069;&#30693;&#35782;&#21457;&#29616;&#31526;&#21495;&#31616;&#27905;&#30340;&#24320;&#25918;&#24335;PDE&#12290;&#29305;&#21035;&#22320;&#65292;&#22522;&#20110;&#22522;&#26412;&#36816;&#31639;&#31526;&#21644;&#25805;&#20316;&#25968;&#30340;&#31526;&#21495;&#24211;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#65292;&#24182;&#19982;&#31232;&#30095;&#22238;&#24402;&#26041;&#27861;&#26080;&#32541;&#32467;&#21512;&#65292;&#29983;&#25104;&#31616;&#27905;&#32780;&#24320;&#25918;&#30340;PDE&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#36866;&#24212;&#24615;&#21644;&#31616;&#27905;&#24615;&#30340;&#23436;&#32654;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#23545;&#25152;&#26377;&#29983;&#25104;&#30340;PDE&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#39640;&#25928;&#22320;&#36827;&#34892;&#26356;&#26032;&#12290;&#23450;&#21046;&#30340;&#32422;&#26463;&#21644;&#35268;&#21017;&#29992;&#20110;&#20445;&#35777;PDE&#22312;&#26415;&#35821;&#19978;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The working mechanisms of complex natural systems tend to abide by concise and profound partial differential equations (PDEs). Methods that directly mine equations from data are called PDE discovery, which reveals consistent physical laws and facilitates our adaptive interaction with the natural world. In this paper, an enhanced deep reinforcement-learning framework is proposed to uncover symbolically concise open-form PDEs with little prior knowledge. Particularly, based on a symbol library of basic operators and operands, a structure-aware recurrent neural network agent is designed and seamlessly combined with the sparse regression method to generate concise and open-form PDE expressions. All of the generated PDEs are evaluated by a meticulously designed reward function by balancing fitness to data and parsimony, and updated by the model-based reinforcement learning in an efficient way. Customized constraints and regulations are formulated to guarantee the rationality of PDEs in term
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2210.01162</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#23567;&#36829;&#21453;&#36830;&#32493;&#25511;&#21046;&#20197;&#23454;&#29616;&#19981;&#21487;&#34892;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#32508;&#21512;&#65292;&#20197;&#23454;&#29616;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;(LTL)&#34920;&#36798;&#30340;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20854;&#20013;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#26410;&#30693;&#65288;&#36879;&#26126;&#30418;&#23376;&#65289;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#26080;&#27861;&#20840;&#23616;&#23436;&#25104;&#12290;&#25105;&#20204;&#19981;&#20462;&#25913;&#32473;&#23450;&#30340;LTL&#20844;&#24335;&#65292;&#32780;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;DRL&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#36829;&#35268;&#28385;&#36275;&#23427;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#35201;&#27714;&#21516;&#26102;&#23454;&#29616;&#33258;&#21160;&#26426;&#28385;&#36275;&#21644;&#26368;&#23567;&#36829;&#35268;&#20195;&#20215;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;LTL&#20219;&#21153;&#30340;DRL&#26234;&#33021;&#20307;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;DRL&#30340;&#36817;&#35270;&#20542;&#21521;&#65292;&#36825;&#22312;&#23398;&#20064;&#21487;&#20197;&#20855;&#26377;&#38271;&#25110;&#26080;&#38480;&#25345;&#32493;&#26102;&#38388;&#30340;&#19968;&#33324;LTL&#20219;&#21153;&#26102;&#32463;&#24120;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20219;&#24847;&#23485;&#24230;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24403;&#36755;&#20837;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#30446;&#26631;&#20026;&#22810;&#25351;&#25968;&#27169;&#22411;&#26102;&#65292;NN&#30340;&#31532;&#19968;&#23618;&#26435;&#37325;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#27169;&#22411;&#20013;$k$&#32500;&#20027;&#23376;&#31354;&#38388;, &#21487;&#20197;&#36890;&#36807;&#22312;&#23376;&#31354;&#38388;&#19978;&#20351;&#29992;&#22343;&#21248;&#25910;&#25947;&#24314;&#31435;&#24191;&#20041;&#35823;&#24046;&#36793;&#30028;&#20026;$O(\sqrt{{kd}/{T}})$, SGD&#35757;&#32451;&#30340;ReLU NN&#21487;&#20197;&#23398;&#20064;&#24418;&#22914;$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$&#30340;&#21333;&#25351;&#25968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2209.14863</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Neural Networks Efficiently Learn Low-Dimensional Representations with SGD. (arXiv:2209.14863v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20219;&#24847;&#23485;&#24230;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24403;&#36755;&#20837;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#30446;&#26631;&#20026;&#22810;&#25351;&#25968;&#27169;&#22411;&#26102;&#65292;NN&#30340;&#31532;&#19968;&#23618;&#26435;&#37325;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#27169;&#22411;&#20013;$k$&#32500;&#20027;&#23376;&#31354;&#38388;, &#21487;&#20197;&#36890;&#36807;&#22312;&#23376;&#31354;&#38388;&#19978;&#20351;&#29992;&#22343;&#21248;&#25910;&#25947;&#24314;&#31435;&#24191;&#20041;&#35823;&#24046;&#36793;&#30028;&#20026;$O(\sqrt{{kd}/{T}})$, SGD&#35757;&#32451;&#30340;ReLU NN&#21487;&#20197;&#23398;&#20064;&#24418;&#22914;$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$&#30340;&#21333;&#25351;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20219;&#24847;&#23485;&#24230;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;$ \boldsymbol{x} \in \mathbb {R}^d $&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#30446;&#26631;$ y \in \mathbb {R}$&#36981;&#24490;&#22810;&#25351;&#25968;&#27169;&#22411;&#65292;&#21363; $ y = g(\langle\boldsymbol{u_1},\boldsymbol{x}\rangle,...,\langle\boldsymbol{u_k},\boldsymbol{x}\rangle)$ &#65292;&#20854;&#20013;&#20989;&#25968;$g$&#20026;&#26377;&#22122;&#22768;&#30340;&#36830;&#25509;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#20351;&#29992;&#24102;&#26377;&#26435;&#37325;&#34928;&#20943;&#30340;&#22312;&#32447;SGD&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;NN&#30340;&#31532;&#19968;&#23618;&#26435;&#37325;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#27169;&#22411;&#20013;$ \boldsymbol{u_1},...,\boldsymbol{u_k}$&#30340;$k$&#32500;&#20027;&#23376;&#31354;&#38388;&#65292;&#24403;$k \ll d$ &#26102;&#65292;&#35813;&#29616;&#35937;&#26377;&#20960;&#20010;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#22312;&#36825;&#20010;&#26356;&#23567;&#30340;&#23376;&#31354;&#38388;&#19978;&#20351;&#29992;&#22343;&#21248;&#25910;&#25947;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22312;SGD&#36827;&#34892;$T$&#27425;&#36845;&#20195;&#21518;&#30340;&#24191;&#20041;&#35823;&#24046;&#36793;&#30028;&#20026;$ O(\sqrt{{kd}/{T}})$&#65292;&#36825;&#19981;&#20381;&#36182;&#20110;NN&#30340;&#23485;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;SGD&#35757;&#32451;&#30340;ReLU NN&#21487;&#20197;&#23398;&#20064;&#24418;&#22914;$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$&#30340;&#21333;&#25351;&#25968;&#30446;&#26631;.
&lt;/p&gt;
&lt;p&gt;
We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\boldsymbol{x}\in \mathbb{R}^d$ is Gaussian and the target $y \in \mathbb{R}$ follows a multiple-index model, i.e., $y=g(\langle\boldsymbol{u_1},\boldsymbol{x}\rangle,...,\langle\boldsymbol{u_k},\boldsymbol{x}\rangle)$ with a noisy link function $g$. We prove that the first-layer weights of the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\boldsymbol{u_1},...,\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $O(\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form $y=f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35782;&#21035;&#21270;&#30103;&#21518;&#30284;&#30151;&#24739;&#32773;&#24613;&#25937;&#25252;&#29702;&#39118;&#38505;&#30340;&#38382;&#39064;&#65292;&#19982;&#20197;&#24448;&#20351;&#29992;&#32467;&#26500;&#21270;&#21355;&#29983;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20004;&#32773;&#24046;&#24322;&#19981;&#22823;&#65292;&#20174;&#32780;&#35828;&#26126;&#20102;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#19981;&#21516;&#24739;&#32773;&#32676;&#20307;&#39118;&#38505;&#20559;&#24046;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2209.13860</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35782;&#21035;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#39640;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes. (arXiv:2209.13860v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35782;&#21035;&#21270;&#30103;&#21518;&#30284;&#30151;&#24739;&#32773;&#24613;&#25937;&#25252;&#29702;&#39118;&#38505;&#30340;&#38382;&#39064;&#65292;&#19982;&#20197;&#24448;&#20351;&#29992;&#32467;&#26500;&#21270;&#21355;&#29983;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20004;&#32773;&#24046;&#24322;&#19981;&#22823;&#65292;&#20174;&#32780;&#35828;&#26126;&#20102;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#19981;&#21516;&#24739;&#32773;&#32676;&#20307;&#39118;&#38505;&#20559;&#24046;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35760;&#24405;&#26159;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#35782;&#21035;&#21270;&#30103;&#24320;&#22987;&#21518;&#30284;&#30151;&#24739;&#32773;&#24613;&#25937;&#25252;&#29702;&#65288;ACU&#65289;&#30340;&#39118;&#38505;&#12290;&#20351;&#29992;&#32467;&#26500;&#21270;&#21355;&#29983;&#25968;&#25454;&#65288;SHD&#65289;&#36827;&#34892;&#39118;&#38505;&#39044;&#27979;&#24050;&#25104;&#20026;&#26631;&#20934;&#65292;&#20294;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#36827;&#34892;&#39044;&#27979;&#26356;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#31508;&#35760;&#32780;&#38750;SHD&#36827;&#34892;ACU&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#25163;&#21160;&#26500;&#24314;&#30340;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SHD&#27169;&#22411;&#30053;&#32988;&#20110;NLP&#27169;&#22411;&#65307; SHD&#30340;l1-&#32602;&#39033;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23454;&#29616;&#30340;C&#32479;&#35745;&#37327;&#20026;0.748&#65288;95&#65285;-CI&#65306;0.735&#65292;0.762&#65289;&#65292;&#32780;&#20855;&#26377;&#35821;&#35328;&#29305;&#24449;&#30340;&#30456;&#21516;&#27169;&#22411;&#23454;&#29616;&#30340;C&#32479;&#35745;&#37327;&#20026;0.730&#65288;95&#65285;-CI&#65306;0.717&#65292;0.745&#65289;&#65292;&#32780;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#23454;&#29616;&#30340;C&#32479;&#35745;&#37327;&#20026;0.702&#65288;95&#65285;-CI&#65306;0.688&#65292;0.717&#65289;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#19981;&#21516;&#24739;&#32773;&#32676;&#20307;&#30340;&#39118;&#38505;&#20559;&#24046;&#26159;&#19981;&#21516;&#30340;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical notes are an essential component of a health record. This paper evaluates how natural language processing (NLP) can be used to identify the risk of acute care use (ACU) in oncology patients, once chemotherapy starts. Risk prediction using structured health data (SHD) is now standard, but predictions using free-text formats are complex. This paper explores the use of free-text notes for the prediction of ACU instead of SHD. Deep Learning models were compared to manually engineered language features. Results show that SHD models minimally outperform NLP models; an l1-penalised logistic regression with SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same model with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a transformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows how language models can be used in clinical applications and underlines how risk bias is different for diverse patient groups, even using only free-text dat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13476</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;:&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#30740;&#31350;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#32972;&#26223;&#19979;&#20165;&#20973;&#20511;&#20960;&#20010;&#26631;&#31614;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#20307;&#21306;&#20998;&#21644;&#19981;&#21464;&#26144;&#23556;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#19977;&#20010;&#24120;&#35265;&#29942;&#39048;&#65306;(1)&#23614;&#37096;&#20998;&#24067;&#65306;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#36981;&#24490;&#38544;&#21547;&#30340;&#38271;&#23614;&#31867;&#20998;&#24067;&#12290;&#30450;&#30446;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#20687;&#32032;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#24615;&#33021;&#24694;&#21270;&#65307;(2)&#19968;&#33268;&#24615;&#65306;&#30001;&#20110;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#20043;&#38388;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#20998;&#21106;&#27169;&#22411;&#26159;&#21542;&#23398;&#20250;&#20102;&#26377;&#24847;&#20041;&#19988;&#19968;&#33268;&#30340;&#35299;&#21078;&#29305;&#24449;&#20173;&#19981;&#28165;&#26970;&#65307;&#20197;&#21450;(3)&#22810;&#26679;&#24615;&#65306;&#25972;&#20010;&#25968;&#25454;&#38598;&#20869;&#37096;&#20999;&#29255;&#30340;&#30456;&#20851;&#24615;&#21463;&#21040;&#30340;&#20851;&#27880;&#26174;&#33879;&#36739;&#23569;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#38598;&#26412;&#36523;&#30340;&#31574;&#30053;&#26041;&#27861;&#65292;&#20174;&#19981;&#21516;&#30340;&#35299;&#21078;&#35270;&#22270;&#20013;&#21457;&#29616;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MOAN&#65292;&#29992;&#20110;&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290; MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#21069;&#32773;&#25552;&#21462;&#20849;&#21516;&#20381;&#36182;&#30340;&#20687;&#32032;&#21306;&#22495;&#65292;&#32780;&#21518;&#32773;&#24378;&#21046;&#32593;&#32476;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35299;&#21078;&#23398;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on contrastive learning have achieved remarkable performance solely by leveraging few labels in the context of medical image segmentation. Existing methods mainly focus on instance discrimination and invariant mapping. However, they face three common pitfalls: (1) tailness: medical image data usually follows an implicit long-tail class distribution. Blindly leveraging all pixels in training hence can lead to the data imbalance issues, and cause deteriorated performance; (2) consistency: it remains unclear whether a segmentation model has learned meaningful and yet consistent anatomical features due to the intra-class variations between different anatomical features; and (3) diversity: the intra-slice correlations within the entire dataset have received significantly less attention. This motivates us to seek a principled approach for strategically making use of the dataset itself to discover similar yet distinct samples from different anatomical views. In this paper, we i
&lt;/p&gt;</description></item><item><title>LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12746</link><description>&lt;p&gt;
LSAP: &#37325;&#26032;&#24605;&#32771;GAN&#28508;&#31354;&#38388;&#20013;&#21453;&#28436;&#30340;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;
&lt;/p&gt;
&lt;p&gt;
LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space. (arXiv:2209.12746v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12746
&lt;/p&gt;
&lt;p&gt;
LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21453;&#28436;&#20027;&#35201;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#22270;&#20687;&#23884;&#20837;&#65292;&#22312;&#36825;&#20010;&#27493;&#39588;&#20013;&#65292;&#32534;&#30721;&#22120;&#25110;&#32773;&#20248;&#21270;&#36807;&#31243;&#23884;&#20837;&#22270;&#20687;&#20197;&#33719;&#21462;&#30456;&#24212;&#30340;&#28508;&#22312;&#30721;&#12290;&#20043;&#21518;&#65292;&#31532;&#20108;&#27493;&#26088;&#22312;&#25913;&#21892;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32467;&#26524;&#32454;&#21270;&#12290;&#23613;&#31649;&#31532;&#20108;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#20445;&#30495;&#24230;&#65292;&#20294;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65292;&#28145;&#24230;&#20381;&#36182;&#20110;&#22312;&#31532;&#19968;&#27493;&#20013;&#33719;&#24471;&#30340;&#21453;&#21521;&#28508;&#22312;&#30721;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#20855;&#26377;&#26356;&#22909;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#28508;&#22312;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#36825;&#20004;&#20010;&#29305;&#24449;&#19982;&#21453;&#21521;&#30721;&#19982;&#21512;&#25104;&#20998;&#24067;&#30340;&#23545;&#40784;&#65288;&#25110;&#19981;&#23545;&#40784;&#65289;&#31243;&#24230;&#26377;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#31354;&#38388;&#23545;&#40784;&#21453;&#28436;&#33539;&#20363;&#65288;LSAP&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#39118;&#26684;&#31354;&#38388;&#65288;$\mathcal{S^N}$&#65289;&#21644;&#26631;&#20934;&#21270;&#20869;&#23481;&#31354;&#38388;&#65288;$\mathcal{C^N}$&#65289;&#65292;&#20998;&#21035;&#22312;&#39118;&#26684;&#21644;&#20869;&#23481;&#19978;&#23545;&#40784;&#27491;&#21521;&#21644;&#36127;&#21521;&#28508;&#22312;&#30721;&#21644;&#21512;&#25104;&#20998;&#24067;&#12290; LSAP&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#36716;&#25442;&#21644;&#22270;&#20687;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LSAP&#20855;&#26377;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29305;&#24615;&#65292;&#22914;&#25913;&#36827;&#30340;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#27169;&#24335;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the methods evolve, inversion is mainly divided into two steps. The first step is Image Embedding, in which an encoder or optimization process embeds images to get the corresponding latent codes. Afterward, the second step aims to refine the inversion and editing results, which we named Result Refinement. Although the second step significantly improves fidelity, perception and editability are almost unchanged, deeply dependent on inverse latent codes attained in the first step. Therefore, a crucial problem is gaining the latent codes with better perception and editability while retaining the reconstruction fidelity. In this work, we first point out that these two characteristics are related to the degree of alignment (or disalignment) of the inverse codes with the synthetic distribution. Then, we propose Latent Space Alignment Inversion Paradigm (LSAP), which consists of evaluation metric and solution for this problem. Specifically, we introduce Normalized Style Space ($\mathcal{S^N
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28304;&#33258;&#30001;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;(UDA)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#21644;&#26080;&#26631;&#31614;&#30446;&#26631;&#22270;&#20687;&#12290;&#26041;&#27861;&#23545;&#27169;&#22411;&#30340;&#29305;&#24449;&#29983;&#25104;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#25429;&#33719;&#22240;&#32032;&#19981;&#30830;&#23450;&#24615;&#12289;&#19968;&#33268;&#24615;&#32422;&#26463;&#21644;&#20004;&#20010;&#19981;&#21516;&#30340;&#33258;&#35757;&#32451;&#38454;&#27573;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.14888</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#33258;&#35757;&#32451;&#30340;&#35201;&#32032;&#23545;&#40784;&#36827;&#34892;&#28304;&#33258;&#30001;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Feature Alignment by Uncertainty and Self-Training for Source-Free Unsupervised Domain Adaptation. (arXiv:2208.14888v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28304;&#33258;&#30001;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;(UDA)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#21644;&#26080;&#26631;&#31614;&#30446;&#26631;&#22270;&#20687;&#12290;&#26041;&#27861;&#23545;&#27169;&#22411;&#30340;&#29305;&#24449;&#29983;&#25104;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#25429;&#33719;&#22240;&#32032;&#19981;&#30830;&#23450;&#24615;&#12289;&#19968;&#33268;&#24615;&#32422;&#26463;&#21644;&#20004;&#20010;&#19981;&#21516;&#30340;&#33258;&#35757;&#32451;&#38454;&#27573;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#20551;&#23450;&#22312;&#27169;&#22411;&#36866;&#24212;&#26399;&#38388;&#26377;&#21487;&#29992;&#30340;&#26631;&#35760;&#28304;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#23494;&#24615;&#38382;&#39064;&#25110;&#31227;&#21160;&#35774;&#22791;&#30340;&#20869;&#23384;&#38480;&#21046;&#65292;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28304;&#33258;&#30001;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#21644;&#26080;&#26631;&#31614;&#30446;&#26631;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21152;&#20837;&#25968;&#25454;&#22686;&#24378;&#26469;&#25429;&#33719;&#22240;&#32032;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#35757;&#32451;&#29305;&#24449;&#29983;&#25104;&#22120;&#12290;&#35813;&#29305;&#24449;&#29983;&#25104;&#22120;&#34987;&#40723;&#21169;&#22312;&#22836;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36793;&#30028;&#20043;&#22806;&#23398;&#20064;&#19968;&#33268;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#27169;&#22411;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#26356;&#24378;&#12290;&#21463;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20419;&#36827;&#39044;&#27979;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#20869;&#31354;&#38388;&#23545;&#40784;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#36866;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#20004;&#20010;&#28304;&#20027;&#23548;&#21644;&#30446;&#26631;&#20027;&#23548;&#30340;&#19981;&#21516;&#33258;&#35757;&#32451;&#38454;&#27573;&#65292;&#20197;&#22686;&#21152;&#28304;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most unsupervised domain adaptation (UDA) methods assume that labeled source images are available during model adaptation. However, this assumption is often infeasible owing to confidentiality issues or memory constraints on mobile devices. Some recently developed approaches do not require source images during adaptation, but they show limited performance on perturbed images. To address these problems, we propose a novel source-free UDA method that uses only a pre-trained source model and unlabeled target images. Our method captures the aleatoric uncertainty by incorporating data augmentation and trains the feature generator with two consistency objectives. The feature generator is encouraged to learn consistent visual features away from the decision boundaries of the head classifier. Thus, the adapted model becomes more robust to image perturbations. Inspired by self-supervised learning, our method promotes inter-space alignment between the prediction space and the feature space while
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#26469;&#33258;&#26102;&#38388;&#30456;&#20851;&#30340;&#23545;&#25163;&#31283;&#20581;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#26469;&#25351;&#23548;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#29983;&#25104;&#32467;&#26524;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2208.08664</link><description>&lt;p&gt;
&#21033;&#29992;&#31283;&#20581;&#20998;&#31867;&#22120;&#24341;&#23548;&#25913;&#36827;&#25193;&#25955;&#24335;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance. (arXiv:2208.08664v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#26469;&#33258;&#26102;&#38388;&#30456;&#20851;&#30340;&#23545;&#25163;&#31283;&#20581;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#26469;&#25351;&#23548;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#29983;&#25104;&#32467;&#26524;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26159;&#19968;&#31867;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#26063;&#65292;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#33719;&#24471;&#31867;&#26465;&#20214;&#29983;&#25104;&#65292;&#24314;&#35758;&#21033;&#29992;&#26469;&#33258;&#26102;&#38388;&#30456;&#20851;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#29702;&#35770;&#19978;&#24456;&#27491;&#30830;&#65292;&#20294;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;&#26799;&#24230;&#30340;&#23545;&#25163;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;&#20256;&#32479;&#20998;&#31867;&#22120;&#21487;&#33021;&#36798;&#21040;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#20998;&#25968;&#65292;&#20294;&#23427;&#20204;&#30340;&#26799;&#24230;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#21487;&#33021;&#20250;&#22952;&#30861;&#29983;&#25104;&#32467;&#26524;&#30340;&#25913;&#21892;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#25163;&#31283;&#20581;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#19982;&#20154;&#31867;&#24863;&#30693;&#19968;&#33268;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#26356;&#22909;&#22320;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#26397;&#30528;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#26041;&#21521;&#36827;&#34892;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#23450;&#20041;&#21644;&#35757;&#32451;&#19968;&#20010;&#26102;&#38388;&#30456;&#20851;&#30340;&#23545;&#25163;&#31283;&#20581;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#12290;&#22312;&#39640;&#24230;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs) are a recent family of generative models that achieve state-of-the-art results. In order to obtain class-conditional generation, it was suggested to guide the diffusion process by gradients from a time-dependent classifier. While the idea is theoretically sound, deep learning-based classifiers are infamously susceptible to gradient-based adversarial attacks. Therefore, while traditional classifiers may achieve good accuracy scores, their gradients are possibly unreliable and might hinder the improvement of the generation results. Recent work discovered that adversarially robust classifiers exhibit gradients that are aligned with human perception, and these could better guide a generative process towards semantically meaningful images. We utilize this observation by defining and training a time-dependent adversarially robust classifier and use it as guidance for a generative diffusion model. In experiments on the highly challenging and di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20197;&#21450;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#24213;&#23618;&#20132;&#20114;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.06950</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#21644;&#25552;&#21319;&#26041;&#27861;&#25311;&#21512;&#20302;&#38454;&#20989;&#25968;ANOVA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#20197;&#21450;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#24213;&#23618;&#20132;&#20114;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#38454;&#20989;&#25968;ANOVA&#27169;&#22411;&#24050;&#32463;&#34987;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37325;&#26032;&#21457;&#29616;&#65292;&#24182;&#31216;&#20043;&#20026;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GAMI-Tree&#65292;&#31867;&#20284;&#20110;EBM&#65292;&#20294;&#20855;&#26377;&#19968;&#20123;&#36235;&#21521;&#26356;&#22909;&#24615;&#33021;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26641;&#65292;&#24182;&#34701;&#20837;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#36807;&#28388;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#24213;&#23618;&#20132;&#20114;&#30340;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36845;&#20195;&#35757;&#32451;&#26041;&#27861;&#25910;&#25947;&#20110;&#20855;&#26377;&#26356;&#22909;&#39044;&#27979;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#24182;&#30830;&#20445;&#30456;&#20114;&#20316;&#29992;&#22312;&#20998;&#23618;&#24847;&#20041;&#19978;&#27491;&#20132;&#20110;&#20027;&#25928;&#24212;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#24191;&#27867;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#23454;&#29616;&#24555;&#36895;&#39640;&#25928;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25991;&#26412;&#25968;&#25454;&#19978;&#20004;&#31867;&#26041;&#27861;&#65306;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#21644;&#25552;&#21462;&#31616;&#21333;&#36923;&#36753;&#35268;&#21017;&#65292;&#21457;&#29616;&#22312;&#30456;&#21516;&#27169;&#22411;&#19979;&#20135;&#29983;&#30340;&#35299;&#37322;&#20063;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35299;&#37322;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.01420</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#27604;&#36739;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#35268;&#21017;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comparing Feature Importance and Rule Extraction for Interpretability on Text Data. (arXiv:2207.01420v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25991;&#26412;&#25968;&#25454;&#19978;&#20004;&#31867;&#26041;&#27861;&#65306;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#21644;&#25552;&#21462;&#31616;&#21333;&#36923;&#36753;&#35268;&#21017;&#65292;&#21457;&#29616;&#22312;&#30456;&#21516;&#27169;&#22411;&#19979;&#20135;&#29983;&#30340;&#35299;&#37322;&#20063;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35299;&#37322;&#24046;&#24322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#28041;&#21450;&#25991;&#26412;&#25968;&#25454;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#36825;&#23548;&#33268;&#20102;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#22312;&#23616;&#37096;&#26041;&#27861;&#20013;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#26063;&#32676;&#65306;&#19968;&#31181;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#21478;&#19968;&#31181;&#21017;&#25552;&#21462;&#31616;&#21333;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19981;&#21516;&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;&#24847;&#22806;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#37027;&#20123;&#25105;&#20204;&#39044;&#35745;&#20250;&#26377;&#23450;&#24615;&#24039;&#21512;&#30340;&#31616;&#21333;&#27169;&#22411;&#19978;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#20135;&#29983;&#30340;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex machine learning algorithms are used more and more often in critical tasks involving text data, leading to the development of interpretability methods. Among local methods, two families have emerged: those computing importance scores for each feature and those extracting simple logical rules. In this paper we show that using different methods can lead to unexpectedly different explanations, even when applied to simple models for which we would expect qualitative coincidence. To quantify this effect, we propose a new approach to compare explanations produced by different methods.
&lt;/p&gt;</description></item><item><title>FibeRed&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;&#37327;&#19995;&#25551;&#36848;&#25299;&#25169;&#22797;&#26434;&#25968;&#25454;&#30340;&#32420;&#32500;&#38477;&#32500;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22823;&#35268;&#27169;&#25299;&#25169;&#29305;&#24449;&#12290;&#31639;&#27861;&#21253;&#21547;&#20174;&#23616;&#37096;&#32447;&#24615;&#38477;&#32500;&#24471;&#21040;&#30340;&#23616;&#37096;&#34920;&#31034;&#19982;&#21021;&#22987;&#20840;&#23616;&#34920;&#31034;&#30456;&#32467;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.06513</link><description>&lt;p&gt;
FibeRed: &#36890;&#36807;&#21521;&#37327;&#19995;&#25551;&#36848;&#25299;&#25169;&#22797;&#26434;&#25968;&#25454;&#30340;&#32420;&#32500;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FibeRed: Fiberwise Dimensionality Reduction of Topologically Complex Data with Vector Bundles. (arXiv:2206.06513v2 [cs.CG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06513
&lt;/p&gt;
&lt;p&gt;
FibeRed&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21521;&#37327;&#19995;&#25551;&#36848;&#25299;&#25169;&#22797;&#26434;&#25968;&#25454;&#30340;&#32420;&#32500;&#38477;&#32500;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22823;&#35268;&#27169;&#25299;&#25169;&#29305;&#24449;&#12290;&#31639;&#27861;&#21253;&#21547;&#20174;&#23616;&#37096;&#32447;&#24615;&#38477;&#32500;&#24471;&#21040;&#30340;&#23616;&#37096;&#34920;&#31034;&#19982;&#21021;&#22987;&#20840;&#23616;&#34920;&#31034;&#30456;&#32467;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#38750;&#24179;&#20961;&#22823;&#35268;&#27169;&#25299;&#25169;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#24456;&#38590;&#29992;&#29616;&#26377;&#30340;&#38477;&#32500;&#31639;&#27861;&#23884;&#20837;&#21040;&#20302;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21521;&#37327;&#19995;&#26469;&#27169;&#25311;&#20855;&#26377;&#25299;&#25169;&#22797;&#26434;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#26679;&#22522;&#31354;&#38388;&#21487;&#20197;&#32771;&#34385;&#21040;&#22823;&#35268;&#27169;&#25299;&#25169;&#65292;&#32780;&#32420;&#32500;&#21487;&#20197;&#32771;&#34385;&#21040;&#23616;&#37096;&#20960;&#20309;&#12290;&#36825;&#20801;&#35768;&#25105;&#20204;&#38477;&#20302;&#32420;&#32500;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#20445;&#30041;&#22823;&#35268;&#27169;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#25968;&#25454;&#38598;&#21450;&#20854;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#21021;&#22987;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#20551;&#35774;&#23427;&#33021;&#24674;&#22797;&#37096;&#20998;&#22823;&#35268;&#27169;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#26032;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#23558;&#23616;&#37096;&#32447;&#24615;&#38477;&#32500;&#24471;&#21040;&#30340;&#23616;&#37096;&#34920;&#31034;&#19982;&#21021;&#22987;&#20840;&#23616;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#20363;&#23376;&#20013;&#35777;&#26126;&#20102;&#36825;&#20010;&#31639;&#27861;&#12290;&#22312;&#36825;&#20123;&#20363;&#23376;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#25299;&#25169;&#32467;&#26500;&#21644;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datasets with non-trivial large scale topology can be hard to embed in low-dimensional Euclidean space with existing dimensionality reduction algorithms. We propose to model topologically complex datasets using vector bundles, in such a way that the base space accounts for the large scale topology, while the fibers account for the local geometry. This allows one to reduce the dimensionality of the fibers, while preserving the large scale topology. We formalize this point of view, and, as an application, we describe an algorithm which takes as input a dataset together with an initial representation of it in Euclidean space, assumed to recover part of its large scale topology, and outputs a new representation that integrates local representations, obtained through local linear dimensionality reduction, along the initial global representation. We demonstrate this algorithm on examples coming from dynamical systems and chemistry. In these examples, our algorithm is able to learn topologica
&lt;/p&gt;</description></item><item><title>WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2205.14375</link><description>&lt;p&gt;
WaveMix: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;&#36164;&#28304;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
WaveMix: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14375
&lt;/p&gt;
&lt;p&gt;
WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WaveMix&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#26082;&#20855;&#26377;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#21448;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;WaveMix&#32593;&#32476;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#21253;&#25324;Cityscapes&#20013;&#30340;&#20998;&#21106;&#21644;Places-365&#12289;&#20116;&#20010;EMNIST&#25968;&#25454;&#38598;&#21644;iNAT-mini&#20013;&#30340;&#20998;&#31867;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;&#20196;&#20154;&#24778;&#22855;&#30340;&#26159;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;WaveMix&#32467;&#26500;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#24403;&#25511;&#21046;&#21442;&#25968;&#25968;&#37327;&#26102;&#65292;WaveMix&#25152;&#38656;&#30340;GPU RAM&#26356;&#23569;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#30465;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#25910;&#30410;&#65292;&#25105;&#20204;&#22312;WaveMix&#22359;&#20013;&#20351;&#29992;&#20102;&#22810;&#32423;&#20108;&#32500;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;2D-DWT&#65289;&#65292;&#23427;&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;:(1)&#23427;&#22522;&#20110;&#19977;&#31181;&#24378;&#22270;&#20687;&#20808;&#39564;&#26465;&#20214;&#37325;&#26032;&#32452;&#32455;&#31354;&#38388;&#20449;&#24687;&#8212;&#8212;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20301;&#31227;&#19981;&#21464;&#24615;&#21644;&#36793;&#32536;&#30340;&#31232;&#30095;&#24615;,(2) i
&lt;/p&gt;
&lt;p&gt;
We propose WaveMix -- a novel neural architecture for computer vision that is resource-efficient yet generalizable and scalable. WaveMix networks achieve comparable or better accuracy than the state-of-the-art convolutional neural networks, vision transformers, and token mixers for several tasks, establishing new benchmarks for segmentation on Cityscapes; and for classification on Places-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix architectures require fewer parameters to achieve these benchmarks compared to the previous state-of-the-art. Moreover, when controlled for the number of parameters, WaveMix requires lesser GPU RAM, which translates to savings in time, cost, and energy. To achieve these gains we used multi-level two-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which has the following advantages: (1) It reorganizes spatial information based on three strong image priors -- scale-invariance, shift-invariance, and sparseness of edges, (2) i
&lt;/p&gt;</description></item><item><title>Anchors &#26159;&#19968;&#31181;&#21518;&#22788;&#29702;&#30340;&#35268;&#21017;&#24615;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20984;&#26174;&#20986;&#19968;&#20010;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#26469;&#24378;&#35843;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#65292;&#24182;&#36890;&#36807; TF-IDF &#21521;&#37327;&#21270;&#27493;&#39588;&#20197;&#21450;&#27169;&#22411;&#23618;&#27425;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20854;&#22312;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#27719;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#29992;&#20316; Anchors &#35789;&#27719;&#12290;</title><link>http://arxiv.org/abs/2205.13789</link><description>&lt;p&gt;
&#19968;&#29255;&#25991;&#23383;&#28023;&#65306;&#38024;&#23545;&#25991;&#26412;&#25968;&#25454;&#30340; Anchors &#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v2 [stat.ML] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13789
&lt;/p&gt;
&lt;p&gt;
Anchors &#26159;&#19968;&#31181;&#21518;&#22788;&#29702;&#30340;&#35268;&#21017;&#24615;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20984;&#26174;&#20986;&#19968;&#20010;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#26469;&#24378;&#35843;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#65292;&#24182;&#36890;&#36807; TF-IDF &#21521;&#37327;&#21270;&#27493;&#39588;&#20197;&#21450;&#27169;&#22411;&#23618;&#27425;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20854;&#22312;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#27719;&#21487;&#20197;&#37325;&#26032;&#21152;&#26435;&#29992;&#20316; Anchors &#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Anchors &#26159;&#19968;&#31181;&#22522;&#20110;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#24182;&#24378;&#35843;&#19968;&#23567;&#32452;&#35789;&#35821;&#65288;&#38170;&#28857;&#65289;&#65292;&#36825;&#20123;&#35789;&#35821;&#23384;&#22312;&#20110;&#25991;&#26723;&#20013;&#26102;&#65292;&#27169;&#22411;&#36755;&#20986;&#31867;&#20284;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545; Anchors &#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#32771;&#34385;&#21040;&#23547;&#25214;&#26368;&#20339;&#38170;&#28857;&#26159;&#35814;&#23613;&#30340;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#20998;&#31867;&#30340;&#31639;&#27861;&#24418;&#24335;&#21270;&#21518;&#65292;&#32467;&#21512;&#19981;&#21516;&#31867;&#21035;&#27169;&#22411;&#30340;&#26174;&#24335;&#32467;&#26524;&#65292;&#25506;&#31350;&#20102; Anchors &#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#20998;&#21035;&#35206;&#30422;&#20102;&#22522;&#26412; if-then &#35268;&#21017;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#36825;&#20004;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#39033;&#20998;&#26512;&#65292;&#27934;&#35265;&#20219;&#20309;&#21487;&#24494;&#20998;&#20998;&#31867;&#22120;&#30340; Anchors &#34892;&#20026;&#29305;&#24449;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#26368;&#39640;&#20559;&#23548;&#25968;&#25152;&#23545;&#24212;&#30340;&#35789;&#35821;&#65292;&#36890;&#36807;&#21453;&#21521;&#25991;&#20214;&#37325;&#26032;&#21152;&#26435;&#65292;&#21487;&#20197;&#20316;&#20026; Anchors &#35789;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anchors (Ribeiro et al., 2018) is a post-hoc, rule-based interpretability method. For text data, it proposes to explain a decision by highlighting a small set of words (an anchor) such that the model to explain has similar outputs when they are present in a document. In this paper, we present the first theoretical analysis of Anchors, considering that the search for the best anchor is exhaustive. After formalizing the algorithm for text classification, we present explicit results on different classes of models when the vectorization step is TF-IDF, and words are replaced by a fixed out-of-dictionary token when removed. Our inquiry covers models such as elementary if-then rules and linear classifiers. We then leverage this analysis to gain insights on the behavior of Anchors for any differentiable classifiers. For neural networks, we empirically show that the words corresponding to the highest partial derivatives of the model with respect to the input, reweighted by the inverse document
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#21333;&#35843;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#26500;&#24314;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#31934;&#24230;&#31526;&#21512;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2205.11775</link><description>&lt;p&gt;
&#21463;&#38480;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Constrained Monotonic Neural Networks. (arXiv:2205.11775v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#21333;&#35843;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#26500;&#24314;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#31934;&#24230;&#31526;&#21512;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#21487;&#20197;&#36924;&#36817;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20219;&#24847;&#20989;&#25968;&#65292;&#20294;&#22312;&#25512;&#24191;&#36807;&#31243;&#20013;&#38656;&#35201;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#24182;&#23545;&#23427;&#20204;&#26045;&#21152;&#39069;&#22806;&#30340;&#38480;&#21046;&#65292;&#20854;&#20013;&#21333;&#35843;&#24615;&#26159;&#26368;&#21463;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#23646;&#24615;&#20043;&#19968;&#65292;&#24182;&#19988;&#26159;&#35813;&#35770;&#25991;&#30340;&#37325;&#28857;&#12290;&#26368;&#26089;&#26500;&#24314;&#21333;&#35843;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26159;&#23558;&#20854;&#26435;&#37325;&#32422;&#26463;&#20026;&#38750;&#36127;&#65292;&#21516;&#26102;&#37319;&#29992;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26080;&#27861;&#19982;&#24120;&#29992;&#30340;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65292;ELU&#65292;SELU&#31561;&#65289;&#19968;&#36215;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#21482;&#33021;&#36924;&#36817;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#24314;&#31435;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#24230;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#65292;&#21516;&#26102;&#28385;&#36275;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to ot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#26469;&#22788;&#29702;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#21463;&#38480;&#31561;&#36317;&#24120;&#25968;&#30340;&#38480;&#21046;&#35201;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#21482;&#35201;&#26080;&#22122;&#22768;&#30446;&#26631;&#30340;&#21463;&#38480;&#31561;&#36317;&#26465;&#20214;&#23567;&#20110;1/3&#65292;&#20219;&#20309;&#38169;&#35823;&#30340;&#23616;&#37096;&#20248;&#21270;&#35299;&#24517;&#39035;&#25509;&#36817;&#20110;&#30495;&#23454;&#35299;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#25214;&#21040;&#36817;&#20284;&#35299;&#12290;</title><link>http://arxiv.org/abs/2203.03899</link><description>&lt;p&gt;
&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#20248;&#21270;: &#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Noisy Low-rank Matrix Optimization: Geometry of Local Minima and Convergence Rate. (arXiv:2203.03899v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#26469;&#22788;&#29702;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#21463;&#38480;&#31561;&#36317;&#24120;&#25968;&#30340;&#38480;&#21046;&#35201;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#21482;&#35201;&#26080;&#22122;&#22768;&#30446;&#26631;&#30340;&#21463;&#38480;&#31561;&#36317;&#26465;&#20214;&#23567;&#20110;1/3&#65292;&#20219;&#20309;&#38169;&#35823;&#30340;&#23616;&#37096;&#20248;&#21270;&#35299;&#24517;&#39035;&#25509;&#36817;&#20110;&#30495;&#23454;&#35299;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#25214;&#21040;&#36817;&#20284;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#20302;&#31209;&#30697;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#22312;&#30697;&#38453;&#24863;&#30693;&#29305;&#20363;&#20013;&#65292;&#35813;&#38382;&#39064;&#24050;&#32463;&#36890;&#36807;&#21463;&#38480;&#31561;&#36317;&#24615;&#36136;&#30340;&#27010;&#24565;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#22823;&#37327;&#20851;&#20110;&#38382;&#39064;&#30340;&#20960;&#20309;&#26223;&#35266;&#21644;&#24120;&#35265;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#32467;&#26524;&#20165;&#22312;&#21463;&#38480;&#31561;&#36317;&#24120;&#25968;&#25509;&#36817;&#20110;0&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22788;&#29702;&#20165;&#20855;&#26377;&#22122;&#22768;&#25968;&#25454;&#30340;&#19968;&#33324;&#30446;&#26631;&#20989;&#25968;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#23545;&#21463;&#38480;&#31561;&#36317;&#24120;&#25968;&#30340;&#38480;&#21046;&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#26080;&#22122;&#22768;&#30446;&#26631;&#30340;&#21463;&#38480;&#31561;&#36317;&#26465;&#20214;&#23567;&#20110;1/3&#65292;&#20219;&#20309;&#38169;&#35823;&#30340;&#23616;&#37096;&#20248;&#21270;&#35299;&#24517;&#39035;&#25509;&#36817;&#20110;&#30495;&#23454;&#35299;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#38797;&#28857;&#29305;&#24615;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#25214;&#21040;&#36817;&#20284;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with low-rank matrix optimization, which has found a wide range of applications in machine learning. This problem in the special case of matrix sensing has been studied extensively through the notion of Restricted Isometry Property (RIP), leading to a wealth of results on the geometric landscape of the problem and the convergence rate of common algorithms. However, the existing results can handle the problem in the case with a general objective function subject to noisy data only when the RIP constant is close to 0. In this paper, we develop a new mathematical framework to solve the above-mentioned problem with a far less restrictive RIP constant. We prove that as long as the RIP constant of the noiseless objective is less than $1/3$, any spurious local solution of the noisy optimization problem must be close to the ground truth solution. By working through the strict saddle property, we also show that an approximate solution can be found in polynomial time. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#22686;&#24378;&#32593;&#32476;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2202.03382</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Corrupted Image Modeling for Self-Supervised Visual Pre-Training. (arXiv:2202.03382v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#22686;&#24378;&#32593;&#32476;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#25439;&#22351;&#22270;&#20687;&#24314;&#27169;&#65292;&#20351;&#29992;&#19968;&#20010;&#36741;&#21161;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#23567;&#22411;&#30340;BEiT&#65288;Vision Transformer&#27169;&#22411;&#65289;&#65292;&#23558;&#36755;&#20837;&#30340;&#22270;&#20687;&#36827;&#34892;&#30772;&#22351;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20154;&#24037;&#30340;[MASK]&#20196;&#29260;&#65292;&#29983;&#25104;&#22120;&#22312;&#36755;&#20986;&#20998;&#24067;&#20013;&#37319;&#26679;&#24688;&#24403;&#30340;&#22791;&#36873;&#39033;&#29992;&#20110;&#26367;&#25442;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#20123;&#22270;&#20687;&#29255;&#27573;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#19968;&#20010;&#22686;&#24378;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#24674;&#22797;&#21407;&#22987;&#22270;&#20687;&#25110;&#39044;&#27979;&#27599;&#20010;&#35270;&#35273;&#20196;&#29260;&#26159;&#21542;&#34987;&#29983;&#25104;&#22120;&#37319;&#26679;&#26367;&#25442;&#12290;&#29983;&#25104;&#22120;&#21644;&#22686;&#24378;&#32593;&#32476;&#21516;&#26102;&#36827;&#34892;&#35757;&#32451;&#65292;&#21327;&#21516;&#26356;&#26032;&#12290;&#39044;&#35757;&#32451;&#21518;&#65292;&#22686;&#24378;&#32593;&#32476;&#21487;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23481;&#37327;&#35270;&#35273;&#32534;&#30721;&#22120;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;ViT&#21644;CNN&#21487;&#20197;&#20351;&#29992;&#32479;&#19968;&#30340;&#38750;&#23402;&#29983;&#26694;&#26550;&#23398;&#20064;&#20016;&#23500;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#26041;&#27861;&#22312;ImageNet&#12289;COIL-100&#21644;PASCAL VOC 2007&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our appro
&lt;/p&gt;</description></item><item><title>Laplacian2Mesh&#26159;&#19968;&#31181;&#20811;&#26381;&#19981;&#35268;&#21017;&#19977;&#35282;&#24418;&#32593;&#26684;&#32467;&#26500;&#22256;&#38590;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30452;&#25509;&#22788;&#29702;&#24418;&#29366;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#24863;&#21463;&#37326;&#21644;&#20010;&#21035;&#29305;&#24449;&#23398;&#20064;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2202.00307</link><description>&lt;p&gt;
Laplacian2Mesh&#65306;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#30340;&#32593;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Laplacian2Mesh: Laplacian-Based Mesh Understanding. (arXiv:2202.00307v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00307
&lt;/p&gt;
&lt;p&gt;
Laplacian2Mesh&#26159;&#19968;&#31181;&#20811;&#26381;&#19981;&#35268;&#21017;&#19977;&#35282;&#24418;&#32593;&#26684;&#32467;&#26500;&#22256;&#38590;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30452;&#25509;&#22788;&#29702;&#24418;&#29366;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#24863;&#21463;&#37326;&#21644;&#20010;&#21035;&#29305;&#24449;&#23398;&#20064;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#24341;&#36215;&#20102;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#22312;&#24418;&#29366;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#20852;&#36259;&#65292;&#22914;&#24418;&#29366;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#24403;&#36755;&#20837;&#20026;&#22810;&#36793;&#24418;&#34920;&#38754;&#26102;&#65292;&#20154;&#20204;&#24517;&#39035;&#24212;&#23545;&#19981;&#35268;&#21017;&#32593;&#26684;&#32467;&#26500;&#12290;&#21463;&#21040;&#20960;&#20309;&#35889;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;Laplacian2Mesh&#65292;&#19968;&#31181;&#26032;&#39062;&#19988;&#28789;&#27963;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#19981;&#35268;&#21017;&#30340;&#19977;&#35282;&#24418;&#32593;&#26684;&#65288;&#39030;&#28857;&#21487;&#33021;&#20855;&#26377;&#20219;&#20309;&#20215;&#20540;&#65289;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#32593;&#26684;&#34920;&#38754;&#26144;&#23556;&#21040;&#22810;&#32500;Laplacian-Beltrami&#31354;&#38388;&#65292;Laplacian2Mesh&#20351;&#20154;&#20204;&#33021;&#22815;&#30452;&#25509;&#20351;&#29992;&#25104;&#29087;&#30340;CNN&#25191;&#34892;&#24418;&#29366;&#20998;&#26512;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#22788;&#29702;&#32593;&#26684;&#32467;&#26500;&#30340;&#19981;&#35268;&#21017;&#36830;&#25509;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23450;&#20041;&#20102;&#19968;&#20010;&#32593;&#26684;&#27719;&#38598;&#25805;&#20316;&#65292;&#20197;&#20351;&#32593;&#32476;&#30340;&#24863;&#21463;&#37326;&#21487;&#20197;&#25193;&#23637;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#39030;&#28857;&#38598;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36947;&#32423;&#33258;&#25105;&#27880;&#24847;&#22359;&#26469;&#23398;&#20064;&#20010;&#21035;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning has sparked a rising interest in computer graphics to perform shape understanding tasks, such as shape classification and semantic segmentation. When the input is a polygonal surface, one has to suffer from the irregular mesh structure. Motivated by the geometric spectral theory, we introduce Laplacian2Mesh, a novel and flexible convolutional neural network (CNN) framework for coping with irregular triangle meshes (vertices may have any valence). By mapping the input mesh surface to the multi-dimensional Laplacian-Beltrami space, Laplacian2Mesh enables one to perform shape analysis tasks directly using the mature CNNs, without the need to deal with the irregular connectivity of the mesh structure. We further define a mesh pooling operation such that the receptive field of the network can be expanded while retaining the original vertex set as well as the connections between them. Besides, we introduce a channel-wise self-attention block to learn the individual im
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#32858;&#31867;&#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#20445;&#35777;&#36890;&#20449;&#36718;&#25968;&#36739;&#23569;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2201.13217</link><description>&lt;p&gt;
&#24555;&#36895;&#20998;&#24067;&#24335;k-Means&#31639;&#27861;&#65292;&#36890;&#20449;&#36718;&#25968;&#36739;&#23569;
&lt;/p&gt;
&lt;p&gt;
Fast Distributed k-Means with a Small Number of Rounds. (arXiv:2201.13217v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13217
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#32858;&#31867;&#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#20445;&#35777;&#36890;&#20449;&#36718;&#25968;&#36739;&#23569;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;k-Means&#31639;&#27861;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#30340;&#32858;&#31867;&#65292;&#24182;&#20445;&#35777;&#35823;&#24046;&#36817;&#20284;&#22240;&#23376;&#21644;&#36890;&#20449;&#36718;&#25968;&#20165;&#21462;&#20915;&#20110;&#21327;&#35843;&#32773;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#20869;&#32622;&#30340;&#20572;&#27490;&#26426;&#21046;&#65292;&#21487;&#20197;&#23613;&#21487;&#33021;&#22320;&#20943;&#23569;&#36890;&#20449;&#36718;&#25968;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;1-4&#36718;&#21363;&#21487;&#23436;&#25104;&#12290;&#19982;k-means||&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#21033;&#29992;&#26356;&#22823;&#30340;&#21327;&#35843;&#33021;&#21147;&#20197;&#33719;&#24471;&#26356;&#23569;&#30340;&#36718;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25152;&#24471;&#21040;&#30340;k-Means&#25104;&#26412;&#36890;&#24120;&#27604;k-means||&#31639;&#27861;&#26356;&#22909;&#65292;&#21363;&#20351;&#21518;&#32773;&#20801;&#35768;&#26356;&#22810;&#30340;&#36890;&#20449;&#36718;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#35201;&#27604;k-means ||&#30701;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for k-means clustering in a distributed setting, where the data is distributed across many machines, and a coordinator communicates with these machines to calculate the output clustering. Our algorithm guarantees a cost approximation factor and a number of communication rounds that depend only on the computational capacity of the coordinator. Moreover, the algorithm includes a built-in stopping mechanism, which allows it to use fewer communication rounds whenever possible. We show both theoretically and empirically that in many natural cases, indeed 1-4 rounds suffice. In comparison with the popular k-means|| algorithm, our approach allows exploiting a larger coordinator capacity to obtain a smaller number of rounds. Our experiments show that the k-means cost obtained by the proposed algorithm is usually better than the cost obtained by k-means||, even when the latter is allowed a larger number of rounds. Moreover, the machine running time in our approach is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#38750;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#26469;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20915;&#31574;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2201.05818</link><description>&lt;p&gt;
&#27979;&#37327;&#38750;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#65306;&#24050;&#30693;&#21644;&#26410;&#30693;&#26410;&#30693;&#30340;&#35748;&#30693;&#12289;&#36923;&#36753;&#21644;&#35745;&#31639;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Measuring Non-Probabilistic Uncertainty: A cognitive, logical and computational assessment of known and unknown unknowns. (arXiv:2201.05818v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#38750;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#26469;&#26816;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20915;&#31574;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#26080;&#27861;&#29992;&#27010;&#29575;&#35770;&#20805;&#20998;&#25551;&#36848;&#30340;&#21407;&#22240;&#26377;&#20004;&#20010;&#12290;&#31532;&#19968;&#20010;&#21407;&#22240;&#26159;&#30001;&#20110;&#29420;&#29305;&#25110;&#20960;&#20046;&#29420;&#29305;&#30340;&#20107;&#20214;&#65292;&#36825;&#20123;&#20107;&#20214;&#20174;&#26410;&#21457;&#29983;&#25110;&#21457;&#29983;&#24471;&#22826;&#23569;&#65292;&#20197;&#33267;&#20110;&#26080;&#27861;&#21487;&#38752;&#22320;&#27979;&#37327;&#39057;&#29575;&#12290;&#31532;&#20108;&#20010;&#21407;&#22240;&#26159;&#24403;&#20154;&#20204;&#25285;&#24515;&#21487;&#33021;&#21457;&#29983;&#26576;&#20123;&#20107;&#24773;&#65292;&#32780;&#33258;&#24049;&#29978;&#33267;&#26080;&#27861;&#24819;&#35937;&#65292;&#20363;&#22914;&#65306; "&#27668;&#20505;&#21464;&#21270;&#12289;&#37329;&#34701;&#21361;&#26426;&#12289;&#22823;&#27969;&#34892;&#12289;&#25112;&#20105;&#65292;&#19979;&#19968;&#20010;&#26159;&#20160;&#20040;&#65311;" &#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#35748;&#30693;&#22320;&#22270;&#23558;&#26368;&#32456;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30772;&#22351;&#20174;&#20855;&#20307;&#12289;&#21487;&#35782;&#21035;&#21644;&#24046;&#24322;&#21270;&#30340;&#26041;&#24335;&#24433;&#21709;&#21040;&#20225;&#19994;&#39640;&#31649;&#12289;&#21592;&#24037;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#19981;&#21516;&#35762;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#35832;&#22914;&#21672;&#35810;&#25253;&#21578;&#25110;&#21521;&#32929;&#19996;&#30340;&#20449;&#20989;&#31561;&#25991;&#26412;&#65292;&#20197;&#26816;&#27979;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#23545;&#36890;&#24120;&#25351;&#23548;&#20915;&#31574;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two reasons why uncertainty may not be adequately described by Probability Theory. The first one is due to unique or nearly-unique events, that either never realized or occurred too seldom for frequencies to be reliably measured. The second one arises when one fears that something may happen, that one is not even able to figure out, e.g., if one asks: "Climate change, financial crises, pandemic, war, what next?"  In both cases, simple one-to-one cognitive maps between available alternatives and possible consequences eventually melt down. However, such destructions reflect into the changing narratives of business executives, employees and other stakeholders in specific, identifiable and differential ways. In particular, texts such as consultants' reports or letters to shareholders can be analysed in order to detect the impact of both sorts of uncertainty onto the causal relations that normally guide decision-making.  We propose structural measures of cognitive maps as a means 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#25913;&#36827;&#20102;InfoMap&#31639;&#27861;&#65292;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#33410;&#28857;&#31038;&#21306;&#65292;&#27492;&#26041;&#27861;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#65292;&#31038;&#21306;&#32467;&#26500;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2112.10953</link><description>&lt;p&gt;
&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#30340;InfoMap&#31639;&#27861;&#22312;&#21560;&#25910;&#38543;&#26426;&#28459;&#27493;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs. (arXiv:2112.10953v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10953
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#25913;&#36827;&#20102;InfoMap&#31639;&#27861;&#65292;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#33410;&#28857;&#31038;&#21306;&#65292;&#27492;&#26041;&#27861;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#65292;&#31038;&#21306;&#32467;&#26500;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#26174;&#33879;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
InfoMap&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#19978;&#23494;&#38598;&#36830;&#25509;&#30340;&#8220;&#31038;&#21306;&#8221;&#33410;&#28857;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#20854;&#24212;&#29992;&#19982;&#21560;&#25910;&#38543;&#26426;&#28459;&#27493;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#21560;&#25910;&#27604;&#20363;&#32553;&#25918;&#22270;&#21644;&#39532;&#23572;&#21487;&#22827;&#26102;&#38388;&#25195;&#25551;&#26469;&#36866;&#24212;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#31227;&#38500;&#29575;&#30340;&#24773;&#20917;&#12290;&#25913;&#36827;&#21518;&#30340;InfoMap&#31639;&#27861;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#32467;&#26500;&#21487;&#33021;&#19982;&#19981;&#32771;&#34385;&#33410;&#28857;&#21560;&#25910;&#29575;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#24182;&#23545;&#26131;&#24863;-&#24863;&#26579;-&#24674;&#22797;&#65288;SI&#65289;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
InfoMap is a popular approach for detecting densely connected "communities" of nodes in networks. To detect such communities, InfoMap uses random walks and ideas from information theory. Motivated by the dynamics of disease spread on networks, whose nodes may have heterogeneous disease-removal rates, we adapt InfoMap to absorbing random walks. To do this, we use absorption-scaled graphs, in which the edge weights are scaled according to absorption rates, along with Markov time sweeping. One of our adaptations of InfoMap converges to the standard version of InfoMap in the limit in which the node-absorption rates approach $0$. The community structure that we obtain using our adaptations of InfoMap can differ markedly from the community structure that one detects using methods that do not take node-absorption rates into account. Additionally, we demonstrate that the community structure that is induced by local dynamics can have important implications for susceptible-infected-recovered (SI
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#27604;&#29305;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#19982;&#21442;&#25968;&#33539;&#22260;&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#28385;&#36275;&#26465;&#20214;&#30340;&#20989;&#25968;&#22343;&#21487;&#34987;&#36924;&#36817;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.09181</link><description>&lt;p&gt;
&#29992;&#19968;&#27604;&#29305;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Approximation of functions with one-bit neural networks. (arXiv:2112.09181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09181
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#27604;&#29305;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#19982;&#21442;&#25968;&#33539;&#22260;&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#28385;&#36275;&#26465;&#20214;&#30340;&#20989;&#25968;&#22343;&#21487;&#34987;&#36924;&#36817;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#22823;&#33268;&#38472;&#36848;&#20102;&#20219;&#20309;&#21512;&#29702;&#30340;&#20989;&#25968;&#37117;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#34987;&#36866;&#24403;&#36873;&#25321;&#30340;&#23454;&#25968;&#32593;&#32476;&#26469;&#20219;&#24847;&#36924;&#36817;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#27604;&#29305;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#20854;&#38750;&#38646;&#21442;&#25968;&#20026;&#22266;&#23450;&#20540; $&#177;a$&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#23450;&#29702;&#26174;&#31034;&#65292;&#23545;&#20110;&#20219;&#20309; $f \in C^s([0,1]^d)$&#65292;&#19988; $\|f\|_\infty &lt;1$ &#21644;&#35823;&#24046; $\varepsilon$&#65292;&#23384;&#22312; $f_{NN}$&#65292;&#20351;&#24471; $\boldsymbol{x}$ &#19981;&#22312; $[0,1]^d$ &#36793;&#30028;&#22788;&#26102;&#65292;$|f(\boldsymbol{x})-f_{NN}(\boldsymbol{x})|\leq \varepsilon$&#65292;&#19988;&#24403; $\varepsilon$ &#36235;&#36817;&#20110;&#38646;&#26102;&#65292;$f_{NN}$ &#21487;&#20197;&#26159;&#30001; $O(\varepsilon^{-2d/s})$ &#20010;&#21442;&#25968;&#30340; $\{\pm 1\}$ &#20108;&#27425;&#32593;&#32476;&#23454;&#29616;&#25110;&#26159;&#30001; $O(\varepsilon^{-2d/s}\log (1/\varepsilon))$ &#20010;&#21442;&#25968;&#30340; $\{\pm \frac 1 2 \}$ ReLU &#32593;&#32476;&#23454;&#29616;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36845;&#20195;&#30340;&#22810;&#20803; Bernstein &#31639;&#23376;&#30340;&#26032;&#36924;&#36817;&#32467;&#26524;&#65292;&#20197;&#21450;&#22122;&#22768;&#25972;&#24418;&#37327;&#21270;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The celebrated universal approximation theorems for neural networks roughly state that any reasonable function can be arbitrarily well-approximated by a network whose parameters are appropriately chosen real numbers. This paper examines the approximation capabilities of one-bit neural networks -- those whose nonzero parameters are $\pm a$ for some fixed $a\not=0$. One of our main theorems shows that for any $f\in C^s([0,1]^d)$ with $\|f\|_\infty&lt;1$ and error $\varepsilon$, there is a $f_{NN}$ such that $|f(\boldsymbol{x})-f_{NN}(\boldsymbol{x})|\leq \varepsilon$ for all $\boldsymbol{x}$ away from the boundary of $[0,1]^d$, and $f_{NN}$ is either implementable by a $\{\pm 1\}$ quadratic network with $O(\varepsilon^{-2d/s})$ parameters or a $\{\pm \frac 1 2 \}$ ReLU network with $O(\varepsilon^{-2d/s}\log (1/\varepsilon))$ parameters, as $\varepsilon\to0$. We establish new approximation results for iterated multivariate Bernstein operators, error estimates for noise-shaping quantization 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CoPE-KB&#27169;&#22411;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#22810;&#20219;&#21153;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26368;&#20248;&#31639;&#27861;CoopKernelFC&#21644;CoopKernelFB&#65292;&#25104;&#21151;&#22320;&#37327;&#21270;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#23545;&#23398;&#20064;&#21152;&#36895;&#24230;&#30340;&#24433;&#21709;&#24182;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2110.15771</link><description>&lt;p&gt;
&#26680;&#36172;&#21338;&#20013;&#30340;&#21327;&#20316;&#32431;&#25506;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Pure Exploration in Kernel Bandit. (arXiv:2110.15771v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CoPE-KB&#27169;&#22411;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#22810;&#20219;&#21153;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26368;&#20248;&#31639;&#27861;CoopKernelFC&#21644;CoopKernelFB&#65292;&#25104;&#21151;&#22320;&#37327;&#21270;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#23545;&#23398;&#20064;&#21152;&#36895;&#24230;&#30340;&#24433;&#21709;&#24182;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21327;&#20316;&#32431;&#25506;&#32034;&#22312;&#26680;&#36172;&#21338;&#20013;&#30340;&#38382;&#39064;&#65288;CoPE-KB&#65289;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#22810;&#20219;&#21153;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#24182;&#36866;&#29992;&#20110;&#35768;&#22810;&#22312;&#32447;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#32593;&#32476;&#35843;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;CoPE-KB&#30340;&#20004;&#31181;&#35774;&#32622;&#65292;&#21363;&#22266;&#23450;&#32622;&#20449;&#24230;&#65288;FC&#65289;&#21644;&#22266;&#23450;&#39044;&#31639;&#65288;FB&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#31181;&#26368;&#20248;&#31639;&#27861;CoopKernelFC&#65288;&#29992;&#20110;FC&#65289;&#21644;CoopKernelFB&#65288;&#29992;&#20110;FB&#65289;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37197;&#22791;&#20102;&#21019;&#26032;&#30340;&#39640;&#25928;&#20869;&#26680;&#20272;&#35745;&#22120;&#65292;&#20197;&#21516;&#26102;&#23454;&#29616;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#22312;&#32479;&#35745;&#21644;&#36890;&#20449;&#24230;&#37327;&#19979;&#24314;&#31435;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;&#29702;&#35770;&#30028;&#38480;&#25104;&#21151;&#22320;&#37327;&#21270;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#23545;&#23398;&#20064;&#21152;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20165;&#21462;&#20915;&#20110;&#20869;&#26680;&#21270;&#29305;&#24449;&#31354;&#38388;&#30340;&#26377;&#25928;&#32500;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#26512;&#25216;&#26415;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23558;&#21327;&#20316;&#32431;&#25506;&#32034;&#21644;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we formulate a Collaborative Pure Exploration in Kernel Bandit problem (CoPE-KB), which provides a novel model for multi-agent multi-task decision making under limited communication and general reward functions, and is applicable to many online learning tasks, e.g., recommendation systems and network scheduling. We consider two settings of CoPE-KB, i.e., Fixed-Confidence (FC) and Fixed-Budget (FB), and design two optimal algorithms CoopKernelFC (for FC) and CoopKernelFB (for FB). Our algorithms are equipped with innovative and efficient kernelized estimators to simultaneously achieve computation and communication efficiency. Matching upper and lower bounds under both the statistical and communication metrics are established to demonstrate the optimality of our algorithms. The theoretical bounds successfully quantify the influences of task similarities on learning acceleration and only depend on the effective dimension of the kernelized feature space. Our analytical techn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#19982;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#21644;&#25193;&#23637;&#27979;&#35797;&#19982;&#25351;&#26631;&#65292;&#20855;&#26377;&#39640;&#25928;&#20934;&#30830;&#21644;&#26377;&#25928;&#24615;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2109.09500</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis. (arXiv:2109.09500v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#19982;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#21644;&#25193;&#23637;&#27979;&#35797;&#19982;&#25351;&#26631;&#65292;&#20855;&#26377;&#39640;&#25928;&#20934;&#30830;&#21644;&#26377;&#25928;&#24615;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#12290;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#23558;Urban&#21644;Bauer&#65288;2021&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25193;&#23637;&#21040;&#39564;&#35777;&#24615;&#22240;&#32032;&#20998;&#26512;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22240;&#23376;&#36733;&#33655;&#21644;&#22240;&#23376;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#23545;&#20110;&#25311;&#21512;&#24230;&#26816;&#39564;&#65292;&#25105;&#20204;&#25506;&#32034;&#22522;&#20110;&#27169;&#25311;&#30340;&#27979;&#35797;&#21644;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#20998;&#31867;&#22120;&#20004;&#20010;&#26679;&#26412;&#27979;&#35797;&#65288;C2ST&#65289;&#65292;&#35813;&#26041;&#27861;&#27979;&#35797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#21306;&#20998;&#26469;&#33258;&#25311;&#21512;&#30340;IFA&#27169;&#22411;&#30340;&#35266;&#27979;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#25193;&#23637;&#21253;&#25324;&#36817;&#20284;&#25311;&#21512;&#26816;&#39564;&#65292;&#20854;&#20013;&#29992;&#25143;&#25351;&#23450;&#35266;&#27979;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20013;&#24212;&#26377;&#22810;&#23569;&#21344;&#21487;&#21306;&#20998;&#37096;&#20998;&#30340;&#30334;&#20998;&#27604;&#65292;&#20197;&#21450;&#30456;&#23545;&#25311;&#21512;&#25351;&#25968;&#65288;RFI&#65289;&#65292;&#35813;&#25351;&#25968;&#31867;&#20284;&#20110;&#32467;&#26500;&#26041;&#31243;&#24314;&#27169;&#20013;&#20351;&#29992;&#30340;RFI&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;1&#65289;Urban&#21644;Bauer&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#25193;&#23637;&#21363;&#20351;&#23384;&#22312;&#39640;&#30456;&#20851;&#22240;&#23376;&#20063;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#65307;&#65288;2&#65289;&#25152;&#25552;&#20986;&#30340;&#25311;&#21512;&#24230;&#25351;&#26631;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#27169;&#22411;&#19981;&#33391;&#25311;&#21512;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#39564;&#35777;IFA&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate novel parameter estimation and goodness-of-fit (GOF) assessment methods for large-scale confirmatory item factor analysis (IFA) with many respondents, items, and latent factors. For parameter estimation, we extend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to the confirmatory setting by showing how to handle constraints on loadings and factor correlations. For GOF assessment, we explore simulation-based tests and indices that extend the classifier two-sample test (C2ST), a method that tests whether a deep neural network can distinguish between observed data and synthetic data sampled from a fitted IFA model. Proposed extensions include a test of approximate fit wherein the user specifies what percentage of observed and synthetic data should be distinguishable as well as a relative fit index (RFI) that is similar in spirit to the RFIs used in structural equation modeling. Via simulation studies, we show that: (1) the confirmatory extension of Urb
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.01636</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01636
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NER&#20219;&#21153;&#38754;&#20020;&#30340;&#26368;&#22823;&#22256;&#38590;&#26159;&#21363;&#20351;&#22312;NE&#31867;&#22411;&#21644;&#25991;&#26723;&#19981;&#29087;&#24713;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201;&#20445;&#25345;&#21487;&#26816;&#27979;&#24615;&#12290;&#24847;&#35782;&#21040;&#29305;&#23450;&#24615;&#20449;&#24687;&#21487;&#33021;&#21253;&#21547;&#21333;&#35789;&#30340;&#28508;&#22312;&#21547;&#20041;&#24182;&#29983;&#25104;&#35789;&#23884;&#20837;&#30340;&#35821;&#20041;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;&#29616;&#26377;&#30340;NER&#26041;&#27861;&#20013;&#65292;NER&#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#24050;&#30693;&#39640;&#28201;&#21513;&#24067;&#26031;&#24577;&#20013;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.04842</link><description>&lt;p&gt;
&#20174;&#39640;&#28201;&#21513;&#24067;&#26031;&#24577;&#20013;&#26368;&#20248;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;
&lt;/p&gt;
&lt;p&gt;
Optimal learning of quantum Hamiltonians from high-temperature Gibbs states. (arXiv:2108.04842v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.04842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#24050;&#30693;&#39640;&#28201;&#21513;&#24067;&#26031;&#24577;&#20013;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#21704;&#23494;&#39039;&#37327;$H$&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#24050;&#30693;&#21453;&#28201;&#24230;$\beta$&#19979;&#33719;&#24471;&#20854;&#21513;&#24067;&#26031;&#24577;$\rho=\exp(-\beta H) / \operatorname{Tr}(\exp(-\beta H))$&#30340;&#22797;&#21046;&#21697;&#65292;&#24182;&#26399;&#26395;&#31934;&#24230;&#20026;$\varepsilon$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#19968;&#33324;&#31867;&#30340;&#21704;&#23494;&#39039;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21704;&#23494;&#39039;&#37327;&#30340;&#31995;&#25968;&#65292;&#24182;&#20445;&#35777;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;$S = O(\log N/(\beta\varepsilon)^{2})$&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;$O(S N)$&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#20026;$\varepsilon$&#12290;&#27492;&#22806;&#65292;&#36824;&#35777;&#26126;&#20102;&#19982;&#25105;&#20204;&#30340;&#31639;&#27861;&#26679;&#26412;&#22797;&#26434;&#24230;&#30456;&#21305;&#37197;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning a Hamiltonian $H$ to precision $\varepsilon$, supposing we are given copies of its Gibbs state $\rho=\exp(-\beta H)/\operatorname{Tr}(\exp(-\beta H))$ at a known inverse temperature $\beta$. Anshu, Arunachalam, Kuwahara, and Soleimanifar (Nature Physics, 2021, arXiv:2004.07266) recently studied the sample complexity (number of copies of $\rho$ needed) of this problem for geometrically local $N$-qubit Hamiltonians. In the high-temperature (low $\beta$) regime, their algorithm has sample complexity poly$(N, 1/\beta,1/\varepsilon)$ and can be implemented with polynomial, but suboptimal, time complexity.  In this paper, we study the same question for a more general class of Hamiltonians. We show how to learn the coefficients of a Hamiltonian to error $\varepsilon$ with sample complexity $S = O(\log N/(\beta\varepsilon)^{2})$ and time complexity linear in the sample size, $O(S N)$. Furthermore, we prove a matching lower bound showing that our algorithm's sam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#27010;&#24565;&#65292;&#25193;&#23637;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#20803;&#20998;&#31867;&#22120;&#32473;&#20986;&#30340;&#32452;&#25104;&#30693;&#35782;&#33258;&#28982;&#22320;&#23545;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#31034;&#20363;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#25104;&#23545;&#20851;&#31995;&#23398;&#20064;&#35753;&#32452;&#21512;&#23884;&#20837;&#25152;&#25552;&#20379;&#30340;&#34920;&#31034;&#26356;&#21152;&#24378;&#20581;&#12290;</title><link>http://arxiv.org/abs/2106.15278</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#23884;&#20837;&#36827;&#34892;&#24320;&#25918;&#38598;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#27010;&#24565;&#65292;&#25193;&#23637;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#20803;&#20998;&#31867;&#22120;&#32473;&#20986;&#30340;&#32452;&#25104;&#30693;&#35782;&#33258;&#28982;&#22320;&#23545;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#31034;&#20363;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#25104;&#23545;&#20851;&#31995;&#23398;&#20064;&#35753;&#32452;&#21512;&#23884;&#20837;&#25152;&#25552;&#20379;&#30340;&#34920;&#31034;&#26356;&#21152;&#24378;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21097;&#20313;&#31867;&#21035;&#30340;&#26631;&#31614;&#19981;&#21487;&#29992;&#65292;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#38480;&#20110;&#22788;&#29702;&#19968;&#23567;&#37096;&#20998;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#31034;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#35782;&#21035;&#25193;&#23637;&#21040;&#24050;&#30693;&#21644;&#26032;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#22810;&#20010;&#24322;&#36136;&#26631;&#31614;&#31354;&#38388;&#30340;&#30417;&#30563;&#20803;&#20998;&#31867;&#22120;&#32473;&#20986;&#30340;&#32452;&#25104;&#30693;&#35782;&#33258;&#28982;&#22320;&#23545;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#31034;&#20363;&#36827;&#34892;&#32858;&#31867;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#25104;&#23545;&#20851;&#31995;&#23398;&#20064;&#65292;&#32452;&#21512;&#23884;&#20837;&#25152;&#25552;&#20379;&#30340;&#34920;&#31034;&#26356;&#21152;&#24378;&#20581;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26469;&#21457;&#29616;&#26032;&#27010;&#24565;&#65292;&#20197;&#22686;&#24378;&#26410;&#35265;&#31867;&#21035;&#30340;&#21306;&#20998;&#24615;&#65292;&#24182;&#23398;&#20064;&#23545;&#26032;&#31867;&#21035;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#24050;&#30693;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24320;&#25918;&#38598;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#34920;&#26126;&#20102;&#22312;&#21457;&#29616;&#26032;&#27010;&#24565;&#21644;&#25552;&#39640;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#30340;&#35782;&#21035;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on both labeled and unlabeled examples, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. The representations given by the combinatorial embedding are made more robust by unsupervised pairwise relation learning. The proposed algorithm discovers novel concepts via a joint optimization for enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#30693;&#35782;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26242;&#24577;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#30452;&#25509;&#23558;&#30005;&#21147;&#31995;&#32479;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2106.13638</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26242;&#24577;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Transient Stability Analysis with Physics-Informed Neural Networks. (arXiv:2106.13638v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.13638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#30693;&#35782;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26242;&#24577;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#35813;&#26041;&#27861;&#30452;&#25509;&#23558;&#30005;&#21147;&#31995;&#32479;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#22823;&#24133;&#21152;&#36895;&#25511;&#21046;&#30005;&#21147;&#31995;&#32479;&#21160;&#24577;&#30340;&#24120;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#27714;&#35299;&#12290;&#22312;&#36827;&#34892;&#26242;&#24577;&#31283;&#23450;&#24615;&#35745;&#31639;&#26041;&#38754;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#36127;&#36733;&#36739;&#22823;&#12289;&#27169;&#22411;&#31616;&#21270;&#12289;&#25110;&#20351;&#29992;&#36807;&#24230;&#20445;&#23432;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22238;&#36991;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#24573;&#30053;&#20102;&#28508;&#22312;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#29289;&#29702;&#30693;&#35782;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65306;&#23427;&#20204;&#23558;&#30005;&#21147;&#31995;&#32479;&#30340;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30452;&#25509;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#29289;&#29702;&#30693;&#35782;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#31995;&#32479;&#26242;&#24577;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31243;&#24207;&#65292;&#20197;&#20419;&#36827;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the possibility to use physics-informed neural networks to drastically accelerate the solution of ordinary differential-algebraic equations that govern the power system dynamics. When it comes to transient stability assessment, the traditionally applied methods either carry a significant computational burden, require model simplifications, or use overly conservative surrogate models. Conventional neural networks can circumvent these limitations but are faced with high demand of high-quality training datasets, while they ignore the underlying governing equations. Physics-informed neural networks are different: they incorporate the power system differential algebraic equations directly into the neural network training and drastically reduce the need for training data. This paper takes a deep dive into the performance of physics-informed neural networks for power system transient stability assessment. Introducing a new neural network training procedure to facilitate a thorough 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36816;&#29992;&#25968;&#23398;&#21644;&#20248;&#21270;&#29702;&#35770;&#26041;&#27861;&#65292;&#23601; ReLU &#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#19979;&#30028;&#20570;&#20102;&#25506;&#31350;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#32593;&#32476;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#32943;&#23450;&#20102;&#19968;&#39033;&#26087;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2105.14835</link><description>&lt;p&gt;
&#20851;&#20110; ReLU &#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#19979;&#30028;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Lower Bounds on the Depth of ReLU Neural Networks. (arXiv:2105.14835v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36816;&#29992;&#25968;&#23398;&#21644;&#20248;&#21270;&#29702;&#35770;&#26041;&#27861;&#65292;&#23601; ReLU &#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#19979;&#30028;&#20570;&#20102;&#25506;&#31350;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#32593;&#32476;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#32943;&#23450;&#20102;&#19968;&#39033;&#26087;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36816;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#12289;&#22810;&#38754;&#20307;&#29702;&#35770;&#21644;&#28909;&#24102;&#20960;&#20309;&#23398;&#31561;&#25216;&#26415;&#65292;&#20026;&#29702;&#35299;&#20855;&#26377; ReLU &#28608;&#27963;&#21644;&#32473;&#23450;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#20570;&#20986;&#20102;&#26356;&#22909;&#30340;&#36129;&#29486;&#12290;&#23613;&#31649;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#35748;&#20026;&#21333;&#23618;&#38544;&#34255;&#23618;&#23601;&#36275;&#20197;&#23398;&#20064;&#20219;&#20309;&#20989;&#25968;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#28155;&#21152;&#26356;&#22810;&#23618;&#65288;&#26080;&#22823;&#23567;&#38480;&#21046;&#65289;&#26102;&#26159;&#21542;&#20005;&#26684;&#22686;&#21152;&#20102;&#21487;&#34920;&#31034;&#20989;&#25968;&#30340;&#31867;&#12290;&#20316;&#20026;&#30740;&#31350;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#32943;&#23450;&#20102; Wang &#21644; Sun&#65288;2005&#65289;&#26377;&#20851;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#19968;&#20010;&#26087;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#20989;&#25968;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to a better understanding of the class of functions that can be represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning any function. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). As a by-product of our investigations, we settle an old conjecture about piecewise linear functions by Wang and Sun (2005) in the affirmative. We also present upper bounds on the sizes of neural networks required to represent functions with logarithmic depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#38169;&#35823;&#23450;&#20301;&#20960;&#20309;&#20998;&#26512;&#31639;&#27861;&#21644;&#36830;&#32493;&#23376;&#31354;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#21270;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#32422;&#26463;&#31561;&#24322;&#24615;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20840;&#23616;&#26368;&#20248;&#35299;&#19982;&#23616;&#37096;&#35299;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2105.08232</link><description>&lt;p&gt;
&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#30340;&#20960;&#20309;&#20998;&#26512;&#22312;&#31934;&#30830;&#21442;&#25968;&#21270;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#21306;&#38388;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes. (arXiv:2105.08232v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#38169;&#35823;&#23450;&#20301;&#20960;&#20309;&#20998;&#26512;&#31639;&#27861;&#21644;&#36830;&#32493;&#23376;&#31354;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#31934;&#30830;&#21442;&#25968;&#21270;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#32422;&#26463;&#31561;&#24322;&#24615;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20840;&#23616;&#26368;&#20248;&#35299;&#19982;&#23616;&#37096;&#35299;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#20302;&#31209;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#30697;&#38453;&#34917;&#20840;&#12289;&#30456;&#20301;&#21516;&#27493;/&#24674;&#22797;&#12289;&#31283;&#20581;PCA&#21644;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#31561;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32447;&#24615;&#27979;&#37327;&#25439;&#22351;&#30340;&#22122;&#22768;&#20302;&#31209;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25628;&#32034;&#31209;r&#31561;&#20110;&#26410;&#30693;&#30495;&#23454;&#31209;r*&#30340;&#24773;&#20917;&#65288;&#31934;&#30830;&#21442;&#25968;&#21270;&#24773;&#20917;&#65289;&#65292;&#20197;&#21450;r&#22823;&#20110;r*&#30340;&#24773;&#20917;&#65288;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#65289;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#32422;&#26463;&#31561;&#24322;&#24615;&#24615;&#36136;&#65288;restricted isometry property&#65292;RIP&#65289;&#22312;&#22609;&#36896;&#38750;&#20984;&#20998;&#35299;&#20844;&#24335;&#30340;&#25972;&#20307;&#26223;&#35266;&#21644;&#24110;&#21161;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#25104;&#21151;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;RIP&#24120;&#25968;&#23567;&#20110; 1/(1+sqrt(r*/r))&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#38750;&#20984;&#38382;&#39064;&#30340;&#20219;&#24847;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#36827;&#34892;&#20102;&#20840;&#23616;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#38169;&#35823;&#23450;&#20301;&#20960;&#20309;&#20998;&#26512;&#65288;Robust Error-Locating Geometric Analysis&#65292;RELGA&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#12290;RELGA&#31639;&#27861;&#36890;&#36807;&#32452;&#21512;&#38169;&#35823;&#23450;&#20301;&#26426;&#21046;&#21644;&#20960;&#20309;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#22122;&#22768;&#27700;&#24179;&#30456;&#23545;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#30697;&#38453;&#24674;&#22797;&#12290;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#36830;&#32493;&#23376;&#31354;&#38388;&#20248;&#21270;&#65288;Successive Subspace Optimization&#65292;SSO&#65289;&#31639;&#27861;&#65292;&#22312;&#22122;&#22768;&#27700;&#24179;&#21644;RIP&#24120;&#25968;&#30340;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#30495;&#23454;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SSO&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#21021;&#22987;&#21270;&#12289;&#38750;&#36864;&#21270;&#24615;&#21644;&#20960;&#20309;&#26465;&#20214;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The matrix sensing problem is an important low-rank optimization problem that has found a wide range of applications, such as matrix completion, phase synchornization/retrieval, robust PCA, and power system state estimation. In this work, we focus on the general matrix sensing problem with linear measurements that are corrupted by random noise. We investigate the scenario where the search rank $r$ is equal to the true rank $r^*$ of the unknown ground truth (the exact parametrized case), as well as the scenario where $r$ is greater than $r^*$ (the overparametrized case). We quantify the role of the restricted isometry property (RIP) in shaping the landscape of the non-convex factorized formulation and assisting with the success of local search algorithms. First, we develop a global guarantee on the maximum distance between an arbitrary local minimizer of the non-convex problem and the ground truth under the assumption that the RIP constant is smaller than $1/(1+\sqrt{r^*/r})$. We then p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Bayesian&#20572;&#26102;&#38382;&#39064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#24494;&#35266;&#32463;&#27982;&#23398;&#20013;&#30340;Bayesian&#25581;&#31034;&#20559;&#22909;&#24605;&#36335;&#65292;&#36890;&#36807;&#35266;&#23519;Bayesian&#20915;&#31574;&#32773;&#30340;&#34892;&#21160;&#65292;&#30830;&#23450;&#20854;&#30340;&#26368;&#20248;&#24615;&#12290;&#24182;&#19988;&#36890;&#36807;&#20004;&#20010;&#20572;&#26102;&#38382;&#39064;&#31034;&#20363;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#24050;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20363;&#23376;&#20013;&#24471;&#21040;&#20102;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2007.03481</link><description>&lt;p&gt;
&#12298;Bayesian&#20572;&#26102;&#38382;&#39064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12299;
&lt;/p&gt;
&lt;p&gt;
Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. (arXiv:2007.03481v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.03481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Bayesian&#20572;&#26102;&#38382;&#39064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#24494;&#35266;&#32463;&#27982;&#23398;&#20013;&#30340;Bayesian&#25581;&#31034;&#20559;&#22909;&#24605;&#36335;&#65292;&#36890;&#36807;&#35266;&#23519;Bayesian&#20915;&#31574;&#32773;&#30340;&#34892;&#21160;&#65292;&#30830;&#23450;&#20854;&#30340;&#26368;&#20248;&#24615;&#12290;&#24182;&#19988;&#36890;&#36807;&#20004;&#20010;&#20572;&#26102;&#38382;&#39064;&#31034;&#20363;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#24050;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20363;&#23376;&#20013;&#24471;&#21040;&#20102;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Bayesian&#20572;&#26102;&#38382;&#39064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#35266;&#23519;Bayesian&#20915;&#31574;&#32773;&#30340;&#34892;&#21160;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#26465;&#20214;&#26469;&#30830;&#23450;&#36825;&#20123;&#34892;&#21160;&#26159;&#21542;&#19982;&#20248;&#21270;&#25104;&#26412;&#20989;&#25968;&#19968;&#33268;&#12290;&#22312;Bayesian&#65288;&#37096;&#20998;&#35266;&#23519;&#65289;&#24773;&#20917;&#19979;&#65292;&#36870;&#21521;&#23398;&#20064;&#32773;&#33021;&#22815;&#26368;&#22909;&#22320;&#30830;&#23450;&#38024;&#23545;&#35266;&#23519;&#21040;&#30340;&#31574;&#30053;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;IRL&#31639;&#27861;&#30830;&#23450;&#26368;&#20248;&#24615;&#65292;&#28982;&#21518;&#26500;&#24314;&#25104;&#26412;&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#26159;&#19968;&#20010;&#38598;&#21512;&#20540;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#19968;&#20010;IRL&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;&#24494;&#35266;&#32463;&#27982;&#23398;&#30340;Bayesian&#25581;&#31034;&#20559;&#22909;&#30340;&#26032;&#24605;&#36335;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#37325;&#35201;&#30340;&#20572;&#26102;&#38382;&#39064;&#31034;&#20363;&#65292;&#21363;&#65292;&#39034;&#24207;&#20551;&#35774;&#26816;&#39564;&#21644;Bayesian&#25628;&#32034;&#65292;&#35828;&#26126;&#20102;&#25152;&#25552;&#35758;&#30340;IRL&#26041;&#26696;&#12290;&#20316;&#20026;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;190000&#20010;&#35270;&#39057;&#30340;&#20803;&#25968;&#25454;&#30340;YouTube&#25968;&#25454;&#38598;&#35828;&#26126;&#20102;&#25152;&#25552;&#35758;&#30340;IRL&#26041;&#27861;&#22914;&#20309;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#22312;&#32447;&#22810;&#23186;&#20307;&#24179;&#21488;&#20013;&#29992;&#25143;&#30340;&#21442;&#19982;&#24230;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#35813;&#31639;&#27861;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#20570;&#20102;&#26368;&#21518;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an inverse reinforcement learning~(IRL) framework for Bayesian stopping time problems. By observing the actions of a Bayesian decision maker, we provide a necessary and sufficient condition to identify if these actions are consistent with optimizing a cost function. In a Bayesian (partially observed) setting, the inverse learner can at best identify optimality wrt the observed strategies. Our IRL algorithm identifies optimality and then constructs set-valued estimates of the cost function.To achieve this IRL objective, we use novel ideas from Bayesian revealed preferences stemming from microeconomics. We illustrate the proposed IRL scheme using two important examples of stopping time problems, namely, sequential hypothesis testing and Bayesian search. As a real-world example, we illustrate using a YouTube dataset comprising metadata from 190000 videos how the proposed IRL method predicts user engagement in online multimedia platforms with high accuracy. Finally, for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;Spectral-CUSUM&#65292;&#21487;&#29992;&#20110;&#20174;&#22024;&#26434;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#26816;&#27979;&#32593;&#32476;&#31038;&#21306;&#32467;&#26500;&#30340;&#31361;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/1910.09083</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#35889;CUSUM&#30340;&#22312;&#32447;&#32593;&#32476;&#32467;&#26500;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spectral CUSUM for Online Network Structure Change Detection. (arXiv:1910.09083v8 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.09083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;Spectral-CUSUM&#65292;&#21487;&#29992;&#20110;&#20174;&#22024;&#26434;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#26816;&#27979;&#32593;&#32476;&#31038;&#21306;&#32467;&#26500;&#30340;&#31361;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22024;&#26434;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#26816;&#27979;&#32593;&#32476;&#31038;&#21306;&#32467;&#26500;&#30340;&#31361;&#21464;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral-CUSUM&#30340;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#24191;&#20041;&#20284;&#28982;&#27604;&#32479;&#35745;&#37327;&#26816;&#27979;&#26410;&#30693;&#32593;&#32476;&#32467;&#26500;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;Spectral-CUSUM&#31243;&#24207;&#30340;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#65288;ARL&#65289;&#21644;&#39044;&#26399;&#26816;&#27979;&#24310;&#36831;&#65288;EDD&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20363;&#23376;&#28436;&#31034;&#20102;Spectral-CUSUM&#31243;&#24207;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#20351;&#29992;&#20256;&#24863;&#22120;&#32593;&#32476;&#25968;&#25454;&#36827;&#34892;&#22320;&#38663;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting abrupt changes in the community structure of a network from noisy observations is a fundamental problem in statistics and machine learning. This paper presents an online change detection algorithm called Spectral-CUSUM to detect unknown network structure changes through a generalized likelihood ratio statistic. We characterize the average run length (ARL) and the expected detection delay (EDD) of the Spectral-CUSUM procedure and prove its asymptotic optimality. Finally, we demonstrate the good performance of the Spectral-CUSUM procedure and compare it with several baseline methods using simulations and real data examples on seismic event detection using sensor network data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#38590;&#20197;&#22312;&#32447;&#39034;&#24207;&#26356;&#26032;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#25311;&#21512;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#36136;&#30340;&#20989;&#25968;&#12290;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1905.10003</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#38750;&#24179;&#31283;&#20989;&#25968;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Sequential Gaussian Processes for Online Learning of Nonstationary Functions. (arXiv:1905.10003v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#30340;&#36830;&#32493;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#38590;&#20197;&#22312;&#32447;&#39034;&#24207;&#26356;&#26032;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20801;&#35768;&#25311;&#21512;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#36136;&#30340;&#20989;&#25968;&#12290;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#22312;&#20272;&#35745;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#20013;&#24471;&#21040;&#35299;&#20915;&#65292;&#36890;&#24120;&#36825;&#20123;&#20989;&#25968;&#26159;&#26102;&#38388;&#30456;&#20851;&#30340;&#20989;&#25968;&#65292;&#24182;&#19988;&#26159;&#23454;&#26102;&#22320;&#38543;&#30528;&#35266;&#27979;&#30340;&#21040;&#26469;&#32780;&#20272;&#35745;&#30340;&#12290;&#39640;&#26031;&#36807;&#31243;&#26159;&#24314;&#27169;&#23454;&#20540;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#23384;&#22312;&#33509;&#24178;&#19981;&#36275;&#65306;1&#65289;&#20256;&#32479;&#39640;&#26031;&#36807;&#31243;&#25512;&#26029;&#30340;&#22797;&#26434;&#24230;$O(N^{3})$&#38543;&#30528;&#35266;&#27979;&#20540;&#30340;&#20010;&#25968;N&#25104;&#22686;&#38271;&#65307;2&#65289;&#36880;&#27493;&#26356;&#26032;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#19981;&#23481;&#26131;&#65307;3&#65289;&#21327;&#26041;&#24046;&#26680;&#36890;&#24120;&#23545;&#20989;&#25968;&#26045;&#21152;&#24179;&#31283;&#24615;&#32422;&#26463;&#65292;&#32780;&#20855;&#26377;&#38750;&#24179;&#31283;&#21327;&#26041;&#24046;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#36890;&#24120;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#26469;&#25311;&#21512;&#26080;&#38480;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#65292;&#20197;&#25429;&#25417;&#38750;&#24179;&#31283;&#34892;&#20026;&#65292;&#21516;&#26102;&#20801;&#35768;&#22312;&#32447;&#12289;&#20998;&#24067;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$ with respect to the number of observations; 2) Updating a GP model sequentially is not trivial; and 3) Covariance kernels typically enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose a sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods fo
&lt;/p&gt;</description></item></channel></rss>