<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#22238;&#24402;&#21644;&#25193;&#25955;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;CO3D&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14817</link><description>&lt;p&gt;
&#25668;&#20687;&#22836;&#20316;&#20026;&#23556;&#32447;: &#36890;&#36807;&#23556;&#32447;&#25193;&#25955;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Cameras as Rays: Pose Estimation via Ray Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#22238;&#24402;&#21644;&#25193;&#25955;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;CO3D&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#30456;&#26426;&#23039;&#21183;&#26159;3D&#37325;&#24314;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#37492;&#20110;&#35270;&#22270;&#31232;&#30095;&#65288;&lt;10&#65289;&#65292;&#35813;&#20219;&#21153;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#36861;&#27714;&#30456;&#26426;&#22806;&#21442;&#30340;&#20840;&#23616;&#21442;&#25968;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30456;&#26426;&#23039;&#21183;&#35270;&#20026;&#23556;&#32447;&#26463;&#30340;&#20998;&#24067;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20801;&#35768;&#19982;&#31354;&#38388;&#22270;&#20687;&#29305;&#24449;&#32039;&#23494;&#32806;&#21512;&#65292;&#25552;&#39640;&#20102;&#23039;&#21183;&#31934;&#24230;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#31181;&#34920;&#31034;&#33258;&#28982;&#36866;&#29992;&#20110;&#38598;&#21512;&#32423;&#21035;&#30340;Transformer&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#22359;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#23556;&#32447;&#19978;&#12290;&#20026;&#20102;&#25429;&#25417;&#31232;&#30095;&#35270;&#35282;&#23039;&#21183;&#25512;&#26029;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37319;&#26679;&#21512;&#29702;&#30340;&#27169;&#24335;&#65292;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26082;&#26159;&#22522;&#20110;&#22238;&#24402;&#65292;&#20063;&#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#65292;&#22312;CO3D&#30456;&#26426;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14817v1 Announce Type: cross  Abstract: Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (&lt;10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20840;&#29699;&#20116;&#20010;&#25968;&#25454;&#38598;&#20013;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#33016;&#29255;&#35786;&#26029;&#20013;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19982;&#33891;&#20107;&#20250;&#35748;&#35777;&#30340;&#25918;&#23556;&#31185;&#21307;&#24072;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#35786;&#26029;&#36793;&#32536;&#21270;&#32676;&#20307;&#26102;&#19968;&#36143;&#23384;&#22312;&#20302;&#35786;&#26029;&#29575;&#65292;&#29978;&#33267;&#22312;&#35832;&#22914;&#40657;&#20154;&#22899;&#24615;&#20043;&#31867;&#30340;&#20132;&#21449;&#20122;&#32452;&#20013;&#30475;&#21040;&#26356;&#39640;&#30340;&#27604;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.14815</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#19987;&#23478;&#32423;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20840;&#29699;&#20116;&#20010;&#25968;&#25454;&#38598;&#20013;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#33016;&#29255;&#35786;&#26029;&#20013;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19982;&#33891;&#20107;&#20250;&#35748;&#35777;&#30340;&#25918;&#23556;&#31185;&#21307;&#24072;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#35786;&#26029;&#36793;&#32536;&#21270;&#32676;&#20307;&#26102;&#19968;&#36143;&#23384;&#22312;&#20302;&#35786;&#26029;&#29575;&#65292;&#29978;&#33267;&#22312;&#35832;&#22914;&#40657;&#20154;&#22899;&#24615;&#20043;&#31867;&#30340;&#20132;&#21449;&#20122;&#32452;&#20013;&#30475;&#21040;&#26356;&#39640;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24050;&#32463;&#22312;&#21307;&#23398;&#24433;&#20687;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#26126;&#30830;&#22521;&#35757;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#24191;&#27867;&#30340;&#30149;&#29702;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19981;&#21453;&#26144;&#25110;&#25918;&#22823;&#20154;&#31867;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#20351;&#22899;&#24615;&#25110;&#40657;&#20154;&#24739;&#32773;&#31561;&#21382;&#21490;&#19978;&#34987;&#36793;&#32536;&#21270;&#30340;&#32676;&#20307;&#22788;&#20110;&#19981;&#21033;&#22320;&#20301;&#12290;&#36825;&#31181;&#20559;&#35265;&#30340;&#20307;&#29616;&#21487;&#33021;&#20250;&#31995;&#32479;&#24615;&#22320;&#24310;&#36831;&#29305;&#23450;&#24739;&#32773;&#20122;&#32452;&#30340;&#37325;&#35201;&#21307;&#30103;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14815v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black fem
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#24433;&#21709;&#23454;&#20307;&#36319;&#36394;&#31561;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#33021;&#22815;&#22312;&#25968;&#23398;&#20219;&#21153;&#19978;&#23454;&#29616;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.14811</link><description>&lt;p&gt;
&#24494;&#35843;&#22686;&#24378;&#29616;&#26377;&#26426;&#21046;&#65306;&#23454;&#20307;&#36319;&#36394;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#24433;&#21709;&#23454;&#20307;&#36319;&#36394;&#31561;&#20869;&#37096;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#33021;&#22815;&#22312;&#25968;&#23398;&#20219;&#21153;&#19978;&#23454;&#29616;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#21270;&#22312;&#35832;&#22914;&#36981;&#24490;&#25351;&#20196;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#25968;&#23398;&#31561;&#24191;&#20041;&#20219;&#21153;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#24494;&#35843;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#20013;&#20869;&#37096;&#35745;&#31639;&#30340;&#35299;&#37322;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23454;&#20307;&#36319;&#36394;&#30340;&#29305;&#24615;&#65292;&#36825;&#26159;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#22312;&#25968;&#23398;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;&#23454;&#29616;&#23454;&#20307;&#36319;&#36394;&#30340;&#26426;&#21046;&#65292;&#24182;&#26174;&#31034;&#20986;&#65288;i&#65289;&#21407;&#22987;&#27169;&#22411;&#21644;&#20854;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#20027;&#35201;&#23454;&#29616;&#23454;&#20307;&#36319;&#36394;&#30340;&#26159;&#30456;&#21516;&#30340;&#30005;&#36335;&#12290;&#20107;&#23454;&#19978;&#65292;&#21407;&#22987;&#27169;&#22411;&#30340;&#23454;&#20307;&#36319;&#36394;&#30005;&#36335;&#22312;&#32463;&#36807;&#24494;&#35843;&#30340;&#29256;&#26412;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#23436;&#25972;&#30340;&#21407;&#22987;&#27169;&#22411;&#12290;&#65288;ii&#65289;&#25152;&#26377;&#27169;&#22411;&#30340;&#30005;&#36335;&#23454;&#29616;&#22823;&#33268;&#30456;&#21516;&#30340;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14811v1 Announce Type: new  Abstract: Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionalit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;GeneOH&#25193;&#25955;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21435;&#22122;&#65292;&#20854;&#20013;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#25509;&#35302;&#30340;HOI&#34920;&#31034;&#21644;&#39046;&#22495;&#36890;&#29992;&#30340;&#21435;&#22122;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.14810</link><description>&lt;p&gt;
GeneOH&#25193;&#25955;: &#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14810
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;GeneOH&#25193;&#25955;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#27867;&#21270;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#21435;&#22122;&#65292;&#20854;&#20013;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#25509;&#35302;&#30340;HOI&#34920;&#31034;&#21644;&#39046;&#22495;&#36890;&#29992;&#30340;&#21435;&#22122;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21435;&#22122;&#25163;-&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;&#19968;&#20010;&#38169;&#35823;&#30340;&#20132;&#20114;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#23545;&#19981;&#27491;&#30830;&#30340;&#25163;&#30340;&#36712;&#36857;&#36827;&#34892;&#32454;&#21270;&#65292;&#20197;&#28040;&#38500;&#20132;&#20114;&#20266;&#24433;&#65292;&#33719;&#24471;&#19968;&#20010;&#24863;&#30693;&#19978;&#30495;&#23454;&#30340;&#24207;&#21015;&#12290;&#36825;&#19968;&#25361;&#25112;&#28041;&#21450;&#22797;&#26434;&#30340;&#20132;&#20114;&#22122;&#22768;&#65292;&#21253;&#25324;&#19981;&#33258;&#28982;&#30340;&#25163;&#37096;&#23039;&#21183;&#21644;&#19981;&#27491;&#30830;&#30340;&#25163;-&#29289;&#20307;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#26032;&#20132;&#20114;&#21644;&#19981;&#21516;&#22122;&#22768;&#27169;&#24335;&#30340;&#31283;&#20581;&#27867;&#21270;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;GeneOH Diffusion&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;:&#19968;&#31181;&#21517;&#20026;GeneOH&#30340;&#21019;&#26032;&#30340;&#22522;&#20110;&#25509;&#35302;&#30340;HOI&#34920;&#31034;&#21644;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36890;&#29992;&#30340;&#21435;&#22122;&#26041;&#26696;&#12290;&#22522;&#20110;&#25509;&#35302;&#30340;&#34920;&#31034;GeneOH&#23545;HOI&#36807;&#31243;&#36827;&#34892;&#20449;&#24687;&#21270;&#21442;&#25968;&#21270;&#65292;&#20419;&#36827;&#22312;&#21508;&#31181;HOI&#22330;&#26223;&#20013;&#23454;&#29616;&#22686;&#24378;&#30340;&#27867;&#21270;&#12290;&#26032;&#30340;&#21435;&#22122;&#26041;&#26696;&#21253;&#25324;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#29992;&#20110;&#25237;&#24433;&#22024;&#26434;&#25968;&#25454;&#26679;&#26412;&#30340;&#32463;&#20856;&#21435;&#22122;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14810v1 Announce Type: cross  Abstract: In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data sample
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.14807</link><description>&lt;p&gt;
&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#20013;&#21160;&#24577;&#19981;&#23433;&#38745;&#22810;&#33218;&#32769;&#34382;&#26426;&#20219;&#21153;&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65288;DLM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#38477;&#20302;&#23381;&#20135;&#22919;&#27515;&#20129;&#29575;&#30340;&#21162;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39044;&#38450;&#20445;&#20581;&#35745;&#21010;&#65292;&#21521;&#39640;&#39118;&#38505;&#20154;&#32676;&#20256;&#25773;&#37325;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DLM&#65306;&#19968;&#31181;&#29992;&#20110;RMAB&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20256;&#36755;&#20223;&#30495;&#22120;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#19982;&#29616;&#26377;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#30340;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14806</link><description>&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#20256;&#36755;&#20223;&#30495;&#30340;&#24046;&#24322;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Difference Learning for Air Quality Forecasting Transport Emulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20256;&#36755;&#20223;&#30495;&#22120;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#19982;&#29616;&#26377;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20581;&#24247;&#21463;&#21040;&#24694;&#21155;&#31354;&#27668;&#36136;&#37327;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21253;&#25324;&#22686;&#21152;&#21628;&#21560;&#36947;&#21644;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#39118;&#38505;&#12290;&#30001;&#20110;&#26368;&#36817;&#26497;&#31471;&#31354;&#27668;&#36136;&#37327;&#20107;&#20214;&#30340;&#22686;&#21152;&#65292;&#20840;&#29699;&#33539;&#22260;&#21644;&#32654;&#22269;&#26412;&#22320;&#37117;&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#25351;&#23548;&#20197;&#26377;&#25928;&#36866;&#24212;&#36825;&#20123;&#20107;&#20214;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#20256;&#36755;&#20223;&#30495;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#29616;&#26377;&#25968;&#20540;&#27169;&#22411;&#30456;&#24403;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14806v1 Announce Type: new  Abstract: Human health is negatively impacted by poor air quality including increased risk for respiratory and cardiovascular disease. Due to a recent increase in extreme air quality events, both globally and locally in the United States, finer resolution air quality forecasting guidance is needed to effectively adapt to these events. The National Oceanic and Atmospheric Administration provides air quality forecasting guidance for the Continental United States. Their air quality forecasting model is based on a 15 km spatial resolution; however, the goal is to reach a three km spatial resolution. This is currently not feasible due in part to prohibitive computational requirements for modeling the transport of chemical species. In this work, we describe a deep learning transport emulator that is able to reduce computations while maintaining skill comparable with the existing numerical model. We show how this method maintains skill in the presence of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#24403;&#21069;LMMs&#21644;&#20154;&#31867;&#22312;MATH-V&#19978;&#30340;&#34920;&#29616;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.14804</link><description>&lt;p&gt;
&#20351;&#29992;MATH-Vision&#25968;&#25454;&#38598;&#27979;&#37327;&#22810;&#27169;&#24577;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#24403;&#21069;LMMs&#21644;&#20154;&#31867;&#22312;MATH-V&#19978;&#30340;&#34920;&#29616;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#35270;&#35273;&#32972;&#26223;&#19979;&#30340;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;MathVista&#65289;&#19978;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#22312;&#38382;&#39064;&#22810;&#26679;&#24615;&#21644;&#28085;&#30422;&#23398;&#31185;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MATH-Vision&#65288;MATH-V&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25910;&#38598;&#20102;&#26469;&#33258;&#30495;&#23454;&#25968;&#23398;&#31454;&#36187;&#30340;3,040&#20010;&#39640;&#36136;&#37327;&#25968;&#23398;&#38382;&#39064;&#21644;&#35270;&#35273;&#32972;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#36328;&#36234;16&#20010;&#19981;&#21516;&#30340;&#25968;&#23398;&#23398;&#31185;&#65292;&#20998;&#20026;5&#20010;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20998;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20026;&#35780;&#20272;LMMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#19988;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24403;&#21069;LMMs&#19982;MATH-V&#19978;&#20154;&#31867;&#34920;&#29616;&#20043;&#38388;&#30340;&#26174;&#33879;&#34920;&#29616;&#24046;&#36317;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#25512;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14804v1 Announce Type: cross  Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements i
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14802</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#19979;&#30340;&#38142;&#36335;&#39044;&#27979;: &#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14802
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#20854;&#22312;&#23545;&#22270;&#34920;&#31034;&#30340;&#30495;&#23454;&#19990;&#30028;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#22312;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#24322;&#36136;&#22270;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#32463;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#23616;&#38480;&#20110;&#38024;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#23450;&#22522;&#20934;&#12290;&#36825;&#31181;&#29421;&#31364;&#30340;&#28966;&#28857;&#38480;&#21046;&#20102;&#38142;&#36335;&#39044;&#27979;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#20004;&#20010;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#26576;&#31181;&#28508;&#22312;&#21407;&#22240;&#32780;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#25552;&#21069;&#39044;&#27979;&#36825;&#31181;&#36830;&#25509;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNNs&#65288;&#22914;GRAFF&#65289;&#23545;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14802v1 Announce Type: new  Abstract: In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification perf
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14800</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#19987;&#23478;&#37117;&#30456;&#31561;: &#28151;&#21512;&#19987;&#23478;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;
&lt;/p&gt;
&lt;p&gt;
Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14800
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#23637;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;LLMs&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;LLMs&#30456;&#27604;&#65292;MoE LLMs&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#65292;&#20173;&#28982;&#24456;&#38590;&#37096;&#32626;&#23427;&#20204;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#19987;&#38376;&#35774;&#35745;&#30340;&#30828;&#20214;&#30340;&#26435;&#37325;&#21098;&#26525;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#21363;&#25554;&#21363;&#29992;&#30340;&#19987;&#23478;&#32423;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#25552;&#39640;MoE LLMs&#30340;&#37096;&#32626;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;MoE LLMs&#19987;&#23478;&#20462;&#21098;&#21644;&#36339;&#36807;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#22686;&#21152;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#39281;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14800v1 Announce Type: cross  Abstract: A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining sat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32500;&#25252;&#19968;&#33268;&#30340;&#29305;&#24449;&#24182;&#24378;&#21270;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QNeRF&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#36753;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14792</link><description>&lt;p&gt;
&#21512;&#24182;&#27880;&#24847;&#21147;&#29305;&#24449;&#20197;&#36827;&#34892;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Consolidating Attention Features for Multi-view Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14792
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32500;&#25252;&#19968;&#33268;&#30340;&#29305;&#24449;&#24182;&#24378;&#21270;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QNeRF&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#36753;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#22270;&#20687;&#32534;&#36753;&#25216;&#26415;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#29978;&#33267;&#31354;&#38388;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#25551;&#32472;&#21333;&#20010;&#22330;&#26223;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#20250;&#23548;&#33268;3D&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20851;&#27880;&#22522;&#20110;&#31354;&#38388;&#25511;&#21046;&#30340;&#20960;&#20309;&#25805;&#20316;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21508;&#20010;&#35270;&#22270;&#19978; consolitdate &#32534;&#36753;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#20004;&#20010;&#35266;&#28857;&#22522;&#30784;&#19978;: (1) &#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20445;&#25345;&#19968;&#33268;&#30340;&#29305;&#24449;&#26377;&#21161;&#20110;&#23454;&#29616;&#22810;&#35270;&#22270;&#32534;&#36753;&#30340;&#19968;&#33268;&#24615;&#65292;(2) &#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#26597;&#35810;&#26174;&#33879;&#24433;&#21709;&#22270;&#20687;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24378;&#21270;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#32534;&#36753;&#22270;&#20687;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; QNeRF&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32534;&#36753;&#22270;&#20687;&#30340;&#20869;&#37096;&#26597;&#35810;&#29305;&#24449;&#35757;&#32451;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;QNeRF &#21487;&#20197;&#21576;&#29616;3D&#19968;&#33268;&#30340;&#26597;&#35810;&#65292;&#28982;&#21518;&#22312;&#36719;&#20214;&#20013;&#36827;&#34892;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14792v1 Announce Type: cross  Abstract: Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly inje
&lt;/p&gt;</description></item><item><title>&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SMA&#65289;&#26159;&#19968;&#31181;&#23436;&#20840;&#39046;&#22495;&#26080;&#20851;&#30340;&#33945;&#38754;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33945;&#38754;&#37319;&#26679;&#32780;&#19981;&#20570;&#20219;&#20309;&#39046;&#22495;&#29305;&#23450;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14789</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14789
&lt;/p&gt;
&lt;p&gt;
&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SMA&#65289;&#26159;&#19968;&#31181;&#23436;&#20840;&#39046;&#22495;&#26080;&#20851;&#30340;&#33945;&#38754;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33945;&#38754;&#37319;&#26679;&#32780;&#19981;&#20570;&#20219;&#20309;&#39046;&#22495;&#29305;&#23450;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#30340;&#20855;&#20307;&#32454;&#33410;&#26159;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#27604;&#22914;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#21453;&#26144;&#20102;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#19981;&#21464;&#24615;&#12290; &#32780;&#33945;&#38754;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#24456;&#26377;&#21069;&#36884;&#65292;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#20110;&#36755;&#20837;&#22686;&#24378;&#65292;&#20294;&#20854;&#33945;&#38754;&#37319;&#26679;&#36807;&#31243;&#20173;&#28982;&#39046;&#22495;&#29305;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#23548;&#33945;&#38754;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SMA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#39046;&#22495;&#26080;&#20851;&#30340;&#33945;&#38754;&#24314;&#27169;&#26041;&#27861;&#12290;SMA&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#33945;&#38754;&#24314;&#27169;&#30446;&#26631;&#23398;&#20064;&#33945;&#38754;&#37319;&#26679;&#65292;&#32780;&#19981;&#20570;&#20219;&#20309;&#39046;&#22495;&#29305;&#23450;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#34507;&#30333;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#31890;&#23376;&#29289;&#29702;&#23398;&#19977;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;SMA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14789v1 Announce Type: cross  Abstract: Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.14781</link><description>&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21363;&#25512;&#26029;&#29992;&#20110;&#19979;&#28216;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#22312;&#25991;&#29486;&#20013;&#40092;&#26377;&#25506;&#35752;&#30340;&#38590;&#35299;&#30340;&#35745;&#31639;&#25512;&#26029;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#19982;&#26368;&#36817;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;(i)&#25512;&#26029;&#21464;&#37327;&#20043;&#38388;&#30340;&#25299;&#25169;&#39034;&#24207;&#20197;&#21450;(ii)&#25512;&#26029;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#24403;&#38480;&#21046;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#20840;&#36793;&#32536;&#21270;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#24314;&#27169;&#26410;&#30693;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20174;&#32780;&#20801;&#35768;&#20854;&#31934;&#30830;&#36793;&#32536;&#21270;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#38500;&#20102;&#22240;&#26524;&#39034;&#24207;&#20043;&#22806;&#65292;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#37117;&#34987;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14781v1 Announce Type: cross  Abstract: Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature. In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based graph learning into an effective Bayesian causal inference framework. Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for whi
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;SCM&#30340;&#27169;&#22411;&#31867;&#65292;&#29992;&#20110;&#22240;&#26524;&#25554;&#34917;&#20219;&#21153;&#65292;&#23558;&#32467;&#26524;&#34920;&#31034;&#20026;&#21453;&#20107;&#23454;&#65292;&#25805;&#20316;&#34920;&#31034;&#20026;&#23545;&#24037;&#20855;&#21464;&#37327;&#36827;&#34892;&#24178;&#39044;&#65292;&#29615;&#22659;&#22522;&#20110;&#21021;&#22987;&#23450;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.14777</link><description>&lt;p&gt;
&#22240;&#26524;&#25554;&#34917;&#29992;&#20110;&#21453;&#20107;&#23454;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65306;&#26725;&#25509;&#22270;&#21644;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14777
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;SCM&#30340;&#27169;&#22411;&#31867;&#65292;&#29992;&#20110;&#22240;&#26524;&#25554;&#34917;&#20219;&#21153;&#65292;&#23558;&#32467;&#26524;&#34920;&#31034;&#20026;&#21453;&#20107;&#23454;&#65292;&#25805;&#20316;&#34920;&#31034;&#20026;&#23545;&#24037;&#20855;&#21464;&#37327;&#36827;&#34892;&#24178;&#39044;&#65292;&#29615;&#22659;&#22522;&#20110;&#21021;&#22987;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22240;&#26524;&#25554;&#34917;&#20219;&#21153;&#65292;&#26088;&#22312;&#39044;&#27979;&#19968;&#31995;&#21015;&#25805;&#20316;&#22312;&#21508;&#31181;&#21487;&#33021;&#29615;&#22659;&#19979;&#30340;&#32467;&#26524;&#12290;&#20197;&#39044;&#27979;&#19981;&#21516;&#33647;&#29289;&#22914;&#20309;&#24433;&#21709;&#19981;&#21516;&#32454;&#32990;&#31867;&#22411;&#20026;&#36816;&#34892;&#31034;&#20363;&#12290;&#25105;&#20204;&#30740;&#31350;&#25351;&#26631;&#21807;&#19968;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25805;&#20316;&#21644;&#29615;&#22659;&#26159;&#20855;&#26377;&#26377;&#38480;&#21487;&#33021;&#20540;&#30340;&#20998;&#31867;&#21464;&#37327;&#12290;&#21363;&#20351;&#22312;&#36825;&#31181;&#31616;&#21333;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#36890;&#24120;&#21482;&#26377;&#24456;&#23569;&#21487;&#33021;&#30340;&#25805;&#20316;-&#29615;&#22659;&#23545;&#24471;&#21040;&#30740;&#31350;&#65292;&#22240;&#27492;&#23384;&#22312;&#23454;&#38469;&#25361;&#25112;&#12290;&#27169;&#22411;&#24517;&#39035;&#23545;&#26032;&#39062;&#30340;&#25805;&#20316;-&#29615;&#22659;&#23545;&#36827;&#34892;&#22806;&#25512;&#65292;&#36825;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#34892;&#30001;&#25805;&#20316;&#32034;&#24341;&#12289;&#21015;&#30001;&#29615;&#22659;&#32034;&#24341;&#12289;&#30697;&#38453;&#26465;&#30446;&#23545;&#24212;&#32467;&#26524;&#30340;&#19968;&#31181;&#30697;&#38453;&#34917;&#20840;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;SCM&#30340;&#27169;&#22411;&#31867;&#65292;&#20854;&#20013;&#32467;&#26524;&#34920;&#31034;&#20026;&#21453;&#20107;&#23454;&#65292;&#25805;&#20316;&#34920;&#31034;&#20026;&#23545;&#24037;&#20855;&#21464;&#37327;&#36827;&#34892;&#24178;&#39044;&#65292;&#29615;&#22659;&#22522;&#20110;&#21021;&#22987;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14777v1 Announce Type: cross  Abstract: We consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. As a running example, we consider predicting how different drugs affect cells from different cell types. We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initia
&lt;/p&gt;</description></item><item><title>Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14776</link><description>&lt;p&gt;
2D Matryoshka&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
2D Matryoshka Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14776
&lt;/p&gt;
&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#24120;&#35265;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#22266;&#23450;&#38271;&#24230;&#30340;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#65292;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#35745;&#31639;&#32422;&#26463;&#21644;&#39044;&#31639;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)(Kusupati&#31561;&#20154;&#65292;2022)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#36739;&#20302;&#30340;&#23884;&#20837;&#32500;&#24230;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#12290;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#36798;&#21040;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#25913;&#36827;&#20102;&#25928;&#29575;&#65292;MRL&#20173;&#35201;&#22312;&#33719;&#24471;&#23884;&#20837;&#20043;&#21069;&#36941;&#21382;&#25152;&#26377;Transformer&#23618;&#65292;&#36825;&#20173;&#28982;&#26159;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36825;&#24341;&#21457;&#20102;&#26159;&#21542;&#22266;&#23450;&#25968;&#37327;&#30340;Transformer&#23618;&#20250;&#24433;&#21709;&#34920;&#31034;&#36136;&#37327;&#20197;&#21450;&#20351;&#29992;&#20013;&#38388;&#23618;&#36827;&#34892;&#21477;&#23376;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.14760</link><description>&lt;p&gt;
&#27867;&#21270;&#22870;&#21169;&#24314;&#27169;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalizing Reward Modeling for Out-of-Distribution Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;(PL)&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26088;&#22312;&#20351;LLMs&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20197;&#24448;&#26377;&#20851;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#30340;&#30740;&#31350;&#24050;&#22312;&#20998;&#24067;&#20869;&#30340;PL&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33719;&#21462;&#20154;&#31867;&#21453;&#39304;&#30340;&#38590;&#24230;&#65292;&#20026;&#27599;&#20010;&#36935;&#21040;&#30340;&#20998;&#24067;&#31163;&#25955;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36229;&#20986;&#20998;&#24067;(OOD) PL&#20013;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#26469;&#22686;&#24378;LLMs&#26377;&#38480;&#20559;&#22909;&#21453;&#39304;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#23454;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;OOD PL&#38382;&#39064;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#24341;&#23548;&#31574;&#30053;&#23398;&#20064;&#20197;&#20351;&#20043;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#22312;&#36935;&#21040;&#27979;&#35797;&#20998;&#24067;&#26102;&#65292;&#20803;&#27979;&#35797;&#36807;&#31243;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model
&lt;/p&gt;</description></item><item><title>&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20013;&#24515;&#27010;&#24565;&#22312;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#28304;&#33258;&#30456;&#21516;&#32622;&#20449;&#38598;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25512;&#24191;&#65292;&#26159;&#23545;&#32479;&#35745;&#23398;&#20064;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#26356;&#19968;&#33324;&#22788;&#29702;&#30340;&#39318;&#35201;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2402.14759</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#25512;&#24191;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#21487;&#23454;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalising realisability in statistical learning theory under epistemic uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14759
&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20013;&#24515;&#27010;&#24565;&#22312;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#28304;&#33258;&#30456;&#21516;&#32622;&#20449;&#38598;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25512;&#24191;&#65292;&#26159;&#23545;&#32479;&#35745;&#23398;&#20064;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#26356;&#19968;&#33324;&#22788;&#29702;&#30340;&#39318;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20013;&#24515;&#27010;&#24565;&#65292;&#22914;&#21487;&#23454;&#29616;&#24615;&#65292;&#22312;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#28304;&#33258;&#30456;&#21516;&#32622;&#20449;&#38598;&#65292;&#21363;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#30340;&#20984;&#38598;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25512;&#24191;&#12290;&#36825;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#23545;&#32479;&#35745;&#23398;&#20064;&#36827;&#34892;&#26356;&#19968;&#33324;&#22788;&#29702;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14759v1 Announce Type: cross  Abstract: The purpose of this paper is to look into how central notions in statistical learning theory, such as realisability, generalise under the assumption that train and test distribution are issued from the same credal set, i.e., a convex set of probability distributions. This can be considered as a first step towards a more general treatment of statistical learning under epistemic uncertainty.
&lt;/p&gt;</description></item><item><title>BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14758</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Batch and match: black-box variational inference with a score-based divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14758
&lt;/p&gt;
&lt;p&gt;
BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20027;&#35201;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#23454;&#29616;&#37117;&#26159;&#22522;&#20110;&#20248;&#21270;&#38543;&#26426;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;BBVI&#26041;&#27861;&#36890;&#24120;&#30001;&#20110;&#20854;&#26799;&#24230;&#20272;&#35745;&#30340;&#39640;&#26041;&#24046;&#32780;&#25910;&#25947;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65288;BaM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#21487;&#20197;&#36890;&#36807;&#23545;&#20855;&#26377;&#20840;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#21464;&#20998;&#26063;&#20351;&#29992;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;BaM&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;&#25209;&#37327;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#20250;&#25351;&#25968;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;BaM&#22312;&#28304;&#33258;&#23618;&#27425;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21518;&#39564;&#25512;&#26029;&#30340;&#39640;&#26031;&#21644;&#38750;&#39640;&#26031;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;BaM&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14758v1 Announce Type: cross  Abstract: Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM ty
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;Pretrained Transformer&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#29978;&#33267;&#27604;&#20043;&#21069;&#35748;&#20026;&#30340;&#26356;&#23567;&#30340;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14753</link><description>&lt;p&gt;
Pretrained Transformer&#30340;&#24341;&#23548;&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prompting a Pretrained Transformer Can Be a Universal Approximator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;Pretrained Transformer&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#29978;&#33267;&#27604;&#20043;&#21069;&#35748;&#20026;&#30340;&#26356;&#23567;&#30340;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Prompting&#12289;Prompt&#35843;&#25972;&#21644;&#21069;&#32512;&#35843;&#25972;transformer&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#25105;&#20204;&#23545;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#24418;&#24335;&#19978;&#65292;&#25552;&#31034;&#21644;&#21069;&#32512;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#26222;&#36941;&#36924;&#36817;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20989;&#25968;&#12290;&#26412;&#25991;&#32943;&#23450;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#27604;&#20808;&#21069;&#35748;&#20026;&#30340;&#35201;&#23567;&#24471;&#22810;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#28155;&#21152;&#21069;&#32512;&#21518;&#21487;&#20197;&#25104;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#20107;&#23454;&#19978;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#38750;&#24120;&#36866;&#21512;&#20110;&#21069;&#32512;&#35843;&#25972;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#27880;&#24847;&#21147;&#22836;&#23601;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;transformer&#30340;&#28145;&#24230;&#20013;&#28155;&#21152;&#21069;&#32512;&#65292;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#37117;&#21487;&#20197;&#34987;&#36924;&#36817;&#65292;&#20854;&#28145;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#38500;&#20102;&#36825;&#20123;&#23494;&#24230;&#31867;&#22411;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;Jack...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14753v1 Announce Type: cross  Abstract: Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it. Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length. Beyond these density-type results, we also offer Jack
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#37325;&#26032;&#23457;&#35270;REINFORCE&#39118;&#26684;&#20248;&#21270;&#23545;&#20110;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#31616;&#21270;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14740</link><description>&lt;p&gt;
&#22238;&#24402;&#22522;&#30784;: &#37325;&#26032;&#23457;&#35270;LLMs&#20013;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#30340;REINFORCE&#39118;&#26684;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14740
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#37325;&#26032;&#23457;&#35270;REINFORCE&#39118;&#26684;&#20248;&#21270;&#23545;&#20110;&#23398;&#20064;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#31616;&#21270;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14740v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: AI&#23545;&#40784;&#34987;&#35270;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#33267;&#20851;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290; \textsc{Proximal Policy Optimization} (PPO)&#24050;&#34987;&#26368;&#26032;&#25991;&#29486;&#23450;&#20301;&#20026;RLHF&#20013;RL&#37096;&#20998;&#30340;&#20856;&#33539;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#26082;&#28041;&#21450;&#39640;&#35745;&#31639;&#25104;&#26412;&#21448;&#28041;&#21450;&#25935;&#24863;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#35748;&#20026;&#35302;&#21457;&#20102;PPO&#21457;&#23637;&#30340;&#22823;&#22810;&#25968;&#21160;&#26426;&#21407;&#21017;&#22312;RLHF&#20013;&#24182;&#38750;&#23454;&#36341;&#19978;&#30340;&#20851;&#27880;&#37325;&#28857;&#65292;&#24182;&#25552;&#20513;&#19968;&#31181;&#26356;&#23569;&#35745;&#31639;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#25345;&#29978;&#33267;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;RL&#30340;&#29615;&#22659;&#20013;&#26681;&#25454;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#30340;\textit{&#20844;&#24335;}&#12290;&#20197;&#31616;&#21333;&#24615;&#20026;&#25351;&#23548;&#21407;&#21017;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PPO&#30340;&#35768;&#22810;&#32452;&#20214;&#22312;RLHF&#29615;&#22659;&#20013;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#36828;&#36828;&#26356;&#31616;&#21333;&#30340;REINFORCE&#39118;&#26684;&#30340;&#20248;&#21270;&#21464;&#20307;&#34920;&#29616;&#20986;&#27604;PPO&#21644;&#26032;&#25552;&#20986;&#30340;&#8220;RL-free&#8221;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14740v1 Announce Type: new  Abstract: AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed "RL-free" methods such a
&lt;/p&gt;</description></item><item><title>Transformers&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#36807;&#31243;&#20013;&#65292;&#20851;&#38190;&#30340;&#35777;&#25454;&#26159;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26799;&#24230;&#32534;&#30721;&#20102;token&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;</title><link>https://arxiv.org/abs/2402.14735</link><description>&lt;p&gt;
Transformers&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
How Transformers Learn Causal Structure with Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14735
&lt;/p&gt;
&lt;p&gt;
Transformers&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#36807;&#31243;&#20013;&#65292;&#20851;&#38190;&#30340;&#35777;&#25454;&#26159;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26799;&#24230;&#32534;&#30721;&#20102;token&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#25104;&#21151;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#20801;&#35768;&#20449;&#24687;&#22312;&#24207;&#21015;&#30340;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#20256;&#36882;&#12290;&#33258;&#27880;&#24847;&#26426;&#21046;&#20351;&#24471;transformers&#33021;&#22815;&#32534;&#30721;&#22240;&#26524;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#20854;&#29305;&#21035;&#36866;&#21512;&#24207;&#21015;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;transformers&#36890;&#36807;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;&#23398;&#20064;&#36825;&#31181;&#22240;&#26524;&#32467;&#26500;&#30340;&#36807;&#31243;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38656;&#35201;&#23398;&#20064;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21270;&#30340;&#20004;&#23618;transformer&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#23398;&#20250;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#31532;&#19968;&#23618;&#27880;&#24847;&#21147;&#20013;&#32534;&#30721;&#28508;&#22312;&#22240;&#26524;&#22270;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26799;&#24230;&#32534;&#30721;&#20102;token&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#30001;&#20110;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#32467;&#26524;&#65292;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#26368;&#22823;&#30340;&#26465;&#30446;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14735v1 Announce Type: new  Abstract: The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14730</link><description>&lt;p&gt;
Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford-Steerable Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;$\mathrm{E}(p, q)$&#31561;&#21464;CNN&#31867;&#12290; CS-CNNs&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;$\mathbb{R}^{p,q}$&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#12290; &#23427;&#20204;&#28085;&#30422;&#20102;&#20363;&#22914;$\mathrm{E}(3)$&#22312;$\mathbb{R}^3$&#19978;&#21644;Poincar\'e&#22312;&#38389;&#21487;&#22827;&#26031;&#22522;&#26102;&#31354;$\mathbb{R}^{1,3}$&#19978;&#30340;&#31561;&#21464;&#24615;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#12290; &#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#26041;&#27861;&#19978;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#32422;&#26463;&#21644;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#26469;&#20445;&#35777;&#36755;&#20986;&#27010;&#29575;&#19981;&#36829;&#21453;&#19987;&#23478;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#24402;&#32435;&#19982;&#28436;&#32462;&#23398;&#20064;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14726</link><description>&lt;p&gt;
&#22312;&#27010;&#24565;&#23398;&#20064;&#26694;&#26550;&#20013;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Incorporating Expert Rules into Neural Networks in the Framework of Concept-Based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#32422;&#26463;&#21644;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#26469;&#20445;&#35777;&#36755;&#20986;&#27010;&#29575;&#19981;&#36829;&#21453;&#19987;&#23478;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#24402;&#32435;&#19982;&#28436;&#32462;&#23398;&#20064;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#23558;&#19987;&#23478;&#35268;&#21017;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#25193;&#23637;&#22522;&#20110;&#27010;&#24565;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#22914;&#20309;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#39044;&#27979;&#27010;&#24565;&#27010;&#29575;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#35813;&#32452;&#21512;&#32972;&#21518;&#30340;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#24418;&#25104;&#32422;&#26463;&#65292;&#20197;&#28385;&#36275;&#19987;&#23478;&#35268;&#21017;&#30340;&#25152;&#26377;&#27010;&#24565;&#20540;&#32452;&#21512;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#31532;&#20108;&#20010;&#24819;&#27861;&#26159;&#20197;&#20984;&#22810;&#38754;&#20307;&#30340;&#24418;&#24335;&#34920;&#31034;&#27010;&#29575;&#20998;&#24067;&#30340;&#21487;&#34892;&#38598;&#65292;&#24182;&#20351;&#29992;&#20854;&#39030;&#28857;&#25110;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14726v1 Announce Type: cross  Abstract: A problem of incorporating the expert rules into machine learning models for extending the concept-based learning is formulated in the paper. It is proposed how to combine logical rules and neural networks predicting the concept probabilities. The first idea behind the combination is to form constraints for a joint probability distribution over all combinations of concept values to satisfy the expert rules. The second idea is to represent a feasible set of probability distributions in the form of a convex polytope and to use its vertices or faces. We provide several approaches for solving the stated problem and for training neural networks which guarantee that the output probabilities of concepts would not violate the expert rules. The solution of the problem can be viewed as a way for combining the inductive and deductive learning. Expert rules are used in a broader sense when any logical function that connects concepts and class labe
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14708</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#29992;&#21345;&#27450;&#35784;&#23545;&#32463;&#27982;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#33410;&#28857;&#30340;&#26412;&#22320;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#8212;&#8212;CaT-GNN&#65288;Causal Temporal Graph Neural Networks&#65289;&#65292;&#21033;&#29992;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#26469;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#21457;&#29616;&#21644;&#24178;&#39044;&#38454;&#27573;&#65292;CaT-GNN&#30830;&#23450;&#20132;&#26131;&#22270;&#20013;&#30340;&#22240;&#26524;&#33410;&#28857;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;CaT-GNN&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;Causal-Inspector&#21644;Causal-Intervener&#12290;Causal-Inspector&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#35782;&#21035;&#22240;&#26524;&#21644;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14708v1 Announce Type: cross  Abstract: Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal \textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14703</link><description>&lt;p&gt;
&#22312;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20013;&#25506;&#35752;&#26410;&#26469;&#21644;&#21382;&#21490;&#30340;&#35781;&#21650;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#22797;&#26434;&#35266;&#27979;&#30340;&#31163;&#32447;&#35780;&#20272;(OPE)&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36991;&#20813;&#23545;&#26102;&#38388;&#36328;&#24230;&#25351;&#25968;&#20381;&#36182;&#30340;&#20272;&#35745;&#22120;&#12290;&#26368;&#36817;&#65292;Uehara&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20063;&#21462;&#20915;&#20110;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#30340;&#26377;&#30028;&#24615;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#25968;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25968;&#37327;&#21487;&#33021;&#20250;&#38543;&#30528;&#38271;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20174;&#32780;&#25273;&#21435;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14703v1 Announce Type: cross  Abstract: We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.14698</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#20998;&#26512;&#29992;&#20110;&#20998;&#31867;&#19982;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65306;&#25104;&#37117;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Big data analytics to classify earthwork-related locations: A Chengdu study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14698
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#65292;&#25104;&#21151;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#35777;&#26126;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26174;&#33879;&#21152;&#21095;&#65292;&#23548;&#33268;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20005;&#37325;&#20581;&#24247;&#21518;&#26524;&#12290;&#22303;&#26041;&#30456;&#20851;&#30340;&#22320;&#28857;&#65288;ERLs&#65289;&#26159;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;ERLs&#30340;&#26377;&#25928;&#31649;&#29702;&#19968;&#30452;&#26159;&#25919;&#24220;&#21644;&#29615;&#22659;&#26426;&#26500;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20027;&#35201;&#21407;&#22240;&#21253;&#25324;&#20854;&#20998;&#31867;&#20998;&#23646;&#19981;&#21516;&#30340;&#30417;&#31649;&#37096;&#38376;&#12289;&#20449;&#24687;&#38556;&#30861;&#12289;&#25968;&#25454;&#26356;&#26032;&#24310;&#36831;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#28304;&#22836;&#28784;&#23576;&#27745;&#26579;&#30340;&#25233;&#21046;&#25514;&#26045;&#30340;&#32570;&#20047;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#21368;&#36710;&#36712;&#36857;&#12289;&#22478;&#24066;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#21644;&#22303;&#22320;&#35206;&#30422;&#25968;&#25454;&#23545;&#22478;&#24066;&#28784;&#23576;&#27745;&#26579;&#28304;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#30740;&#31350;&#20102;&#29305;&#24449;&#19982;&#28784;&#23576;&#27745;&#26579;&#28304;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#25104;&#21151;&#23454;&#26045;&#22312;&#19968;&#20010;&#21517;&#20026;&#30340;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14698v1 Announce Type: cross  Abstract: Air pollution has significantly intensified, leading to severe health consequences worldwide. Earthwork-related locations (ERLs) constitute significant sources of urban dust pollution. The effective management of ERLs has long posed challenges for governmental and environmental agencies, primarily due to their classification under different regulatory authorities, information barriers, delays in data updating, and a lack of dust suppression measures for various sources of dust pollution. To address these challenges, we classified urban dust pollution sources using dump truck trajectory, urban point of interest (POI), and land cover data. We compared several prediction models and investigated the relationship between features and dust pollution sources using real data. The results demonstrate that high-accuracy classification can be achieved with a limited number of features. This method was successfully implemented in the system called
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#31639;&#27861;&#25552;&#21319;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#33021;&#20943;&#23569;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#32593;&#32476;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38388;</title><link>https://arxiv.org/abs/2402.14694</link><description>&lt;p&gt;
&#20026;&#38750;&#20174;&#19994;&#20154;&#21592;&#24555;&#36895;&#20171;&#32461;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Quick Introduction to Quantum Machine Learning for Non-Practitioners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14694
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#31639;&#27861;&#25552;&#21319;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#33021;&#20943;&#23569;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#32593;&#32476;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#25506;&#35752;&#20102;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#31639;&#27861;&#21487;&#33021;&#25913;&#36827;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#37327;&#23376;&#35745;&#31639;&#21033;&#29992;&#21463;&#37327;&#23376;&#21147;&#23398;&#25511;&#21046;&#30340;&#31890;&#23376;&#36827;&#34892;&#35745;&#31639;&#65292;&#21033;&#29992;&#21472;&#21152;&#21644;&#32416;&#32544;&#31561;&#29305;&#24615;&#36827;&#34892;&#20449;&#24687;&#34920;&#31034;&#21644;&#25805;&#32437;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#23558;&#36825;&#20123;&#21407;&#29702;&#24212;&#29992;&#20110;&#22686;&#24378;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#28508;&#22312;&#22320;&#20943;&#23569;&#20102;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#32593;&#32476;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#35770;&#25991;&#28085;&#30422;&#20102;&#22522;&#26412;&#30340;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#65292;&#21253;&#25324;&#21472;&#21152;&#12289;&#30456;&#20301;&#31354;&#38388;&#21644;&#32416;&#32544;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#36825;&#20123;&#29305;&#24615;&#30340;&#37327;&#23376;&#38376;&#27010;&#24565;&#12290;&#23427;&#36824;&#22238;&#39038;&#20102;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27010;&#24565;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#26799;&#24230;&#19979;&#38477;&#21644;&#21453;&#21521;&#20256;&#25773;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#20316;&#20026;&#31070;&#32463;&#20803;&#30340;&#21487;&#35757;&#32451;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14694v1 Announce Type: cross  Abstract: This paper provides an introduction to quantum machine learning, exploring the potential benefits of using quantum computing principles and algorithms that may improve upon classical machine learning approaches. Quantum computing utilizes particles governed by quantum mechanics for computational purposes, leveraging properties like superposition and entanglement for information representation and manipulation. Quantum machine learning applies these principles to enhance classical machine learning models, potentially reducing network size and training time on quantum hardware. The paper covers basic quantum mechanics principles, including superposition, phase space, and entanglement, and introduces the concept of quantum gates that exploit these properties. It also reviews classical deep learning concepts, such as artificial neural networks, gradient descent, and backpropagation, before delving into trainable quantum circuits as neural 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#36890;&#36807;&#21152;&#20837;&#26126;&#30830;&#30340;&#21608;&#26399;&#20449;&#21495;&#65292;&#25552;&#39640;&#20102;&#22768;&#38899;&#36136;&#37327;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#39640;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.14692</link><description>&lt;p&gt;
PeriodGrad: &#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#21487;&#25511;&#38899;&#39640;&#31070;&#32463;&#22768;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#36890;&#36807;&#21152;&#20837;&#26126;&#30830;&#30340;&#21608;&#26399;&#20449;&#21495;&#65292;&#25552;&#39640;&#20102;&#22768;&#38899;&#36136;&#37327;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#39640;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#65292;&#21152;&#20837;&#20102;&#26126;&#30830;&#30340;&#21608;&#26399;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#35843;&#33410;&#20449;&#21495;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;DDPM&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#20316;&#20026;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27874;&#24418;&#12290;&#22522;&#20110;DDPM&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#20855;&#26377;&#20351;&#29992;&#31616;&#21333;&#26102;&#38388;&#22495;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#27468;&#22768;&#21512;&#25104;&#65292;&#38656;&#35201;&#31070;&#32463;&#22768;&#30721;&#22120;&#29983;&#25104;&#20855;&#26377;&#28789;&#27963;&#38899;&#39640;&#25511;&#21046;&#30340;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#27874;&#24418;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;DDPM&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24456;&#38590;&#29983;&#25104;&#35821;&#38899;&#27874;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#21152;&#20837;&#26126;&#30830;&#30340;&#21608;&#26399;&#20449;&#21495;&#20934;&#30830;&#25429;&#25417;&#35821;&#38899;&#27874;&#24418;&#30340;&#21608;&#26399;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25913;&#21892;&#20102;&#22768;&#38899;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#27604;&#20256;&#32479;&#22522;&#20110;DDPM&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#26356;&#22909;&#30340;&#38899;&#39640;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14692v1 Announce Type: cross  Abstract: This paper presents a neural vocoder based on a denoising diffusion probabilistic model (DDPM) incorporating explicit periodic signals as auxiliary conditioning signals. Recently, DDPM-based neural vocoders have gained prominence as non-autoregressive models that can generate high-quality waveforms. The neural vocoders based on DDPM have the advantage of training with a simple time-domain loss. In practical applications, such as singing voice synthesis, there is a demand for neural vocoders to generate high-fidelity speech waveforms with flexible pitch control. However, conventional DDPM-based neural vocoders struggle to generate speech waveforms under such conditions. Our proposed model aims to accurately capture the periodic structure of speech waveforms by incorporating explicit periodic signals. Experimental results show that our model improves sound quality and provides better pitch control than conventional DDPM-based neural voco
&lt;/p&gt;</description></item><item><title>Q-Probe&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20989;&#25968;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#37325;&#26032;&#21152;&#26435;&#20505;&#36873;&#23436;&#25104;&#65292;&#20174;&#32780;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#33719;&#24471;&#26174;&#33879;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.14688</link><description>&lt;p&gt;
Q-Probe: &#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Q-Probe: A Lightweight Approach to Reward Maximization for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14688
&lt;/p&gt;
&lt;p&gt;
Q-Probe&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20989;&#25968;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#37325;&#26032;&#21152;&#26435;&#20505;&#36873;&#23436;&#25104;&#65292;&#20174;&#32780;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#33719;&#24471;&#26174;&#33879;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Q-probing&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;Q-probing&#20301;&#20110;&#20687;&#24494;&#35843;&#36825;&#26679;&#36739;&#37325;&#30340;&#26041;&#27861;&#21644;&#20687;&#23569;&#37327;&#25552;&#31034;&#36825;&#26679;&#36739;&#36731;&#30340;&#26041;&#27861;&#20043;&#38388;&#65292;&#20294;&#20063;&#21487;&#20197;&#19982;&#20219;&#19968;&#31181;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#20854;&#24819;&#27861;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#19978;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21487;&#29992;&#20110;&#37325;&#26032;&#21152;&#26435;&#20505;&#36873;&#23436;&#25104;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#37319;&#26679;&#36807;&#31243;&#31561;&#21516;&#20110;Q-probe&#30340;KL&#32422;&#26463;&#26368;&#22823;&#21270;&#12290;&#20026;&#20102;&#35757;&#32451;Q-probes&#65292;&#25105;&#20204;&#32771;&#34385;&#22870;&#21169;&#24314;&#27169;&#25110;&#22522;&#20110;&#37325;&#35201;&#24615;&#21152;&#26435;&#31574;&#30053;&#26799;&#24230;&#30340;&#19968;&#31867;&#26032;&#22411;&#30452;&#25509;&#31574;&#30053;&#23398;&#20064;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#30475;&#21040;&#22312;&#20855;&#26377;&#22522;&#20110;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#65288;&#20195;&#30721;&#29983;&#25104;&#65289;&#20197;&#21450;&#30001;&#20559;&#22909;&#25968;&#25454;&#23450;&#20041;&#30340;&#38544;&#24335;&#22870;&#21169;&#30340;&#39046;&#22495;&#20013;&#33719;&#24471;&#25910;&#30410;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;Q-probe&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14688v1 Announce Type: new  Abstract: We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26041;&#24046;&#20999;&#25442;&#30340;&#33258;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#29702;&#35770;&#21644;&#19987;&#23478;&#32858;&#21512;&#26041;&#27861;&#26469;&#23398;&#20064;&#26041;&#24046;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#30005;&#37327;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>https://arxiv.org/abs/2402.14684</link><description>&lt;p&gt;
&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#26041;&#24046;&#20999;&#25442;&#30340;&#33258;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive time series forecasting with markovian variance switching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26041;&#24046;&#20999;&#25442;&#30340;&#33258;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#29702;&#35770;&#21644;&#19987;&#23478;&#32858;&#21512;&#26041;&#27861;&#26469;&#23398;&#20064;&#26041;&#24046;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#30005;&#37327;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#20013;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#22312;&#21046;&#24230;&#21464;&#21270;&#19979;&#36827;&#34892;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20256;&#32479;&#26041;&#27861;&#20551;&#35774;&#20855;&#26377;&#22312;&#26102;&#38388;&#19978;&#24658;&#23450;&#30340;&#26041;&#24046;&#30340;&#32447;&#24615;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;LGSSM&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#36807;&#31243;&#19981;&#33021;&#34987;&#36825;&#26679;&#30340;&#27169;&#22411;&#25429;&#25417;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#20999;&#25442;&#26041;&#24046;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#21160;&#24577;&#31995;&#32479;&#36890;&#24120;&#26159;&#26080;&#27861;&#35299;&#20915;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#38543;&#26102;&#38388;&#21576;&#25351;&#25968;&#22686;&#38271;&#65307;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;VB&#65289;&#25216;&#26415;&#24050;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#29702;&#35770;&#30340;&#26032;&#30340;&#20272;&#35745;&#26041;&#24046;&#30340;&#26041;&#27861;&#65307;&#25105;&#20204;&#35843;&#25972;&#19987;&#23478;&#32858;&#21512;&#26041;&#27861;&#26469;&#38543;&#26102;&#38388;&#23398;&#20064;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#20197;&#21450;&#29992;&#20110;&#30005;&#37327;&#36127;&#33655;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#35823;&#24046;&#20272;&#35745;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#30340;&#19987;&#23478;&#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14684v1 Announce Type: cross  Abstract: Adaptive time series forecasting is essential for prediction under regime changes. Several classical methods assume linear Gaussian state space model (LGSSM) with variances constant in time. However, there are many real-world processes that cannot be captured by such models. We consider a state-space model with Markov switching variances. Such dynamical systems are usually intractable because of their computational complexity increasing exponentially with time; Variational Bayes (VB) techniques have been applied to this problem. In this paper, we propose a new way of estimating variances based on online learning theory; we adapt expert aggregation methods to learn the variances over time. We apply the proposed method to synthetic data and to the problem of electricity load forecasting. We show that this method is robust to misspecification and outperforms traditional expert aggregation.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.14683</link><description>&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Visual Hallucinations of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14683
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24187;&#35273;&#65288;VH&#65289;&#24847;&#21619;&#30528;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#23545;&#22270;&#20687;&#24819;&#35937;&#20986;&#38169;&#35823;&#30340;&#32454;&#33410;&#12290;&#29616;&#26377;&#30740;&#31350;&#21457;&#29616;VH&#23454;&#20363;&#20165;&#23384;&#22312;&#20110;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;MLLM&#22312;VH&#19979;&#30340;&#24615;&#33021;&#29702;&#35299;&#23384;&#22312;&#20559;&#24046;&#65292;&#21407;&#22240;&#22312;&#20110;&#36825;&#31867;VH&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VHTest&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;VH&#23454;&#20363;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VHTest&#22312;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;COCO&#65289;&#20013;&#25214;&#21040;&#19968;&#20123;&#21021;&#22987;&#30340;VH&#23454;&#20363;&#65292;&#20026;&#27599;&#20010;VH&#27169;&#24335;&#29983;&#25104;&#19968;&#20010;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;DALL-E-3&#65289;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;VH&#22270;&#20687;&#12290;&#25105;&#20204;&#21033;&#29992;VHTest&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;VH&#27169;&#24335;&#20013;1,200&#20010;VH&#23454;&#20363;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#65288;&#20363;&#22914;GPT-4V&#12289;LLaVA-1.5&#21644;MiniGPT-v2&#65289;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#22823;&#37096;&#20998;&#23454;&#20363;&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14683v1 Announce Type: cross  Abstract: Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14664</link><description>&lt;p&gt;
&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#36125;&#21494;&#26031;&#31163;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#20013;&#65292;&#21160;&#20316;&#32463;&#24120;&#26159;&#30456;&#20851;&#30340;&#65292;&#36825;&#20026;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#26356;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#21644;&#23398;&#20064;&#65288;OPL&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#26469;&#25429;&#25417;&#36825;&#20123;&#30456;&#20851;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sDM&#65292;&#19968;&#20010;&#20026;OPE&#21644;OPL&#35774;&#35745;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#26082;&#26377;&#31639;&#27861;&#22522;&#30784;&#21448;&#26377;&#29702;&#35770;&#22522;&#30784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;sDM&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#32780;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#21463;&#22312;&#32447;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#24615;&#33021;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20559;&#31163;&#20256;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#30340;&#34920;&#29616;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#22909;&#22788;&#12290;&#23454;&#35777;&#35777;&#25454;&#23637;&#31034;&#20102;sDM&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14664v1 Announce Type: cross  Abstract: In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.14648</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#37325;&#26032;&#24605;&#32771;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14648
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#19968;&#30452;&#26159;&#25269;&#25239;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#65292;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21364;&#23545;&#25239;&#24615;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#30830;&#23450;&#20102;&#22952;&#30861;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21464;&#24615;&#25439;&#22833;&#21644;&#20998;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#65292;&#34920;&#26126;&#23384;&#22312;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30001;&#20110;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#20998;&#24067;&#21457;&#25955;&#32780;&#20986;&#29616;&#30340;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AR-AT&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20572;&#27490;&#26799;&#24230;&#25805;&#20316;&#21644;&#19968;&#20010;&#39044;&#27979;&#22120;&#26469;&#36991;&#20813;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#28789;&#24863;&#26469;&#33258;&#26368;&#36817;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
&lt;/p&gt;</description></item><item><title>CoLoRA&#36890;&#36807;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#39044;&#27979;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#28436;&#21464;&#30340;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14646</link><description>&lt;p&gt;
CoLoRA:&#29992;&#20110;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#31616;&#21270;&#38544;&#24335;&#31070;&#32463;&#24314;&#27169;&#30340;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14646
&lt;/p&gt;
&lt;p&gt;
CoLoRA&#36890;&#36807;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#39044;&#27979;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#28436;&#21464;&#30340;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;CoLoRA&#65289;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#23427;&#39044;&#20808;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#19978;&#36830;&#32493;&#22320;&#35843;&#25972;&#20302;&#31209;&#26435;&#37325;&#65292;&#20197;&#24555;&#36895;&#39044;&#27979;&#26032;&#29289;&#29702;&#21442;&#25968;&#21644;&#26032;&#21021;&#22987;&#26465;&#20214;&#19979;&#35299;&#22330;&#30340;&#28436;&#21464;&#12290;&#33258;&#36866;&#24212;&#21487;&#20197;&#26159;&#32431;&#31929;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#26041;&#31243;&#39537;&#21160;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#25552;&#20379;Galerkin&#26368;&#20248;&#30340;&#36924;&#36817;&#12290;&#30001;&#20110;CoLoRA&#22312;&#26102;&#38388;&#19978;&#23616;&#37096;&#36924;&#36817;&#35299;&#22330;&#65292;&#26435;&#37325;&#30340;&#31209;&#21487;&#20197;&#20445;&#25345;&#36739;&#23567;&#65292;&#36825;&#24847;&#21619;&#30528;&#21482;&#38656;&#35201;&#31163;&#32447;&#35757;&#32451;&#20960;&#26465;&#36712;&#36857;&#65292;&#22240;&#27492;CoLoRA&#38750;&#24120;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;CoLoRA&#30340;&#39044;&#27979;&#36895;&#24230;&#24555;&#19978;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20854;&#20934;&#30830;&#24230;&#21644;&#21442;&#25968;&#25928;&#29575;&#20063;&#27604;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14646v1 Announce Type: new  Abstract: This work introduces reduced models based on Continuous Low Rank Adaptation (CoLoRA) that pre-train neural networks for a given partial differential equation and then continuously adapt low-rank weights in time to rapidly predict the evolution of solution fields at new physics parameters and new initial conditions. The adaptation can be either purely data-driven or via an equation-driven variational approach that provides Galerkin-optimal approximations. Because CoLoRA approximates solution fields locally in time, the rank of the weights can be kept small, which means that only few training trajectories are required offline so that CoLoRA is well suited for data-scarce regimes. Predictions with CoLoRA are orders of magnitude faster than with classical methods and their accuracy and parameter efficiency is higher compared to other neural network approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#22256;&#38590;&#24615;&#30340;&#35777;&#25454;&#65292;&#20551;&#35774;&#26684;&#38382;&#39064;&#30340;&#26368;&#22351;&#24773;&#20917;&#22256;&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14645</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#26684;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Sparse Linear Regression and Lattice Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#22256;&#38590;&#24615;&#30340;&#35777;&#25454;&#65292;&#20551;&#35774;&#26684;&#38382;&#39064;&#30340;&#26368;&#22351;&#24773;&#20917;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#65288;SLR&#65289;&#26159;&#32479;&#35745;&#23398;&#20013;&#19968;&#20010;&#30740;&#31350;&#33391;&#22909;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#35774;&#35745;&#30697;&#38453; $X\in\mathbb{R}^{m\times n}$ &#21644;&#21709;&#24212;&#21521;&#37327; $y=X\theta^*+w$&#65292;&#20854;&#20013; $\theta^*$ &#26159; $k$-&#31232;&#30095;&#21521;&#37327;&#65288;&#21363;&#65292;$\|\theta^*\|_0\leq k$&#65289;&#65292;$w$ &#26159;&#23567;&#30340;&#12289;&#20219;&#24847;&#30340;&#22122;&#22768;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010; $k$-&#31232;&#30095;&#30340; $\widehat{\theta} \in \mathbb{R}^n$&#65292;&#20351;&#24471;&#22343;&#26041;&#39044;&#27979;&#35823;&#24046; $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$ &#26368;&#23567;&#21270;&#12290;&#34429;&#28982; $\ell_1$-&#26494;&#24347;&#26041;&#27861;&#22914;&#22522; Pursuit&#12289;Lasso &#21644; Dantzig &#36873;&#25321;&#22120;&#22312;&#35774;&#35745;&#30697;&#38453;&#26465;&#20214;&#33391;&#22909;&#26102;&#35299;&#20915;&#20102; SLR&#65292;&#20294;&#27809;&#26377;&#24050;&#30693;&#36890;&#29992;&#31639;&#27861;&#65292;&#20063;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#35774;&#32622;&#20013;&#30340;&#22256;&#38590;&#24615;&#30340;&#27491;&#24335;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14645v1 Announce Type: new  Abstract: Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector $y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is, $\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$. While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bo
&lt;/p&gt;</description></item><item><title>latrend&#26694;&#26550;&#20026;&#32437;&#21521;&#25968;&#25454;&#32858;&#31867;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26041;&#27861;&#24212;&#29992;&#26694;&#26550;&#65292;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#23454;&#29616;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.14621</link><description>&lt;p&gt;
latrend: &#29992;&#20110;&#32858;&#31867;&#32437;&#21521;&#25968;&#25454;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
latrend: A Framework for Clustering Longitudinal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14621
&lt;/p&gt;
&lt;p&gt;
latrend&#26694;&#26550;&#20026;&#32437;&#21521;&#25968;&#25454;&#32858;&#31867;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26041;&#27861;&#24212;&#29992;&#26694;&#26550;&#65292;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#23454;&#29616;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#25968;&#25454;&#30340;&#32858;&#31867;&#29992;&#20110;&#25506;&#32034;&#19981;&#21516;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20849;&#21516;&#36235;&#21183;&#65292;&#20197;&#25968;&#20540;&#27979;&#37327;&#20026;&#20852;&#36259;&#12290;&#22810;&#24180;&#26469;&#24341;&#20837;&#20102;&#21508;&#31181;R&#21253;&#65292;&#29992;&#20110;&#35782;&#21035;&#32437;&#21521;&#27169;&#24335;&#30340;&#32858;&#31867;&#65292;&#20197;&#19968;&#31181;&#25110;&#22810;&#31181;&#36235;&#21183;&#26469;&#24635;&#32467;&#20027;&#39064;&#20043;&#38388;&#36712;&#36857;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;R&#21253;"latrend"&#20316;&#20026;&#32437;&#21521;&#32858;&#31867;&#26041;&#27861;&#30340;&#32479;&#19968;&#24212;&#29992;&#26694;&#26550;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#26368;&#23567;&#32534;&#30721;&#37327;&#24773;&#20917;&#19979;&#27604;&#36739;&#21508;&#31181;&#26041;&#27861;&#12290;&#35813;&#21253;&#36824;&#20316;&#20026;&#24120;&#29992;&#21253;"dtwclust"&#12289;"flexmix"&#12289;"kml"&#12289;"lcmm"&#12289;"mclust"&#12289;"mixAK"&#21644;"mixtools"&#30340;&#25509;&#21475;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#26041;&#27861;&#35268;&#33539;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#21487;&#20197;&#21033;&#29992;&#26694;&#26550;&#25552;&#20379;&#30340;&#26631;&#20934;&#24037;&#20855;&#26469;&#24555;&#36895;&#23454;&#29616;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14621v1 Announce Type: new  Abstract: Clustering of longitudinal data is used to explore common trends among subjects over time for a numeric measurement of interest. Various R packages have been introduced throughout the years for identifying clusters of longitudinal patterns, summarizing the variability in trajectories between subject in terms of one or more trends. We introduce the R package "latrend" as a framework for the unified application of methods for longitudinal clustering, enabling comparisons between methods with minimal coding. The package also serves as an interface to commonly used packages for clustering longitudinal data, including "dtwclust", "flexmix", "kml", "lcmm", "mclust", "mixAK", and "mixtools". This enables researchers to easily compare different approaches, implementations, and method specifications. Furthermore, researchers can build upon the standard tools provided by the framework to quickly implement new cluster methods, enabling rapid protot
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.14609</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Complex Qeury Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25191;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#30340;&#33021;&#21147;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#22270;&#25512;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#36923;&#36753;&#26597;&#35810;&#34920;&#31034;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#36923;&#36753;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#24182;&#19981;&#33021;&#24212;&#29992;&#20110;&#22810;&#20010;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#20849;&#20139;&#24102;&#26377;&#25935;&#24863;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#33021;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#20351;&#24471;&#20849;&#20139;&#21644;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#25512;&#29702;&#20197;&#26816;&#32034;&#26597;&#35810;&#31572;&#26696;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#26597;&#35810;&#12290;&#19968;&#20010;&#23454;&#20307;&#21487;&#33021;&#28041;&#21450;&#21040;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#23545;&#20110;&#21457;&#29616;&#30693;&#35782;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
&lt;/p&gt;</description></item><item><title>&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#30340;&#24341;&#20837;&#32531;&#35299;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#26356;&#23569;&#30340;&#33033;&#20914;&#65292;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14603</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Balanced Resonate-and-Fire Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14603
&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#30340;&#24341;&#20837;&#32531;&#35299;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#26356;&#23569;&#30340;&#33033;&#20914;&#65292;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;20&#24180;&#37324;&#24341;&#20837;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;RF&#65289;&#31070;&#32463;&#20803;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#19988;&#31526;&#21512;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#23574;&#23792;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#20849;&#25391;&#33180;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#22312;&#26102;&#38388;&#22495;&#20869;&#25552;&#21462;&#39057;&#29575;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;RF&#20844;&#24335;&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#65292;&#38480;&#21046;&#20102;&#26377;&#25928;&#23398;&#20064;&#24182;&#38459;&#30861;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#21407;&#21017;&#20248;&#21183;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24179;&#34913;&#30340;RF&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#65292;&#23427;&#32531;&#35299;&#20102;&#26222;&#36890;RF&#31070;&#32463;&#20803;&#30340;&#19968;&#20123;&#22266;&#26377;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;BRF&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#30340;&#33033;&#20914;&#20165;&#20026;&#29616;&#20195;RSNNs&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#20195;RSNNs&#65292;&#38656;&#35201;&#30340;&#21442;&#25968;&#26126;&#26174;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;BRF-RSNN&#22987;&#32456;&#25552;&#20379;&#26356;&#24555;&#36895;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#36830;&#25509;&#22810;&#20010;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14603v1 Announce Type: cross  Abstract: The resonate-and-fire (RF) neuron, introduced over two decades ago, is a simple, efficient, yet biologically plausible spiking neuron model, which can extract frequency patterns within the time domain due to its resonating membrane dynamics. However, previous RF formulations suffer from intrinsic shortcomings that limit effective learning and prevent exploiting the principled advantage of RF neurons. Here, we introduce the balanced RF (BRF) neuron, which alleviates some of the intrinsic limitations of vanilla RF neurons and demonstrates its effectiveness within recurrent spiking neural networks (RSNNs) on various sequence learning tasks. We show that networks of BRF neurons achieve overall higher task performance, produce only a fraction of the spikes, and require significantly fewer parameters as compared to modern RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable training convergence, even when bridging many 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.14601</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#25945;&#32946;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bringing Generative AI to Adaptive Learning in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14601
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#28608;&#22686;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21457;&#23637;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#36825;&#19968;&#27010;&#24565;&#22312;&#25945;&#32946;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#39046;&#22495;&#30340;&#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#28508;&#21147;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#32467;&#21512;&#23558;&#20026;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#24418;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#24182;&#22522;&#20110;&#32622;&#20449;&#24230;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#29305;&#24449;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21270;&#35760;&#24518;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.14598</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#29305;&#24449;&#33258;&#21160;&#36866;&#24212;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired Distributed Memorization Learning for Efficient Feature-free Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14598
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#24182;&#22522;&#20110;&#32622;&#20449;&#24230;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#29305;&#24449;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21270;&#35760;&#24518;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34920;&#29616;&#20986;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#31243;&#24207;&#12290;&#21463;&#20154;&#31867;&#22823;&#33041;&#20998;&#24067;&#24335;&#35760;&#24518;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#31216;&#20026;DML&#65292;&#20197;&#25903;&#25345;&#36716;&#31227;&#27169;&#22411;&#30340;&#24555;&#36895;&#39046;&#22495;&#36866;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DML&#37319;&#29992;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#26469;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#36825;&#20123;&#20449;&#21495;&#20316;&#20026;&#20914;&#21160;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#30340;&#32622;&#20449;&#24230;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;DML&#33021;&#22815;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24378;&#21270;&#35760;&#24518;&#65292;&#24555;&#36895;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#23545;&#28145;&#23618;&#29305;&#24449;&#36827;&#34892;&#32321;&#37325;&#30340;&#24494;&#35843;&#65292;&#36825;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22522;&#20110;&#22235;&#20010;&#20132;&#21449;&#39046;&#22495;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14598v1 Announce Type: cross  Abstract: Compared with gradient based artificial neural networks, biological neural networks usually show a more powerful generalization ability to quickly adapt to unknown environments without using any gradient back-propagation procedure. Inspired by the distributed memory mechanism of human brains, we propose a novel gradient-free Distributed Memorization Learning mechanism, namely DML, to support quick domain adaptation of transferred models. In particular, DML adopts randomly connected neurons to memorize the association of input signals, which are propagated as impulses, and makes the final decision by associating the distributed memories based on their confidence. More importantly, DML is able to perform reinforced memorization based on unlabeled data to quickly adapt to a new domain without heavy fine-tuning of deep features, which makes it very suitable for deploying on edge devices. Experiments based on four cross-domain real-world da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23398;&#29983;&#23398;&#20064;&#39118;&#26684;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#21487;&#20197;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20135;&#29983;&#21487;&#38752;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14597</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#33258;&#23398;&#26631;&#35760;&#36827;&#34892;&#23398;&#20064;&#39118;&#26684;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning Style Identification Using Semi-Supervised Self-Taught Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14597
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23398;&#29983;&#23398;&#20064;&#39118;&#26684;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#21487;&#20197;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20135;&#29983;&#21487;&#38752;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#26159;&#19968;&#20010;&#24517;&#39035;&#36866;&#24212;&#31361;&#22914;&#20854;&#26469;&#30340;&#21464;&#21270;&#21644;&#30001;&#22823;&#27969;&#34892;&#30149;&#12289;&#25112;&#20105;&#21644;&#19982;&#27668;&#20505;&#21464;&#21270;&#26377;&#20851;&#30340;&#33258;&#28982;&#28798;&#23475;&#31561;&#20107;&#20214;&#24341;&#36215;&#30340;&#24178;&#25200;&#30340;&#21160;&#24577;&#39046;&#22495;&#12290;&#24403;&#36825;&#20123;&#20107;&#20214;&#21457;&#29983;&#26102;&#65292;&#20256;&#32479;&#35838;&#22530;&#20197;&#20256;&#32479;&#25110;&#34701;&#21512;&#26041;&#24335;&#25552;&#20379;&#30340;&#25945;&#23398;&#21487;&#20197;&#36716;&#21464;&#20026;&#23436;&#20840;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#30340;&#39640;&#25928;&#23398;&#20064;&#29615;&#22659;&#12290;&#34429;&#28982;&#23398;&#20064;&#31649;&#29702;&#31995;&#32479;&#25903;&#25345;&#25945;&#24072;&#30340;&#29983;&#20135;&#21147;&#21644;&#21019;&#36896;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21521;&#35838;&#31243;&#20013;&#25152;&#26377;&#23398;&#20064;&#32773;&#25552;&#20379;&#30456;&#21516;&#30340;&#20869;&#23481;&#65292;&#24573;&#30053;&#20102;&#20182;&#20204;&#29420;&#29305;&#30340;&#23398;&#20064;&#39118;&#26684;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#26469;&#26816;&#27979;&#23398;&#29983;&#30340;&#23398;&#20064;&#39118;&#26684;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#24120;&#29992;&#30340;Felder Silverman&#23398;&#20064;&#39118;&#26684;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#20135;&#29983;&#21487;&#38752;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#38376;&#19981;&#21516;&#35838;&#31243;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14597v1 Announce Type: cross  Abstract: Education is a dynamic field that must be adaptable to sudden changes and disruptions caused by events like pandemics, war, and natural disasters related to climate change. When these events occur, traditional classrooms with traditional or blended delivery can shift to fully online learning, which requires an efficient learning environment that meets students' needs. While learning management systems support teachers' productivity and creativity, they typically provide the same content to all learners in a course, ignoring their unique learning styles. To address this issue, we propose a semi-supervised machine learning approach that detects students' learning styles using a data mining technique. We use the commonly used Felder Silverman learning style model and demonstrate that our semi-supervised method can produce reliable classification models with few labeled data. We evaluate our approach on two different courses and achieve an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Google&#24191;&#21578;&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#24182;&#23558;&#20915;&#31574;&#20256;&#25773;&#22238;&#20854;&#32676;&#38598;&#65292;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14590</link><description>&lt;p&gt;
&#25193;&#23637;LLM&#23457;&#26680;&#20197;&#36827;&#34892;Google&#24191;&#21578;&#20869;&#23481;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Scaling Up LLM Reviews for Google Ads Content Moderation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Google&#24191;&#21578;&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#24182;&#23558;&#20915;&#31574;&#20256;&#25773;&#22238;&#20854;&#32676;&#38598;&#65292;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#20869;&#23481;&#31649;&#29702;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#25104;&#26412;&#21644;&#24310;&#36831;&#20351;&#23427;&#20204;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#22914;Google Ads&#23384;&#20648;&#24211;&#65289;&#19978;&#30340;&#20020;&#26102;&#20351;&#29992;&#25104;&#26412;&#36807;&#39640;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;LLM&#23457;&#26680;&#20197;&#22312;Google Ads&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#36807;&#36807;&#28388;&#21644;&#37325;&#22797;&#39033;&#21024;&#38500;&#26469;&#36873;&#25321;&#20505;&#36873;&#39033;&#65292;&#24182;&#20026;&#27492;&#21019;&#24314;&#24191;&#21578;&#32676;&#38598;&#65292;&#25105;&#20204;&#36873;&#25321;&#27599;&#20010;&#32676;&#38598;&#30340;&#19968;&#20010;&#20195;&#34920;&#24615;&#24191;&#21578;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20165;&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20195;&#34920;&#24615;&#24191;&#21578;&#30340;LLM&#20915;&#31574;&#20256;&#25773;&#22238;&#23427;&#20204;&#30340;&#32676;&#38598;&#12290;&#35813;&#26041;&#27861;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#19982;&#22522;&#32447;&#38750;LLM&#27169;&#22411;&#30456;&#27604;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#32858;&#31867;&#21644;&#26631;&#31614;&#20256;&#25773;&#20013;&#20351;&#29992;&#30340;&#34920;&#31034;; &#25105;&#20204;&#21457;&#29616;&#20132;&#21449;&#27169;&#24577;&#30456;&#20284;&#24615;&#34920;&#31034;&#20135;&#29983;&#27604;&#21333;&#19968;&#27169;&#24577;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m
&lt;/p&gt;</description></item><item><title>AI&#38899;&#20048;&#22312;&#27169;&#20223;&#20154;&#31867;&#38899;&#20048;&#36807;&#31243;&#20013;&#21487;&#33021;&#24418;&#25104;&#33258;&#24049;&#30340;&#20559;&#29233;&#65292;&#21487;&#33021;&#23545;&#25152;&#26377;&#38899;&#20048;&#21382;&#21490;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#22312;&#20445;&#30041;&#19990;&#30028;&#38899;&#20048;&#25991;&#21270;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14589</link><description>&lt;p&gt;
&#36991;&#20813;AI&#23545;&#25152;&#26377;&#38899;&#20048;&#21382;&#21490;&#30340;&#27888;&#21202;&#29256;&#26412;&#36827;&#34892;&#24378;&#21152;
&lt;/p&gt;
&lt;p&gt;
Avoiding an AI-imposed Taylor's Version of all music history
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14589
&lt;/p&gt;
&lt;p&gt;
AI&#38899;&#20048;&#22312;&#27169;&#20223;&#20154;&#31867;&#38899;&#20048;&#36807;&#31243;&#20013;&#21487;&#33021;&#24418;&#25104;&#33258;&#24049;&#30340;&#20559;&#29233;&#65292;&#21487;&#33021;&#23545;&#25152;&#26377;&#38899;&#20048;&#21382;&#21490;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#22312;&#20445;&#30041;&#19990;&#30028;&#38899;&#20048;&#25991;&#21270;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26410;&#26469;&#38899;&#20048;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#32039;&#23494;&#22320;&#36981;&#24490;&#20154;&#31867;&#38899;&#20048;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23545;&#25968;&#25454;&#24211;&#20013;&#30340;&#29305;&#23450;&#20154;&#31867;&#33402;&#26415;&#23478;&#20135;&#29983;&#33258;&#24049;&#30340;&#20559;&#29233;&#65292;&#32780;&#36825;&#20123;&#20559;&#35265;&#22312;&#26368;&#22351;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#25152;&#26377;&#38899;&#20048;&#21382;&#21490;&#30340;&#28508;&#22312;&#23384;&#22312;&#23041;&#32961;&#12290; AI&#30340;&#36229;&#32423;&#31881;&#19997;&#21487;&#33021;&#20250;&#37319;&#21462;&#34892;&#21160;&#26469;&#25197;&#26354;&#21382;&#21490;&#35760;&#24405;&#21644;&#29616;&#26377;&#24405;&#38899;&#65292;&#20197;&#31526;&#21512;&#20182;&#20204;&#33258;&#24049;&#30340;&#21916;&#22909;&#65292;&#19990;&#30028;&#38899;&#20048;&#25991;&#21270;&#30340;&#22810;&#26679;&#24615;&#30340;&#20445;&#23384;&#21487;&#33021;&#20250;&#25104;&#20026;&#19968;&#20010;&#27604;12&#24179;&#22343;&#24459;&#25110;&#20854;&#20182;&#35199;&#26041;&#21516;&#36136;&#21270;&#38382;&#39064;&#26356;&#21152;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;AI&#35206;&#30422;&#36719;&#20214;&#30340;&#25216;&#26415;&#33021;&#21147;&#65292;&#24182;&#21046;&#20316;&#20102;&#19968;&#20123;&#35199;&#26041;&#27969;&#34892;&#38899;&#20048;&#21382;&#21490;&#19978;&#33879;&#21517;&#26354;&#30446;&#30340;&#27888;&#21202;&#29256;&#26412;&#20316;&#20026;&#25361;&#34885;&#24615;&#30340;&#20363;&#23376;&#65307;&#36825;&#20123;&#21046;&#20316;&#30340;&#36136;&#37327;&#24182;&#19981;&#24433;&#21709;&#24635;&#20307;&#35770;&#28857;&#65288;&#29978;&#33267;&#21487;&#33021;&#20250;&#30475;&#21040;&#26410;&#26469;&#30340;AI&#23581;&#35797;&#23558;&#35746;&#20070;&#38025;&#30340;&#22768;&#38899;&#24378;&#21152;&#21040;&#25152;&#26377;&#29616;&#26377;&#38899;&#39057;&#25991;&#20214;&#19978;&#65292;&#26356;&#19981;&#29992;&#35828;&#27888;&#21202;&#183;&#26031;&#23041;&#22827;&#29305;&#20102;&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#26410;&#26469;&#21361;&#38505;&#30340;&#28508;&#22312;&#38450;&#24481;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14589v1 Announce Type: cross  Abstract: As future musical AIs adhere closely to human music, they may form their own attachments to particular human artists in their databases, and these biases may in the worst case lead to potential existential threats to all musical history. AI super fans may act to corrupt the historical record and extant recordings in favour of their own preferences, and preservation of the diversity of world music culture may become even more of a pressing issue than the imposition of 12 tone equal temperament or other Western homogenisations. We discuss the technical capability of AI cover software and produce Taylor's Versions of famous tracks from Western pop history as provocative examples; the quality of these productions does not affect the overall argument (which might even see a future AI try to impose the sound of paperclips onto all existing audio files, let alone Taylor Swift). We discuss some potential defenses against the danger of future m
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;CBA&#31639;&#27861;&#65292;&#20854;&#21033;&#29992;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#30340;&#20551;&#35774;&#33719;&#24471;&#20102;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#32463;&#20856;Exp4&#31639;&#27861;&#30340;&#22870;&#21169;&#30028;&#38480;&#65292;&#25104;&#20026;&#39318;&#20010;&#23545;&#19968;&#33324;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#30340;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#23454;&#29616;&#30028;&#38480;&#30340;&#30740;&#31350;&#32773;&#65292;&#24182;&#22312;&#19987;&#23478;&#26696;&#20363;&#20013;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14585</link><description>&lt;p&gt;
&#20855;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#19987;&#23478;&#24314;&#35758;&#19979;&#30340;&#36172;&#24466;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bandits with Abstention under Expert Advice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14585
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CBA&#31639;&#27861;&#65292;&#20854;&#21033;&#29992;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#30340;&#20551;&#35774;&#33719;&#24471;&#20102;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#32463;&#20856;Exp4&#31639;&#27861;&#30340;&#22870;&#21169;&#30028;&#38480;&#65292;&#25104;&#20026;&#39318;&#20010;&#23545;&#19968;&#33324;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#30340;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#23454;&#29616;&#30028;&#38480;&#30340;&#30740;&#31350;&#32773;&#65292;&#24182;&#22312;&#19987;&#23478;&#26696;&#20363;&#20013;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36172;&#24466;&#21453;&#39304;&#19979;&#21033;&#29992;&#19987;&#23478;&#24314;&#35758;&#36827;&#34892;&#39044;&#27979;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20551;&#35774;&#19968;&#31181;&#34892;&#21160;&#65292;&#21363;&#23398;&#20064;&#32773;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#65292;&#22312;&#27599;&#27425;&#35797;&#39564;&#20013;&#37117;&#27809;&#26377;&#22870;&#21169;&#25110;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CBA&#31639;&#27861;&#65292;&#21033;&#29992;&#36825;&#19968;&#20551;&#35774;&#33719;&#24471;&#20102;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#32463;&#20856;Exp4&#31639;&#27861;&#30340;&#22870;&#21169;&#30028;&#38480;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#25105;&#20204;&#30340;&#38382;&#39064;&#35270;&#20026;&#22312;&#23398;&#20064;&#32773;&#26377;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#36873;&#39033;&#26102;&#23545;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#36827;&#34892;&#32858;&#21512;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23545;&#19968;&#33324;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#30340;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#23454;&#29616;&#30028;&#38480;&#30340;&#30740;&#31350;&#32773;&#12290;&#22312;&#19987;&#23478;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#30028;&#38480;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#20043;&#21069;&#22312;&#19987;&#23478;Exp&#65288;&#23558;&#24323;&#26435;&#35270;&#20026;&#21478;&#19968;&#31181;&#34892;&#21160;&#65289;&#30340;&#36793;&#30028;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#24212;&#29992;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#26377;&#38480;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#29699;&#30340;&#24182;&#38598;&#12290;&#22312;&#36825;&#20010;&#19978;&#19979;&#25991;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;CBA&#30340;&#26377;&#25928;&#23454;&#29616;&#65292;re
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14585v1 Announce Type: new  Abstract: We study the classic problem of prediction with expert advice under bandit feedback. Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. We propose the CBA algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. We can view our problem as the aggregation of confidence-rated predictors when the learner has the option of abstention from play. Importantly, we are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action). As an example application, we discuss learning unions of balls in a finite metric space. In this contextual setting, we devise an efficient implementation of CBA, re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Q&#23398;&#20064;&#35206;&#30422;&#26102;&#38388;&#24863;&#30693;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#36710;&#36733;&#32593;&#32476;&#21644;HD&#22320;&#22270;&#26356;&#26032;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20197;&#20811;&#26381;&#32593;&#32476;&#25317;&#22622;&#12290;</title><link>https://arxiv.org/abs/2402.14582</link><description>&lt;p&gt;
&#36890;&#36807;&#35206;&#30422;&#24863;&#30693;&#21644;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#39640;&#28165;&#22320;&#22270;&#26356;&#26032;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancement of High-definition Map Update Service Through Coverage-aware and Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Q&#23398;&#20064;&#35206;&#30422;&#26102;&#38388;&#24863;&#30693;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#36710;&#36733;&#32593;&#32476;&#21644;HD&#22320;&#22270;&#26356;&#26032;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20197;&#20811;&#26381;&#32593;&#32476;&#25317;&#22622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#65288;HD&#65289;&#22320;&#22270;&#31995;&#32479;&#23558;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#21040;&#26356;&#39640;&#27700;&#24179;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#24471;&#30410;&#20110;&#30456;&#27604;&#20256;&#32479;&#20108;&#32500;&#22320;&#22270;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#21019;&#24314;HD&#22320;&#22270;&#38656;&#35201;&#22823;&#37327;&#30340;&#36335;&#38754;&#21644;&#38750;&#36335;&#38754;&#25968;&#25454;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#21407;&#22987;&#25968;&#25454;&#38598;&#36890;&#36807;&#36710;&#36733;&#32593;&#32476;&#25910;&#38598;&#24182;&#19978;&#20256;&#21040;&#22522;&#20110;&#20113;&#30340;HD&#22320;&#22270;&#26381;&#21153;&#25552;&#20379;&#21830;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21160;&#24577;&#25299;&#25169;&#65292;&#36890;&#36807;&#36710;&#36733;&#26080;&#32447;&#36890;&#36947;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;&#38543;&#30528;&#36710;&#36742;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23545;&#26381;&#21153;&#36136;&#37327;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#25104;&#20026;&#21327;&#21516;&#39550;&#39542;&#20013;&#23454;&#26102;HD&#22320;&#22270;&#31995;&#32479;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Q&#23398;&#20064;&#35206;&#30422;&#26102;&#38388;&#24863;&#30693;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#36710;&#36733;&#32593;&#32476;&#21644;HD&#22320;&#22270;&#26356;&#26032;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20197;&#20811;&#26381;&#32593;&#32476;&#25317;&#22622;&#12290;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#27169;&#25311;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14582v1 Announce Type: cross  Abstract: High-definition (HD) Map systems will play a pivotal role in advancing autonomous driving to a higher level, thanks to the significant improvement over traditional two-dimensional (2D) maps. Creating an HD Map requires a huge amount of on-road and off-road data. Typically, these raw datasets are collected and uploaded to cloud-based HD map service providers through vehicular networks. Nevertheless, there are challenges in transmitting the raw data over vehicular wireless channels due to the dynamic topology. As the number of vehicles increases, there is a detrimental impact on service quality, which acts as a barrier to a real-time HD Map system for collaborative driving in Autonomous Vehicles (AV). In this paper, to overcome network congestion, a Q-learning coverage-time-awareness algorithm is presented to optimize the quality of service for vehicular networks and HD map updates. The algorithm is evaluated in an environment that imita
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;Transformer&#22312;&#31185;&#23398;&#22270;&#34920;&#20013;&#36827;&#34892;&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;LayoutLMv3&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;UDOP&#65292;&#26368;&#39640;&#36798;&#21040;&#20102;82.87&#30340;F1-macro&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14579</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;Transformer&#22312;&#31185;&#23398;&#22270;&#34920;&#20013;&#36827;&#34892;&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Role Classification in Scientific Charts Using Multimodal Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14579
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;Transformer&#22312;&#31185;&#23398;&#22270;&#34920;&#20013;&#36827;&#34892;&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;LayoutLMv3&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;UDOP&#65292;&#26368;&#39640;&#36798;&#21040;&#20102;82.87&#30340;F1-macro&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35282;&#33394;&#20998;&#31867;&#28041;&#21450;&#23545;&#31185;&#23398;&#22270;&#34920;&#20013;&#30340;&#25991;&#26412;&#20803;&#32032;&#30340;&#35821;&#20041;&#35282;&#33394;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#22270;&#34920;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#27169;&#22411;LayoutLMv3&#21644;UDOP&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;Transformer&#21033;&#29992;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#19977;&#31181;&#27169;&#24577;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#26041;&#27861;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#22270;&#34920;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LayoutLMv3&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#22343;&#20248;&#20110;UDOP&#12290;LayoutLMv3&#22312;ICPR22&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;82.87&#30340;&#26368;&#39640;F1-macro&#20998;&#25968;&#65292;&#36229;&#36807;&#20102;ICPR22 CHART-Infographics&#25361;&#25112;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22312;&#19968;&#20010;&#21512;&#25104;&#30340;&#22024;&#26434;&#25968;&#25454;&#38598;ICPR22-N&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#19977;&#20010;&#22270;&#34920;&#25968;&#25454;&#38598;CHIME-R&#12289;DeGruyter&#21644;EconBiz&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#28155;&#21152;&#20102;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14579v1 Announce Type: cross  Abstract: Text role classification involves classifying the semantic role of textual elements within scientific charts. For this task, we propose to finetune two pretrained multimodal document layout analysis models, LayoutLMv3 and UDOP, on chart datasets. The transformers utilize the three modalities of text, image, and layout as input. We further investigate whether data augmentation and balancing methods help the performance of the models. The models are evaluated on various chart datasets, and results show that LayoutLMv3 outperforms UDOP in all experiments. LayoutLMv3 achieves the highest F1-macro score of 82.87 on the ICPR22 test dataset, beating the best-performing model from the ICPR22 CHART-Infographics challenge. Moreover, the robustness of the models is tested on a synthetic noisy dataset ICPR22-N. Finally, the generalizability of the models is evaluated on three chart datasets, CHIME-R, DeGruyter, and EconBiz, for which we added labe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MultiVAW&#26041;&#27861;&#65292;&#23558;Vovk-Azoury-Warmuth&#31639;&#27861;&#25193;&#23637;&#21040;&#22810;&#20803;&#35774;&#32622;&#65292;&#21516;&#26102;&#24212;&#29992;&#20110;&#22312;&#32447;&#23618;&#27425;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25918;&#23485;&#20256;&#32479;&#20998;&#26512;&#25152;&#20570;&#30340;&#20551;&#35774;</title><link>https://arxiv.org/abs/2402.14578</link><description>&lt;p&gt;
&#29992;&#20110;&#23618;&#27425;&#39044;&#27979;&#30340;&#22810;&#20803;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multivariate Online Linear Regression for Hierarchical Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14578
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MultiVAW&#26041;&#27861;&#65292;&#23558;Vovk-Azoury-Warmuth&#31639;&#27861;&#25193;&#23637;&#21040;&#22810;&#20803;&#35774;&#32622;&#65292;&#21516;&#26102;&#24212;&#29992;&#20110;&#22312;&#32447;&#23618;&#27425;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#25918;&#23485;&#20256;&#32479;&#20998;&#26512;&#25152;&#20570;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#22312;&#32447;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#20801;&#35768;&#21709;&#24212;&#26159;&#22810;&#20803;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MultiVAW&#65292;&#19968;&#31181;&#23558;&#33879;&#21517;&#30340;Vovk-Azoury-Warmuth&#31639;&#27861;&#25193;&#23637;&#21040;&#22810;&#20803;&#35774;&#32622;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#26102;&#38388;&#19978;&#20063;&#20855;&#26377;&#23545;&#25968;&#36951;&#25022;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#20110;&#22312;&#32447;&#23618;&#27425;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20010;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#21152;&#20197;&#24674;&#22797;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#36890;&#24120;&#29992;&#20110;&#20854;&#20998;&#26512;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14578v1 Announce Type: cross  Abstract: In this paper, we consider a deterministic online linear regression model where we allow the responses to be multivariate. To address this problem, we introduce MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth algorithm to the multivariate setting, and show that it also enjoys logarithmic regret in time. We apply our results to the online hierarchical forecasting problem and recover an algorithm from this literature as a special case, allowing us to relax the hypotheses usually made for its analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#20013;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#30340;&#29305;&#24615;&#65292;&#32508;&#21512;&#32771;&#34385;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.14576</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36793;&#32536;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Edge Caching Based on Deep Reinforcement Learning and Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#20013;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#30340;&#29305;&#24615;&#65292;&#32508;&#21512;&#32771;&#34385;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32593;&#32476;&#20013;&#20887;&#20313;&#25968;&#25454;&#20256;&#36755;&#26085;&#30410;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#27969;&#37327;&#28608;&#22686;&#24050;&#32463;&#20351;&#20013;&#32487;&#38142;&#36335;&#21644;&#39592;&#24178;&#32593;&#32476;&#25215;&#21387;&#65292;&#20419;&#20351;&#23545;&#36793;&#32536;&#36335;&#30001;&#22120;&#30340;&#32531;&#23384;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25506;&#32034;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#22788;&#29702;&#32531;&#23384;&#38382;&#39064;&#65292;&#20551;&#35774;&#22266;&#23450;&#26102;&#38388;&#38388;&#38548;&#30340;&#20915;&#31574;&#65307;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#28041;&#21450;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#65292;&#23613;&#31649;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#22312;&#30830;&#23450;&#26368;&#20339;&#32531;&#23384;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#30456;&#20851;&#30340;&#29616;&#26377;&#24037;&#20316;&#24182;&#26410;&#32771;&#34385;&#25152;&#26377;&#36825;&#20123;&#25991;&#20214;&#29305;&#24449;&#26469;&#24418;&#25104;&#32531;&#23384;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#21033;&#29992;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#26469;&#24314;&#27169;&#32531;&#23384;&#38382;&#39064;&#65292;&#20197;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#30340;&#36830;&#32493;&#26102;&#38388;&#29305;&#24615;&#65292;&#20801;&#35768;&#22312;&#25991;&#20214;&#35831;&#27714;&#26102;&#38543;&#26426;&#36827;&#34892;&#32531;&#23384;&#20915;&#31574;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#19981;&#21516;&#25991;&#20214;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14576v1 Announce Type: cross  Abstract: This paper addresses the escalating challenge of redundant data transmission in networks. The surge in traffic has strained backhaul links and backbone networks, prompting the exploration of caching solutions at the edge router. Existing work primarily relies on Markov Decision Processes (MDP) for caching issues, assuming fixed-time interval decisions; however, real-world scenarios involve random request arrivals, and despite the critical role of various file characteristics in determining an optimal caching policy, none of the related existing work considers all these file characteristics in forming a caching policy. In this paper, first, we formulate the caching problem using a semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature of real-world scenarios allowing for caching decisions at random times upon file requests. Then, we propose a double deep Q-learning-based caching approach that comprehensively accou
&lt;/p&gt;</description></item><item><title>CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.14551</link><description>&lt;p&gt;
CLCE&#65306;&#19968;&#31181;&#20248;&#21270;&#23398;&#20064;&#34701;&#21512;&#30340;&#25913;&#36827;&#20132;&#21449;&#29109;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14551
&lt;/p&gt;
&lt;p&gt;
CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#20027;&#35201;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21021;&#22987;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#65288;CE&#65289;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;CE&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CLCE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;CE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#20197;&#21327;&#21516;&#26041;&#24335;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14551v1 Announce Type: cross  Abstract: State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24322;&#26041;&#24046;Aleatoric&#21644;&#35748;&#30693;&#26041;&#24046;&#23884;&#20837;&#21040;&#23398;&#20064;BNN&#21442;&#25968;&#30340;&#26041;&#24046;&#20013;&#65292;&#25913;&#21892;&#20102;&#36731;&#37327;&#32423;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14532</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#24322;&#26041;&#24046;&#19981;&#30830;&#23450;&#24615;&#30340;&#36731;&#37327;&#32423;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21464;&#20998;&#25512;&#26029;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Variational Inference of Lightweight Bayesian Neural Networks with Heteroscedastic Uncertainties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24322;&#26041;&#24046;Aleatoric&#21644;&#35748;&#30693;&#26041;&#24046;&#23884;&#20837;&#21040;&#23398;&#20064;BNN&#21442;&#25968;&#30340;&#26041;&#24046;&#20013;&#65292;&#25913;&#21892;&#20102;&#36731;&#37327;&#32423;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#20013;&#33719;&#24471;&#24322;&#26041;&#24046;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#38500;&#20102;&#39044;&#27979;&#22343;&#20540;&#22806;&#65292;&#24322;&#26041;&#24046;Aleatoric&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;BNN&#30340;&#36755;&#20986;&#36827;&#34892;&#23398;&#20064;&#65292;&#28982;&#32780;&#36825;&#26679;&#20570;&#21487;&#33021;&#38656;&#35201;&#21521;&#32593;&#32476;&#20013;&#28155;&#21152;&#26356;&#22810;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24322;&#26041;&#24046;Aleatoric&#21644;&#35748;&#30693;&#26041;&#24046;&#22343;&#21487;&#20197;&#23884;&#20837;&#21040;&#23398;&#20064;BNN&#21442;&#25968;&#30340;&#26041;&#24046;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#36731;&#37327;&#32423;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#30697;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;BNNs&#30340;&#26080;&#38656;&#21462;&#26679;&#30340;&#21464;&#20998;&#25512;&#26029;&#30456;&#23545;&#31616;&#21333;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14532v1 Announce Type: new  Abstract: Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural Network (BNN) is vital to many applications. Often, heteroscedastic aleatoric uncertainties are learned as outputs of the BNN in addition to the predictive means, however doing so may necessitate adding more learnable parameters to the network. In this work, we demonstrate that both the heteroscedastic aleatoric and epistemic variance can be embedded into the variances of learned BNN parameters, improving predictive performance for lightweight networks. By complementing this approach with a moment propagation approach to inference, we introduce a relatively simple framework for sampling-free variational inference suitable for lightweight BNNs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#22240;&#32452;&#23398;&#25110;&#36716;&#24405;&#32452;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20351;&#29992; TensorFlow Federated &#21644; Flower &#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#22521;&#35757;&#30142;&#30149;&#39044;&#21518;&#21644;&#32454;&#32990;&#31867;&#22411;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14527</link><description>&lt;p&gt;
&#22522;&#22240;&#32452;&#23398;&#25110;&#36716;&#24405;&#32452;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#27169;&#22411;&#36136;&#37327;&#21644;&#24615;&#33021;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#22240;&#32452;&#23398;&#25110;&#36716;&#24405;&#32452;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20351;&#29992; TensorFlow Federated &#21644; Flower &#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#22521;&#35757;&#30142;&#30149;&#39044;&#21518;&#21644;&#32454;&#32990;&#31867;&#22411;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22522;&#22240;&#32452;&#23398;&#25110;&#36716;&#24405;&#32452;&#25968;&#25454;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#23545;&#35768;&#22810;&#26032;&#39062;&#30340;&#20581;&#24247;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#31934;&#20934;&#21307;&#23398;&#21487;&#20197;&#26681;&#25454;&#20010;&#20307;&#29983;&#29289;&#26631;&#24535;&#29289;&#12289;&#32454;&#32990;&#21644;&#20998;&#23376;&#29366;&#24577;&#31561;&#20010;&#20307;&#20449;&#24687;&#26469;&#37327;&#36523;&#23450;&#21046;&#21307;&#23398;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#25152;&#38656;&#25968;&#25454;&#25935;&#24863;&#12289;&#24222;&#22823;&#12289;&#24322;&#36136;&#65292;&#24182;&#19988;&#36890;&#24120;&#20998;&#24067;&#22312;&#26080;&#27861;&#20351;&#29992;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#22320;&#28857;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#30417;&#31649;&#21407;&#22240;&#65292;&#22312;&#21487;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#22788;&#32858;&#21512;&#25152;&#26377;&#25968;&#25454;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#36825;&#19968;&#22256;&#22659;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#23454;&#29616;&#20102;&#22312;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20998;&#25955;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550; TensorFlow Federated &#21644; Flower &#36827;&#34892;&#27604;&#36739;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#26696;&#20363;&#26159;&#22521;&#35757;&#30142;&#30149;&#39044;&#21518;&#21644;&#32454;&#32990;&#31867;&#22411;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#24067;&#24335;&#36716;&#24405;&#32452;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14527v1 Announce Type: new  Abstract: Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications. For example, precision medicine tailors medical treatments to patients on the basis of individual biomarkers, cellular and molecular states, etc. However, the data required is sensitive, voluminous, heterogeneous, and typically distributed across locations where dedicated machine learning hardware is not available. Due to privacy and regulatory reasons, it is also problematic to aggregate all data at a trusted third party.Federated learning is a promising solution to this dilemma, because it enables decentralized, collaborative machine learning without exchanging raw data. In this paper, we perform comparative experiments with the federated learning frameworks TensorFlow Federated and Flower. Our test case is the training of disease prognosis and cell type classification models. We train the models with distributed transcriptom
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#29983;&#25104;&#21463;&#36816;&#21160;&#32422;&#26463;&#30340;&#31867;&#20154;&#21452;&#25163;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#27969;&#30021;&#33258;&#28982;&#30340;&#26426;&#22120;&#20154;&#23545;&#20154;&#31867;&#29289;&#20307;&#20132;&#25509;&#12290;</title><link>https://arxiv.org/abs/2402.14525</link><description>&lt;p&gt;
&#21463;&#36816;&#21160;&#32422;&#26463;&#30340;&#31867;&#20154;&#21452;&#25163;&#26426;&#22120;&#20154;&#23545;&#20154;&#31867;&#30340;&#20132;&#25509;
&lt;/p&gt;
&lt;p&gt;
Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#29983;&#25104;&#21463;&#36816;&#21160;&#32422;&#26463;&#30340;&#31867;&#20154;&#21452;&#25163;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#27969;&#30021;&#33258;&#28982;&#30340;&#26426;&#22120;&#20154;&#23545;&#20154;&#31867;&#29289;&#20307;&#20132;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#25163;&#20132;&#25509;&#23545;&#20110;&#20256;&#36755;&#22823;&#22411;&#12289;&#26131;&#21464;&#24418;&#25110;&#26131;&#25439;&#22351;&#30340;&#29289;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21463;&#36816;&#21160;&#32422;&#26463;&#30340;&#31867;&#20154;&#21452;&#25163;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#20197;&#30830;&#20445;&#26426;&#22120;&#20154;&#23545;&#20154;&#31867;&#29289;&#20307;&#20132;&#25509;&#30340;&#27969;&#30021;&#21644;&#33258;&#28982;&#12290;&#25105;&#20204;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;&#20249;&#20276;&#30340;&#21160;&#20316;&#21453;&#24212;&#24615;&#22320;&#29983;&#25104;&#21512;&#36866;&#30340;&#21709;&#24212;&#36712;&#36857;&#12290;&#36825;&#20123;&#36712;&#36857;&#36890;&#36807;&#20219;&#21153;&#31354;&#38388;&#32422;&#26463;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#30340;&#20132;&#25509;&#12290;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35748;&#20026;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#65292;&#19982;&#22522;&#32447;&#36870;&#36816;&#21160;&#23398;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14525v1 Announce Type: cross  Abstract: Bimanual handovers are crucial for transferring large, deformable or delicate objects. This paper proposes a framework for generating kinematically constrained human-like bimanual robot motions to ensure seamless and natural robot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to reactively generate suitable response trajectories for a robot based on the observed human partner's motion. The trajectories are adapted with task space constraints to ensure accurate handovers. Results from a pilot study show that our approach is perceived as more human--like compared to a baseline Inverse Kinematics approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#19981;&#21516;&#27169;&#22411;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#24471;&#20219;&#21153;&#23884;&#20837;&#21487;&#20197;&#36328;&#36234;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#22312;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.14522</link><description>&lt;p&gt;
&#36328;&#36234;&#22810;&#20010;&#27169;&#22411;&#30340;&#32479;&#19968;&#20219;&#21153;&#23884;&#20837;&#65306;&#24357;&#21512;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#23427;&#27169;&#22411;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#19981;&#21516;&#27169;&#22411;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#24471;&#20219;&#21153;&#23884;&#20837;&#21487;&#20197;&#36328;&#36234;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#22312;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23884;&#20837;&#26159;&#19968;&#31181;&#25429;&#25417;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#27169;&#22411;&#32534;&#36753;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#39046;&#22495;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#20219;&#21153;&#23884;&#20837;&#65288;FUTE&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#21327;&#35843;&#26469;&#33258;&#21508;&#31181;&#27169;&#22411;&#65288;&#21253;&#25324;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20855;&#26377;&#19981;&#21516;&#25552;&#31034;&#30340;LLMs&#65289;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#20854;&#22788;&#20110;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#12290;&#36825;&#31181;&#32479;&#19968;&#24615;&#20351;&#24471;&#21487;&#20197;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#35299;&#20915;&#22810;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#33539;&#22260;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14522v1 Announce Type: new  Abstract: Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-mo
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14515</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#21644;&#26497;&#22823;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14515
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28909;&#38376;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#19982;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#23494;&#20999;&#32852;&#31995;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;QNN&#21487;&#20197;&#34920;&#31034;&#20026;&#26377;&#38480;&#20613;&#37324;&#21494;&#32423;&#25968;&#65292;&#20854;&#20013;&#39057;&#29575;&#38598;&#34987;&#31216;&#20026;&#39057;&#35889;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#39057;&#35889;&#24182;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#27169;&#22411;&#65292;&#23384;&#22312;&#21508;&#31181;&#26497;&#22823;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#20445;&#25345;&#39057;&#35889;&#30340;&#20855;&#26377;&#30456;&#21516;&#38754;&#31215;$A = RL$&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#30340;&#21452;&#23556;&#65292;&#20854;&#20013;$R$&#34920;&#31034;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#65292;$L$&#34920;&#31034;&#23618;&#25968;&#65292;&#25105;&#20204;&#22240;&#27492;&#31216;&#20043;&#20026;&#38754;&#31215;&#20445;&#25345;&#21464;&#25442;&#19979;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#30340;&#22312;&#32467;&#26524;&#20013;$R$&#21644;$L$&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#22823;&#39057;&#35889;&#30340;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14515v1 Announce Type: cross  Abstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum. We analyse this frequency spectrum and prove, for a large class of models, various maximality results. Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depen
&lt;/p&gt;</description></item><item><title>Equilibrium K-Means&#65288;EKM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#32858;&#31867;&#20013;&#24515;&#22312;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14490</link><description>&lt;p&gt;
&#20351;&#29992;Equilibrium K-Means&#36827;&#34892;&#19981;&#24179;&#34913;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imbalanced Data Clustering using Equilibrium K-Means
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14490
&lt;/p&gt;
&lt;p&gt;
Equilibrium K-Means&#65288;EKM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#32858;&#31867;&#20013;&#24515;&#22312;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#25351;&#30340;&#26159;&#25968;&#25454;&#28857;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#34913;&#65292;&#36825;&#32473;&#20256;&#32479;&#30340;&#30828;&#32858;&#31867;&#31639;&#27861;&#21644;&#27169;&#31946;&#32858;&#31867;&#31639;&#27861;&#65288;&#22914;&#30828;K&#22343;&#20540;&#65288;HKM&#65292;&#25110;&#32773;Lloyd&#31639;&#27861;&#65289;&#21644;&#27169;&#31946;K&#22343;&#20540;&#65288;FKM&#65292;&#25110;&#32773;Bezdek&#31639;&#27861;&#65289;&#65289;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#8212;&#8212;Equilibrium K-Means&#65288;EKM&#65289;&#65292;&#23427;&#22312;&#20004;&#20010;&#27493;&#39588;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#65292;&#20943;&#23569;&#20102;&#32858;&#31867;&#20013;&#24515;&#21521;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;HKM&#12289;FKM&#21644;EKM&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#34920;&#26126;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#20855;&#26377;&#26126;&#30830;&#20851;&#31995;&#30340;&#29275;&#39039;&#26041;&#27861;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;EKM&#20855;&#26377;&#19982;FKM&#30456;&#21516;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#23545;&#20854;&#25104;&#21592;&#23450;&#20041;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#30340;&#29289;&#29702;&#24847;&#20041;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#21313;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;EKM&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14490v1 Announce Type: new  Abstract: Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#25193;&#23637;&#25299;&#25169;&#20266;&#36317;&#31163;&#65288;ETD&#65289;&#30340;&#20266;&#36317;&#31163;&#65292;&#20855;&#26377;&#21487;&#35843;&#33410;&#30340;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#36817;&#20284;&#20999;&#29255;&#12290;</title><link>https://arxiv.org/abs/2402.14489</link><description>&lt;p&gt;
&#19968;&#31867;&#29992;&#20110;&#24555;&#36895;&#27604;&#36739;&#25345;&#20037;&#22270;&#30340;&#25299;&#25169;&#20266;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
A Class of Topological Pseudodistances for Fast Comparison of Persistence Diagrams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#25193;&#23637;&#25299;&#25169;&#20266;&#36317;&#31163;&#65288;ETD&#65289;&#30340;&#20266;&#36317;&#31163;&#65292;&#20855;&#26377;&#21487;&#35843;&#33410;&#30340;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#36817;&#20284;&#20999;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#20037;&#22270;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#24182;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#12290;&#25345;&#20037;&#22270;&#25968;&#25454;&#30340;&#27604;&#36739;&#38656;&#35201;&#35745;&#31639;&#22823;&#37327;&#25345;&#20037;&#22270;&#20043;&#38388;&#30340;&#27604;&#36739;&#24230;&#37327;&#65292;&#36825;&#20123;&#24230;&#37327;&#38656;&#35201;&#26082;&#20934;&#30830;&#21448;&#29702;&#35770;&#19978;&#21487;&#38752;&#65292;&#21516;&#26102;&#35745;&#31639;&#36895;&#24230;&#20063;&#35201;&#24555;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;&#26356;&#23494;&#38598;&#30340;&#22810;&#32500;&#25345;&#20037;&#22270;&#65292;&#36825;&#26679;&#30340;&#27604;&#36739;&#24230;&#37327;&#23578;&#26410;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#20266;&#36317;&#31163;&#65292;&#31216;&#20026;&#25193;&#23637;&#25299;&#25169;&#20266;&#36317;&#31163;&#65288;ETD&#65289;&#65292;&#20855;&#26377;&#21487;&#35843;&#33410;&#30340;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#36817;&#20284;&#20999;&#29255;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14489v1 Announce Type: cross  Abstract: Persistence diagrams (PD)s play a central role in topological data analysis, and are used in an ever increasing variety of applications. The comparison of PD data requires computing comparison metrics among large sets of PDs, with metrics which are accurate, theoretically sound, and fast to compute. Especially for denser multi-dimensional PDs, such comparison metrics are lacking. While on the one hand, Wasserstein-type distances have high accuracy and theoretical guarantees, they incur high computational cost. On the other hand, distances between vectorizations such as Persistence Statistics (PS)s have lower computational cost, but lack the accuracy guarantees and in general they are not guaranteed to distinguish PDs (i.e. the two PS vectors of different PDs may be equal). In this work we introduce a class of pseudodistances called Extended Topological Pseudodistances (ETD)s, which have tunable complexity, and can approximate Sliced an
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22312;&#38544;&#34255;&#21160;&#20316;&#27169;&#22411;&#19979;&#30340;&#21512;&#21516;&#19982;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#30340;&#26377;&#30028;&#21512;&#21516;&#65292;&#23545;&#20110;&#19968;&#33324;&#24773;&#20917;&#30340;&#26597;&#35810;&#27425;&#25968;&#20855;&#26377;&#22810;&#39033;&#24335;&#19978;&#30028;&#65292;&#24182;&#19988;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.14486</link><description>&lt;p&gt;
&#36793;&#30028;&#21512;&#21516;&#26159;&#21542;&#21487;&#23398;&#20064;&#24182;&#36817;&#20284;&#26368;&#20248;?
&lt;/p&gt;
&lt;p&gt;
Are Bounded Contracts Learnable and Approximately Optimal?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14486
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22312;&#38544;&#34255;&#21160;&#20316;&#27169;&#22411;&#19979;&#30340;&#21512;&#21516;&#19982;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#30340;&#26377;&#30028;&#21512;&#21516;&#65292;&#23545;&#20110;&#19968;&#33324;&#24773;&#20917;&#30340;&#26597;&#35810;&#27425;&#25968;&#20855;&#26377;&#22810;&#39033;&#24335;&#19978;&#30028;&#65292;&#24182;&#19988;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#30340;&#38544;&#34255;&#21160;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#22996;&#25176;&#26041;&#36890;&#36807;&#21512;&#21516;&#28608;&#21169;&#20195;&#29702;&#20154;&#25353;&#21512;&#21516;&#24320;&#23637;&#39033;&#30446;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#26377;&#30028;&#25903;&#20184;&#30340;&#21512;&#21516;&#26159;&#21542;&#21487;&#23398;&#20064;&#24182;&#36817;&#20284;&#26368;&#20248;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#20004;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#30028;&#30340;&#26597;&#35810;&#27425;&#25968;&#20869;&#25214;&#21040;&#20960;&#20046;&#26368;&#20248;&#30340;&#26377;&#30028;&#21512;&#21516;&#65292;&#22522;&#20110;&#25991;&#29486;&#20013;&#30340;&#20004;&#20010;&#26631;&#20934;&#20551;&#35774;&#65306;&#20195;&#29702;&#20154;&#30340;&#26356;&#26114;&#36149;&#30340;&#34892;&#21160;&#23548;&#33268;&#22996;&#25176;&#26041;&#30340;&#26356;&#22909;&#30340;&#32467;&#26524;&#20998;&#24067;&#65292;&#24182;&#19988;&#20195;&#29702;&#20154;&#30340;&#25104;&#26412;/&#21162;&#21147;&#20855;&#26377;&#36882;&#20943;&#22238;&#25253;&#12290;&#25105;&#20204;&#30340;&#22810;&#39033;&#24335;&#26597;&#35810;&#22797;&#26434;&#24230;&#19978;&#30028;&#34920;&#26126;&#65292;&#26631;&#20934;&#20551;&#35774;&#36275;&#20197;&#23454;&#29616;&#23545;&#19968;&#33324;&#24773;&#20917;&#24050;&#30693;&#19979;&#30028;&#30340;&#25351;&#25968;&#25913;&#36827;&#12290;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#20381;&#36182;&#20110;&#23545;&#21512;&#21516;&#31354;&#38388;&#30340;&#31163;&#25955;&#21270;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14486v1 Announce Type: cross  Abstract: This paper considers the hidden-action model of the principal-agent problem, in which a principal incentivizes an agent to work on a project using a contract. We investigate whether contracts with bounded payments are learnable and approximately optimal. Our main results are two learning algorithms that can find a nearly optimal bounded contract using a polynomial number of queries, under two standard assumptions in the literature: a costlier action for the agent leads to a better outcome distribution for the principal, and the agent's cost/effort has diminishing returns. Our polynomial query complexity upper bound shows that standard assumptions are sufficient for achieving an exponential improvement upon the known lower bound for general instances. Unlike the existing algorithms, which relied on discretizing the contract space, our algorithms directly learn the underlying outcome distributions. As for the approximate optimality of bo
&lt;/p&gt;</description></item><item><title>SpanSeq &#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#25968;&#25454;&#24207;&#21015;&#30340;&#25968;&#25454;&#24211;&#20998;&#21306;&#26041;&#27861;&#65292;&#33021;&#22815;&#36991;&#20813;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#25968;&#25454;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.14482</link><description>&lt;p&gt;
SpanSeq&#65306;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#39033;&#30446;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#24207;&#21015;&#25968;&#25454;&#25286;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14482
&lt;/p&gt;
&lt;p&gt;
SpanSeq &#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#25968;&#25454;&#24207;&#21015;&#30340;&#25968;&#25454;&#24211;&#20998;&#21306;&#26041;&#27861;&#65292;&#33021;&#22815;&#36991;&#20813;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22686;&#21152;&#24456;&#22823;&#65292;&#24182;&#19988;&#38543;&#30528;&#35832;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#39044;&#35745;&#23558;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SpanSeq&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#29983;&#29289;&#24207;&#21015;&#65288;&#22522;&#22240;&#12289;&#34507;&#30333;&#36136;&#21644;&#22522;&#22240;&#32452;&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#24211;&#20998;&#21306;&#26041;&#27861;&#65292;&#26088;&#22312;&#36991;&#20813;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14482v1 Announce Type: new  Abstract: The use of deep learning models in computational biology has increased massively in recent years, and is expected to do so further with the current advances in fields like Natural Language Processing. These models, although able to draw complex relations between input and target, are also largely inclined to learn noisy deviations from the pool of data used during their development. In order to assess their performance on unseen data (their capacity to generalize), it is common to randomly split the available data in development (train/validation) and test sets. This procedure, although standard, has lately been shown to produce dubious assessments of generalization due to the existing similarity between samples in the databases used. In this work, we present SpanSeq, a database partition method for machine learning that can scale to most biological sequences (genes, proteins and genomes) in order to avoid data leakage between sets. We a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#21160;&#22240;&#26524;&#21457;&#29616;&#65288;AutoCD&#65289;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#23436;&#20840;&#33258;&#21160;&#21270;&#24212;&#29992;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;5G&#30005;&#20449;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14481</link><description>&lt;p&gt;
&#26397;&#21521;&#33258;&#21160;&#22240;&#26524;&#25512;&#26029;&#65306;&#22522;&#20110;5G&#30005;&#20449;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Causal Discovery: a case study on 5G telecommunication data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#21160;&#22240;&#26524;&#21457;&#29616;&#65288;AutoCD&#65289;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#23436;&#20840;&#33258;&#21160;&#21270;&#24212;&#29992;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;5G&#30005;&#20449;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21160;&#22240;&#26524;&#21457;&#29616;&#65288;AutoCD&#65289;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20026;&#26088;&#22312;&#23436;&#20840;&#33258;&#21160;&#21270;&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#27861;&#24212;&#29992;&#30340;&#20219;&#20309;&#31995;&#32479;&#12290; AutoCD&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19987;&#23478;&#20154;&#31867;&#20998;&#26512;&#24072;&#20250;&#25552;&#20379;&#30340;&#25152;&#26377;&#22240;&#26524;&#20449;&#24687;&#65292;&#24182;&#22238;&#31572;&#29992;&#25143;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#36825;&#26679;&#19968;&#20010;&#24179;&#21488;&#30340;&#26550;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#26102;&#38388;&#30005;&#20449;&#25968;&#25454;&#12290;&#35813;&#31995;&#32479;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#37327;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14481v1 Announce Type: new  Abstract: We introduce the concept of Automated Causal Discovery (AutoCD), defined as any system that aims to fully automate the application of causal discovery and causal reasoning methods. AutoCD's goal is to deliver all causal information that an expert human analyst would and answer a user's causal queries. We describe the architecture of such a platform, and illustrate its performance on synthetic data sets. As a case study, we apply it on temporal telecommunication data. The system is general and can be applied to a plethora of causal discovery problems.
&lt;/p&gt;</description></item><item><title>DynGMA&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#36817;&#20284;&#65292;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#22312;&#23398;&#20064;&#23436;&#20840;&#26410;&#30693;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#19981;&#21464;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14475</link><description>&lt;p&gt;
DynGMA&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#23398;&#20064;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#20581;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGMA: a robust approach for learning stochastic differential equations from data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14475
&lt;/p&gt;
&lt;p&gt;
DynGMA&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#36817;&#20284;&#65292;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#22312;&#23398;&#20064;&#23436;&#20840;&#26410;&#30693;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#19981;&#21464;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#30693;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#20284;&#21442;&#25968;&#21270;SDE&#36716;&#31227;&#23494;&#24230;&#30340;&#26041;&#27861;&#65306;&#21463;&#21160;&#21147;&#31995;&#32479;&#38543;&#26426;&#25668;&#21160;&#29702;&#35770;&#21551;&#21457;&#30340;&#39640;&#26031;&#23494;&#24230;&#36817;&#20284;&#65292;&#20197;&#21450;&#23427;&#30340;&#25193;&#23637;&#65292;&#21160;&#21147;&#39640;&#26031;&#28151;&#21512;&#36817;&#20284;&#65288;DynGMA&#65289;&#12290;&#21463;&#30410;&#20110;&#31283;&#20581;&#30340;&#23494;&#24230;&#36817;&#20284;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#23436;&#20840;&#26410;&#30693;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#30697;&#19981;&#21464;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14475v1 Announce Type: new  Abstract: Learning unknown stochastic differential equations (SDEs) from observed data is a significant and challenging task with applications in various fields. Current approaches often use neural networks to represent drift and diffusion functions, and construct likelihood-based loss by approximating the transition density to train these networks. However, these methods often rely on one-step stochastic numerical schemes, necessitating data with sufficiently high time resolution. In this paper, we introduce novel approximations to the transition density of the parameterized SDE: a Gaussian density approximation inspired by the random perturbation theory of dynamical systems, and its extension, the dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invari
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#25551;&#36848;&#12289;&#35299;&#37322;&#21644;&#35843;&#35797;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;GAMs&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#32467;&#21512;LLMs&#30340;&#28789;&#27963;&#24615;&#21644;GAMs&#20934;&#30830;&#25551;&#36848;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#38598;&#25688;&#35201;&#12289;&#38382;&#31572;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.14474</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21644;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Data Science with LLMs and Interpretable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#25551;&#36848;&#12289;&#35299;&#37322;&#21644;&#35843;&#35797;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;GAMs&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#32467;&#21512;LLMs&#30340;&#28789;&#27963;&#24615;&#21644;GAMs&#20934;&#30830;&#25551;&#36848;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#38598;&#25688;&#35201;&#12289;&#38382;&#31572;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26088;&#22312;&#34987;&#20154;&#31867;&#36731;&#26494;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20351;&#29992;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#25551;&#36848;&#12289;&#35299;&#37322;&#21644;&#35843;&#35797;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;(GAMs)&#12290;&#23558;LLMs&#30340;&#28789;&#27963;&#24615;&#19982;GAMs&#20934;&#30830;&#25551;&#36848;&#30340;&#24191;&#27867;&#32479;&#35745;&#27169;&#24335;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#23454;&#29616;&#25968;&#25454;&#38598;&#25688;&#35201;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;LLMs&#36824;&#21487;&#20197;&#25913;&#21892;&#39046;&#22495;&#19987;&#23478;&#19982;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#20026;&#28508;&#22312;&#29616;&#35937;&#29983;&#25104;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#24320;&#28304;LLM-GAM&#25509;&#21475;\url{https://github.com/interpretml/TalkToEBM}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14474v1 Announce Type: cross  Abstract: Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans. In this work, we show that large language models (LLMs) are remarkably good at working with interpretable models, too. In particular, we show that LLMs can describe, interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility of LLMs with the breadth of statistical patterns accurately described by GAMs enables dataset summarization, question answering, and model critique. LLMs can also improve the interaction between domain experts and interpretable models, and generate hypotheses about the underlying phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an open-source LLM-GAM interface.
&lt;/p&gt;</description></item><item><title>&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#21453;&#20107;&#23454;&#31034;&#20363;&#20197;&#25429;&#33719;&#24322;&#24120;&#30340;&#22810;&#26679;&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23545;&#35302;&#21457;&#24322;&#24120;&#26816;&#27979;&#22120;&#26426;&#21046;&#30340;&#39640;&#32423;&#35821;&#20041;&#35299;&#37322;&#65292;&#20801;&#35768;&#25506;&#32034;&#8220;&#20551;&#35774;&#24773;&#26223;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.14469</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#24322;&#24120;&#65306;&#22914;&#26524;&#24322;&#24120;&#26159;&#27491;&#24120;&#30340;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reimagining Anomalies: What If Anomalies Were Normal?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14469
&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#21453;&#20107;&#23454;&#31034;&#20363;&#20197;&#25429;&#33719;&#24322;&#24120;&#30340;&#22810;&#26679;&#27010;&#24565;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23545;&#35302;&#21457;&#24322;&#24120;&#26816;&#27979;&#22120;&#26426;&#21046;&#30340;&#39640;&#32423;&#35821;&#20041;&#35299;&#37322;&#65292;&#20801;&#35768;&#25506;&#32034;&#8220;&#20551;&#35774;&#24773;&#26223;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20294;&#20854;&#22797;&#26434;&#24615;&#32473;&#29702;&#35299;&#20026;&#20309;&#23454;&#20363;&#34987;&#39044;&#27979;&#20026;&#24322;&#24120;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#24322;&#24120;&#29983;&#25104;&#22810;&#20010;&#21453;&#20107;&#23454;&#31034;&#20363;&#65292;&#25429;&#33719;&#24322;&#24120;&#30340;&#22810;&#26679;&#27010;&#24565;&#12290;&#21453;&#20107;&#23454;&#31034;&#20363;&#26159;&#23545;&#24322;&#24120;&#30340;&#20462;&#25913;&#65292;&#34987;&#24322;&#24120;&#26816;&#27979;&#22120;&#35270;&#20026;&#27491;&#24120;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#35302;&#21457;&#24322;&#24120;&#26816;&#27979;&#22120;&#26426;&#21046;&#30340;&#39640;&#32423;&#35821;&#20041;&#35299;&#37322;&#65292;&#20801;&#35768;&#29992;&#25143;&#25506;&#32034;&#8220;&#20551;&#35774;&#24773;&#26223;&#8221;&#12290;&#23545;&#19981;&#21516;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#21487;&#20197;&#23454;&#29616;&#23545;&#26816;&#27979;&#22120;&#30340;&#39640;&#36136;&#37327;&#35821;&#20041;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14469v1 Announce Type: cross  Abstract: Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness. A counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector. The method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore "what-if scenarios." Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#20102;&#27874;&#35199;&#22810;&#23612;&#20122;&#28023;&#33609;&#22312;&#22320;&#20013;&#28023;&#27700;&#22495;&#20013;&#23545;&#27700;&#29983;&#29289;&#22320;&#29699;&#21270;&#23398;&#29305;&#24615;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#28023;&#27915;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14459</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#20102;&#27874;&#35199;&#22810;&#23612;&#20122;&#28023;&#33609;&#23545;&#22320;&#20013;&#28023;&#27700;&#22495;&#30340;&#22823;&#35268;&#27169;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on Mediterranean Sea Water
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#20102;&#27874;&#35199;&#22810;&#23612;&#20122;&#28023;&#33609;&#22312;&#22320;&#20013;&#28023;&#27700;&#22495;&#20013;&#23545;&#27700;&#29983;&#29289;&#22320;&#29699;&#21270;&#23398;&#29305;&#24615;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#28023;&#27915;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#35199;&#22810;&#23612;&#20122;&#28023;&#33609;&#26159;&#22320;&#20013;&#28023;&#30340;&#19968;&#31181;&#21463;&#20445;&#25252;&#30340;&#29305;&#26377;&#28023;&#33609;&#65292;&#20419;&#36827;&#29983;&#29289;&#22810;&#26679;&#24615;&#65292;&#20648;&#23384;&#30899;&#65292;&#37322;&#25918;&#27687;&#27668;&#65292;&#24182;&#20026;&#35768;&#22810;&#28023;&#27915;&#29983;&#29289;&#25552;&#20379;&#26646;&#24687;&#22320;&#12290;&#20511;&#21161;&#22686;&#24378;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;174&#20010;&#29305;&#24449;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#27874;&#35199;&#22810;&#23612;&#20122;&#28023;&#33609;&#30340;&#30830;&#20999;&#20301;&#32622;&#19982;&#27700;&#20307;&#29983;&#29289;&#22320;&#29699;&#21270;&#23398;&#29305;&#24615;&#20043;&#38388;&#23384;&#22312;&#31283;&#22266;&#30340;&#30456;&#20851;&#24615;&#12290;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#26174;&#31034;&#65292;&#19982;&#30899;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#22914;&#20928;&#29983;&#29289;&#37327;&#29983;&#20135;&#21644;&#20108;&#27687;&#21270;&#30899;&#19979;&#38477;&#34920;&#38754;&#36136;&#37327;&#36890;&#37327;&#65292;&#22312;&#23384;&#22312;&#27874;&#35199;&#22810;&#23612;&#20122;&#28023;&#33609;&#30340;&#21306;&#22495;&#20013;&#20854;&#20540;&#21457;&#29983;&#25913;&#21464;&#65292;&#20174;&#32780;&#21487;&#29992;&#20110;&#38388;&#25509;&#23450;&#20301;&#27874;&#35199;&#22810;&#23612;&#20122;&#28023;&#33609;&#33609;&#22320;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#26893;&#29289;&#23545;&#29615;&#22659;&#20135;&#29983;&#20840;&#29699;&#24433;&#21709;&#30340;&#35777;&#25454;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#31181;&#26893;&#29289;&#22312;&#28023;&#27915;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#23545;&#20854;&#20445;&#25252;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14459v1 Announce Type: cross  Abstract: Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that fosters biodiversity, stores carbon, releases oxygen, and provides habitat to numerous sea organisms. Leveraging augmented research, we collected a comprehensive dataset of 174 features compiled from diverse data sources. Through machine learning analysis, we discovered the existence of a robust correlation between the exact location of P. oceanica and water biogeochemical properties. The model's feature importance, showed that carbon-related variables as net biomass production and downward surface mass flux of carbon dioxide have their values altered in the areas with P. oceanica, which in turn can be used for indirect location of P. oceanica meadows. The study provides the evidence of the plant's ability to exert a global impact on the environment and underscores the crucial role of this plant in sea ecosystems, emphasizing the need for its conservation and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#21160;&#25511;&#21046;&#31574;&#30053;&#22788;&#29702;&#28909;&#21644;&#30142;&#30149;&#20256;&#36755;&#21021;&#22987;&#36793;&#30028;&#20540;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#25913;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#39537;&#21160;&#20256;&#36755;&#22330;&#30340;&#27969;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.14446</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#21453;&#24212;&#25193;&#25955;&#38382;&#39064;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reinforcement Learning Control of Reaction-Diffusion Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#21160;&#25511;&#21046;&#31574;&#30053;&#22788;&#29702;&#28909;&#21644;&#30142;&#30149;&#20256;&#36755;&#21021;&#22987;&#36793;&#30028;&#20540;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#25913;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#39537;&#21160;&#20256;&#36755;&#22330;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21644;&#35745;&#31639;&#24037;&#20855;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#34987;&#35777;&#26126;&#26159;&#21487;&#38752;&#30340;&#12290;&#29305;&#21035;&#26159;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#20316;&#20026;&#39640;&#32423;&#25903;&#25345;&#24037;&#20855;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22788;&#29702;&#25511;&#21046;&#38382;&#39064;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#22312;&#28216;&#25103;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#25104;&#21151;&#28608;&#21457;&#20102;&#23545;&#33021;&#22815;&#20811;&#26381;&#24403;&#21069;&#22256;&#38590;&#30340;&#26032;&#39046;&#22495;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#21160;&#25511;&#21046;&#31574;&#30053;&#22312;&#28909;&#21644;&#30142;&#30149;&#20256;&#36755;&#21021;&#22987;&#36793;&#30028;&#20540;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35843;&#25972;&#20102;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#39537;&#21160;&#20256;&#36755;&#22330;&#30340;&#27969;&#21160;&#12290;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#21453;&#24212;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14446v1 Announce Type: cross  Abstract: Mathematical and computational tools have proven to be reliable in decision-making processes. In recent times, in particular, machine learning-based methods are becoming increasingly popular as advanced support tools. When dealing with control problems, reinforcement learning has been applied to decision-making in several applications, most notably in games. The success of these methods in finding solutions to complex problems motivates the exploration of new areas where they can be employed to overcome current difficulties. In this paper, we explore the use of automatic control strategies to initial boundary value problems in thermal and disease transport. Specifically, in this work, we adapt an existing reinforcement learning algorithm using a stochastic policy gradient method and we introduce two novel reward functions to drive the flow of the transported field. The new model-based framework exploits the interactions between a react
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#33021;&#22815;&#36827;&#34892;&#26799;&#24230;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#25216;&#26415;&#23548;&#20986;&#20102;&#23545;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#37327;&#21270;&#20102;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#24102;&#26469;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14434</link><description>&lt;p&gt;
&#24182;&#34892;&#20013;&#28857;&#38543;&#26426;&#21270;&#30340; Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Parallelized Midpoint Randomization for Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14434
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#33021;&#22815;&#36827;&#34892;&#26799;&#24230;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#25216;&#26415;&#23548;&#20986;&#20102;&#23545;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#37327;&#21270;&#20102;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#24102;&#26469;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#21487;&#20197;&#36827;&#34892;&#26799;&#24230;&#30340;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#30001;&#24179;&#28369;&#21644;&#24378;log-&#20985;&#23494;&#24230;&#34920;&#24449;&#30340;&#30446;&#26631;&#20998;&#24067;&#19978;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36816;&#29992;&#26368;&#36817;&#24320;&#21457;&#29992;&#20110;&#20998;&#26512;&#20854;&#32431;&#39034;&#24207;&#29256;&#26412;&#30340;&#35777;&#26126;&#25216;&#26415;&#12290;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#30028;&#38480;&#37327;&#21270;&#20102;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#25152;&#23454;&#29616;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#65292;&#36825;&#21487;&#33021;&#26159;&#30456;&#24403;&#21487;&#35266;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14434v1 Announce Type: cross  Abstract: We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible. Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities. We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version. Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities. These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Twin-sight&#30340;&#21452;&#27169;&#22411;&#33539;&#24335;&#65292;&#20197;&#22686;&#24378;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#37051;&#22495;&#20445;&#25345;&#32422;&#26463;&#26469;&#25552;&#39640;&#36825;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;</title><link>https://arxiv.org/abs/2402.14430</link><description>&lt;p&gt;
&#20855;&#26377;&#26497;&#31471;&#26631;&#31614;&#19981;&#36275;&#30340;&#32852;&#37030;&#27169;&#22411;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training of Federated Models with Extremely Label Deficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14430
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Twin-sight&#30340;&#21452;&#27169;&#22411;&#33539;&#24335;&#65292;&#20197;&#22686;&#24378;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#37051;&#22495;&#20445;&#25345;&#32422;&#26463;&#26469;&#25552;&#39640;&#36825;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#21327;&#20316;&#35757;&#32451;&#20855;&#26377;&#26631;&#31614;&#19981;&#36275;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#40065;&#26834;&#30340;FSSL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26631;&#35760;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#20174;&#32780;&#20135;&#29983;&#26799;&#24230;&#20914;&#31361;&#12290;&#20026;&#20102;&#20943;&#36731;&#26799;&#24230;&#20914;&#31361;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#27169;&#22411;&#33539;&#24335;&#65292;&#31216;&#20026;Twin-sight&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#26469;&#33258;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#19981;&#21516;&#35270;&#35282;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;&#30456;&#20114;&#25351;&#23548;&#12290;&#29305;&#21035;&#22320;&#65292;Twin-sight&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#30417;&#30563;&#30446;&#26631;&#20989;&#25968;&#30340;&#30417;&#30563;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#26080;&#30417;&#30563;&#30446;&#26631;&#20989;&#25968;&#35757;&#32451;&#19968;&#20010;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;&#20026;&#20102;&#22686;&#24378;&#36825;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;Twin-sight&#24341;&#20837;&#20102;&#19968;&#20010;&#20445;&#25345;&#37051;&#22495;&#30340;&#32422;&#26463;&#65292;&#20419;&#36827;&#37051;&#22495;&#30340;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14430v1 Announce Type: new  Abstract: Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency. Advanced FSSL methods predominantly focus on training a single model on each client. However, this approach could lead to a discrepancy between the objective functions of labeled and unlabeled data, resulting in gradient conflicts. To alleviate gradient conflict, we propose a novel twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data. In particular, Twin-sight concurrently trains a supervised model with a supervised objective function while training an unsupervised model using an unsupervised objective function. To enhance the synergy between these two models, Twin-sight introduces a neighbourhood-preserving constraint, which encourages the preservation of the nei
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Text-to-Pressure&#65288;T2P&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#19982;&#29983;&#25104;&#21160;&#20316;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14427</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#20154;&#31867;&#27963;&#21160;&#30340;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Text me the data: Generating Ground Pressure Sequence from Textual Descriptions for HAR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14427
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Text-to-Pressure&#65288;T2P&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#19982;&#29983;&#25104;&#21160;&#20316;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#65292;&#20026;&#35757;&#32451;&#39640;&#25928;&#27169;&#22411;&#65292;&#24517;&#39035;&#26377;&#22823;&#37327;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#33719;&#21462;&#22320;&#38754;&#21387;&#21147;&#25968;&#25454;&#26412;&#36523;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12289;&#32791;&#26102;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text-to-Pressure&#65288;T2P&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20154;&#31867;&#27963;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#22823;&#37327;&#22320;&#38754;&#21387;&#21147;&#24207;&#21015;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#30690;&#37327;&#37327;&#21270;&#19982;&#31616;&#21333;&#25991;&#26412;&#26465;&#20214;&#33258;&#22238;&#24402;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#30340;&#31163;&#25955;&#28508;&#22312;&#30456;&#20851;&#24615;&#33719;&#24471;&#39640;&#36136;&#37327;&#29983;&#25104;&#30340;&#21387;&#21147;&#24207;&#21015;&#19982;&#21387;&#21147;&#22320;&#22270;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#19982;&#29983;&#25104;&#21160;&#20316;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#19978;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#34920;&#29616;&#65292;R squared &#20540;&#20026;0.722&#65292;Masked R squared &#20540;&#20026;0.892&#65292;FID &#20998;&#25968;&#20026;1.83&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14427v1 Announce Type: cross  Abstract: In human activity recognition (HAR), the availability of substantial ground truth is necessary for training efficient models. However, acquiring ground pressure data through physical sensors itself can be cost-prohibitive, time-consuming. To address this critical need, we introduce Text-to-Pressure (T2P), a framework designed to generate extensive ground pressure sequences from textual descriptions of human activities using deep learning techniques. We show that the combination of vector quantization of sensor data along with simple text conditioned auto regressive strategy allows us to obtain high-quality generated pressure sequences from textual descriptions with the help of discrete latent correlation between text and pressure maps. We achieved comparable performance on the consistency between text and generated motion with an R squared value of 0.722, Masked R squared value of 0.892, and FID score of 1.83. Additionally, we trained 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#30340;&#20840;&#23616;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#26469;&#20943;&#23569;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2402.14402</link><description>&lt;p&gt;
&#20840;&#23616;&#23433;&#20840;&#39034;&#24207;&#23398;&#20064;&#36890;&#36807;&#39640;&#25928;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Global Safe Sequential Learning via Efficient Knowledge Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#30340;&#20840;&#23616;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#26469;&#20943;&#23569;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14402v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#20363;&#22914;&#20027;&#21160;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35768;&#22810;&#21307;&#23398;&#25110;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#36873;&#25321;&#21463;&#20808;&#39564;&#26410;&#30693;&#30340;&#23433;&#20840;&#26465;&#20214;&#38480;&#21046;&#12290;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#23433;&#20840;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#24314;&#27169;&#23433;&#20840;&#27010;&#29575;&#65292;&#24182;&#22312;&#20855;&#26377;&#36739;&#39640;&#23433;&#20840;&#32622;&#20449;&#24230;&#30340;&#21306;&#22495;&#20013;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#23433;&#20840;&#24314;&#27169;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#28040;&#32791;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23433;&#20840;&#32622;&#20449;&#24230;&#38598;&#20013;&#22312;&#32473;&#23450;&#30340;&#35266;&#27979;&#20540;&#21608;&#22260;&#65292;&#23548;&#33268;&#23616;&#37096;&#25506;&#32034;&#12290;&#30001;&#20110;&#22312;&#23433;&#20840;&#20851;&#38190;&#23454;&#39564;&#20013;&#36890;&#24120;&#23384;&#22312;&#21487;&#36716;&#31227;&#30340;&#28304;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#39034;&#24207;&#23398;&#20064;&#26469;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#65292;&#20197;&#20943;&#23569;&#24341;&#20837;&#28304;&#25968;&#25454;&#24102;&#26469;&#30340;&#39069;&#22806;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14402v1 Announce Type: new  Abstract: Sequential learning methods such as active learning and Bayesian optimization select the most informative data to learn about a task. In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence. However, accurate safety modeling requires prior knowledge or consumes data. In addition, the safety confidence centers around the given observations which leads to local exploration. As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. In this pap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#25955;&#24674;&#22797;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#39640;&#32423;&#21644;&#20302;&#32423;&#35270;&#35273;&#29305;&#24449;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14401</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35270;&#35273;&#34917;&#20607;&#24341;&#23548;&#21644;&#35270;&#35273;&#24046;&#24322;&#20998;&#26512;&#29992;&#20110;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#25955;&#24674;&#22797;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#39640;&#32423;&#21644;&#20302;&#32423;&#35270;&#35273;&#29305;&#24449;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#30001;&#33021;&#24341;&#23548;&#30340;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;(NR-IQA)&#26041;&#27861;&#20173;&#28982;&#22312;&#25214;&#21040;&#22312;&#22270;&#20687;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#29305;&#24449;&#20449;&#24687;&#21644;&#25429;&#33719;&#39640;&#32423;&#29305;&#24449;&#20449;&#24687;&#20043;&#38388;&#36798;&#21040;&#24179;&#34913;&#20197;&#21450;&#39640;&#32423;&#29305;&#24449;&#20449;&#24687;&#30340;&#26377;&#25928;&#21033;&#29992;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#20808;&#25216;&#26415;(SOTA)&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#24314;&#27169;&#22797;&#26434;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#22270;&#20687;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#22320;&#23398;&#20064;&#39640;&#32423;&#21644;&#20302;&#32423;&#35270;&#35273;&#29305;&#24449;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#25193;&#25955;&#27169;&#22411;&#25506;&#32034;&#21040;NR-IQA&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#24674;&#22797;&#32593;&#32476;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#22686;&#24378;&#22270;&#20687;&#21644;&#21253;&#21547;&#22122;&#22768;&#30340;&#22270;&#20687;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#21435;&#22122;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#20316;&#20026;&#39640;&#32423;&#35270;&#35273;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14401v1 Announce Type: cross  Abstract: Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual informat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14400</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14400
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#38656;&#35201;&#21450;&#26102;&#24178;&#39044;&#30340;&#21307;&#23398;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21457;&#30340;&#36816;&#21160;&#27963;&#21160;&#65292;&#21363;&#8220;&#21160;&#21147;&#23398;&#8221;&#65292;&#34987;&#35777;&#26126;&#21487;&#25552;&#20379;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#39044;&#27979;&#26410;&#26469;&#31070;&#32463;&#21457;&#32946;&#30340;&#26367;&#20195;&#24615;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#35780;&#20272;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#23450;&#24615;&#21644;&#20027;&#35266;&#30340;&#65292;&#20391;&#37325;&#20110;&#23545;&#36890;&#36807;&#35270;&#35273;&#35782;&#21035;&#30340;&#29305;&#23450;&#24180;&#40836;&#25163;&#21183;&#30340;&#25551;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26681;&#25454;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#26469;&#39044;&#27979;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#25104;&#29087;&#12290;&#25105;&#20204;&#21033;&#29992;&#22788;&#29702;&#36807;&#30340;3D&#23156;&#20799;&#35270;&#39057;&#24405;&#20687;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;&#65292;&#25552;&#21462;&#35299;&#21078;&#26631;&#24535;&#29289;&#30340;&#26102;&#31354;&#31995;&#21015;&#65292;&#24182;&#24212;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#23454;&#38469;&#24180;&#40836;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20110;&#25163;&#21160;&#35774;&#35745;&#29305;&#24449;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14400v1 Announce Type: cross  Abstract: Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30028;&#38480;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29305;&#23450;&#35760;&#24405;&#32423;&#23041;&#32961;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#36991;&#20813;&#20102;&#36890;&#36807;DP&#36827;&#34892;&#38388;&#25509;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.14397</link><description>&lt;p&gt;
DP-SGD&#31639;&#27861;&#23545;&#25239;&#35760;&#24405;&#23618;&#25512;&#26029;&#30340;&#23553;&#38381;&#24418;&#24335;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Bounds for DP-SGD against Record-level Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14397
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30028;&#38480;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29305;&#23450;&#35760;&#24405;&#32423;&#23041;&#32961;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#36991;&#20813;&#20102;&#36890;&#36807;DP&#36827;&#34892;&#38388;&#25509;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#31639;&#27861;&#65288;&#22914;DP-SGD&#65289;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#21508;&#31181;&#38544;&#31169;&#25915;&#20987;&#20855;&#26377;&#38887;&#24615;&#12290;&#34429;&#28982;&#21487;&#20197;&#20165;&#22522;&#20110;&#65288;&#949;&#65292;&#948;&#65289;-DP&#20445;&#35777;&#25512;&#23548;&#20986;&#26576;&#20123;&#25915;&#20987;&#30340;&#30028;&#38480;&#65292;&#20294;&#26377;&#24847;&#20041;&#30340;&#30028;&#38480;&#38656;&#35201;&#36275;&#22815;&#23567;&#30340;&#38544;&#31169;&#39044;&#31639;&#65288;&#21363;&#27880;&#20837;&#22823;&#37327;&#22122;&#22768;&#65289;&#65292;&#36825;&#23548;&#33268;&#25928;&#29992;&#22823;&#24133;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38024;&#23545;&#29305;&#23450;&#35760;&#24405;&#32423;&#23041;&#32961;&#65288;&#22914;&#25104;&#21592;&#20851;&#31995;&#21644;&#23646;&#24615;&#25512;&#26029;&#65289;&#30340;&#38544;&#31169;&#65292;&#32780;&#26080;&#38656;&#32463;&#36807;DP&#30340;&#38388;&#25509;&#36830;&#32467;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#27969;&#34892;&#30340;DP-SGD&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#31616;&#21333;&#30340;&#38381;&#24335;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#23558;DP-SGD&#24314;&#27169;&#20026;&#19968;&#20010;&#20449;&#24687;&#35770;&#36890;&#36947;&#65292;&#20854;&#36755;&#20837;&#26159;&#25915;&#20987;&#32773;&#24819;&#35201;&#25512;&#26029;&#30340;&#31192;&#23494;&#65288;&#22914;&#25968;&#25454;&#35760;&#24405;&#30340;&#25104;&#21592;&#20851;&#31995;&#65289;&#65292;&#36755;&#20986;&#26159;&#36845;&#20195;&#20248;&#21270;&#20135;&#29983;&#30340;&#20013;&#38388;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14397v1 Announce Type: cross  Abstract: Machine learning models trained with differentially-private (DP) algorithms such as DP-SGD enjoy resilience against a wide range of privacy attacks. Although it is possible to derive bounds for some attacks based solely on an $(\varepsilon,\delta)$-DP guarantee, meaningful bounds require a small enough privacy budget (i.e., injecting a large amount of noise), which results in a large loss in utility. This paper presents a new approach to evaluate the privacy of machine learning models against specific record-level threats, such as membership and attribute inference, without the indirection through DP. We focus on the popular DP-SGD algorithm, and derive simple closed-form bounds. Our proofs model DP-SGD as an information theoretic channel whose inputs are the secrets that an attacker wants to infer (e.g., membership of a data record) and whose outputs are the intermediate model parameters produced by iterative optimization. We obtain b
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AlphaTensor-Quantum&#26041;&#27861;&#65292;&#22312;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#20013;&#20248;&#21270;T&#38376;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23569;&#30005;&#36335;&#30340;T&#35745;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14396</link><description>&lt;p&gt;
&#20351;&#29992;AlphaTensor&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Optimization with AlphaTensor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14396
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AlphaTensor-Quantum&#26041;&#27861;&#65292;&#22312;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#20013;&#20248;&#21270;T&#38376;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23569;&#30005;&#36335;&#30340;T&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30005;&#36335;&#20248;&#21270;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#20013;&#26368;&#26114;&#36149;&#30340;&#38376;&#65288;&#21363;T&#38376;&#65289;&#65292;&#35299;&#20915;T&#35745;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#21270;&#23454;&#29616;&#32473;&#23450;&#30005;&#36335;&#25152;&#38656;&#30340;T&#38376;&#25968;&#37327;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AlphaTensor-Quantum&#26041;&#27861;&#65292;&#21033;&#29992;&#20248;&#21270;T&#35745;&#25968;&#19982;&#24352;&#37327;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19982;&#29616;&#26377;&#30340;T&#35745;&#25968;&#20248;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;AlphaTensor-Quantum&#33021;&#22815;&#25972;&#21512;&#20851;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24182;&#21033;&#29992;&#24037;&#20855;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20248;&#21270;&#30005;&#36335;&#30340;T&#35745;&#25968;&#12290;AlphaTensor-Quantum&#22312;&#19968;&#32452;&#31639;&#26415;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;T&#35745;&#25968;&#20248;&#21270;&#26041;&#27861;&#65288;&#21363;&#20351;&#22312;&#19981;&#20351;&#29992;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27604;&#36739;&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#21457;&#29616;&#20102;&#19968;&#31181;&#31867;&#20284;Karat&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14396v1 Announce Type: cross  Abstract: A key challenge in realizing fault-tolerant quantum computers is circuit optimization. Focusing on the most expensive gates in fault-tolerant quantum computation (namely, the T gates), we address the problem of T-count optimization, i.e., minimizing the number of T gates that are needed to implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a method based on deep reinforcement learning that exploits the relationship between optimizing T-count and tensor decomposition. Unlike existing methods for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific knowledge about quantum computation and leverage gadgets, which significantly reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms the existing methods for T-count optimization on a set of arithmetic benchmarks (even when compared without making use of gadgets). Remarkably, it discovers an efficient algorithm akin to Karat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#33258;&#24213;&#21521;&#19978;&#30340;&#35821;&#27861;&#24402;&#32435;&#21551;&#21457;&#30340;&#22270;&#35299;&#26512;&#31639;&#27861;&#65292;&#20351;&#24471;Graph Parsing Network&#65288;GPN&#65289;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#27599;&#20010;&#29420;&#29305;&#22270;&#30340;&#20010;&#24615;&#21270;&#27744;&#21270;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.14393</link><description>&lt;p&gt;
&#22270;&#35299;&#26512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Parsing Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#33258;&#24213;&#21521;&#19978;&#30340;&#35821;&#27861;&#24402;&#32435;&#21551;&#21457;&#30340;&#22270;&#35299;&#26512;&#31639;&#27861;&#65292;&#20351;&#24471;Graph Parsing Network&#65288;GPN&#65289;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#27599;&#20010;&#29420;&#29305;&#22270;&#30340;&#20010;&#24615;&#21270;&#27744;&#21270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#27744;&#21270;&#23558;&#22270;&#20449;&#24687;&#21387;&#32553;&#20026;&#19968;&#31181;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#26368;&#20808;&#36827;&#30340;&#22270;&#27744;&#21270;&#26041;&#27861;&#37319;&#29992;&#20998;&#23618;&#26041;&#27861;&#65292;&#36880;&#27493;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#12290;&#36825;&#20123;&#26041;&#27861;&#24517;&#39035;&#22312;&#20869;&#23384;&#25928;&#29575;&#21644;&#20445;&#30041;&#33410;&#28857;&#20449;&#24687;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#36825;&#21462;&#20915;&#20110;&#23427;&#20204;&#26159;&#21542;&#20351;&#29992;&#33410;&#28857;&#20002;&#24323;&#25110;&#33410;&#28857;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#20026;&#25152;&#26377;&#22270;&#39044;&#23450;&#20041;&#20102;&#22266;&#23450;&#30340;&#27744;&#21270;&#27604;&#20363;&#25110;&#27744;&#21270;&#23618;&#25968;&#65292;&#36825;&#38459;&#30861;&#20102;&#27599;&#20010;&#21333;&#29420;&#22270;&#24418;&#30340;&#20010;&#24615;&#21270;&#27744;&#21270;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#21463;&#33258;&#24213;&#21521;&#19978;&#30340;&#35821;&#27861;&#24402;&#32435;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#35299;&#26512;&#31639;&#27861;&#26469;&#25512;&#26029;&#27744;&#21270;&#32467;&#26500;&#65292;&#28982;&#21518;&#39537;&#21160;&#22270;&#27744;&#21270;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#22270;&#35299;&#26512;&#32593;&#32476;&#65288;GPN&#65289;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#27599;&#20010;&#29420;&#29305;&#22270;&#30340;&#20010;&#24615;&#21270;&#27744;&#21270;&#32467;&#26500;&#12290;GPN&#21463;&#30410;&#20110;&#22270;&#35299;&#26512;&#31639;&#27861;&#29983;&#25104;&#30340;&#31163;&#25955;&#20998;&#37197;&#65292;&#20174;&#32780;&#22312;&#20445;&#30041;&#33410;&#28857;&#20449;&#24687;&#30340;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14393v1 Announce Type: new  Abstract: Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering. Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph. In this work, inspired by bottom-up grammar induction, we propose an efficient graph parsing algorithm to infer the pooling structure, which then drives graph pooling. The resulting Graph Parsing Network (GPN) adaptively learns personalized pooling structure for each individual graph. GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node inform
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#27688;&#22522;&#37240;&#27531;&#22522;&#30340;&#24494;&#29615;&#22659;&#65292;&#32467;&#21512;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#39640;&#25928;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24494;&#29615;&#22659;&#24863;&#30693;&#34507;&#30333;&#36136;&#23884;&#20837;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14391</link><description>&lt;p&gt;
MAPE-PPI&#65306;&#36890;&#36807;&#24494;&#29615;&#22659;&#24863;&#30693;&#34507;&#30333;&#23884;&#20837;&#21521;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#27688;&#22522;&#37240;&#27531;&#22522;&#30340;&#24494;&#29615;&#22659;&#65292;&#32467;&#21512;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#39640;&#25928;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24494;&#29615;&#22659;&#24863;&#30693;&#34507;&#30333;&#36136;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;(PPIs)&#22312;&#21508;&#31181;&#29983;&#29289;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#23545;&#29983;&#21629;&#27963;&#21160;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23454;&#39564;PPI&#27979;&#23450;&#38656;&#27714;&#22686;&#38271;&#21644;&#25104;&#26412;&#22686;&#21152;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;PPI&#39044;&#27979;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#34892;PPI&#39044;&#27979;&#65292;&#28982;&#32780;&#34507;&#30333;&#36136;&#32467;&#26500;&#25165;&#26159;&#20915;&#23450;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#21516;&#26102;&#32771;&#34385;&#20004;&#31181;&#34507;&#30333;&#36136;&#27169;&#24577;&#65292;&#25105;&#20204;&#36890;&#36807;&#34507;&#30333;&#27688;&#22522;&#37240;&#27531;&#22522;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#19978;&#19979;&#25991;&#23450;&#20041;&#20102;&#24494;&#29615;&#22659;&#65292;&#25551;&#36848;&#20102;&#21608;&#22260;&#21270;&#23398;&#24615;&#36136;&#21644;&#20960;&#20309;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#24037;&#20316;&#20013;&#23450;&#20041;&#30340;&#24494;&#29615;&#22659;&#20027;&#35201;&#22522;&#20110;&#23454;&#39564;&#27979;&#23450;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#65292;&#20854;&#8220;&#35789;&#27719;&#34920;&#8221;&#36890;&#24120;&#26497;&#23567;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#28085;&#30422;&#24494;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;PPI&#39044;&#27979;&#30340;&#24494;&#29615;&#22659;&#24863;&#30693;&#34507;&#30333;&#36136;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14391v1 Announce Type: new  Abstract: Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the "vocabulary" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI p
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#32593;&#26684;&#25628;&#32034;&#23558;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;KNN&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#31561;&#31639;&#27861;&#26234;&#33021;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14389</link><description>&lt;p&gt;
&#20445;&#38556;&#20132;&#26131;&#65306;&#19968;&#31181;&#20351;&#29992;IHT-LR&#21644;&#32593;&#26684;&#25628;&#32034;&#30340;&#28151;&#21512;&#21487;&#38752;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Securing Transactions: A Hybrid Dependable Ensemble Machine Learning Model using IHT-LR and Grid Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14389
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#32593;&#26684;&#25628;&#32034;&#23558;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;KNN&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#31561;&#31639;&#27861;&#26234;&#33021;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#26426;&#26500;&#21644;&#20225;&#19994;&#38754;&#20020;&#30528;&#26469;&#33258;&#27450;&#35784;&#20132;&#26131;&#30340;&#25345;&#32493;&#25361;&#25112;&#65292;&#20419;&#20351;&#38656;&#35201;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#26816;&#27979;&#20449;&#29992;&#21345;&#27450;&#35784;&#23545;&#20110;&#35782;&#21035;&#21644;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#20132;&#26131;&#33267;&#20851;&#37325;&#35201;&#12290;&#21450;&#26102;&#21457;&#29616;&#27450;&#35784;&#21487;&#20351;&#35843;&#26597;&#20154;&#21592;&#36805;&#36895;&#37319;&#21462;&#34892;&#21160;&#65292;&#20197;&#20943;&#23569;&#36827;&#19968;&#27493;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#35843;&#26597;&#36807;&#31243;&#36890;&#24120;&#32791;&#26102;&#65292;&#38480;&#21046;&#20102;&#27599;&#22825;&#21487;&#20197;&#24443;&#24213;&#26816;&#26597;&#30340;&#35686;&#25253;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#20934;&#30830;&#30340;&#35686;&#25253;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#35823;&#25253;&#21644;&#28431;&#25253;&#30340;&#27450;&#35784;&#26696;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28151;&#21512;&#38598;&#25104;(ENS)&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32593;&#26684;&#25628;&#32034;&#26234;&#33021;&#22320;&#32467;&#21512;&#22810;&#20010;&#31639;&#27861;&#24182;&#36827;&#34892;&#36866;&#24403;&#21152;&#26435;&#20248;&#21270;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;(DT)&#12289;&#38543;&#26426;&#26862;&#26519;(RF)&#12289;K-&#26368;&#36817;&#37051;(KNN)&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#65292;&#20197;&#22686;&#24378;&#27450;&#35784;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14389v1 Announce Type: new  Abstract: Financial institutions and businesses face an ongoing challenge from fraudulent transactions, prompting the need for effective detection methods. Detecting credit card fraud is crucial for identifying and preventing unauthorized transactions.Timely detection of fraud enables investigators to take swift actions to mitigate further losses. However, the investigation process is often time-consuming, limiting the number of alerts that can be thoroughly examined each day. Therefore, the primary objective of a fraud detection model is to provide accurate alerts while minimizing false alarms and missed fraud cases. In this paper, we introduce a state-of-the-art hybrid ensemble (ENS) dependable Machine learning (ML) model that intelligently combines multiple algorithms with proper weighted optimization using Grid search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor (KNN), and Multilayer Perceptron (MLP), to enhance fraud 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#39118;&#36895;&#22270;&#65292;WindDragon&#31995;&#32479;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#30701;&#26399;&#39118;&#21147;&#39044;&#27979;&#65292;&#20026;&#30005;&#32593;&#36816;&#33829;&#21644;&#31995;&#32479;&#24179;&#34913;&#25552;&#20379;&#20851;&#38190;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.14385</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#25913;&#36827;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#30340;WindDragon&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
WindDragon: Enhancing wind power forecasting with Automated Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14385
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#39118;&#36895;&#22270;&#65292;WindDragon&#31995;&#32479;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#30701;&#26399;&#39118;&#21147;&#39044;&#27979;&#65292;&#20026;&#30005;&#32593;&#36816;&#33829;&#21644;&#31995;&#32479;&#24179;&#34913;&#25552;&#20379;&#20851;&#38190;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#21040;2050&#24180;&#38646;&#30899;&#25490;&#25918;&#30340;&#30446;&#26631;&#38656;&#35201;&#23558;&#22823;&#37327;&#39118;&#21147;&#32435;&#20837;&#30005;&#32593;&#20013;&#12290;&#36825;&#31181;&#33021;&#28304;&#30001;&#20110;&#20854;&#21464;&#21270;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#23545;&#31995;&#32479;&#36816;&#33829;&#21830;&#26500;&#25104;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#23545;&#20110;&#30005;&#32593;&#36816;&#33829;&#21644;&#31995;&#32479;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#36827;&#34892;&#30701;&#26399;&#65288;1&#33267;6&#23567;&#26102;&#65289;&#39118;&#21147;&#39044;&#27979;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#39118;&#36895;&#22270;&#26469;&#20934;&#30830;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14385v1 Announce Type: new  Abstract: Achieving net zero carbon emissions by 2050 requires the integration of increasing amounts of wind power into power grids. This energy source poses a challenge to system operators due to its variability and uncertainty. Therefore, accurate forecasting of wind power is critical for grid operation and system balancing. This paper presents an innovative approach to short-term (1 to 6 hour horizon) windpower forecasting at a national level. The method leverages Automated Deep Learning combined with Numerical Weather Predictions wind speed maps to accurately forecast wind power.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#36827;&#34892;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#36719;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#21644;&#24182;&#34892;&#37325;&#26500;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#37325;&#26500;&#25439;&#22833;&#21644;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#30456;&#32467;&#21512;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#21152;&#36895;&#20102;&#26816;&#27979;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#30340;&#24322;&#24120;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14384</link><description>&lt;p&gt;
&#20855;&#26377;&#36719;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#21644;&#24182;&#34892;&#37325;&#26500;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#36827;&#34892;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#36719;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#21644;&#24182;&#34892;&#37325;&#26500;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#37325;&#26500;&#25439;&#22833;&#21644;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#30456;&#32467;&#21512;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#21152;&#36895;&#20102;&#26816;&#27979;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#30340;&#24322;&#24120;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#32500;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(DCGAN)&#26469;&#36827;&#34892;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#39034;&#24207;&#24322;&#24120;&#26816;&#27979;&#12290;&#24322;&#24120;&#26816;&#27979;&#28041;&#21450;&#26799;&#24230;&#19979;&#38477;&#20197;&#37325;&#26500;&#33021;&#28304;&#23376;&#24207;&#21015;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#32593;&#32476;&#35782;&#21035;&#32039;&#23494;&#29983;&#25104;&#23427;&#20204;&#30340;&#22122;&#22768;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;Soft-DTW&#20316;&#20026;&#37325;&#26500;&#25439;&#22833;&#30340;&#21487;&#24494;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#20248;&#20110;&#27431;&#27663;&#36317;&#31163;&#12290;&#23558;&#37325;&#26500;&#25439;&#22833;&#21644;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#27010;&#29575;&#20998;&#24067;&#30456;&#32467;&#21512;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#22810;&#20010;&#28857;&#30340;&#37325;&#26500;&#21152;&#36895;&#26816;&#27979;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#35782;&#21035;&#24314;&#31569;&#29289;&#20013;&#30340;&#24322;&#24120;&#33021;&#28304;&#28040;&#32791;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;15&#26635;&#24314;&#31569;&#29289;&#30340;&#23567;&#26102;&#33021;&#28304;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#21152;&#20197;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14384v1 Announce Type: new  Abstract: In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data. Anomaly detection involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network. Soft-DTW is used as a differentiable alternative for the reconstruction loss and is found to be superior to Euclidean distance. Combining reconstruction loss and the latent space's prior probability distribution serves as the anomaly score. Our novel method accelerates detection by parallel computation of reconstruction of multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26041;&#27861;SPMiner&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30446;&#26631;&#22270;&#20013;&#36817;&#20284;&#25214;&#21040;&#39057;&#32321;&#23376;&#22270;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#39034;&#24207;&#23884;&#20837;&#31354;&#38388;&#21644;&#39640;&#25928;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#20986;&#22312;&#30446;&#26631;&#22270;&#20013;&#20986;&#29616;&#26368;&#39057;&#32321;&#30340;&#32593;&#32476;&#23376;&#22270;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14367</link><description>&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#29992;&#20110;&#39057;&#32321;&#23376;&#22270;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Frequent Subgraph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26041;&#27861;SPMiner&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30446;&#26631;&#22270;&#20013;&#36817;&#20284;&#25214;&#21040;&#39057;&#32321;&#23376;&#22270;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#39034;&#24207;&#23884;&#20837;&#31354;&#38388;&#21644;&#39640;&#25928;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#20986;&#22312;&#30446;&#26631;&#22270;&#20013;&#20986;&#29616;&#26368;&#39057;&#32321;&#30340;&#32593;&#32476;&#23376;&#22270;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#21035;&#39057;&#32321;&#23376;&#22270;&#65292;&#20063;&#34987;&#31216;&#20026;&#32593;&#32476;&#27169;&#24335;&#65292;&#22312;&#20998;&#26512;&#21644;&#39044;&#27979;&#30495;&#23454;&#32593;&#32476;&#30340;&#23646;&#24615;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#22823;&#22411;&#24120;&#35265;&#27169;&#24335;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#19981;&#20165;&#26159;&#22240;&#20026;&#20854;NP&#38590;&#30340;&#23376;&#22270;&#35745;&#25968;&#65292;&#32780;&#19988;&#36824;&#22240;&#20026;&#21487;&#33021;&#30340;&#23376;&#22270;&#27169;&#24335;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Subgraph Pattern Miner&#65288;SPMiner&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30446;&#26631;&#22270;&#20013;&#36817;&#20284;&#25214;&#21040;&#39057;&#32321;&#23376;&#22270;&#12290;SPMiner&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#39034;&#24207;&#23884;&#20837;&#31354;&#38388;&#21644;&#39640;&#25928;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#20986;&#22312;&#30446;&#26631;&#22270;&#20013;&#20986;&#29616;&#26368;&#39057;&#32321;&#30340;&#32593;&#32476;&#23376;&#22270;&#27169;&#24335;&#12290;SPMiner&#39318;&#20808;&#23558;&#30446;&#26631;&#22270;&#20998;&#35299;&#20026;&#35768;&#22810;&#37325;&#21472;&#30340;&#23376;&#22270;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#23376;&#22270;&#32534;&#30721;&#20026;&#19968;&#20010;&#39034;&#24207;&#23884;&#20837;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;SPMiner&#22312;&#39034;&#24207;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#21333;&#35843;&#28459;&#27493;&#20197;&#35782;&#21035;&#39057;&#32321;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14367v1 Announce Type: new  Abstract: Identifying frequent subgraphs, also called network motifs, is crucial in analyzing and predicting properties of real-world networks. However, finding large commonly-occurring motifs remains a challenging problem not only due to its NP-hard subroutine of subgraph counting, but also the exponential growth of the number of possible subgraphs patterns. Here we present Subgraph Pattern Miner (SPMiner), a novel neural approach for approximately finding frequent subgraphs in a large target graph. SPMiner combines graph neural networks, order embedding space, and an efficient search strategy to identify network subgraph patterns that appear most frequently in the target graph. SPMiner first decomposes the target graph into many overlapping subgraphs and then encodes each subgraph into an order embedding space. SPMiner then uses a monotonic walk in the order embedding space to identify frequent motifs. Compared to existing approaches and possibl
&lt;/p&gt;</description></item><item><title>OpenTab &#26159;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#25512;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#34920;&#26684;&#26816;&#32034;&#22120;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;SQL&#31243;&#24207;&#21644;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#23454;&#29616;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14361</link><description>&lt;p&gt;
OpenTab&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#36827;&#20026;&#24320;&#25918;&#39046;&#22495;&#30340;&#34920;&#26684;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
OpenTab: Advancing Large Language Models as Open-domain Table Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14361
&lt;/p&gt;
&lt;p&gt;
OpenTab &#26159;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#25512;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#34920;&#26684;&#26816;&#32034;&#22120;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;SQL&#31243;&#24207;&#21644;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#23454;&#29616;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#38656;&#35201;&#26410;&#32463;&#35757;&#32451;&#30340;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290; &#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#19968;&#20010;&#26816;&#32034;&#22120;&#26469;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#25193;&#23637;LLM&#30340;&#30693;&#35782;&#33539;&#22260;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#27169;&#24577;&#21644;&#22823;&#34920;&#26684;&#23610;&#23544;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#25991;&#26412;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;LLMs&#22312;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#19978;&#24182;&#19981;&#29702;&#24819;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenTab&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#24320;&#25918;&#39046;&#22495;&#34920;&#26684;&#25512;&#29702;&#26694;&#26550;&#12290; &#24635;&#20307;&#32780;&#35328;&#65292;OpenTab&#21033;&#29992;&#34920;&#26684;&#26816;&#32034;&#22120;&#26469;&#33719;&#21462;&#30456;&#20851;&#34920;&#26684;&#65292;&#28982;&#21518;&#29983;&#25104;SQL&#31243;&#24207;&#20197;&#39640;&#25928;&#22320;&#35299;&#26512;&#26816;&#32034;&#21040;&#30340;&#34920;&#26684;&#12290; &#21033;&#29992;&#20174;SQL&#25191;&#34892;&#20013;&#23548;&#20986;&#30340;&#20013;&#38388;&#25968;&#25454;&#65292;&#23427;&#36827;&#34892;&#22522;&#20110;&#20107;&#23454;&#30340;&#25512;&#29702;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290; &#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;OpenTab&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14361v1 Announce Type: new  Abstract: Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#30340;&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#28508;&#31354;&#38388;&#22810;&#32423;&#30417;&#30563;&#32593;&#32476;(SPDNet)&#65292;&#22686;&#24378;&#20998;&#21106;&#20197;&#26356;&#20934;&#30830;&#20272;&#35745;EAT&#20307;&#31215;</title><link>https://arxiv.org/abs/2402.14349</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#30340;&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#30340;&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#28508;&#31354;&#38388;&#22810;&#32423;&#30417;&#30563;&#32593;&#32476;(SPDNet)&#65292;&#22686;&#24378;&#20998;&#21106;&#20197;&#26356;&#20934;&#30830;&#20272;&#35745;EAT&#20307;&#31215;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#22806;&#33026;&#32938;&#32452;&#32455;(EAT)&#26159;&#19968;&#31181;&#21487;&#20197;&#20998;&#27852;&#22823;&#37327;&#33026;&#32852;&#32032;&#20174;&#32780;&#24433;&#21709;&#24515;&#32908;&#21644;&#20896;&#29366;&#21160;&#33033;&#30340;&#20869;&#33039;&#33026;&#32938;&#12290;EAT&#30340;&#20307;&#31215;&#21644;&#23494;&#24230;&#21487;&#20197;&#20316;&#20026;&#29420;&#31435;&#39118;&#38505;&#26631;&#35760;&#30340;&#27979;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30913;&#20849;&#25391;&#22270;&#20687;&#27979;&#37327;&#20307;&#31215;&#26159;&#35780;&#20272;EAT&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;EAT&#19982;&#24515;&#21253;&#31215;&#28082;&#20043;&#38388;&#23545;&#27604;&#24230;&#20302;&#20197;&#21450;&#36816;&#21160;&#20266;&#24433;&#30340;&#23384;&#22312;&#65292;&#20998;&#21106;EAT&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#28508;&#31354;&#38388;&#22810;&#32423;&#30417;&#30563;&#32593;&#32476;(SPDNet)&#65292;&#37319;&#29992;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#21644;&#23545;&#25239;&#26657;&#20934;&#23398;&#20064;&#20197;&#22686;&#24378;&#20998;&#21106;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;EAT&#20307;&#31215;&#12290;&#32593;&#32476;&#39318;&#20808;&#36890;&#36807;&#22312;&#29305;&#24449;&#28508;&#31354;&#38388;&#20013;&#23558;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#20026;&#39640;&#26031;&#20998;&#24067;&#26469;&#35299;&#20915;&#30001;&#20110;&#24320;&#25918;&#24335;&#21307;&#30103;&#29615;&#22659;&#20013;&#21307;&#23398;&#22270;&#20687;&#30340;&#20302;&#36136;&#37327;&#25110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#32780;&#23548;&#33268;EAT&#36793;&#32536;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#20854;&#36125;&#21494;&#26031;&#20272;&#35745;&#20316;&#20026;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14349v1 Announce Type: cross  Abstract: Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DepL&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#23398;&#20064;&#32534;&#25490;&#65292;&#33021;&#22815;&#30830;&#20445;&#20197;&#26368;&#20302;&#35757;&#32451;&#25104;&#26412;&#36798;&#21040;&#30446;&#26631;&#23398;&#20064;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14346</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20998;&#24067;&#24335;&#21387;&#32553;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dependable Distributed Training of Compressed Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DepL&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#23398;&#20064;&#32534;&#25490;&#65292;&#33021;&#22815;&#30830;&#20445;&#20197;&#26368;&#20302;&#35757;&#32451;&#25104;&#26412;&#36798;&#21040;&#30446;&#26631;&#23398;&#20064;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#29616;&#26377;&#24037;&#20316;&#19968;&#30452;&#24573;&#35270;&#20102;&#23454;&#29616;&#23398;&#20064;&#36136;&#37327;&#30340;&#20998;&#24067;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#20854;&#24179;&#22343;&#20540;&#12290; &#36825;&#23548;&#33268;&#20102;&#25152;&#24471;ML&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24046;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#27604;&#39044;&#26399;&#30340;&#35201;&#24046;&#24471;&#22810;&#12290; &#25105;&#20204;&#36890;&#36807;&#25552;&#20986;DepL&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#23398;&#20064;&#32534;&#25490;&#26694;&#26550;&#65292;&#33021;&#22815;&#23601;&#65288;i&#65289;&#29992;&#20110;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35201;&#20351;&#29992;&#30340;&#27169;&#22411;&#21450;&#20309;&#26102;&#22312;&#23427;&#20204;&#20043;&#38388;&#20999;&#25442;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#35201;&#21033;&#29992;&#30340;&#33410;&#28857;&#38598;&#32676;&#21450;&#20854;&#36164;&#28304;&#20570;&#20986;&#39640;&#36136;&#37327;&#39640;&#25928;&#30340;&#20915;&#31574;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#21487;&#33021;&#30340;&#21487;&#29992;&#27169;&#22411;&#20026;&#23436;&#25972;&#30340;DNN&#21450;&#20854;&#21387;&#32553;&#29256;&#26412;&#12290; &#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;DepL&#20445;&#35777;&#20197;&#30446;&#26631;&#27010;&#29575;&#23454;&#29616;&#30446;&#26631;&#23398;&#20064;&#36136;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25104;&#26412;&#26368;&#20302;&#12290; &#25105;&#20204;&#35777;&#26126;DepL&#20855;&#26377;&#24120;&#25968;&#31454;&#20105;&#27604;&#29575;&#21644;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14346v1 Announce Type: cross  Abstract: The existing work on the distributed training of machine learning (ML) models has consistently overlooked the distribution of the achieved learning quality, focusing instead on its average value. This leads to a poor dependability}of the resulting ML models, whose performance may be much worse than expected. We fill this gap by proposing DepL, a framework for dependable learning orchestration, able to make high-quality, efficient decisions on (i) the data to leverage for learning, (ii) the models to use and when to switch among them, and (iii) the clusters of nodes, and the resources thereof, to exploit. For concreteness, we consider as possible available models a full DNN and its compressed versions. Unlike previous studies, DepL guarantees that a target learning quality is reached with a target probability, while keeping the training cost at a minimum. We prove that DepL has constant competitive ratio and polynomial complexity, and s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22788;&#29702;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#23454;&#26045;&#20102;&#20351;&#29992;&#29109;&#20998;&#25968;&#21644;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#25932;&#23545;&#29702;&#30001;&#30340;&#31283;&#20581;&#24615;&#33021;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.14337</link><description>&lt;p&gt;
AURA&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14337
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22788;&#29702;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#23454;&#26045;&#20102;&#20351;&#29992;&#29109;&#20998;&#25968;&#21644;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#25932;&#23545;&#29702;&#30001;&#30340;&#31283;&#20581;&#24615;&#33021;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31574;&#32972;&#21518;&#30340;&#29702;&#30001;&#19981;&#20165;&#35299;&#37322;&#20102;&#27169;&#22411;&#20915;&#31574;&#65292;&#32780;&#19988;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#26080;&#25032;&#21487;&#20987;&#30340;&#29702;&#30001;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#27492;&#22806;&#65292;&#20272;&#35745;&#29702;&#30001;&#36275;&#22815;&#24544;&#23454;&#20197;&#40723;&#21169;&#27169;&#22411;&#34920;&#29616;&#30340;&#31243;&#24230;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#36843;&#20351;&#27169;&#22411;&#22312;&#19981;&#29702;&#24819;&#30340;&#29702;&#30001;&#19979;&#36755;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#19988;&#19982;&#27169;&#22411;&#23436;&#20840;&#26377;&#33021;&#21147;&#30340;&#24773;&#20917;&#30456;&#27604;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22914;&#20309;&#24212;&#23545;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#12290;&#25105;&#20204;&#39318;&#20808;&#29992;&#32473;&#23450;&#29702;&#30001;&#30340;&#29109;&#20998;&#25968;&#26469;&#23450;&#20041;&#27169;&#31946;&#30340;&#29702;&#30001;&#65292;&#20351;&#29992;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#20316;&#20026;&#20449;&#24687;&#37327;&#12290;&#28982;&#21518;&#26681;&#25454;&#29702;&#30001;&#30340;&#27169;&#31946;&#24615;&#26469;&#24341;&#23548;&#27169;&#22411;&#36873;&#25321;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#35770;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29702;&#30001;&#30340;&#25932;&#23545;&#36136;&#37327;&#20135;&#29983;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14337v1 Announce Type: new  Abstract: Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationale
&lt;/p&gt;</description></item><item><title>HyperFast&#26159;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#21363;&#26102;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#38656;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#39640;&#24230;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14335</link><description>&lt;p&gt;
&#36229;&#24555;&#36895;&#65306;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#21363;&#26102;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperFast: Instant Classification for Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14335
&lt;/p&gt;
&lt;p&gt;
HyperFast&#26159;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#21363;&#26102;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#38656;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#39640;&#24230;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#31561;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#26159;&#22823;&#22810;&#25968;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#30340;&#39318;&#36873;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#22312;&#26377;&#38480;&#35774;&#32622;&#19979;&#30340;&#29609;&#20855;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HyperFast&#65292;&#19968;&#20010;&#20026;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#31435;&#21363;&#20998;&#31867;&#34920;&#26684;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#20803;&#35757;&#32451;&#30340;&#36229;&#32593;&#32476;&#12290;HyperFast&#29983;&#25104;&#19968;&#20010;&#38024;&#23545;&#26410;&#35265;&#25968;&#25454;&#38598;&#23450;&#21046;&#30340;&#29305;&#23450;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#30452;&#25509;&#29992;&#20110;&#20998;&#31867;&#25512;&#26029;&#65292;&#26080;&#38656;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;OpenML&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23558;HyperFast&#19982;&#31454;&#20105;&#24615;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#12289;&#20256;&#32479;ML&#26041;&#27861;&#12289;AutoML&#31995;&#32479;&#21644;&#25552;&#21319;&#26426;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;HyperFast&#23637;&#29616;&#20986;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14335v1 Announce Type: cross  Abstract: Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines. HyperFast shows highly competitive results, wh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14332</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#21040;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#30340;&#23610;&#23544;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#25105;&#20204;&#20250;&#24471;&#21040;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#35201;&#26377;&#25928;&#22320;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#26410;&#30693;&#30340;&#22522;&#20934;&#32858;&#31867;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26114;&#36149;&#30340;oracle&#26597;&#35810;&#26469;&#35775;&#38382;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20986;&#23558;&#19982;&#22522;&#26412;&#20107;&#23454;&#32467;&#26500;&#19978;&#25509;&#36817;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32858;&#31867;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#25105;&#20204;&#21487;&#20197;&#65288;1&#65289;&#23545;&#22823;&#35268;&#27169;&#32858;&#31867;&#23454;&#20363;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#65288;2&#65289;&#22312;&#36739;&#23567;&#23454;&#20363;&#19978;&#35780;&#20272;&#19968;&#32452;&#20505;&#36873;&#31639;&#27861;&#65292;&#65288;3&#65289;&#20445;&#35777;&#22312;&#23567;&#23454;&#20363;&#19978;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#20026;&#19977;&#31181;&#32463;&#20856;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#23610;&#23544;&#27867;&#21270;&#20445;&#35777;&#65306;&#21333;&#38142;&#25509;&#12289;k-means++&#21644;Gonzalez&#30340;k&#20013;&#24515;&#21551;&#21457;&#24335;&#65288;&#19968;&#31181;&#24179;&#28369;&#30340;&#21464;&#31181;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14332v1 Announce Type: new  Abstract: In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuris
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MolEdit3D&#65292;&#23558;3D&#20998;&#23376;&#29983;&#25104;&#19982;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#32534;&#36753;&#20998;&#23376;&#26102;&#36873;&#25321;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14315</link><description>&lt;p&gt;
&#36890;&#36807;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21644;&#37319;&#26679;&#36827;&#34892;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Structure-Based Drug Design via 3D Molecular Generative Pre-training and Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MolEdit3D&#65292;&#23558;3D&#20998;&#23376;&#29983;&#25104;&#19982;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#32534;&#36753;&#20998;&#23376;&#26102;&#36873;&#25321;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#26088;&#22312;&#22312;&#20808;&#39564;&#30693;&#35782;&#19979;&#29983;&#25104;&#20855;&#26377;&#39640;&#20146;&#21644;&#21147;&#30340;&#37197;&#20307;&#65292;&#24182;&#20102;&#35299;3D&#38774;&#26631;&#32467;&#26500;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#32473;&#23450;&#30446;&#26631;&#32467;&#21512;&#20301;&#28857;&#30340;3D&#37197;&#20307;&#20998;&#24067;&#65292;&#35201;&#20040;&#36845;&#20195;&#20462;&#25913;&#20998;&#23376;&#20197;&#20248;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#27963;&#24615;&#20272;&#35745;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;3D&#20998;&#23376;&#29983;&#25104;&#19982;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#32534;&#36753;&#20998;&#23376;&#26102;&#36873;&#25321;&#22312;2D&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#20998;&#23376;&#23545;&#25509;&#26469;&#20272;&#35745;&#27963;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14315v1 Announce Type: cross  Abstract: Structure-based drug design aims at generating high affinity ligands with prior knowledge of 3D target structures. Existing methods either use conditional generative model to learn the distribution of 3D ligands given target binding sites, or iteratively modify molecules to optimize a structure-based activity estimator. The former is highly constrained by data quantity and quality, which leaves optimization-based approaches more promising in practical scenario. However, existing optimization-based approaches choose to edit molecules in 2D space, and use molecular docking to estimate the activity using docking predicted 3D target-ligand complexes. The misalignment between the action space and the objective hinders the performance of these models, especially for those employ deep learning for acceleration. In this work, we propose MolEdit3D to combine 3D molecular generation with optimization frameworks. We develop a novel 3D graph editi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;FPGA&#30340;&#25512;&#29702;&#21152;&#36895;&#22120;&#65292;&#33021;&#22815;&#39640;&#25928;&#25903;&#25345;&#20219;&#24847;&#20869;&#26680;&#22823;&#23567;&#30340;CNN&#65292;&#36890;&#36807;Z&#27969;&#26041;&#27861;&#20248;&#21270;&#25968;&#25454;&#27969;&#12289;Kseg&#26041;&#26696;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#65292;&#20197;&#21450;VF&#21644;HF&#26041;&#27861;&#20248;&#21270;CNN&#37096;&#32626;&#12290;</title><link>https://arxiv.org/abs/2402.14307</link><description>&lt;p&gt;
&#22522;&#20110;FPGA&#30340;&#21152;&#36895;&#22120;&#65292;&#25903;&#25345;&#20219;&#24847;&#21367;&#31215;&#26680;&#22823;&#23567;&#30340;CNN
&lt;/p&gt;
&lt;p&gt;
An FPGA-Based Accelerator Enabling Efficient Support for CNNs with Arbitrary Kernel Sizes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14307
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;FPGA&#30340;&#25512;&#29702;&#21152;&#36895;&#22120;&#65292;&#33021;&#22815;&#39640;&#25928;&#25903;&#25345;&#20219;&#24847;&#20869;&#26680;&#22823;&#23567;&#30340;CNN&#65292;&#36890;&#36807;Z&#27969;&#26041;&#27861;&#20248;&#21270;&#25968;&#25454;&#27969;&#12289;Kseg&#26041;&#26696;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#65292;&#20197;&#21450;VF&#21644;HF&#26041;&#27861;&#20248;&#21270;CNN&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24341;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#30340;&#20851;&#38190;&#25805;&#20316;&#65292;&#20855;&#26377;&#22823;&#22411;&#21367;&#31215;&#26680;&#65292;&#22312;&#21508;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35774;&#35745;&#22312;&#25903;&#25345;&#22823;&#22411;&#21367;&#31215;&#26680;&#26102;&#35745;&#31639;&#25928;&#29575;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;FPGA&#30340;&#25512;&#29702;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#39640;&#25928;&#37096;&#32626;&#20855;&#26377;&#20219;&#24847;&#20869;&#26680;&#22823;&#23567;&#30340;CNN&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Z&#27969;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#37325;&#29992;&#26426;&#20250;&#26469;&#20248;&#21270;&#35745;&#31639;&#25968;&#25454;&#27969;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#35774;&#35745;&#32467;&#21512;&#21367;&#31215;&#26680;&#20998;&#21106;&#65288;Kseg&#65289;&#26041;&#26696;&#65292;&#20026;&#22823;&#22411;&#21367;&#31215;&#26680;&#25552;&#20379;&#20102;&#25193;&#23637;&#25903;&#25345;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#37325;&#21472;&#25968;&#25454;&#30340;&#23384;&#20648;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#26032;&#20852;CNN&#20013;&#20856;&#22411;&#22359;&#32467;&#26500;&#30340;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#22402;&#30452;&#34701;&#21512;&#65288;VF&#65289;&#21644;&#27700;&#24179;&#34701;&#21512;&#65288;HF&#65289;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;CNN&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14307v1 Announce Type: cross  Abstract: Convolutional neural networks (CNNs) with large kernels, drawing inspiration from the key operations of vision transformers (ViTs), have demonstrated impressive performance in various vision-based applications. To address the issue of computational efficiency degradation in existing designs for supporting large-kernel convolutions, an FPGA-based inference accelerator is proposed for the efficient deployment of CNNs with arbitrary kernel sizes. Firstly, a Z-flow method is presented to optimize the computing data flow by maximizing data reuse opportunity. Besides, the proposed design, incorporating the kernel-segmentation (Kseg) scheme, enables extended support for large-kernel convolutions, significantly reducing the storage requirements for overlapped data. Moreover, based on the analysis of typical block structures in emerging CNNs, vertical-fused (VF) and horizontal-fused (HF) methods are developed to optimize CNN deployments from bo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20351;&#29992;Expohedron&#35299;&#20915;&#24085;&#32047;&#25176;&#26368;&#20248;&#25928;&#29992;-&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;Birkhoff-von Neumann&#20998;&#35299;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14305</link><description>&lt;p&gt;
&#26377;&#25928;&#23454;&#29616;&#22312;&#37325;&#22797;&#25490;&#21517;&#20013;&#32676;&#20307;&#38388;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#25928;&#29992;&#8212;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14305
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20351;&#29992;Expohedron&#35299;&#20915;&#24085;&#32047;&#25176;&#26368;&#20248;&#25928;&#29992;-&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;Birkhoff-von Neumann&#20998;&#35299;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#35745;&#31639;&#20855;&#26377;&#24085;&#32047;&#25176;&#26368;&#20248;&#24179;&#34913;&#20445;&#35777;&#30340;&#19968;&#31995;&#21015;&#25490;&#21517;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#65288;1&#65289;&#26368;&#22823;&#21270;&#28040;&#36153;&#32773;&#25928;&#29992;&#21644;&#65288;2&#65289;&#26368;&#23567;&#21270;&#29289;&#21697;&#29983;&#20135;&#32773;&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#36825;&#26679;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#20351;&#29992;&#26631;&#37327;&#21270;&#26041;&#27861;&#21644;&#32447;&#24615;&#35268;&#21010;&#26469;&#35299;&#20915;&#65292;&#22522;&#20110;&#34920;&#31034;&#29289;&#21697;&#21487;&#33021;&#25490;&#21517;&#20998;&#24067;&#30340;&#21452;&#38543;&#26426;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#26041;&#27861;&#20381;&#36182;&#20110;Birkhoff-von Neumann&#65288;BvN&#65289;&#20998;&#35299;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(n^5)$&#65292;&#20854;&#20013;$n$&#26159;&#29289;&#21697;&#25968;&#37327;&#65292;&#36825;&#20351;&#24471;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#20013;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Expohedron&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064; - &#19968;&#20010;&#34920;&#24449;&#25152;&#26377;&#21487;&#36798;&#21040;&#29289;&#21697;&#26333;&#20809;&#30340;&#25490;&#21015;&#22810;&#38754;&#20307;&#12290;&#22312;Expohedron&#19978;&#65292;&#25105;&#20204;&#32472;&#21046;&#20102;&#24085;&#32047;&#25176;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#22312;&#26368;&#22823;&#21270;&#25928;&#29992;&#21644;&#26368;&#23567;&#21270;&#19981;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14305v1 Announce Type: cross  Abstract: In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items. Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems. To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve which captures the trade-off betwee
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GenSERP&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#25972;&#29702;&#25628;&#32034;&#32467;&#26524;&#24182;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#36830;&#36143;&#30340;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.14301</link><description>&lt;p&gt;
GenSERP: &#29992;&#20110;&#25972;&#20010;&#39029;&#38754;&#21576;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GenSERP: Large Language Models for Whole Page Presentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GenSERP&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#25972;&#29702;&#25628;&#32034;&#32467;&#26524;&#24182;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#36830;&#36143;&#30340;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#26368;&#23567;&#21270;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#30340;&#32452;&#32455;&#24037;&#20316;&#24102;&#26469;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GenSERP&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#21644;&#35270;&#35273;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21160;&#24577;&#32452;&#32455;&#20013;&#38388;&#25628;&#32034;&#32467;&#26524;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#29983;&#25104;&#30340;&#32842;&#22825;&#31572;&#26696;&#12289;&#32593;&#31449;&#25688;&#35201;&#12289;&#22810;&#23186;&#20307;&#25968;&#25454;&#12289;&#30693;&#35782;&#38754;&#26495;&#31561;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;&#20197;&#36830;&#36143;&#30340;SERP&#24067;&#23616;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14301v1 Announce Type: cross  Abstract: The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query. Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result. (2) An answer generation phase where the LLM populates the layouts with the retrieved content. In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the pag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.14294</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#20114;&#25442;&#24615;&#23454;&#29616;&#39640;&#21442;&#25968;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-arity PAC learning via exchangeability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#32500;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21363;&#22312;&#8220;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#8221;&#23384;&#22312;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#12290; &#22312;&#36825;&#20010;&#29702;&#35770;&#20013;&#65292;&#20551;&#35774;&#21487;&#20197;&#26159;&#22270;&#24418;&#12289;&#36229;&#22270;&#65292;&#25110;&#32773;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26159;&#26377;&#38480;&#20851;&#31995;&#35821;&#35328;&#20013;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;i.i.d.&#25277;&#26679;&#34987;&#25277;&#26679;&#20135;&#29983;&#21487;&#20114;&#25442;&#20998;&#24067;&#30340;&#35825;&#23548;&#23376;&#32467;&#26500;&#21462;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#65292;&#36890;&#36807;&#34920;&#24449;&#39640;&#32500;&#65288;agnostic&#65289;PAC&#21487;&#23398;&#24615;&#65292;&#20197;&#32431;&#32452;&#21512;&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#21450;&#36866;&#24403;&#29256;&#26412;&#30340;&#22343;&#21248;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14294v1 Announce Type: new  Abstract: We develop a theory of high-arity PAC learning, which is statistical learning in the presence of "structured correlation". In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.
&lt;/p&gt;</description></item><item><title>CEV-LM &#26159;&#19968;&#20010;&#36731;&#37327;&#12289;&#21322;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#32534;&#36753;&#21521;&#37327;&#25511;&#21046;&#25991;&#26412;&#30340;&#36895;&#24230;&#12289;&#38899;&#37327;&#21644;&#32469;&#22280;&#24230;&#37327;&#65292;&#20174;&#32780;&#26356;&#31934;&#20934;&#22320;&#23450;&#21046;&#29983;&#25104;&#30340;&#25991;&#26412;&#24418;&#29366;&#65292;&#27604;&#29616;&#26377;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14290</link><description>&lt;p&gt;
CEV-LM: &#21463;&#25511;&#32534;&#36753;&#21521;&#37327;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22609;&#36896;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14290
&lt;/p&gt;
&lt;p&gt;
CEV-LM &#26159;&#19968;&#20010;&#36731;&#37327;&#12289;&#21322;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#32534;&#36753;&#21521;&#37327;&#25511;&#21046;&#25991;&#26412;&#30340;&#36895;&#24230;&#12289;&#38899;&#37327;&#21644;&#32469;&#22280;&#24230;&#37327;&#65292;&#20174;&#32780;&#26356;&#31934;&#20934;&#22320;&#23450;&#21046;&#29983;&#25104;&#30340;&#25991;&#26412;&#24418;&#29366;&#65292;&#27604;&#29616;&#26377;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25511;&#21046;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#26631;&#20934;&#65292;&#38656;&#35201;&#26356;&#22810;&#22320;&#23450;&#21046;&#29983;&#25104;&#30340;&#32039;&#20945;&#24615;&#12289;&#38024;&#23545;&#24615;&#21644;&#20449;&#24687;&#24615;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#21463;&#20247;/&#24212;&#29992;&#31243;&#24207;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#26041;&#27861;&#20027;&#35201;&#35843;&#25972;&#25991;&#26412;&#30340;&#35821;&#20041;&#65288;&#22914;&#24773;&#24863;&#12289;&#20027;&#39064;&#65289;&#12289;&#32467;&#26500;&#65288;&#22914;&#21477;&#27861;&#26641;&#12289;&#35789;&#24615;&#65289;&#21644;&#35789;&#27719;&#65288;&#22914;&#20851;&#38190;&#35789;/&#30701;&#35821;&#21253;&#21547;&#65289;&#65292;&#20294;&#26080;&#27861;&#23454;&#29616;&#22797;&#26434;&#30340;&#30446;&#26631;&#65292;&#22914;&#25511;&#21046;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35835;&#24615;,&#25105;&#20204;&#24341;&#20837;&#20102;CEV-LM&#8212;&#8212;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#21322;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#32534;&#36753;&#21521;&#37327;&#26469;&#25511;&#21046;&#19977;&#20010;&#34917;&#20805;&#24230;&#37327;&#65288;&#36895;&#24230;&#12289;&#38899;&#37327;&#21644;&#32469;&#22280;&#65289;&#65292;&#20197;&#37327;&#21270;&#25991;&#26412;&#30340;&#24418;&#29366;&#65288;&#20363;&#22914;&#20869;&#23481;&#30340;&#33410;&#22863;&#65289;&#12290; &#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;CTG&#27169;&#22411;&#65292;&#21457;&#29616;CEV-LM &#21487;&#26174;&#33879;&#26356;&#26377;&#38024;&#23545;&#24615;&#21644;&#31934;&#30830;&#22320;&#25511;&#21046;&#36825;&#19977;&#20010;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14290v1 Announce Type: new  Abstract: As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three m
&lt;/p&gt;</description></item><item><title>TinyLLaVA&#26694;&#26550;&#20351;&#24471;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26356;&#22909;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#35757;&#32451;&#26041;&#26696;&#36798;&#21040;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#27169;&#22411;TinyLLaVA-3.1B&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;7B&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14289</link><description>&lt;p&gt;
TinyLLaVA&#65306;&#23567;&#35268;&#27169;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TinyLLaVA: A Framework of Small-scale Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14289
&lt;/p&gt;
&lt;p&gt;
TinyLLaVA&#26694;&#26550;&#20351;&#24471;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#26356;&#22909;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#35757;&#32451;&#26041;&#26696;&#36798;&#21040;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#26368;&#20339;&#27169;&#22411;TinyLLaVA-3.1B&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;7B&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLaVA&#26694;&#26550;&#65292;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#23567;&#35268;&#27169;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#25552;&#20379;&#20102;&#32479;&#19968;&#35270;&#35282;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#36830;&#25509;&#27169;&#22359;&#12289;&#35821;&#35328;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35757;&#32451;&#26041;&#26696;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#26356;&#22909;&#36136;&#37327;&#30340;&#25968;&#25454;&#32467;&#21512;&#26356;&#22909;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;LMMs&#33021;&#22815;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#19982;&#26356;&#22823;&#30340;LMMs&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#23567;&#35268;&#27169;LMMs&#12290;&#25105;&#20204;&#26368;&#20339;&#27169;&#22411;TinyLLaVA-3.1B&#22312;&#19982;&#29616;&#26377;&#30340;7B&#27169;&#22411;&#65288;&#22914;LLaVA-1.5&#21644;Qwen-VL&#65289;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#22312;&#25968;&#25454;&#25193;&#23637;&#12289;&#35757;&#32451;&#35774;&#32622;&#21644;&#27169;&#22411;&#36873;&#25321;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#20195;&#30721;&#23558;&#34987;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14289v1 Announce Type: cross  Abstract: We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs). We empirically study the effects of different vision encoders, connection modules, language models, training data and training recipes. Our extensive experiments showed that better quality of data combined with better training recipes, smaller LMMs can consistently achieve on-par performances compared to bigger LMMs. Under our framework, we train a family of small-scale LMMs. Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as baselines for future research in terms of data scaling, training setups and model selections. Our model weights and codes will be made public.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;</title><link>https://arxiv.org/abs/2402.14285</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#25193;&#25955;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14285
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#29983;&#25104;&#38050;&#29748;&#21367;&#35889;&#65289;&#65292;&#25216;&#26415;&#37325;&#28857;&#25918;&#22312;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#19978;&#12290;&#38899;&#20048;&#35268;&#21017;&#36890;&#24120;&#20197;&#31526;&#21495;&#24418;&#24335;&#34920;&#36798;&#22312;&#38899;&#31526;&#29305;&#24449;&#19978;&#65292;&#22914;&#38899;&#31526;&#23494;&#24230;&#25110;&#21644;&#24358;&#36827;&#34892;&#65292;&#35768;&#22810;&#35268;&#21017;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#22312;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#24341;&#23548;&#25193;&#25955;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;&#25511;&#21046;&#24341;&#23548;&#65288;SCG&#65289;&#65292;&#23427;&#20165;&#38656;&#35201;&#23545;&#35268;&#21017;&#20989;&#25968;&#36827;&#34892;&#21069;&#21521;&#35780;&#20272;&#65292;&#21487;&#20197;&#19982;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#19968;&#36215;&#24037;&#20316;&#65292;&#20174;&#32780;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#30340;&#26080;&#35757;&#32451;&#24341;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#21487;&#20197;&#19982;SCG&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#32452;&#21512;&#12290;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#26631;&#20934;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#22522;&#20110;&#22320;&#26631;&#30340;&#23450;&#20301;&#19982;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25112;&#22330;&#19978;&#39044;&#27979;&#31227;&#21160;&#23454;&#20307;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;GPS&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14280</link><description>&lt;p&gt;
&#22312;&#26080;GPS&#29615;&#22659;&#20013;&#20351;&#29992;&#22522;&#20110;&#22320;&#26631;&#30340;&#23450;&#20301;&#36827;&#34892;&#23433;&#20840;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Secure Navigation using Landmark-based Localization in a GPS-denied Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#22522;&#20110;&#22320;&#26631;&#30340;&#23450;&#20301;&#19982;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25112;&#22330;&#19978;&#39044;&#27979;&#31227;&#21160;&#23454;&#20307;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22312;&#26080;GPS&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25112;&#22330;&#24773;&#26223;&#20013;&#65292;&#20381;&#36182;GPS&#36827;&#34892;&#23548;&#33322;&#21487;&#33021;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#24369;&#28857;&#12290;&#23545;&#25163;&#32463;&#24120;&#20351;&#29992;&#31574;&#30053;&#26469;&#21542;&#35748;&#25110;&#27450;&#39575;GPS&#20449;&#21495;&#65292;&#36825;&#38656;&#35201;&#26367;&#20195;&#26041;&#27861;&#26469;&#23450;&#20301;&#21644;&#23548;&#33322;&#31227;&#21160;&#37096;&#38431;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#22320;&#26631;&#30340;&#23450;&#20301;&#65288;LanBLoc&#65289;&#19982;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EKF&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#39044;&#27979;&#27839;&#30528;&#25112;&#22330;&#31227;&#21160;&#23454;&#20307;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#30001;&#37096;&#38431;&#25511;&#21046;&#20013;&#24515;&#29983;&#25104;&#30340;&#23433;&#20840;&#36712;&#36857;&#20449;&#24687;&#65292;&#32771;&#34385;&#21487;&#35782;&#21035;&#30340;&#22320;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14280v1 Announce Type: cross  Abstract: In modern battlefield scenarios, the reliance on GPS for navigation can be a critical vulnerability. Adversaries often employ tactics to deny or deceive GPS signals, necessitating alternative methods for the localization and navigation of mobile troops. Range-free localization methods such as DV-HOP rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in a dynamic and sparse network topology. Vision-based approaches like SLAM and Visual Odometry use sensor fusion techniques for map generation and pose estimation that are more sophisticated and computationally expensive. This paper proposes a novel framework that integrates landmark-based localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the future state of moving entities along the battlefield. Our framework utilizes safe trajectory information generated by the troop control center by considering identifiable landmark
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30828;&#26679;&#26412;&#21152;&#26435;&#25345;&#32493;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IR-DRO&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#65292;&#20197;&#25913;&#21892;LLM&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14270</link><description>&lt;p&gt;
&#29275;&#22836;&#35282;&#65306;&#30828;&#26679;&#26412;&#21152;&#26435;&#25345;&#32493;&#35757;&#32451;&#25913;&#21892;LLM&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14270
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30828;&#26679;&#26412;&#21152;&#26435;&#25345;&#32493;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IR-DRO&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#65292;&#20197;&#25913;&#21892;LLM&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30701;&#32570;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#19968;&#20010;&#36731;&#37327;&#32423;&#25345;&#32493;&#35757;&#32451;LLMs&#30340;&#32463;&#39564;&#31574;&#30053;&#24320;&#22987;&#65292;&#20351;&#29992;&#23427;&#20204;&#30340;&#21407;&#22987;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#23548;&#33268;&#20013;&#31561;&#25439;&#22833;&#30340;&#26679;&#26412;&#30340;&#36873;&#25321;&#24615;&#20445;&#30041;&#12290;&#36825;&#20123;&#26679;&#26412;&#34987;&#35748;&#20026;&#26159;&#20449;&#24687;&#20016;&#23500;&#30340;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#19982;&#26368;&#39640;&#25439;&#22833;&#30340;&#26679;&#26412;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#23558;&#30001;&#20110;&#19982;&#25968;&#25454;&#22122;&#22768;&#21644;&#22797;&#26434;&#24615;&#30340;&#30456;&#20851;&#24615;&#32780;&#34987;&#20002;&#24323;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#31574;&#30053;&#24418;&#24335;&#21270;&#20026;&#22522;&#20110;&#23454;&#20363;&#21152;&#26435;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;IR-DRO&#65289;&#30340;&#21407;&#21017;&#26694;&#26550;&#12290;IR-DRO&#26088;&#22312;&#36890;&#36807;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;&#26426;&#21046;&#21160;&#24577;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#30340;&#37325;&#28857;&#26679;&#26412;&#65292;&#30001;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#31616;&#21270;&#65292;&#20197;&#20415;&#36731;&#26494;&#25972;&#21512;&#21040;&#24050;&#24314;&#31435;&#30340;&#35757;&#32451;&#21327;&#35758;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14270v1 Announce Type: new  Abstract: In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protoco
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;</title><link>https://arxiv.org/abs/2402.14264</link><description>&lt;p&gt;
&#21452;&#31283;&#20581;&#23398;&#20064;&#22312;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#32467;&#26500;&#19981;&#21487;&#30693;&#24615;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14264
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#26368;&#26680;&#24515;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#24191;&#27867;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20272;&#35745;&#31574;&#30053;&#65292;&#26368;&#36817;&#36824;&#32435;&#20837;&#20102;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#37319;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#32479;&#35745;&#19979;&#30028;&#32467;&#26500;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#24178;&#25200;&#20989;&#25968;&#27809;&#26377;&#32467;&#26500;&#24615;&#36136;&#20551;&#35774;&#65292;&#38500;&#20102;&#35775;&#38382;&#40657;&#30418;&#20272;&#35745;&#22120;&#20197;&#36798;&#21040;&#23567;&#35823;&#24046;&#65307;&#24403;&#21482;&#24895;&#24847;&#32771;&#34385;&#20351;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#20998;&#31867;&#31070;&#35861;&#20316;&#20026;&#40657;&#30418;&#23376;&#36807;&#31243;&#30340;&#20272;&#35745;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#23545;&#20110;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.14259</link><description>&lt;p&gt;
&#21333;&#35789;&#24207;&#21015;&#29109;&#65306;&#36208;&#21521;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22312;&#30830;&#20445;&#23433;&#20840;&#20851;&#38190;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#21487;&#38752;&#24615;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#23578;&#26410;&#24314;&#31435;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26080;&#20851;&#30340;&#35789;&#27719;&#21644;&#35821;&#24207;&#21547;&#26377;&#26377;&#38480;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#36825;&#26159;&#30001;&#20110;&#29983;&#25104;&#19981;&#24179;&#31561;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#35821;&#20041;&#30456;&#20851;&#24615;&#22312;&#21333;&#35789;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#27604;&#20363;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26102;&#26356;&#21152;&#24378;&#35843;&#20851;&#38190;&#35789;&#21644;&#26356;&#30456;&#20851;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;5&#20010;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#21033;&#29992;7&#31181;&#8220;&#29616;&#25104;&#30340;&#8221;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;WSE&#19982;6&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;WSE&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14259v1 Announce Type: cross  Abstract: Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on ac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35814;&#32454;&#30340;&#21464;&#37327;&#32423;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#27599;&#20010;&#21464;&#37327;&#23545;&#24615;&#33021;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#20026;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#24178;&#39044;&#25514;&#26045;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;</title><link>https://arxiv.org/abs/2402.14254</link><description>&lt;p&gt;
&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#24046;&#24322;&#30340;&#20998;&#23618;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A hierarchical decomposition for explaining ML performance discrepancies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35814;&#32454;&#30340;&#21464;&#37327;&#32423;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#27599;&#20010;&#21464;&#37327;&#23545;&#24615;&#33021;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#20026;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#24178;&#39044;&#25514;&#26045;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24615;&#33021;&#24448;&#24448;&#26377;&#25152;&#19981;&#21516;&#12290;&#20102;&#35299;&#23427;&#20204;&#30340;&#24615;&#33021;&#24046;&#24322;&#30340;&#21407;&#22240;&#23545;&#20110;&#30830;&#23450;&#20309;&#31181;&#24178;&#39044;&#25514;&#26045;&#65288;&#20363;&#22914;&#31639;&#27861;&#25110;&#36816;&#33829;&#65289;&#26368;&#26377;&#25928;&#20197;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#23558;&#24635;&#24615;&#33021;&#24046;&#24322;&#20998;&#35299;&#20026;&#29305;&#24449;&#20998;&#24067;$p(X)$&#21464;&#21270;&#30340;&#24433;&#21709;&#19982;&#32467;&#26524;&#26465;&#20214;&#20998;&#24067;$p(Y|X)$&#21464;&#21270;&#30340;&#24433;&#21709;&#30340;$\textit{&#27719;&#24635;&#20998;&#35299;}$&#65307;&#28982;&#32780;&#65292;&#36825;&#26679;&#31895;&#31961;&#30340;&#35299;&#37322;&#21482;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#26041;&#27861;&#26469;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;$\textit{&#35814;&#32454;&#30340;&#21464;&#37327;&#32423;&#20998;&#35299;}$&#21487;&#20197;&#37327;&#21270;&#27599;&#20010;&#21464;&#37327;&#23545;&#27719;&#24635;&#20998;&#35299;&#20013;&#27599;&#20010;&#39033;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#26377;&#20851;&#20840;&#22240;&#26524;&#22270;&#30340;&#23436;&#25972;&#30693;&#35782;&#25110;&#36827;&#34892;&#24378;&#21442;&#25968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14254v1 Announce Type: new  Abstract: Machine learning (ML) algorithms can often differ in performance across domains. Understanding $\textit{why}$ their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Existing methods focus on $\textit{aggregate decompositions}$ of the total performance gap into the impact of a shift in the distribution of features $p(X)$ versus the impact of a shift in the conditional distribution of the outcome $p(Y|X)$; however, such coarse explanations offer only a few options for how one can close the performance gap. $\textit{Detailed variable-level decompositions}$ that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. However, existing methods assume knowledge of the full causal graph or make strong parametric assumpti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#33258;&#35757;&#32451;&#65288;KIST&#65289;&#30340;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#23478;&#24635;&#32467;&#30340;&#24322;&#24120;&#30693;&#35782;&#38598;&#25104;&#21040;&#37325;&#24314;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#24322;&#24120;&#26679;&#26412;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24322;&#24120;&#23450;&#20301;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14246</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#35757;&#32451;&#30340;&#37325;&#24314;&#24322;&#24120;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Reconstruction-Based Anomaly Localization via Knowledge-Informed Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#33258;&#35757;&#32451;&#65288;KIST&#65289;&#30340;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#23478;&#24635;&#32467;&#30340;&#24322;&#24120;&#30693;&#35782;&#38598;&#25104;&#21040;&#37325;&#24314;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#24322;&#24120;&#26679;&#26412;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24322;&#24120;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#23450;&#20301;&#28041;&#21450;&#22312;&#22270;&#20687;&#20013;&#23450;&#20301;&#24322;&#24120;&#21306;&#22495;&#65292;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#24037;&#19994;&#20219;&#21153;&#12290;&#30001;&#20110;&#20854;&#20302;&#22797;&#26434;&#24615;&#21644;&#39640;&#35299;&#37322;&#24615;&#65292;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#29992;&#20110;&#24322;&#24120;&#23450;&#20301;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#27491;&#24120;&#26679;&#26412;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#22914;&#26524;&#22312;&#24322;&#24120;&#23450;&#20301;&#36807;&#31243;&#20013;&#36866;&#24403;&#21033;&#29992;&#24322;&#24120;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#26377;&#24369;&#26631;&#35760;&#30340;&#24322;&#24120;&#26679;&#26412;&#21487;&#29992;&#65292;&#36825;&#38480;&#21046;&#20102;&#25913;&#36827;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#39046;&#22495;&#19987;&#23478;&#24635;&#32467;&#30340;&#19968;&#20123;&#24322;&#24120;&#30693;&#35782;&#12290;&#21033;&#29992;&#36825;&#26679;&#30340;&#30693;&#35782;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#21033;&#29992;&#24322;&#24120;&#26679;&#26412;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#23450;&#20301;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#33258;&#35757;&#32451;&#65288;KIST&#65289;&#30340;&#26032;&#39062;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#30693;&#35782;&#38598;&#25104;&#21040;&#37325;&#24314;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14246v1 Announce Type: new  Abstract: Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task. Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability. Most existing reconstruction-based methods only use normal samples to construct model. If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved. However, usually only weakly labeled anomalous samples are available, which limits the improvement. In many cases, we can obtain some knowledge of anomalies summarized by domain experts. Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance. In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction mo
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25552;&#20379;&#33258;&#21160;&#20559;&#22909;&#21453;&#39304;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#26524;</title><link>https://arxiv.org/abs/2402.14245</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#22686;&#24378;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14245
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25552;&#20379;&#33258;&#21160;&#20559;&#22909;&#21453;&#39304;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23558;&#30001;LLMs&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#25351;&#20196;&#19982;&#25191;&#34892;&#25152;&#38656;&#30340;&#21521;&#37327;&#21270;&#25805;&#20316;&#23545;&#40784;&#65292;&#24120;&#24120;&#38656;&#35201;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#32454;&#33410;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#36825;&#31181;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#32454;&#24494;&#20043;&#22788;&#30340;&#38656;&#27714;&#65292;&#21463;&#21040;&#22522;&#20110;&#20559;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#33258;&#21160;&#20559;&#22909;&#21453;&#39304;&#65292;&#20165;&#20174;&#22270;&#20687;&#36755;&#20837;&#20013;&#24341;&#23548;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#21517;&#20026;CriticGPT&#30340;&#22810;&#27169;&#24577;LLM&#65292;&#33021;&#22815;&#29702;&#35299;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#36712;&#36857;&#35270;&#39057;&#65292;&#20316;&#20026;&#19968;&#20010;&#35780;&#35770;&#21592;&#25552;&#20379;&#20998;&#26512;&#21644;&#20559;&#22909;&#21453;&#39304;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#22870;&#21169;&#24314;&#27169;&#30340;&#35282;&#24230;&#39564;&#35777;&#20102;CriticGPT&#29983;&#25104;&#30340;&#20559;&#22909;&#26631;&#31614;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14245v1 Announce Type: cross  Abstract: Recently, there has been considerable attention towards leveraging large language models (LLMs) to enhance decision-making processes. However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details. To circumvent the need for such task-specific granularity, inspired by preference-based policy learning approaches, we investigate the utilization of multimodal LLMs to provide automated preference feedback solely from image inputs to guide decision-making. In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback. Subsequently, we validate the effectiveness of preference labels generated by CriticGPT from a reward modeling perspective. Experimental evaluation of the a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14244</link><description>&lt;p&gt;
MENTOR&#65306;&#22312;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14244
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#20026;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#24182;&#20381;&#27425;&#23436;&#25104;&#30340;&#23618;&#27425;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#25214;&#21040;&#36866;&#24403;&#30340;&#23376;&#30446;&#26631;&#26469;&#30830;&#20445;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#25972;&#21512;&#21040;&#20854;&#20013;&#65288;MENTOR&#65289;&#12290;MENTOR&#20805;&#24403;&#8220;&#23548;&#24072;&#8221;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#39640;&#23618;&#31574;&#30053;&#23398;&#20064;&#20013;&#65292;&#20197;&#25214;&#21040;&#26356;&#22909;&#30340;&#23376;&#30446;&#26631;&#12290;&#33267;&#20110;&#20302;&#23618;&#31574;&#30053;&#65292;MENTOR&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#31574;&#30053;&#20197;&#20998;&#21035;&#36827;&#34892;&#25506;&#32034;-&#24320;&#21457;&#35299;&#32806;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20154;&#31867;&#21487;&#20197;&#31616;&#21333;&#22320;&#23558;&#20219;&#21153;&#25286;&#20998;&#25104;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14244v1 Announce Type: new  Abstract: Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14236</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#30340;&#33258;&#21160;&#35774;&#35745;&#19982;&#20248;&#21270;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20998;&#24067;&#24335;&#28388;&#27874;&#30005;&#36335;(DFC)&#22797;&#26434;&#19988;&#32791;&#26102;&#65292;&#30005;&#36335;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#30005;&#23376;&#24037;&#31243;&#24072;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;DFC&#30340;&#35774;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#24037;&#31243;&#24072;&#35774;&#35745;&#32463;&#39564;&#30340;&#20381;&#36182;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;&#30005;&#36335;&#35774;&#35745;&#30456;&#20851;&#30340;&#20027;&#35266;&#24615;&#21644;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19982;&#20256;&#32479;&#30340;&#24037;&#31243;&#24072;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35774;&#35745;&#25928;&#29575;&#21644;&#36136;&#37327;&#19978;&#37117;&#26377;&#26126;&#26174;&#25913;&#21892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35774;&#35745;&#22797;&#26434;&#25110;&#24555;&#36895;&#21457;&#23637;&#30340;DFC&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14236v1 Announce Type: cross  Abstract: Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#25509;&#36817;&#26368;&#20248;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#22312;&#26410;&#30693;&#25351;&#25968;&#35774;&#23450;&#19979;&#30340;&#20855;&#26377;&#33258;&#25105;&#36873;&#25321;&#20559;&#24046;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#39640;&#25928;&#22320;&#24674;&#22797;&#21442;&#25968;&#21521;&#37327;&#65292;&#20855;&#26377;&#26174;&#33879;&#20248;&#21270;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14229</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#25105;&#36873;&#25321;&#20559;&#24046;&#30340;&#39640;&#25928;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Linear Regression with Self-Selection Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14229
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#25509;&#36817;&#26368;&#20248;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#22312;&#26410;&#30693;&#25351;&#25968;&#35774;&#23450;&#19979;&#30340;&#20855;&#26377;&#33258;&#25105;&#36873;&#25321;&#20559;&#24046;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#39640;&#25928;&#22320;&#24674;&#22797;&#21442;&#25968;&#21521;&#37327;&#65292;&#20855;&#26377;&#26174;&#33879;&#20248;&#21270;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#26410;&#30693;&#25351;&#25968;&#35774;&#23450;&#20013;&#20855;&#26377;&#33258;&#25105;&#36873;&#25321;&#20559;&#24046;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26368;&#36817;&#30001;Cherapanamjeri&#12289;Daskalakis&#12289;Ilyas&#21644;Zampetakis[STOC 2023]&#30340;&#30740;&#31350;&#24341;&#20837;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#35266;&#23519;&#21040;$m$&#20010;i.i.d.&#26679;&#26412;$(\mathbf{x}_{\ell},z_{\ell})_{\ell=1}^m$&#65292;&#20854;&#20013;$z_{\ell}=\max_{i\in [k]}\{\mathbf{x}_{\ell}^T\mathbf{w}_i+\eta_{i,\ell}\}$&#65292;&#20294;&#26368;&#22823;&#21270;&#25351;&#25968;$i_{\ell}$&#26159;&#19981;&#21487;&#35266;&#27979;&#30340;&#12290;&#36825;&#37324;&#65292;$\mathbf{x}_{\ell}$&#34987;&#20551;&#35774;&#20026;$\mathcal{N}(0,I_n)$&#65292;&#22122;&#22768;&#20998;&#24067;$\mathbf{\eta}_{\ell}\sim \mathcal{D}$&#26159;&#20197;$\mathbf{x}_{\ell}$&#20026;&#20013;&#24515;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#25509;&#36817;&#26368;&#20248;&#30340;&#26679;&#26412;&#39640;&#25928;&#65288;&#20197;$k$&#20026;&#24230;&#37327;&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#24674;&#22797;$\mathbf{w}_1,\ldots,\mathbf{w}_k\in \mathbb{R}^n$&#65292;&#20854;$\ell_2$-&#35823;&#24046;&#20026;$\varepsilon$&#65292;&#20855;&#26377;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;$\tilde{O}(n)\cdot \mathsf{poly}(k,1/\varepsilon)$&#21644;&#26174;&#33879;&#25913;&#21892;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;$\mathsf{poly}(n,k,1/\varepsilon)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14229v1 Announce Type: cross  Abstract: We consider the problem of linear regression with self-selection bias in the unknown-index setting, as introduced in recent work by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$ i.i.d. samples $(\mathbf{x}_{\ell},z_{\ell})_{\ell=1}^m$ where $z_{\ell}=\max_{i\in [k]}\{\mathbf{x}_{\ell}^T\mathbf{w}_i+\eta_{i,\ell}\}$, but the maximizing index $i_{\ell}$ is unobserved. Here, the $\mathbf{x}_{\ell}$ are assumed to be $\mathcal{N}(0,I_n)$ and the noise distribution $\mathbf{\eta}_{\ell}\sim \mathcal{D}$ is centered and independent of $\mathbf{x}_{\ell}$. We provide a novel and near optimally sample-efficient (in terms of $k$) algorithm to recover $\mathbf{w}_1,\ldots,\mathbf{w}_k\in \mathbb{R}^n$ up to additive $\ell_2$-error $\varepsilon$ with polynomial sample complexity $\tilde{O}(n)\cdot \mathsf{poly}(k,1/\varepsilon)$ and significantly improved time complexity $\mathsf{poly}(n,k,1/\varep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;</title><link>https://arxiv.org/abs/2402.14228</link><description>&lt;p&gt;
COPR:&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#21017;&#21270;&#23454;&#29616;&#25345;&#32493;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COPR: Continual Human Preference Learning via Optimal Policy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#30028; &#25688;&#35201;: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#36890;&#24120;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#37492;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#25345;&#32493;&#23545;&#40784;&#30456;&#23545;&#20110;&#20256;&#32479;&#38745;&#24577;&#23545;&#40784;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#21644;&#23454;&#38469;&#12290;&#28982;&#32780;&#65292;&#20351;RLHF&#19982;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20860;&#23481;&#30001;&#20110;&#20854;&#22797;&#26434;&#36807;&#31243;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30452;&#25509;&#23398;&#20064;&#26032;&#30340;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#23548;&#33268;&#21382;&#21490;&#20559;&#22909;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#23548;&#33268;&#26080;&#21161;&#25110;&#26377;&#23475;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#12290;COPR&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#37319;&#29992;Lagrange&#23545;&#20598;&#65288;LD&#65289;&#26041;&#27861;&#26681;&#25454;&#21382;&#21490;&#19978;&#30340;&#26368;&#20248;&#31574;&#30053;&#21160;&#24577;&#22320;&#27491;&#21017;&#21270;&#24403;&#21069;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21547;&#24322;&#24120;&#20540;3D&#21644;4D&#25968;&#25454;&#30340;&#40065;&#26834;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#20351;&#29992;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#20540;&#19981;&#22826;&#25935;&#24863;&#65292;&#36866;&#29992;&#20110;&#22810;&#32500;&#22024;&#26434;&#25110;&#19981;&#30830;&#23450;&#25968;&#25454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14227</link><description>&lt;p&gt;
&#20855;&#26377;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#30340;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14227
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21547;&#24322;&#24120;&#20540;3D&#21644;4D&#25968;&#25454;&#30340;&#40065;&#26834;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#20351;&#29992;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#20540;&#19981;&#22826;&#25935;&#24863;&#65292;&#36866;&#29992;&#20110;&#22810;&#32500;&#22024;&#26434;&#25110;&#19981;&#30830;&#23450;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#65289;&#65292;&#29992;&#20110;&#23454;&#26102;&#22788;&#29702;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;3D&#21644;4D&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#31639;&#27861;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#65288;MCC&#65289;&#32467;&#21512;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#30340;&#12290;&#23613;&#31649;&#22343;&#26041;&#35823;&#24046;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#37117;&#26159;&#21487;&#34892;&#30340;&#20195;&#20215;&#20989;&#25968;&#65292;&#20294;&#32467;&#26524;&#34920;&#26126;&#38750;&#20108;&#27425;&#26368;&#22823;&#30456;&#20851;&#24615;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#20540;&#19981;&#22826;&#25935;&#24863;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#32500;&#22024;&#26434;&#25110;&#19981;&#30830;&#23450;&#25968;&#25454;&#30340;&#24212;&#29992;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#22522;&#20110;&#26032;&#39062;&#30340;&#24191;&#20041;HR&#65288;GHR&#65289;&#24494;&#31215;&#20998;&#23548;&#20986;&#65292;&#23427;&#20801;&#35768;&#23545;&#22235;&#20803;&#25968;&#21464;&#37327;&#30340;&#23454;&#20989;&#25968;&#36827;&#34892;&#24494;&#20998;&#65292;&#24182;&#25552;&#20379;&#20056;&#27861;&#21644;&#38142;&#24335;&#27861;&#21017;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#38597;&#19988;&#31616;&#27905;&#30340;&#23548;&#20986;&#12290;&#22312;&#33016;&#37096;&#20869;&#37096;&#26631;&#35760;&#29289;&#30340;&#36816;&#21160;&#39044;&#27979;&#32972;&#26223;&#19979;&#36827;&#34892;&#30340;&#20223;&#30495;&#32467;&#26524;&#28085;&#30422;&#20102;&#32954;&#30284;&#25918;&#30103;&#20013;&#30340;&#24120;&#35268;&#21644;&#19981;&#35268;&#21017;&#21628;&#21560;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14227v1 Announce Type: new  Abstract: We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequenc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#20960;&#20309;&#20284;&#28982;&#35299;&#20915;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#23384;&#22312;&#20005;&#37325;&#30340;&#27424;&#37319;&#26679;&#65292;&#20063;&#33021;&#23454;&#29616;&#65292;&#19988;&#22312;&#20154;&#21475;&#35268;&#27169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14220</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#20960;&#20309;&#20998;&#24067;&#20272;&#35745;&#26410;&#30693;&#20154;&#21475;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Estimating Unknown Population Sizes Using the Hypergeometric Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#20960;&#20309;&#20284;&#28982;&#35299;&#20915;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#23384;&#22312;&#20005;&#37325;&#30340;&#27424;&#37319;&#26679;&#65292;&#20063;&#33021;&#23454;&#29616;&#65292;&#19988;&#22312;&#20154;&#21475;&#35268;&#27169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#36229;&#20960;&#20309;&#20998;&#24067;&#25551;&#36848;&#20174;&#21010;&#20998;&#20026;&#22810;&#20010;&#31867;&#21035;&#30340;&#31163;&#25955;&#20803;&#32032;&#24635;&#20307;&#20013;&#36827;&#34892;&#26080;&#25918;&#22238;&#25277;&#26679;&#12290;&#22312;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#31354;&#30333;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#30340;&#25361;&#25112;&#65292;&#24403;&#24635;&#20307;&#35268;&#27169;&#21644;&#20854;&#26500;&#25104;&#31867;&#21035;&#30340;&#22823;&#23567;&#22343;&#26410;&#30693;&#26102;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#20960;&#20309;&#20284;&#28982;&#35299;&#20915;&#36825;&#19968;&#20272;&#35745;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#20351;&#23384;&#22312;&#20005;&#37325;&#30340;&#27424;&#37319;&#26679;&#20063;&#33021;&#23454;&#29616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#37322;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20854;&#20013;&#22320;&#38754;&#30495;&#23454;&#20540;&#26159;&#26377;&#26465;&#20214;&#30340;&#36830;&#32493;&#28508;&#21464;&#37327;&#28151;&#21512;&#20998;&#24067;&#65292;&#27604;&#22914;&#21327;&#21516;&#36807;&#28388;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#12290;&#23454;&#35777;&#25968;&#25454;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#21475;&#35268;&#27169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#29992;&#20110;&#24314;&#27169;&#35745;&#25968;&#25968;&#25454;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14220v1 Announce Type: new  Abstract: The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26368;&#22823;&#21270;&#30456;&#21516;&#21050;&#28608;&#19979;&#21508;&#20010;&#20010;&#20307;&#30340;EEG&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#27492;&#23454;&#29616;&#20010;&#20307;&#38388;&#20849;&#20139;&#26102;&#31354;&#33041;&#30005;&#22270;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14213</link><description>&lt;p&gt;
&#20010;&#20307;&#38388;&#20849;&#20139;&#33041;&#30005;&#22270;&#26102;&#31354;&#34920;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#33258;&#28982;&#31070;&#32463;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14213
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26368;&#22823;&#21270;&#30456;&#21516;&#21050;&#28608;&#19979;&#21508;&#20010;&#20010;&#20307;&#30340;EEG&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#27492;&#23454;&#29616;&#20010;&#20307;&#38388;&#20849;&#20139;&#26102;&#31354;&#33041;&#30005;&#22270;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#21050;&#28608;&#35825;&#23548;&#30340;&#31070;&#32463;&#34920;&#24449;&#25581;&#31034;&#20102;&#20154;&#31867;&#22914;&#20309;&#23545;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#22806;&#22260;&#21050;&#28608;&#20570;&#20986;&#21453;&#24212;&#12290;&#29702;&#35299;&#33258;&#28982;&#21050;&#28608;&#22788;&#29702;&#30340;&#19968;&#33324;&#31070;&#32463;&#26426;&#21046;&#30340;&#20851;&#38190;&#22312;&#20110;&#23545;&#40784;&#21508;&#20010;&#20010;&#20307;&#30340;&#31070;&#32463;&#27963;&#21160;&#24182;&#25552;&#21462;&#20010;&#20307;&#38388;&#30340;&#20849;&#20139;&#31070;&#32463;&#34920;&#24449;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20197;&#20854;&#20016;&#23500;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#32780;&#38395;&#21517;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20010;&#20307;&#38388;&#20849;&#20139;&#26102;&#31354;&#33041;&#30005;&#22270;&#34920;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;CL-SSTER&#65289;&#12290;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;CL-SSTER&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26368;&#22823;&#21270;&#30456;&#21516;&#21050;&#28608;&#19979;&#21508;&#20010;&#20010;&#20307;&#30340;EEG&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#65292;&#19982;&#19981;&#21516;&#21050;&#28608;&#30340;&#30456;&#23545;&#24212;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#21367;&#31215;&#21516;&#26102;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14213v1 Announce Type: cross  Abstract: Neural representations induced by naturalistic stimuli offer insights into how humans respond to peripheral stimuli in daily life. The key to understanding the general neural mechanisms underlying naturalistic stimuli processing involves aligning neural activities across individuals and extracting inter-subject shared neural representations. Targeting the Electroencephalogram (EEG) technique, known for its rich spatial and temporal information, this study presents a general framework for Contrastive Learning of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER). Harnessing the representational capabilities of contrastive learning, CL-SSTER utilizes a neural network to maximize the similarity of EEG representations across individuals for identical stimuli, contrasting with those for varied stimuli. The network employed spatial and temporal convolutions to simultaneously learn the spatial and temporal patterns inhere
&lt;/p&gt;</description></item><item><title>Moonwalk&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;-&#36870;-Jacobian&#20056;&#31215;&#30340;&#26032;&#25216;&#26415;&#65292;&#21152;&#36895;&#21069;&#21521;&#26799;&#24230;&#35745;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20445;&#25345;&#30495;&#23454;&#26799;&#24230;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#35745;&#31639;&#26102;&#38388;&#38477;&#20302;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2402.14212</link><description>&lt;p&gt;
Moonwalk&#65306;&#36870;&#21521;-&#21069;&#21521;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Moonwalk: Inverse-Forward Differentiation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14212
&lt;/p&gt;
&lt;p&gt;
Moonwalk&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;-&#36870;-Jacobian&#20056;&#31215;&#30340;&#26032;&#25216;&#26415;&#65292;&#21152;&#36895;&#21069;&#21521;&#26799;&#24230;&#35745;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#20445;&#25345;&#30495;&#23454;&#26799;&#24230;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#35745;&#31639;&#26102;&#38388;&#38477;&#20302;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#34429;&#28982;&#22312;&#26799;&#24230;&#35745;&#31639;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#22312;&#35299;&#20915;&#20869;&#23384;&#28040;&#32791;&#21644;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#21069;&#21521;&#26799;&#24230;&#35745;&#31639;&#20316;&#20026;&#21487;&#36870;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#30340;&#28508;&#21147;&#65292;&#24182;&#19981;&#24102;&#26469;&#37325;&#22823;&#32570;&#28857;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;-&#36870;-Jacobian&#20056;&#31215;&#30340;&#26032;&#25216;&#26415;&#65292;&#21152;&#36895;&#20102;&#21069;&#21521;&#26799;&#24230;&#30340;&#35745;&#31639;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20943;&#23569;&#20869;&#23384;&#21644;&#20445;&#25345;&#30495;&#23454;&#26799;&#24230;&#20934;&#30830;&#24615;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Moonwalk&#22312;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#19982;&#26420;&#32032;&#21069;&#21521;&#30340;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#30456;&#27604;&#65292;&#22312;&#27809;&#26377;&#20998;&#37197;&#26356;&#22810;&#20869;&#23384;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#23454;&#35777;&#30340;&#35282;&#24230;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;Moonwalk&#19982;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#30456;&#32467;&#21512;&#26469;&#21152;&#36895;&#65292;&#20197;&#23454;&#29616;&#19982;&#21453;&#21521;&#20256;&#25773;&#30456;&#24403;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#23567;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14212v1 Announce Type: cross  Abstract: Backpropagation, while effective for gradient computation, falls short in addressing memory consumption, limiting scalability. This work explores forward-mode gradient computation as an alternative in invertible networks, showing its potential to reduce the memory footprint without substantial drawbacks. We introduce a novel technique based on a vector-inverse-Jacobian product that accelerates the computation of forward gradients while retaining the advantages of memory reduction and preserving the fidelity of true gradients. Our method, Moonwalk, has a time complexity linear in the depth of the network, unlike the quadratic time complexity of na\"ive forward, and empirically reduces computation time by several orders of magnitude without allocating more memory. We further accelerate Moonwalk by combining it with reverse-mode differentiation to achieve time complexity comparable with backpropagation while maintaining a much smaller mem
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Patched Spectrogram Transformer&#23454;&#29616;&#30340;PS3DT&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#22120;&#22312;ASVspoof2019&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14205</link><description>&lt;p&gt;
&#20351;&#29992;Patched Spectrogram Transformer &#23454;&#29616;&#30340;&#21387;&#32553;&#31283;&#20581;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Compression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14205
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Patched Spectrogram Transformer&#23454;&#29616;&#30340;PS3DT&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#22120;&#22312;ASVspoof2019&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#29983;&#25104;&#24037;&#20855;&#29616;&#24050;&#21487;&#20197;&#36731;&#26494;&#33719;&#24471;&#12290;&#21512;&#25104;&#35821;&#38899;&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#37329;&#34701;&#27450;&#35784;&#12289;&#20882;&#20805;&#20182;&#20154;&#20197;&#21450;&#35823;&#23548;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21487;&#20197;&#26816;&#27979;&#21512;&#25104;&#35821;&#38899;&#30340;&#21462;&#35777;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#22330;&#26223;&#65288;&#20363;&#22914;&#26816;&#27979;&#22312;&#31038;&#20132;&#24179;&#21488;&#19978;&#20849;&#20139;&#30340;&#21512;&#25104;&#35821;&#38899;&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT)&#30340;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#22120;&#65292;&#23427;&#23558;&#26102;&#22495;&#35821;&#38899;&#20449;&#21495;&#36716;&#25442;&#20026;&#26757;&#23572;&#39057;&#35889;&#22270;&#65292;&#24182;&#20351;&#29992;&#21464;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;ASVspoof2019&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;PS3DT&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#20351;&#29992;&#39057;&#35889;&#22270;&#36827;&#34892;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PS3DT&#22312;ASVspoof2019&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#20854;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14205v1 Announce Type: cross  Abstract: Many deep learning synthetic speech generation tools are readily available. The use of synthetic speech has caused financial fraud, impersonation of people, and misinformation to spread. For this reason forensic methods that can detect synthetic speech have been proposed. Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic speech shared on social platforms. In this paper we propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a synthetic speech detector that converts a time domain speech signal to a mel-spectrogram and processes it in patches using a transformer neural network. We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic speech detection. We also investigate generaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.14202</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#27604;&#36739;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Comparing Graph Transformers via Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#25442;&#22120;&#30340;&#21306;&#20998;&#33021;&#21147;&#19982;&#20301;&#32622;&#32534;&#30721;&#30340;&#36873;&#25321;&#32039;&#23494;&#30456;&#20851;&#65306;&#29992;&#20110;&#22686;&#24378;&#22522;&#26412;&#21464;&#25442;&#22120;&#19982;&#22270;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26377;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#20301;&#32622;&#32534;&#30721;&#65306;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#12290;APEs&#20026;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#21464;&#25442;&#22120;&#30340;&#36755;&#20837;&#12290;&#32780;RPEs&#21017;&#20026;&#27599;&#23545;&#33410;&#28857;&#65288;&#20363;&#22914;&#65292;&#22270;&#36317;&#31163;&#65289;&#20998;&#37197;&#19968;&#20010;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#22686;&#24378;&#27880;&#24847;&#21147;&#22359;&#12290;&#20808;&#39564;&#19978;&#65292;&#30446;&#21069;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#26356;&#26377;&#21033;&#20110;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#36825;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#20301;&#32622;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;APEs&#21644;RPEs&#30340;&#22270;&#21464;&#25442;&#22120;&#22312;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#20854;&#21306;&#20998;&#33021;&#21147;&#30340;&#21516;&#26102;&#20132;&#25442;APEs&#21644;RPEs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in
&lt;/p&gt;</description></item><item><title>BeTAIL&#32467;&#21512;&#20102;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#31574;&#30053;&#21644;&#22312;&#32447;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#65292;&#20197;&#23398;&#20064;&#20174;&#20154;&#31867;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#21040;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32416;&#27491;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14194</link><description>&lt;p&gt;
BeTAIL&#65306;&#20174;&#20154;&#31867;&#36187;&#36710;&#28216;&#25103;&#20013;&#23398;&#20064;&#30340;&#34892;&#20026;&#36716;&#25442;&#22120;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14194
&lt;/p&gt;
&lt;p&gt;
BeTAIL&#32467;&#21512;&#20102;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#31574;&#30053;&#21644;&#22312;&#32447;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#65292;&#20197;&#23398;&#20064;&#20174;&#20154;&#31867;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#21040;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32416;&#27491;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#22914;&#33258;&#20027;&#36187;&#36710;&#65292;&#34987;&#27169;&#20223;&#30340;&#31574;&#30053;&#24517;&#39035;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#20154;&#31867;&#20915;&#31574;&#24314;&#27169;&#12290;&#24207;&#21015;&#24314;&#27169;&#38750;&#24120;&#26377;&#25928;&#22320;&#25429;&#25417;&#36816;&#21160;&#24207;&#21015;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#20294;&#22312;&#36866;&#24212;&#26032;&#29615;&#22659;&#25110;&#20998;&#24067;&#36716;&#31227;&#26041;&#38754;&#21364;&#24456;&#38590;&#12290;&#30456;&#21453;&#65292;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25928;&#24212;&#65292;&#20294;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#22788;&#29702;&#22797;&#26434;&#36816;&#21160;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BeTAIL&#65306;&#34892;&#20026;&#36716;&#25442;&#22120;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65292;&#23427;&#23558;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#30340;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#31574;&#30053;&#19982;&#22312;&#32447;AIL&#30456;&#32467;&#21512;&#12290;BeTAIL&#23558;&#19968;&#20010;AIL&#21097;&#20313;&#31574;&#30053;&#28155;&#21152;&#21040;BeT&#31574;&#30053;&#20013;&#65292;&#20197;&#24314;&#27169;&#20154;&#31867;&#19987;&#23478;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#24182;&#32416;&#27491;&#20998;&#24067;&#22806;&#29366;&#24577;&#25110;&#29615;&#22659;&#20013;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14194v1 Announce Type: new  Abstract: Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environmen
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.14184</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#26679;&#24615;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14184
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26159;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#24320;&#28304;&#20013;&#23384;&#22312;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#65292;&#38598;&#25104;&#26377;&#21161;&#20110;&#25552;&#21319;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#38598;&#25104;&#20013;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#65292;&#23545;&#27599;&#20010;&#27169;&#22411;&#36171;&#20104;&#30456;&#21516;&#26435;&#37325;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19981;&#20165;&#21333;&#20010;&#27169;&#22411;&#34920;&#29616;&#30693;&#35782;&#65292;&#36824;&#20351;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;&#38598;&#25104;&#12290;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14184v1 Announce Type: cross  Abstract: Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#32447;&#24615;&#21464;&#25442;&#22120;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22122;&#38899;&#24178;&#25200;&#25968;&#25454;&#26102;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36229;&#36234;&#20102;&#35768;&#22810;&#21512;&#29702;&#30340;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.14180</link><description>&lt;p&gt;
&#32447;&#24615;&#21464;&#25442;&#22120;&#26159;&#22810;&#21151;&#33021;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Linear Transformers are Versatile In-Context Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14180
&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#21464;&#25442;&#22120;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22122;&#38899;&#24178;&#25200;&#25968;&#25454;&#26102;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36229;&#36234;&#20102;&#35768;&#22810;&#21512;&#29702;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21464;&#25442;&#22120;&#65292;&#29305;&#21035;&#26159;&#32447;&#24615;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#22312;&#21069;&#21521;&#25512;&#29702;&#27493;&#39588;&#20013;&#23545;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#38544;&#21547;&#22320;&#25191;&#34892;&#31867;&#20284;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#20219;&#20309;&#32447;&#24615;&#21464;&#25442;&#22120;&#37117;&#20445;&#25345;&#38544;&#24335;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#21487;&#35299;&#37322;&#20026;&#25191;&#34892;&#19968;&#31181;&#21464;&#24418;&#30340;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21464;&#25442;&#22120;&#22312;&#35757;&#32451;&#25968;&#25454;&#21463;&#21040;&#19981;&#21516;&#27700;&#24179;&#22122;&#38899;&#24178;&#25200;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#32447;&#24615;&#21464;&#25442;&#22120;&#21457;&#29616;&#20102;&#19968;&#31181;&#22797;&#26434;&#19988;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36229;&#36234;&#25110;&#19982;&#35768;&#22810;&#21512;&#29702;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#21305;&#25932;&#12290;&#25105;&#20204;&#21453;&#21521;&#24037;&#31243;&#20102;&#36825;&#20010;&#31639;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#21644;&#22122;&#38899;&#27700;&#24179;&#30340;&#33258;&#36866;&#24212;&#37325;&#32553;&#25918;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14180v1 Announce Type: new  Abstract: Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14169</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
A Temporal Bias Correction using a Machine Learning Attention model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#27169;&#22411;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#30456;&#27604;&#23384;&#22312;&#20559;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#24433;&#21709;&#30740;&#31350;&#20043;&#21069;&#36827;&#34892;&#26657;&#20934;&#12290;&#20351;&#26657;&#20934;&#25104;&#20026;&#21487;&#33021;&#30340;&#32479;&#35745;&#26041;&#27861;&#38598;&#21512;&#34987;&#31216;&#20026;&#20559;&#24046;&#26657;&#27491;&#65288;BC&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;BC&#26041;&#27861;&#22312;&#35843;&#25972;&#26102;&#38388;&#20559;&#24046;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65288;&#22914;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#21644;&#39057;&#29575;&#65289;&#26080;&#27861;&#20934;&#30830;&#26657;&#27491;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20123;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BC&#26041;&#27861;&#26469;&#26657;&#27491;&#26102;&#38388;&#20559;&#24046;&#12290;&#36825;&#24471;&#30410;&#20110;&#23558;BC&#37325;&#26032;&#26500;&#24819;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#23558;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27010;&#29575;&#20851;&#27880;&#27169;&#22411;&#35843;&#25972;&#21040;BC&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23612;&#26085;&#21033;&#20122;&#38463;&#24067;&#36158;&#30340;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#26696;&#20363;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
&lt;/p&gt;</description></item><item><title>T-Stitch&#25552;&#20986;&#20102;&#19968;&#31181;&#36712;&#36857;&#25340;&#25509;&#30340;&#37319;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#19981;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#36890;&#36807;&#22312;&#21021;&#22987;&#38454;&#27573;&#20351;&#29992;&#36739;&#23567;&#30340;DPM&#26469;&#29983;&#25104;&#20840;&#23616;&#32467;&#26500;&#65292;&#28982;&#21518;&#20999;&#25442;&#21040;&#36739;&#22823;&#30340;DPM&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.14167</link><description>&lt;p&gt;
T-Stitch&#65306;&#20351;&#29992;&#36712;&#36857;&#25340;&#25509;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14167
&lt;/p&gt;
&lt;p&gt;
T-Stitch&#25552;&#20986;&#20102;&#19968;&#31181;&#36712;&#36857;&#25340;&#25509;&#30340;&#37319;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#19981;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#36890;&#36807;&#22312;&#21021;&#22987;&#38454;&#27573;&#20351;&#29992;&#36739;&#23567;&#30340;DPM&#26469;&#29983;&#25104;&#20840;&#23616;&#32467;&#26500;&#65292;&#28982;&#21518;&#20999;&#25442;&#21040;&#36739;&#22823;&#30340;DPM&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#36827;&#34892;&#37319;&#26679;&#23545;&#20110;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#27493;&#39588;&#21644;&#22823;&#22411;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Trajectory Stitching T-Stitch&#30340;&#37319;&#26679;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;T-Stitch&#19981;&#21516;&#20110;&#20165;&#20165;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;DPM&#36827;&#34892;&#25972;&#20010;&#37319;&#26679;&#36712;&#36857;&#65292;&#32780;&#26159;&#39318;&#20808;&#21033;&#29992;&#36739;&#23567;&#30340;DPM&#20316;&#20026;&#21021;&#22987;&#27493;&#39588;&#20013;&#36739;&#20026;&#24265;&#20215;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#28982;&#21518;&#22312;&#21518;&#26399;&#20999;&#25442;&#21040;&#22823;&#22411;DPM&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#23398;&#20064;&#30456;&#20284;&#30340;&#32534;&#30721;&#65292;&#24182;&#19988;&#36739;&#23567;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#26089;&#26399;&#27493;&#39588;&#20013;&#29983;&#25104;&#33391;&#22909;&#30340;&#20840;&#23616;&#32467;&#26500;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;T-Stitch&#26080;&#38656;&#35757;&#32451;&#65292;&#36890;&#24120;&#36866;&#29992;&#20110;&#19981;&#21516;&#26550;&#26500;&#65292;&#24182;&#19988;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24555;&#36895;&#37319;&#26679;&#25216;&#26415;&#30456;&#36741;&#30456;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14167v1 Announce Type: cross  Abstract: Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. Our key insight is that different diffusion models learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps. Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.14160</link><description>&lt;p&gt;
&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;&#65306;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#21152;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#37325;&#22797;&#25277;&#26679;&#26368;&#22823;&#21270;&#26641;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21152;&#36895;LLM&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#21152;&#36895;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#33609;&#31295;&#20196;&#29260;&#24207;&#21015;&#65292;&#35813;&#24207;&#21015;&#36827;&#19968;&#27493;&#30001;&#30446;&#26631;LLM&#24182;&#34892;&#39564;&#35777;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#33609;&#31295;&#20196;&#29260;&#26641;&#25512;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#21333;&#24207;&#21015;&#25512;&#27979;&#35299;&#30721;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#26641;&#30340;&#27599;&#20010;&#32423;&#21035;&#29420;&#31435;&#29983;&#25104;&#20196;&#29260;&#65292;&#27809;&#26377;&#21033;&#29992;&#25972;&#20010;&#26641;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22266;&#23450;&#24207;&#21015;&#38271;&#24230;&#24050;&#32463;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20316;&#21697;&#22312;&#22266;&#23450;&#30446;&#26631;&#35745;&#31639;&#36164;&#28304;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#26159;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36882;&#24402;&#25512;&#27979;&#35299;&#30721;(RSD)&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#23427;&#23545;&#19981;&#37325;&#22797;&#25277;&#26679;&#30340;&#33609;&#31295;&#20196;&#29260;&#36827;&#34892;&#26368;&#22823;&#21270;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#23454;&#29616;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14160v1 Announce Type: cross  Abstract: Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree's entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to LLM for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the dive
&lt;/p&gt;</description></item><item><title>BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14151</link><description>&lt;p&gt;
BIRCO&#65306;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14151
&lt;/p&gt;
&lt;p&gt;
BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#22522;&#20934;(BIRCO)&#12290; BIRCO&#35780;&#20272;IR&#31995;&#32479;&#26681;&#25454;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#26816;&#32034;&#25991;&#26723;&#30340;&#33021;&#21147;&#12290; &#35813;&#22522;&#20934;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#22823;&#23567;&#20351;&#20854;&#36866;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;LLM&#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26356;&#22797;&#26434;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290; &#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#20219;&#21153;&#19978;&#22343;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#21644;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14148</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19982;&#25705;&#25830;&#65306;&#28369;&#21160;&#12289;&#20445;&#25345;&#12289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Networks and Friction: Slide, Hold, Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14148
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#26550;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20855;&#26377;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#23450;&#24459;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;&#29992;&#20110;&#35757;&#32451;&#32593;&#32476;&#30340;&#25968;&#25454;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#26041;&#31243;&#32467;&#21512;&#29366;&#24577;&#28436;&#21270;&#32769;&#21270;&#23450;&#24459;&#29983;&#25104;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#21046;&#23450;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26126;&#30830;&#32771;&#34385;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#26465;&#20214;&#12289;&#30452;&#25509;&#25928;&#24212;&#20197;&#21450;&#29366;&#24577;&#21464;&#37327;&#30340;&#28436;&#21464;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;GRU&#26550;&#26500;&#30340;RNN&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#39044;&#27979;&#25705;&#25830;&#31995;&#25968;&#30001;&#20110;&#36895;&#24230;&#36339;&#36291;&#32780;&#20135;&#29983;&#30340;&#21464;&#21270;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14148v1 Announce Type: cross  Abstract: In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#27599;&#20010;&#20010;&#20307;&#37096;&#20998;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#22312;&#27979;&#35797;&#39118;&#38505;&#19978;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.14145</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#39046;&#22495;&#30340;&#26412;&#22320;&#20998;&#24067;&#20559;&#31227;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14145
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#27599;&#20010;&#20010;&#20307;&#37096;&#20998;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#22312;&#27979;&#35797;&#39118;&#38505;&#19978;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#32473;&#22312;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#21478;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#25968;&#25454;&#20998;&#24067;&#38543;&#25972;&#20010;&#24635;&#20307;&#30340;&#22810;&#20010;&#37096;&#20998;&#21464;&#21270;&#30340;&#24773;&#24418;&#65292;&#24182;&#20165;&#22312;&#27599;&#20010;&#37096;&#20998;&#20869;&#23545;&#35757;&#32451;&#19982;&#27979;&#35797;&#65288;&#37096;&#32626;&#65289;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#36827;&#34892;&#23616;&#37096;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#20056;&#24130;&#31283;&#20581;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#20013;&#27599;&#20010;&#20010;&#20307;&#37096;&#20998;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#25311;&#21512;&#22522;&#20110;&#20174;&#22810;&#20010;&#37096;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#27169;&#22411;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#28982;&#21518;&#23545;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;&#32454;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#19982;&#24120;&#29992;&#30340;&#29616;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#36215;&#23454;&#26045;&#12290;&#25105;&#20204;&#22312;&#27979;&#35797;&#39118;&#38505;&#19978;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#27867;&#21270;&#30028;&#38480;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14145v1 Announce Type: cross  Abstract: Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments
&lt;/p&gt;</description></item><item><title>NeuroFlux&#26159;&#19968;&#20010;&#20026;&#20869;&#23384;&#21463;&#38480;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#35757;&#32451;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#36741;&#21161;&#32593;&#32476;&#21644;&#22359;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#21019;&#26032;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2402.14139</link><description>&lt;p&gt;
NeuroFlux: &#20351;&#29992;&#33258;&#36866;&#24212;&#23616;&#37096;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14139
&lt;/p&gt;
&lt;p&gt;
NeuroFlux&#26159;&#19968;&#20010;&#20026;&#20869;&#23384;&#21463;&#38480;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#35757;&#32451;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#36741;&#21161;&#32593;&#32476;&#21644;&#22359;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#21019;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#21644;&#36793;&#32536;&#29615;&#22659;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#35774;&#22791;&#20869;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroFlux&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#20869;&#23384;&#21463;&#38480;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#35757;&#32451;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#26426;&#36935;&#65306;&#31532;&#19968;&#26159;&#37319;&#29992;&#21487;&#21464;&#25968;&#37327;&#28388;&#27874;&#22120;&#30340;&#33258;&#36866;&#24212;&#36741;&#21161;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;GPU&#20869;&#23384;&#30340;&#20351;&#29992;&#65307;&#31532;&#20108;&#26159;&#38024;&#23545;&#22359;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#26082;&#28385;&#36275;GPU&#20869;&#23384;&#38480;&#21046;&#65292;&#21448;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14139v1 Announce Type: new  Abstract: Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the CNNs into
&lt;/p&gt;</description></item><item><title>GDTM&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23460;&#20869;&#22320;&#29702;&#31354;&#38388;&#36319;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#20998;&#24067;&#24335;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#21644;&#21487;&#37325;&#26032;&#37197;&#32622;&#20256;&#24863;&#22120;&#33410;&#28857;&#20301;&#32622;&#65292;&#21487;&#20197;&#29992;&#20110;&#30740;&#31350;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20307;&#31995;&#32467;&#26500;&#20248;&#21270;&#21644;&#27169;&#22411;&#23545;&#19981;&#33391;&#20256;&#24863;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#20301;&#32622;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14136</link><description>&lt;p&gt;
GDTM&#65306;&#19968;&#20010;&#20855;&#26377;&#20998;&#24067;&#24335;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#30340;&#23460;&#20869;&#22320;&#29702;&#31354;&#38388;&#36319;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14136
&lt;/p&gt;
&lt;p&gt;
GDTM&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23460;&#20869;&#22320;&#29702;&#31354;&#38388;&#36319;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#20998;&#24067;&#24335;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#21644;&#21487;&#37325;&#26032;&#37197;&#32622;&#20256;&#24863;&#22120;&#33410;&#28857;&#20301;&#32622;&#65292;&#21487;&#20197;&#29992;&#20110;&#30740;&#31350;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20307;&#31995;&#32467;&#26500;&#20248;&#21270;&#21644;&#27169;&#22411;&#23545;&#19981;&#33391;&#20256;&#24863;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#20301;&#32622;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#23450;&#20301;&#31227;&#21160;&#29289;&#20307;&#65292;&#21363;&#22320;&#29702;&#31354;&#38388;&#36319;&#36394;&#65292;&#23545;&#20110;&#33258;&#20027;&#24314;&#31569;&#22522;&#30784;&#35774;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#22320;&#29702;&#31354;&#38388;&#36319;&#36394;&#36890;&#24120;&#21033;&#29992;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#34701;&#21512;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#20855;&#26377;&#26469;&#33258;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#26102;&#38388;&#23545;&#40784;&#12289;&#21516;&#27493;&#25968;&#25454;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#24182;&#19981;readily&#21487;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GDTM&#65292;&#19968;&#20010;&#20855;&#26377;&#20998;&#24067;&#24335;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#21644;&#21487;&#37325;&#26032;&#37197;&#32622;&#20256;&#24863;&#22120;&#33410;&#28857;&#20301;&#32622;&#30340;&#20061;&#23567;&#26102;&#22810;&#27169;&#24577;&#29289;&#20307;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20351;&#24471;&#33021;&#22815;&#25506;&#32034;&#20960;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#20363;&#22914;&#20248;&#21270;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#21450;&#30740;&#31350;&#27169;&#22411;&#23545;&#24694;&#21155;&#20256;&#24863;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#25918;&#32622;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20195;&#30721;&#12289;&#31034;&#20363;&#25968;&#25454;&#21644;&#26816;&#26597;&#28857;&#21487;&#22312;https://github.com/nesl/GDTM &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14136v1 Announce Type: cross  Abstract: Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#24369;&#20449;&#21495;&#30340;&#26816;&#27979;&#31934;&#24230;&#65292;&#24182;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30452;&#25509;&#33719;&#21462;&#20301;&#32622;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.14131</link><description>&lt;p&gt;
&#29992;&#20110;&#26816;&#27979;&#24369;&#20449;&#21495;&#21644;&#25552;&#21462;&#29289;&#29702;&#20449;&#24687;&#30340;&#38543;&#26426;&#26862;&#26519;&#65306;&#30913;&#23548;&#33322;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Random forests for detecting weak signals and extracting physical information: a case study of magnetic navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14131
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#24369;&#20449;&#21495;&#30340;&#26816;&#27979;&#31934;&#24230;&#65292;&#24182;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30452;&#25509;&#33719;&#21462;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;&#24211;&#21162;&#26426;&#22120;&#21644;&#26102;&#28382;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#25506;&#27979;&#22320;&#29699;&#30340;&#24322;&#24120;&#30913;&#22330;&#65292;&#35813;&#30913;&#22330;&#28153;&#27809;&#22312;&#22797;&#26434;&#30340;&#20449;&#21495;&#20013;&#65292;&#29992;&#20110;&#22312;&#26080;GPS&#29615;&#22659;&#20013;&#36827;&#34892;&#30913;&#23548;&#33322;&#12290;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#22330;&#30340;&#31934;&#24230;&#23545;&#24212;&#20110;&#22312;10&#21040;40&#31859;&#33539;&#22260;&#20869;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#26816;&#27979;&#24369;&#20449;&#21495;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#30452;&#25509;&#33719;&#21462;&#20301;&#32622;&#20449;&#24687;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#22810;&#20010;&#20915;&#31574;&#26641;&#30340;&#36755;&#20986;&#32467;&#21512;&#36215;&#26469;&#65292;&#32473;&#20986;&#24863;&#20852;&#36259;&#29289;&#29702;&#37327;&#30340;&#26368;&#20339;&#20540;&#12290;&#29305;&#21035;&#26159;&#65292;&#20174;&#39134;&#26426;&#39550;&#39542;&#33329;&#22312;&#21508;&#31181;&#26426;&#21160;&#38454;&#27573;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#65292;&#30001;&#20110;&#22320;&#29699;&#30913;&#22330;&#30340;&#20854;&#20182;&#20803;&#32032;&#24341;&#36215;&#30340;&#24378;&#32972;&#26223;&#22797;&#26434;&#20449;&#21495;&#26159;&#36896;&#25104;&#22797;&#26434;&#20449;&#21495;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#23558;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#26469;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14131v1 Announce Type: cross  Abstract: It was recently demonstrated that two machine-learning architectures, reservoir computing and time-delayed feed-forward neural networks, can be exploited for detecting the Earth's anomaly magnetic field immersed in overwhelming complex signals for magnetic navigation in a GPS-denied environment. The accuracy of the detected anomaly field corresponds to a positioning accuracy in the range of 10 to 40 meters. To increase the accuracy and reduce the uncertainty of weak signal detection as well as to directly obtain the position information, we exploit the machine-learning model of random forests that combines the output of multiple decision trees to give optimal values of the physical quantities of interest. In particular, from time-series data gathered from the cockpit of a flying airplane during various maneuvering stages, where strong background complex signals are caused by other elements of the Earth's magnetic field and the fields p
&lt;/p&gt;</description></item><item><title>DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;</title><link>https://arxiv.org/abs/2402.14123</link><description>&lt;p&gt;
DeiSAM&#65306;&#36890;&#36807;&#25351;&#31034;&#25552;&#31034;&#20998;&#21106;&#20219;&#20309;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
DeiSAM: Segment Anything with Deictic Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14123
&lt;/p&gt;
&lt;p&gt;
DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#38646;-shot&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#35782;&#21035;&#20855;&#20307;&#23545;&#35937;&#65292;&#20154;&#31867;&#26412;&#33021;&#22320;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#31034;&#24615;&#25551;&#36848;&#65292;&#21363;&#26681;&#25454;&#19978;&#19979;&#25991;&#25351;&#31216;&#26576;&#29289;&#65292;&#27604;&#22914;&#8220;&#22312;&#26700;&#23376;&#19978;&#24182;&#22312;&#26479;&#23376;&#21518;&#38754;&#30340;&#29289;&#20307;&#8221;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#32570;&#20047;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35299;&#37322;&#36825;&#31181;&#25351;&#31034;&#24615;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeiSAM&#8212;&#8212;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#12290;&#32473;&#23450;&#22797;&#26434;&#30340;&#25991;&#26412;&#20998;&#21106;&#25551;&#36848;&#65292;DeiSAM&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#22330;&#26223;&#22270;&#36827;&#34892;&#21487;&#21306;&#20998;&#30340;&#21069;&#21521;&#25512;&#29702;&#12290;&#38543;&#21518;&#65292;DeiSAM&#36890;&#36807;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14123v1 Announce Type: cross  Abstract: Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as "The object that is on the desk and behind the cup.". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by match
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#38382;&#39064;&#65292;&#20026;&#20102;&#39640;&#25928;&#22320;&#25214;&#21040;&#21487;&#20197;&#22312;&#26679;&#26412;&#19978;&#23454;&#29616;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#65292;&#38656;&#35201;&#33267;&#23569; $\Omega(k \log (d/k))$ &#20010;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.14103</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#19981;&#24403;&#23398;&#20064;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#38382;&#39064;&#65292;&#20026;&#20102;&#39640;&#25928;&#22320;&#25214;&#21040;&#21487;&#20197;&#22312;&#26679;&#26412;&#19978;&#23454;&#29616;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#65292;&#38656;&#35201;&#33267;&#23569; $\Omega(k \log (d/k))$ &#20010;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#19981;&#24403;&#23398;&#20064;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#26469;&#33258;&#32500;&#24230;&#20026; $d$ &#30340; $k$-&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#30340; $n$ &#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35810;&#38382;&#20102;&#22312;&#26102;&#38388;&#22810;&#39033;&#24335;&#20013;&#30340;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#23545;&#36825; $n$ &#20010;&#26679;&#26412;&#36798;&#21040;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#12290;&#20449;&#24687;&#29702;&#35770;&#19978;&#65292;&#36825;&#21487;&#20197;&#29992; $\Theta(k \log (d/k))$ &#20010;&#26679;&#26412;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#24456;&#26174;&#33879;&#65292;&#20294;&#27809;&#26377;&#24050;&#30693;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#38468;&#21152;&#23545;&#27169;&#22411;&#30340;&#20854;&#20182;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23569;&#20110; $\Theta(d)$ &#20010;&#26679;&#26412;&#36798;&#21040;&#30456;&#21516;&#30340;&#20445;&#35777;&#12290;&#31867;&#20284;&#22320;&#65292;&#29616;&#26377;&#30340;&#22256;&#38590;&#32467;&#26524;&#35201;&#20040;&#20165;&#38480;&#20110;&#36866;&#24403;&#35774;&#32622;&#65292;&#22312;&#35813;&#35774;&#32622;&#20013;&#20272;&#35745;&#20540;&#20063;&#24517;&#39035;&#26159;&#31232;&#30095;&#30340;&#65292;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14103v1 Announce Type: new  Abstract: We study computational-statistical gaps for improper learning in sparse linear regression. More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. Information-theoretically this can be achieved using $\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\Theta(d)$ samples without additional restrictions on the model. Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.   We give evidence that efficient algorithms for this task require at least (roughly) $\Omega(
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#20803;&#21160;&#24577;&#20146;&#21644;&#20851;&#31995;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#19981;&#21516;&#26102;&#38388;&#28857;&#31070;&#32463;&#20803;&#20043;&#38388;&#24418;&#25104;&#30340;&#31038;&#21306;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#32452;&#12290;</title><link>https://arxiv.org/abs/2402.14102</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#29983;&#29289;&#32593;&#32476;&#20013;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#32452;&#30340;&#21160;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning dynamic representations of the functional connectome in neurobiological networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#20803;&#21160;&#24577;&#20146;&#21644;&#20851;&#31995;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#19981;&#21516;&#26102;&#38388;&#28857;&#31070;&#32463;&#20803;&#20043;&#38388;&#24418;&#25104;&#30340;&#31038;&#21306;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22238;&#36335;&#30340;&#38745;&#24577;&#31361;&#35302;&#36830;&#25509;&#19982;&#20854;&#21151;&#33021;&#30340;&#21160;&#24577;&#24418;&#25104;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#19981;&#21516;&#20110;&#38745;&#24577;&#36830;&#25509;&#65292;&#19981;&#21516;&#31070;&#32463;&#20803;&#21487;&#20197;&#22312;&#19981;&#21516;&#26102;&#38388;&#31215;&#26497;&#21442;&#19982;&#21508;&#31181;&#32452;&#21512;&#65292;&#23454;&#29616;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#27963;&#29983;&#29983;&#21160;&#30340;&#21160;&#29289;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#21160;&#24577;&#20146;&#21644;&#21147;&#65292;&#24182;&#25581;&#31034;&#19981;&#21516;&#26102;&#38388;&#28857;&#31070;&#32463;&#20803;&#20043;&#38388;&#24418;&#25104;&#30340;&#31038;&#21306;&#12290;&#25512;&#26029;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;(NTF)&#32452;&#32455;&#26469;&#33258;&#22823;&#33041;&#20840;&#38754;&#38041;&#27963;&#21160;&#30340;&#31070;&#32463;&#20803;&#30165;&#36857;&#20043;&#38388;&#30340;&#25104;&#23545;&#38750;&#32447;&#24615;&#20146;&#21644;&#21147;&#12290;&#27599;&#20010;&#22240;&#23376;&#25351;&#23450;&#20102;&#21738;&#20123;&#31070;&#32463;&#20803;&#32676;&#20307;&#22312;&#29305;&#23450;&#26102;&#38388;&#38388;&#38548;&#21644;&#21160;&#29289;&#19978;&#26368;&#26377;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#23558;&#20801;&#35768;&#21152;&#26435;&#31038;&#21306;&#26816;&#27979;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;NTF&#20135;&#29983;&#30340;&#21151;&#33021;&#22522;&#24207;&#65292;&#20197;&#25581;&#31034;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14102v1 Announce Type: cross  Abstract: The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an unsupervised approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times. The inference occurs in two major steps. First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF). Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals. Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome. Since time 
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#30340;&#27969;&#24418;&#19981;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#19982;&#25968;&#25454;&#20998;&#24067;&#30456;&#24046;&#29978;&#36828;&#12290;</title><link>https://arxiv.org/abs/2402.14098</link><description>&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26377;&#36259;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Modern GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14098
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#30340;&#27969;&#24418;&#19981;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#19982;&#25968;&#25454;&#20998;&#24067;&#30456;&#24046;&#29978;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#21270;&#26679;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#36825;&#24341;&#21457;&#20102;&#35768;&#22810;&#20154;&#35748;&#20026;&#8220;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25429;&#25417;&#20102;&#35757;&#32451;&#25968;&#25454;&#27969;&#24418;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#35299;&#37322;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#30340;&#27969;&#24418;&#19981;&#36866;&#21512;&#35757;&#32451;&#20998;&#24067;&#65306;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#27969;&#24418;&#19981;&#32463;&#36807;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#26159;&#26356;&#25509;&#36817;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30001;&#28508;&#22312;&#32534;&#30721;&#19978;&#30340;&#20808;&#39564;&#38544;&#21547;&#30340;&#22270;&#20687;&#20998;&#24067;&#65292;&#24182;&#30740;&#31350;&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26159;&#21542;&#23398;&#20064;&#20102;&#19968;&#20010;&#36924;&#36817;&#35757;&#32451;&#20998;&#24067;&#30340;&#23494;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#19982;&#25968;&#25454;&#20998;&#24067;&#30456;&#24046;&#29978;&#36828;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#30340;&#23494;&#24230;&#20998;&#37197;&#32473;&#20998;&#24067;&#20043;&#22806;&#30340;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#29992;&#20110;&#35757;&#32451;&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#38598;&#36890;&#24120;&#19981;&#23646;&#20110;&#20856;&#22411;&#25551;&#36848;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14098v1 Announce Type: new  Abstract: Modern GANs achieve remarkable performance in terms of generating realistic and diverse samples. This has led many to believe that ``GANs capture the training data manifold''. In this work we show that this interpretation is wrong. We empirically show that the manifold learned by modern GANs does not fit the training distribution: specifically the manifold does not pass through the training examples and passes closer to out-of-distribution images than to in-distribution images. We also investigate the distribution over images implied by the prior over the latent codes and study whether modern GANs learn a density that approximates the training distribution. Surprisingly, we find that the learned density is very far from the data distribution and that GANs tend to assign higher density to out-of-distribution images. Finally, we demonstrate that the set of images used to train modern GANs are often not part of the typical set described by 
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14081</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36816;&#21160;&#20195;&#30721;&#30340;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14081
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20855;&#26377;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#38271;&#24230;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24773;&#20917;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#23454;&#20363;&#21487;&#20197;&#30475;&#20316;&#26159;&#22024;&#26434;&#21160;&#24577;&#27169;&#22411;&#30340;&#19968;&#20010;&#26679;&#26412;&#23454;&#29616;&#65292;&#20854;&#29305;&#28857;&#26159;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#25968;&#25454;&#26159;&#28151;&#21512;&#30340;&#65292;&#30001;&#22810;&#20010;&#38543;&#26426;&#36807;&#31243;&#24314;&#27169;&#30340;&#20960;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#32452;&#25104;&#65292;&#20351;&#24471;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#25968;&#25454;&#22238;&#24402;&#21040;&#27599;&#31181;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33258;&#21160;&#20998;&#37197;&#19968;&#20010;&#31216;&#20026;&#20854;&#36816;&#21160;&#20195;&#30721;&#30340;&#31614;&#21517;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#20998;&#37197;&#30340;&#36816;&#21160;&#20195;&#30721;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#30456;&#20851;&#24615;&#30340;&#31232;&#30095;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14081v1 Announce Type: cross  Abstract: While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#35745;&#31639;&#26679;&#26412;&#26041;&#24046;&#65292;&#25552;&#39640;&#20102;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20013;&#30340;&#35268;&#33539;&#21270;&#32622;&#20449;&#39044;&#27979;&#25928;&#29575;&#21644;&#35206;&#30422;&#29575;</title><link>https://arxiv.org/abs/2402.14080</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#35268;&#33539;&#21270;&#32622;&#20449;&#39044;&#27979;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65306;&#22522;&#20110;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#30340;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Normalized Conformal Prediction and Uncertainty Quantification for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14080
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#35745;&#31639;&#26679;&#26412;&#26041;&#24046;&#65292;&#25552;&#39640;&#20102;&#25239;&#30284;&#33647;&#29289;&#25935;&#24863;&#24615;&#39044;&#27979;&#20013;&#30340;&#35268;&#33539;&#21270;&#32622;&#20449;&#39044;&#27979;&#25928;&#29575;&#21644;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27491;&#22312;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#20851;&#38190;&#20915;&#31574;&#20219;&#21153;&#65292;&#28982;&#32780;&#23427;&#20204;&#34987;&#35757;&#32451;&#20026;&#25552;&#20379;&#28857;&#39044;&#27979;&#32780;&#27809;&#26377;&#25552;&#20379;&#20449;&#24515;&#24230;&#12290;&#22914;&#26524;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32467;&#21512;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#32622;&#20449;&#39044;&#27979;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39044;&#27979;&#21306;&#38388;&#37197;&#23545;&#65292;&#20174;&#32780;&#21487;&#20197;&#30475;&#21040;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#29992;&#20110;&#32622;&#20449;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#26410;&#33021;&#25552;&#20379;&#23545;&#25152;&#26377;&#26679;&#26412;&#21516;&#26679;&#20934;&#30830;&#30340;&#24322;&#26041;&#24046;&#38388;&#38548;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#33719;&#24471;&#30340;&#26041;&#24046;&#26469;&#20272;&#35745;&#27599;&#20010;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#22238;&#24402;&#26862;&#26519;&#30340;&#26041;&#24046;&#22914;&#20309;&#25552;&#39640;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#20219;&#21153;&#19978;&#35268;&#33539;&#21270;&#35825;&#23548;&#32622;&#20449;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14080v1 Announce Type: cross  Abstract: Deep learning models are being adopted and applied on various critical decision-making tasks, yet they are trained to provide point predictions without providing degrees of confidence. The trustworthiness of deep learning models can be increased if paired with uncertainty estimations. Conformal Prediction has emerged as a promising method to pair machine learning models with prediction intervals, allowing for a view of the model's uncertainty. However, popular uncertainty estimation methods for conformal prediction fail to provide heteroskedastic intervals that are equally accurate for all samples. In this paper, we propose a method to estimate the uncertainty of each sample by calculating the variance obtained from a Deep Regression Forest. We show that the deep regression forest variance improves the efficiency and coverage of normalized inductive conformal prediction on a drug response prediction task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23631;&#24149;&#25130;&#22270;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;Patch-and-Text Prediction&#65288;PTP&#65289;&#30446;&#26631;&#26469;&#25913;&#21892;&#25991;&#26412;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14073</link><description>&lt;p&gt;
&#20174;&#23631;&#24149;&#25130;&#22270;&#20013;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Language Understanding from Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23631;&#24149;&#25130;&#22270;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;Patch-and-Text Prediction&#65288;PTP&#65289;&#30446;&#26631;&#26469;&#25913;&#21892;&#25991;&#26412;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#20852;&#30340;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#65288;LMs&#65289;&#21487;&#20197;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#22312;&#21333;&#20010;&#35270;&#35273;&#35270;&#22270;&#20869;&#65292;&#26377;&#26395;&#25299;&#23485;&#22270;&#34920;&#29702;&#35299;&#21644;UI&#23548;&#33322;&#31561;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#27169;&#22411;&#20026;&#23631;&#24149;&#25130;&#22270;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#23631;&#24149;&#25130;&#22270;LMs&#22312;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#26126;&#26174;&#33853;&#21518;&#20110;&#20165;&#25991;&#26412;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#27169;&#22411;&#36755;&#20837;&#26159;&#32431;&#25991;&#26412;&#28210;&#26579;&#30340;&#23631;&#24149;&#25130;&#22270;&#65292;&#24182;&#38598;&#20013;&#22312;&#25552;&#39640;&#23631;&#24149;&#25130;&#22270;LMs&#30340;&#25991;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-and-Text Prediction&#65288;PTP&#65289;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#36974;&#30422;&#21644;&#24674;&#22797;&#23631;&#24149;&#25130;&#22270;&#20013;&#30340;&#22270;&#20687;&#22359;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#65292;&#28041;&#21450;&#36974;&#30422;&#29575;&#12289;&#22359;&#22823;&#23567;&#20197;&#21450;&#29992;&#20110;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20165;&#37319;&#29992;&#35270;&#35273;&#36755;&#20837;&#65292;&#23601;&#22312;8&#20010;GLUE&#20013;&#30340;6&#20010;&#19978;&#23454;&#29616;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14073v1 Announce Type: new  Abstract: An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT on 6 out of 8 GLUE 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14049</link><description>&lt;p&gt;
&#29992;&#20110;&#26497;&#31471;&#25968;&#25454;&#32553;&#25918;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Models for Extreme Downscaling of Climate Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#25361;&#25112;&#38656;&#35201;&#20934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#22320;&#26144;&#23556;&#27668;&#20505;&#21644;&#22825;&#27668;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#21482;&#33021;&#20197;&#38750;&#24120;&#31895;&#31961;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25552;&#20379;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#26497;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#25152;&#33268;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#20854;&#21464;&#20307;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#21319;&#33258;&#28982;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#22312;&#25913;&#36827;&#31185;&#23398;&#25968;&#25454;&#38598;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#29992;&#20110;&#26497;&#31471;&#32553;&#25918;&#32593;&#26684;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14049v1 Announce Type: cross  Abstract: Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty
&lt;/p&gt;</description></item><item><title>PolyNet&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#35268;&#21017;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14048</link><description>&lt;p&gt;
PolyNet&#65306;&#23398;&#20064;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14048
&lt;/p&gt;
&lt;p&gt;
PolyNet&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#35268;&#21017;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#36805;&#36895;&#25509;&#36817;&#20154;&#31867;&#35774;&#35745;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32553;&#23567;&#24046;&#36317;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#24517;&#39035;&#39640;&#25928;&#22320;&#25506;&#32034;&#35299;&#31354;&#38388;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#22810;&#26679;&#21270;&#35299;&#29983;&#25104;&#26469;&#20154;&#20026;&#22686;&#21152;&#25506;&#32034;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#25439;&#23475;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#24182;&#19988;&#38590;&#20197;&#20026;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#35774;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PolyNet&#65292;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#20316;&#21697;&#19981;&#21516;&#65292;PolyNet&#20165;&#20351;&#29992;&#21333;&#20010;&#35299;&#30721;&#22120;&#65292;&#24182;&#19988;&#35757;&#32451;&#22270;&#24335;&#19981;&#36890;&#36807;&#20154;&#20026;&#35268;&#21017;&#24378;&#21046;&#23454;&#26045;&#22810;&#26679;&#21270;&#35299;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#35780;&#20272;PolyNet&#65292;&#24182;&#35266;&#23519;&#21040;&#38544;&#24335;&#22810;&#26679;&#24615;&#26426;&#21046;&#20801;&#35768;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14048v1 Announce Type: cross  Abstract: Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems. In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows P
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14047</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple and Effective Transfer Learning for Neuro-Symbolic Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27867;&#21270;&#21644;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#24863;&#30693;&#26144;&#23556;&#21040;&#31526;&#21495;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#32773;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14047v1 Announce Type: cross  Abstract: Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute reasoning tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic reasoning. Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via transfer learning, where the weights of the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14045</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#25512;&#36827;&#20302;&#31209;&#21644;&#23616;&#37096;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#30340;&#22823;&#23481;&#37327;&#21644;&#22797;&#26434;&#24615;&#26159;&#23384;&#20648;&#12289;&#20256;&#36755;&#21644;&#22788;&#29702;&#30340;&#29942;&#39048;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21450;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#24212;&#29992;&#24050;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#23637;&#31034;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;LRMA&#21644;LLRMA&#30340;&#20316;&#21697;&#12290;&#25991;&#29486;&#30340;&#35814;&#32454;&#20998;&#26512;&#30830;&#35748;&#20102;&#24212;&#29992;&#20110;&#21508;&#31181;&#25104;&#20687;&#27169;&#24577;&#30340;LRMA&#21644;LLRMA&#26041;&#27861;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;LRMA&#21644;LLRMA&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#33258;2015&#24180;&#20197;&#26469;&#65292;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26126;&#26174;&#20559;&#21521;&#20110;LLRMA&#65292;&#26174;&#31034;&#20102;&#30456;&#23545;&#20110;LRMA&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;LLRMA&#25152;&#20351;&#29992;&#30340;&#27973;&#23618;&#30456;&#20284;&#24615;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20808;&#36827;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#26469;&#22788;&#29702;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14045v1 Announce Type: cross  Abstract: The large volume and complexity of medical imaging datasets are bottlenecks for storage, transmission, and processing. To tackle these challenges, the application of low-rank matrix approximation (LRMA) and its derivative, local LRMA (LLRMA) has demonstrated potential.   This paper conducts a systematic literature review to showcase works applying LRMA and LLRMA in medical imaging. A detailed analysis of the literature identifies LRMA and LLRMA methods applied to various imaging modalities. This paper addresses the challenges and limitations associated with existing LRMA and LLRMA methods.   We note a significant shift towards a preference for LLRMA in the medical imaging field since 2015, demonstrating its potential and effectiveness in capturing complex structures in medical data compared to LRMA. Acknowledging the limitations of shallow similarity methods used with LLRMA, we suggest advanced semantic image segmentation for similarit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14042</link><description>&lt;p&gt;
&#20351;&#29992;GANs&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24310;&#20280;&#19982;&#20445;&#25252;&#8212;&#8212;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;: &#20445;&#25252;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#23545;&#20110;&#39640;&#36136;&#37327;&#20307;&#39564;(QoE)&#21644;&#21487;&#25509;&#21463;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#29702;&#25935;&#24863;&#25968;&#25454;&#30340;&#26381;&#21153;&#65292;&#22914;&#22522;&#20110;IT&#30340;&#20581;&#24247;&#26381;&#21153;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#21311;&#21517;&#21270;&#25216;&#26415;&#23481;&#26131;&#34987;&#25968;&#25454;&#37325;&#26032;&#35782;&#21035;&#65292;&#20294;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36880;&#28176;&#21462;&#20195;&#20102;&#21311;&#21517;&#21270;&#65292;&#22240;&#20026;&#23427;&#30456;&#23545;&#32791;&#26102;&#21644;&#36164;&#28304;&#32791;&#36153;&#36739;&#23569;&#65292;&#24182;&#19988;&#26356;&#33021;&#25269;&#25239;&#25968;&#25454;&#27844;&#28431;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#36981;&#24490;&#24046;&#20998;&#38544;&#31169;&#29616;&#35937;&#30340;GAN&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#26368;&#26032;GAN&#22522;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20998;&#21457;&#12290; &#39044;&#27979;&#24314;&#27169;&#12289;&#33258;&#30456;&#20851;&#24615;&#21644;&#20998;&#24067;&#20998;&#26512;&#34987;&#29992;&#26469;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#36136;&#37327;(QoG)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 Announce Type: cross  Abstract: Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. T
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19987;&#19994;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26816;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#19987;&#19994;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#27491;&#30830;&#30340;&#21307;&#29983;&#65292;&#37325;&#28857;&#26159;&#22788;&#29702;&#38463;&#25289;&#20271;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#31867;&#21035;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.14039</link><description>&lt;p&gt;
&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#22810;&#31867;&#20998;&#24067;&#24773;&#22659;&#19979;&#30340;&#36828;&#31243;&#21307;&#30103;&#19987;&#19994;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Specialty detection in the context of telemedicine in a highly imbalanced multi-class distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19987;&#19994;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26816;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#19987;&#19994;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#27491;&#30830;&#30340;&#21307;&#29983;&#65292;&#37325;&#28857;&#26159;&#22788;&#29702;&#38463;&#25289;&#20271;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#31867;&#21035;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Covid-19&#22823;&#27969;&#34892;&#23548;&#33268;&#20102;&#23545;&#36828;&#31243;&#21307;&#30103;&#26381;&#21153;&#30340;&#35748;&#35782;&#21644;&#38656;&#27714;&#22686;&#21152;&#65292;&#36827;&#32780;&#38656;&#35201;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#24182;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#20943;&#23569;&#36816;&#33829;&#36127;&#25285;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19987;&#19994;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26816;&#27979;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#19987;&#19994;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#27491;&#30830;&#30340;&#21307;&#29983;&#12290;&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#22788;&#29702;&#38463;&#25289;&#20271;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#31867;&#21035;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#19968;&#20123;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#19987;&#19994;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#20381;&#36182;&#20110;&#19987;&#19994;&#26816;&#27979;&#30340;&#38544;&#34255;&#19994;&#21153;&#39046;&#22495;&#65292;&#20363;&#22914;&#20026;&#19981;&#21516;&#19987;&#19994;&#23450;&#21046;&#21644;&#20010;&#24615;&#21270;&#21672;&#35810;&#27969;&#31243;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14039v1 Announce Type: cross  Abstract: The Covid-19 pandemic has led to an increase in the awareness of and demand for telemedicine services, resulting in a need for automating the process and relying on machine learning (ML) to reduce the operational load. This research proposes a specialty detection classifier based on a machine learning model to automate the process of detecting the correct specialty for each question and routing it to the correct doctor. The study focuses on handling multiclass and highly imbalanced datasets for Arabic medical questions, comparing some oversampling techniques, developing a Deep Neural Network (DNN) model for specialty detection, and exploring the hidden business areas that rely on specialty detection such as customizing and personalizing the consultation flow for different specialties. The proposed module is deployed in both synchronous and asynchronous medical consultations to provide more real-time classification, minimize the doctor 
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14035</link><description>&lt;p&gt;
&#22996;&#21592;&#20250;&#30340;&#26234;&#24935;&#65306;&#20174;&#22522;&#30784;&#27169;&#22411;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14035
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#20174;&#19994;&#32773;&#20204;&#19968;&#30452;&#22312;&#24320;&#21457;&#19987;&#38376;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;&#20026;&#20102;&#20139;&#21463;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#22909;&#22788;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#36335;&#24452;&#26159;&#23558;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#65292;&#21518;&#32773;&#36890;&#24120;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#26381;&#21153;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#25216;&#26415;&#21487;&#20197;&#22312;&#36825;&#37324;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#27169;&#22411;&#23398;&#20250;&#27169;&#20223;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#23481;&#37327;&#19978;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#27169;&#22411;&#29305;&#24449;&#19978;&#30340;&#36825;&#20123;&#24046;&#24322;&#23548;&#33268;&#20102;&#33976;&#39311;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21019;&#24314;&#19968;&#20010;&#25945;&#23398;&#22996;&#21592;&#20250;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;&#21644;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#34394;&#25311;&#37051;&#23621;&#65288;VN&#65289;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23454;&#20307;&#23884;&#20837;&#20013;&#30340;&#37051;&#23621;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#25972;&#21512;&#36828;&#36317;&#31163;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.14033</link><description>&lt;p&gt;
VN&#32593;&#32476;&#65306;&#21033;&#29992;&#34394;&#25311;&#37051;&#23621;&#23884;&#20837;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
VN Network: Embedding Newly Emerging Entities with Virtual Neighbors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#34394;&#25311;&#37051;&#23621;&#65288;VN&#65289;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23454;&#20307;&#23884;&#20837;&#20013;&#30340;&#37051;&#23621;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#25972;&#21512;&#36828;&#36317;&#31163;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#24341;&#36215;&#20102;&#36817;&#24180;&#26469;&#30340;&#22823;&#37327;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#25152;&#26377;&#27979;&#35797;&#23454;&#20307;&#22312;&#35757;&#32451;&#26399;&#38388;&#22343;&#21487;&#33719;&#24471;&#65292;&#36825;&#20351;&#24471;&#20026;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#37325;&#26032;&#35757;&#32451;&#23884;&#20837;&#21464;&#24471;&#32791;&#26102;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#26410;&#30693;&#23454;&#20307;&#30340;&#29616;&#26377;&#37051;&#23621;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#34394;&#25311;&#37051;&#23621;&#65288;VN&#65289;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#20943;&#23569;&#37051;&#23621;&#31232;&#30095;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#35268;&#21017;&#25512;&#26029;&#24471;&#20986;&#30340;&#34394;&#25311;&#37051;&#23621;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21463;&#35268;&#21017;&#38480;&#21046;&#30340;&#38382;&#39064;&#20026;&#36825;&#20123;&#37051;&#23621;&#20998;&#37197;&#36719;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#23427;&#20204;&#35270;&#20026;&#27627;&#19981;&#21547;&#31946;&#30340;&#30495;&#23454;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#36339;&#25110;&#20004;&#36339;&#37051;&#23621;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#24573;&#30053;&#21487;&#33021;&#26377;&#29992;&#30340;&#36828;&#36317;&#31163;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#36923;&#36753;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14033v1 Announce Type: cross  Abstract: Embedding entities and relations into continuous vector spaces has attracted a surge of interest in recent years. Most embedding methods assume that all test entities are available during training, which makes it time-consuming to retrain embeddings for newly emerging entities. To address this issue, recent works apply the graph neural network on the existing neighbors of the unseen entities. In this paper, we propose a novel framework, namely Virtual Neighbor (VN) network, to address three key challenges. Firstly, to reduce the neighbor sparsity problem, we introduce the concept of the virtual neighbors inferred by rules. And we assign soft labels to these neighbors by solving a rule-constrained problem, rather than simply regarding them as unquestionably true. Secondly, many existing methods only use one-hop or two-hop neighbors for aggregation and ignore the distant information that may be helpful. Instead, we identify both logic an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#24207;&#26041;&#24046;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#28155;&#21152;&#26041;&#24046;&#27491;&#21017;&#21270;&#39033;&#26469;&#20445;&#25345;&#28508;&#31354;&#38388;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21462;&#38750;&#32447;&#24615;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14031</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#24207;&#26041;&#24046;&#30340;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Autoencoder with Ordered Variance for Nonlinear Model Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#24207;&#26041;&#24046;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#28155;&#21152;&#26041;&#24046;&#27491;&#21017;&#21270;&#39033;&#26469;&#20445;&#25345;&#28508;&#31354;&#38388;&#30340;&#39034;&#24207;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21462;&#38750;&#32447;&#24615;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#26377;&#24207;&#26041;&#24046;&#65288;AEO&#65289;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#20854;&#20013;&#36890;&#36807;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#65292;&#28155;&#21152;&#26041;&#24046;&#27491;&#21017;&#21270;&#39033;&#20197;&#24378;&#21046;&#22312;&#28508;&#31354;&#38388;&#20013;&#20445;&#25345;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;ResNets&#23545;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20462;&#25913;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;ResNet AEO&#65288;RAEO&#65289;&#12290;&#35813;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;AEO&#21644;RAEO&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#25552;&#21462;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14031v1 Announce Type: cross  Abstract: This paper presents a novel autoencoder with ordered variance (AEO) in which the loss function is modified with a variance regularization term to enforce order in the latent space. Further, the autoencoder is modified using ResNets, which results in a ResNet AEO (RAEO). The paper also illustrates the effectiveness of AEO and RAEO in extracting nonlinear relationships among input variables in an unsupervised setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.14029</link><description>&lt;p&gt;
&#20923;&#32467;&#32593;&#32476;&#20013;&#30340;&#37096;&#20998;&#25628;&#32034;&#36275;&#20197;&#25214;&#21040;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;
&lt;/p&gt;
&lt;p&gt;
Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31264;&#23494;&#32593;&#32476;&#21253;&#21547;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23376;&#32593;&#32476;--&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLTs&#65289;&#12290;&#26368;&#36817;&#65292;Gadhikar&#31561;&#20154;&#65288;2023&#24180;&#65289;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;SLTs&#20063;&#21487;&#20197;&#22312;&#38543;&#26426;&#20462;&#21098;&#30340;&#28304;&#32593;&#32476;&#20013;&#25214;&#21040;&#65292;&#20174;&#32780;&#20943;&#23569;SLT&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29978;&#33267;&#27604;&#28304;&#32593;&#32476;&#26356;&#31232;&#30095;&#30340;SLTs&#30340;&#25628;&#32034;&#65292;&#23548;&#33268;&#30001;&#20110;&#24847;&#22806;&#30340;&#39640;&#31232;&#30095;&#24615;&#32780;&#20934;&#30830;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#30340;&#20219;&#24847;&#27604;&#29575;&#20943;&#23569;SLT&#25628;&#32034;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20923;&#32467;&#19968;&#37096;&#20998;&#21021;&#22987;&#26435;&#37325;&#30340;&#38543;&#26426;&#23376;&#38598;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#25628;&#32034;&#31354;&#38388;&#20043;&#22806;--&#21363;&#65292;&#36890;&#36807;&#27704;&#20037;&#20462;&#21098;&#23427;&#20204;&#25110;&#23558;&#23427;&#20204;&#38145;&#23450;&#20026;SLT&#30340;&#22266;&#23450;&#37096;&#20998;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;&#25105;&#20204;&#19982;&#38543;&#26426;&#20923;&#32467;&#21464;&#37327;&#30340;&#23376;&#38598;&#21644;&#36924;&#36817;&#65292;&#22312;&#36825;&#31181;&#20943;&#23569;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#65292;SLT&#30340;&#23384;&#22312;&#22312;&#29702;&#35770;&#19978;&#26159;&#24471;&#21040;&#20445;&#35777;&#30340;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#24490;&#29615;ANN&#34920;&#29616;&#26368;&#20339;&#65292;&#30452;&#26041;&#22270;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;ANNs&#12290;</title><link>https://arxiv.org/abs/2402.14027</link><description>&lt;p&gt;
&#23398;&#20064;&#22240;&#26524;&#20107;&#20214;&#32452;&#21512;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Learning causation event conjunction sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14027
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#24490;&#29615;ANN&#34920;&#29616;&#26368;&#20339;&#65292;&#30452;&#26041;&#22270;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;ANNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#22240;&#26524;&#20851;&#31995;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#25110;&#22810;&#20010;&#22240;&#26524;&#20107;&#20214;&#30340;&#32452;&#21512;&#65292;&#20197;&#20219;&#24847;&#39034;&#24207;&#21457;&#29983;&#65292;&#24182;&#21487;&#33021;&#26377;&#38750;&#22240;&#26524;&#20107;&#20214;&#20171;&#20837;&#65292;&#26368;&#32456;&#23548;&#33268;&#19968;&#20010;&#25928;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#24490;&#29615;&#21644;&#38750;&#24490;&#29615;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#65292;&#20197;&#21450;&#22522;&#20110;&#30452;&#26041;&#22270;&#30340;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#27880;&#24847;&#21147;&#24490;&#29615;ANN&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#30452;&#26041;&#22270;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;ANNs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14027v1 Announce Type: new  Abstract: This is an examination of some methods that learn causations in event sequences. A causation is defined as a conjunction of one or more cause events occurring in an arbitrary order, with possible intervening non-causal events, that lead to an effect. The methods include recurrent and non-recurrent artificial neural networks (ANNs), as well as a histogram-based algorithm. An attention recurrent ANN performed the best of the ANNs, while the histogram algorithm was significantly superior to all the ANNs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#39564;&#35777;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21475;&#20869;X&#20809;&#20013;&#26816;&#27979;&#29273;&#40831;&#24322;&#24120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24179;&#22343;&#25935;&#24863;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#32780;&#24179;&#22343;&#29305;&#24322;&#24615;&#30053;&#26377;&#19979;&#38477;</title><link>https://arxiv.org/abs/2402.14022</link><description>&lt;p&gt;
&#20351;&#29992;&#37197;&#23545;&#25968;&#25454;&#23545;&#21475;&#20869;X&#20809;&#20013;&#29273;&#40831;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#32479;&#35745;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical validation of a deep learning algorithm for dental anomaly detection in intraoral radiographs using paired data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#39564;&#35777;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21475;&#20869;X&#20809;&#20013;&#26816;&#27979;&#29273;&#40831;&#24322;&#24120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24179;&#22343;&#25935;&#24863;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#32780;&#24179;&#22343;&#29305;&#24322;&#24615;&#30053;&#26377;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#20020;&#24202;&#39564;&#35777;&#30740;&#31350;&#35774;&#32622;&#65292;&#38024;&#23545;&#21475;&#20869;X&#32447;&#22270;&#20687;&#20013;&#26816;&#27979;&#29273;&#40831;&#24322;&#24120;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30340;&#32479;&#35745;&#20998;&#26512;&#21644;&#32467;&#26524;&#65292;&#20855;&#20307;&#21253;&#25324;&#40843;&#40831;&#12289;&#26681;&#23574;&#30149;&#21464;&#12289;&#26681;&#31649;&#27835;&#30103;&#32570;&#38519;&#12289;&#20896;&#20462;&#22797;&#36793;&#32536;&#32570;&#38519;&#12289;&#29273;&#21608;&#39592;&#36136;&#27969;&#22833;&#21644;&#29273;&#30707;&#12290;&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29273;&#21307;&#30340;&#26816;&#27979;&#24615;&#33021;&#19982;&#36825;&#20123;&#29273;&#21307;&#22312;&#27809;&#26377;&#31639;&#27861;&#24110;&#21161;&#19979;&#35780;&#20272;&#22270;&#20687;&#26102;&#30340;&#20808;&#21069;&#34920;&#29616;&#12290;&#36890;&#36807;&#35745;&#31639;&#37197;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#36793;&#38469;&#21033;&#28070;&#21644;&#24615;&#33021;&#25439;&#22833;&#65292;&#21487;&#20197;&#37327;&#21270;&#20551;&#35774;&#30340;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#21464;&#21270;&#12290;&#20351;&#29992;McNemar&#26816;&#39564;&#21644;&#20108;&#39033;&#20551;&#35774;&#26816;&#39564;&#24191;&#27867;&#35777;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;&#24179;&#22343;&#25935;&#24863;&#24615;&#20174;60.7%&#22686;&#21152;&#21040;85.9%&#65292;&#32780;&#24179;&#22343;&#29305;&#24322;&#24615;&#30053;&#26377;&#38477;&#20302;&#65292;&#20174;$9
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14022v1 Announce Type: cross  Abstract: This article describes the clinical validation study setup, statistical analysis and results for a deep learning algorithm which detects dental anomalies in intraoral radiographic images, more specifically caries, apical lesions, root canal treatment defects, marginal defects at crown restorations, periodontal bone loss and calculus. The study compares the detection performance of dentists using the deep learning algorithm to the prior performance of these dentists evaluating the images without algorithmic assistance. Calculating the marginal profit and loss of performance from the annotated paired image data allows for a quantification of the hypothesized change in sensitivity and specificity. The statistical significance of these results is extensively proven using both McNemar's test and the binomial hypothesis test. The average sensitivity increases from $60.7\%$ to $85.9\%$, while the average specificity slightly decreases from $9
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;AdaGrad&#22312;&#38750;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#22122;&#22768;&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#27010;&#29575;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#19988;&#21487;&#20197;&#22312;&#22122;&#22768;&#21442;&#25968;&#36275;&#22815;&#23567;&#26102;&#21152;&#36895;&#33267;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13794</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;AdaGrad&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Convergence of AdaGrad with Relaxed Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13794
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;AdaGrad&#22312;&#38750;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#22122;&#22768;&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#27010;&#29575;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#65292;&#19988;&#21487;&#20197;&#22312;&#22122;&#22768;&#21442;&#25968;&#36275;&#22815;&#23567;&#26102;&#21152;&#36895;&#33267;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;AdaGrad&#22312;&#38750;&#20984;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;AdaGrad&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#20854;&#20013;&#22122;&#22768;&#30340;&#22823;&#23567;&#30001;&#20989;&#25968;&#20540;&#24046;&#21644;&#26799;&#24230;&#22823;&#23567;&#25511;&#21046;&#12290;&#36825;&#20010;&#27169;&#22411;&#28085;&#30422;&#20102;&#24191;&#27867;&#33539;&#22260;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#26377;&#30028;&#22122;&#22768;&#12289;&#27425;&#39640;&#26031;&#22122;&#22768;&#12289;&#20223;&#23556;&#26041;&#24046;&#22122;&#22768;&#21644;&#39044;&#26399;&#20809;&#28369;&#24230;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#35777;&#26126;&#26356;&#21152;&#29616;&#23454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#25910;&#25947;&#36895;&#24230;&#65292;&#26681;&#25454;&#36890;&#29992;&#22122;&#22768;&#65292;&#21487;&#20197;&#36798;&#21040;( \tilde{\mathcal{O}}(1/\sqrt{T}))&#12290;&#36825;&#20010;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#23545;&#38382;&#39064;&#21442;&#25968;&#30340;&#20102;&#35299;&#65292;&#24403;&#19982;&#20989;&#25968;&#20540;&#24046;&#21644;&#22122;&#22768;&#27700;&#24179;&#30456;&#20851;&#30340;&#21442;&#25968;&#36275;&#22815;&#23567;&#26102;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#21040;(\tilde{\mathcal{O}}(1/T))&#65292;&#20854;&#20013;(T)&#34920;&#31034;&#24635;&#36845;&#20195;&#27425;&#25968;&#12290;&#25910;&#25947;&#36895;&#24230;&#22240;&#27492;&#21305;&#37197;&#20102;&#19979;&#38480;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13794v1 Announce Type: cross  Abstract: In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\tilde{\mathcal{O}}(1/\sqrt{T})). This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\tilde{\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rat
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20004;&#31181;&#28145;&#24230;&#21367;&#31215;&#26550;&#26500;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#20013;&#21152;&#20837;&#23545;&#25239;&#26679;&#26412;&#21644;&#26102;&#38388;&#22686;&#24378;&#26679;&#26412;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13651</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#22810;&#26222;&#21202;&#38647;&#36798;&#20998;&#31867;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Deep Neural Networks for Micro-Doppler Radar Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13651
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20004;&#31181;&#28145;&#24230;&#21367;&#31215;&#26550;&#26500;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#20013;&#21152;&#20837;&#23545;&#25239;&#26679;&#26412;&#21644;&#26102;&#38388;&#22686;&#24378;&#26679;&#26412;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#20998;&#31867;&#22120;&#22312;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#30340;&#24040;&#22823;&#33021;&#21147;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#32780;&#26080;&#27861;&#24456;&#22909;&#27867;&#21270;&#30340;&#39118;&#38505;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#28145;&#24230;&#21367;&#31215;&#26550;&#26500;&#22312;&#30456;&#21516;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#24403;&#36981;&#24490;&#26631;&#20934;&#35757;&#32451;&#23454;&#36341;&#26102;&#65292;&#20004;&#20010;&#20998;&#31867;&#22120;&#37117;&#23637;&#29616;&#20986;&#23545;&#36755;&#20837;&#34920;&#31034;&#30340;&#24494;&#23567;&#26102;&#38388;&#20559;&#31227;&#30340;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#22686;&#24378;&#24102;&#26377;&#26368;&#23567;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#23567;&#30340;&#26102;&#38388;&#20559;&#31227;&#21644;&#23545;&#25239;&#26679;&#26412;&#37117;&#26159;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26080;&#27861;&#24456;&#22909;&#27867;&#21270;&#30340;&#29305;&#24449;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#22312;&#23545;&#25239;&#26679;&#26412;&#21644;&#26102;&#38388;&#22686;&#24378;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#24433;&#21709;&#65292;&#36827;&#32780;&#23548;&#33268;&#26356;&#22909;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#28436;&#31034;&#20102;&#25805;&#20316;&#22312;&#33410;&#22863;-&#36895;&#24230;&#22270;&#34920;&#31034;&#32780;&#19981;&#26159;&#22810;&#26222;&#21202;-&#26102;&#38388;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13651v1 Announce Type: cross  Abstract: With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.13528</link><description>&lt;p&gt;
&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#65306;&#20174;&#32467;&#26500;&#28798;&#38590;&#21709;&#24212;&#20013;&#25366;&#25496;&#26410;&#26469;&#22833;&#25928;&#25285;&#24551;
&lt;/p&gt;
&lt;p&gt;
Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#38598;&#20013;&#20110;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#19982;&#32467;&#26500;&#22833;&#36133;&#30456;&#20851;&#30340;&#35752;&#35770;&#65292;&#20197;&#25913;&#36827;&#28798;&#38590;&#21709;&#24212;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#20013;&#35752;&#35770;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#26159;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#12290;&#22914;&#26524;&#36825;&#20123;&#25285;&#24551;&#34987;&#20256;&#36798;&#32473;&#36866;&#24403;&#30340;&#26426;&#26500;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#8212;&#8212;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#32654;&#22269;&#20960;&#36215;&#26368;&#36817;&#30340;&#32467;&#26500;&#22833;&#25928;&#20107;&#20214;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20221;&#39318;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;Reddit&#21644;YouTube&#20013;&#25366;&#25496;&#30340;2,662&#20010;&#31038;&#20132;&#32593;&#32476;&#23454;&#20363;&#65292;&#29992;&#20110;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13528v1 Announce Type: cross  Abstract: Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377;RoPE&#30340;transformer&#25512;&#26029;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;&#26469;&#38477;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#65292;&#26368;&#22823;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.13388</link><description>&lt;p&gt;
Transformer &#25216;&#24039;&#65306;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Transformer tricks: Precomputing the first layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377;RoPE&#30340;transformer&#25512;&#26029;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;&#26469;&#38477;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#65292;&#26368;&#22823;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31616;&#30701;&#30340;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377; RoPE&#65288;&#22914; LLaMA&#12289;Mistral &#21644; PaLM&#65289;&#30340; transformer &#25512;&#26029;&#30340;&#25216;&#24039;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#31532;&#19968;&#20010; transformer &#23618;&#30340;&#22823;&#37096;&#20998;&#20869;&#23481;&#21487;&#20197;&#39044;&#20808;&#35745;&#31639;&#65292;&#20174;&#32780;&#23548;&#33268;&#31245;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#20302;&#30340;&#27599;&#20196;&#29260;&#25104;&#26412;&#12290;&#22240;&#20026;&#36825;&#31181;&#25216;&#24039;&#20165;&#20248;&#21270;&#20102;&#19968;&#23618;&#65292;&#30456;&#23545;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#21482;&#26377; 4 &#23618;&#30340;&#27169;&#22411;&#65288;&#22914; Whisper tiny&#65289;&#65292;&#26368;&#22823;&#33410;&#30465;&#20165;&#38480;&#20110; 25%&#65292;&#32780;&#23545;&#20110; 32 &#23618;&#27169;&#22411;&#65288;&#22914; Mistral-7B&#65289;&#65292;&#33410;&#30465;&#21017;&#26159; 3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13388v1 Announce Type: new  Abstract: This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.13352</link><description>&lt;p&gt;
KetGPT -- &#20351;&#29992;Transformer&#23545;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#65292;&#34920;&#31034;&#20026;&#37327;&#23376;&#30005;&#36335;&#65292;&#21487;&#29992;&#20316;&#35780;&#20272;&#37327;&#23376;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#65292;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#30005;&#36335;&#24182;&#19981;&#26159;&#20195;&#34920;&#24615;&#22522;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#37327;&#23376;&#31995;&#32479;&#21046;&#36896;&#30340;&#30495;&#23454;&#37327;&#23376;&#31639;&#27861;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#8220;&#26377;&#29992;&#8221;&#30340;&#37327;&#23376;&#22522;&#20934;&#26500;&#25104;&#20102;&#25512;&#21160;&#37327;&#23376;&#32534;&#35793;&#22120;&#21644;&#30828;&#20214;&#24320;&#21457;&#19982;&#27604;&#36739;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KetGPT&#65292;&#19968;&#31181;&#20197;OpenQASM&#35821;&#35328;&#29983;&#25104;&#21512;&#25104;&#30005;&#36335;&#30340;&#24037;&#20855;&#65292;&#20854;&#32467;&#26500;&#26159;&#22522;&#20110;&#25512;&#23548;&#33258;&#37327;&#23376;&#30005;&#36335;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
&lt;/p&gt;</description></item><item><title>LS&#20449;&#24687;&#20934;&#21017;&#26088;&#22312;&#22686;&#24378;WBIC&#21644;sBIC&#30340;&#21151;&#33021;&#65292;&#26377;&#25928;&#22788;&#29702;&#38750;&#27491;&#21017;&#24773;&#20917;&#65292;&#20855;&#26377;&#31283;&#23450;&#24615;&#65292;&#20026;&#22855;&#24322;&#24773;&#20917;&#19979;&#30340;&#20449;&#24687;&#20934;&#21017;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.12762</link><description>&lt;p&gt;
&#22312;&#22855;&#24322;&#24615;&#19979;&#30340;&#23398;&#20064;&#65306;&#25913;&#36827;WBIC&#21644;sBIC&#30340;&#20449;&#24687;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Learning under Singularity: An Information Criterion improving WBIC and sBIC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12762
&lt;/p&gt;
&lt;p&gt;
LS&#20449;&#24687;&#20934;&#21017;&#26088;&#22312;&#22686;&#24378;WBIC&#21644;sBIC&#30340;&#21151;&#33021;&#65292;&#26377;&#25928;&#22788;&#29702;&#38750;&#27491;&#21017;&#24773;&#20917;&#65292;&#20855;&#26377;&#31283;&#23450;&#24615;&#65292;&#20026;&#22855;&#24322;&#24773;&#20917;&#19979;&#30340;&#20449;&#24687;&#20934;&#21017;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#20934;&#21017;&#65288;IC&#65289;&#65292;&#31216;&#20026;&#22312;&#22855;&#24322;&#24615;&#19979;&#30340;&#23398;&#20064;&#65288;LS&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#24191;&#27867;&#36866;&#29992;&#30340;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65288;WBIC&#65289;&#21644;&#22855;&#24322;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65288;sBIC&#65289;&#30340;&#21151;&#33021;&#12290; LS&#22312;&#27809;&#26377;&#27491;&#21017;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#34920;&#29616;&#20986;&#31283;&#23450;&#24615;&#12290;Watanabe&#23450;&#20041;&#20102;&#19968;&#20010;&#32479;&#35745;&#27169;&#22411;&#25110;&#23398;&#20064;&#26426;&#22120;&#20026;&#27491;&#21017;&#65292;&#22914;&#26524;&#20174;&#21442;&#25968;&#21040;&#27010;&#29575;&#20998;&#24067;&#30340;&#26144;&#23556;&#26159;&#19968;&#23545;&#19968;&#30340;&#65292;&#24182;&#19988;&#20854;Fisher&#20449;&#24687;&#30697;&#38453;&#26159;&#27491;&#23450;&#30340;&#12290;&#30456;&#21453;&#65292;&#19981;&#31526;&#21512;&#36825;&#20123;&#26465;&#20214;&#30340;&#27169;&#22411;&#34987;&#31216;&#20026;&#22855;&#24322;&#12290; &#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22855;&#24322;&#24773;&#20917;&#19979;&#30340;&#20449;&#24687;&#20934;&#21017;&#65292;&#21253;&#25324;WBIC&#21644;sBIC&#12290; WBIC&#36866;&#29992;&#20110;&#38750;&#27491;&#21017;&#24773;&#20917;&#65292;&#20294;&#22312;&#26679;&#26412;&#37327;&#24456;&#22823;&#19988;&#24050;&#30693;&#23398;&#20064;&#31995;&#25968;&#20272;&#35745;&#20887;&#20313;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290; &#30456;&#21453;&#65292;sBIC&#22312;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12762v1 Announce Type: cross  Abstract: We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC). LS is effective without regularity constraints and demonstrates stability. Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite. In contrast, models not meeting these conditions are termed singular. Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients. Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11773</link><description>&lt;p&gt;
&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Dynamic Multi-Network Mining of Tensor Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#23376;&#24207;&#21015;&#32858;&#31867;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#35299;&#37322;&#32467;&#26524;&#32858;&#31867;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36890;&#24120;&#25105;&#20204;&#27809;&#26377;&#20851;&#20110;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#38754;&#23545;&#30001;&#21253;&#21547;&#26102;&#38388;&#25139;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#24335;&#32452;&#25104;&#30340;&#22823;&#37327;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#22914;&#20309;&#20026;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#23376;&#24207;&#21015;&#32858;&#31867;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35265;&#35299;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;&#65288;DMM&#65289;&#65292;&#23427;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#30001;l1&#33539;&#25968;&#32422;&#26463;&#30340;&#19968;&#32452;&#21508;&#31181;&#38271;&#24230;&#30340;&#27573;&#32452;&#65288;&#21363;&#32858;&#31867;&#65289;&#29305;&#24449;&#21270;&#30340;&#20381;&#36182;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#12290;(a) &#21487;&#35299;&#37322;&#24615;&#65306;&#23427;&#20351;&#29992;&#22810;&#20010;&#32593;&#32476;&#23545;&#32858;&#31867;&#36827;&#34892;&#29305;&#24449;&#25551;&#36848;&#65292;&#27599;&#20010;&#32593;&#32476;&#26159;&#30456;&#24212;&#38750;&#26102;&#38388;&#27169;&#24335;&#30340;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#20379;&#21487;&#35265;&#19988;&#21487;&#35299;&#37322;&#30340;&#20851;&#38190;&#20851;&#31995;&#35265;&#35299;&#12290; (b) &#31934;&#30830;&#24615;&#65306;&#23427;&#21457;&#29616;&#20102;&#32858;&#31867;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11773v1 Announce Type: cross  Abstract: Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clus
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.11525</link><description>&lt;p&gt;
&#29992;RLHF&#25512;&#36827;&#32763;&#35793;&#20559;&#22909;&#24314;&#27169;&#65306;&#36808;&#21521;&#25104;&#26412;&#25928;&#30410;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#30495;&#23454;&#24615;&#12289;&#34920;&#36798;&#21147;&#21644;&#20248;&#38597;&#26159;&#26426;&#22120;&#32763;&#35793;&#20013;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#24182;&#19981;&#20005;&#26684;&#31526;&#21512;&#20154;&#31867;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#32763;&#35793;&#20043;&#38388;&#30340;&#27604;&#36739;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26426;&#22120;&#32763;&#35793;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25351;&#23548;&#38543;&#21518;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLHF&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#31181;&#25913;&#36827;&#20063;&#26377;&#30410;&#20110;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>BlackJAX&#26159;&#19968;&#20010;&#23454;&#29616;&#22312;JAX&#20013;&#32452;&#21512;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#24211;&#65292;&#37319;&#29992;&#20989;&#25968;&#24335;&#26041;&#27861;&#25552;&#39640;&#26131;&#29992;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22359;&#21270;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#23574;&#31471;&#26041;&#27861;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#24819;&#35201;&#20102;&#35299;&#24037;&#20316;&#21407;&#29702;&#30340;&#20154;&#12290;</title><link>https://arxiv.org/abs/2402.10797</link><description>&lt;p&gt;
BlackJAX: JAX&#20013;&#30340;&#32452;&#21512;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
BlackJAX: Composable Bayesian inference in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10797
&lt;/p&gt;
&lt;p&gt;
BlackJAX&#26159;&#19968;&#20010;&#23454;&#29616;&#22312;JAX&#20013;&#32452;&#21512;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#24211;&#65292;&#37319;&#29992;&#20989;&#25968;&#24335;&#26041;&#27861;&#25552;&#39640;&#26131;&#29992;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22359;&#21270;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#23574;&#31471;&#26041;&#27861;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#24819;&#35201;&#20102;&#35299;&#24037;&#20316;&#21407;&#29702;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BlackJAX&#26159;&#19968;&#20010;&#24211;&#65292;&#23454;&#29616;&#20102;&#22312;&#36125;&#21494;&#26031;&#35745;&#31639;&#20013;&#24120;&#29992;&#30340;&#25277;&#26679;&#21644;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#37319;&#29992;&#20989;&#25968;&#24335;&#26041;&#27861;&#23454;&#29616;&#31639;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#26131;&#29992;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22359;&#21270;&#12290;BlackJAX&#20351;&#29992;Python&#32534;&#20889;&#65292;&#21033;&#29992;JAX&#22312;CPU&#12289;GPU&#21644;TPU&#19978;&#32534;&#35793;&#21644;&#36816;&#34892;&#31867;&#20284;Numpy&#30340;&#25277;&#26679;&#22120;&#21644;&#21464;&#20998;&#26041;&#27861;&#12290;&#35813;&#24211;&#36890;&#36807;&#30452;&#25509;&#22788;&#29702;&#65288;&#38750;&#27491;&#21017;&#21270;&#65289;&#30446;&#26631;&#23545;&#25968;&#23494;&#24230;&#20989;&#25968;&#65292;&#19982;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#24456;&#22909;&#22320;&#38598;&#25104;&#12290;BlackJAX&#26088;&#22312;&#25104;&#20026;&#22522;&#26412;&#32479;&#35745;&#8220;&#22522;&#20803;&#8221;&#30340;&#20302;&#32423;&#21487;&#32452;&#21512;&#23454;&#29616;&#30340;&#38598;&#21512;&#65292;&#21487;&#32452;&#21512;&#25191;&#34892;&#23450;&#20041;&#33391;&#22909;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#39640;&#32423;&#20363;&#31243;&#20197;&#25552;&#39640;&#26131;&#29992;&#24615;&#12290;&#23427;&#38754;&#21521;&#38656;&#35201;&#23574;&#31471;&#26041;&#27861;&#30340;&#29992;&#25143;&#12289;&#24076;&#26395;&#21019;&#24314;&#22797;&#26434;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20197;&#21450;&#24819;&#35201;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#24037;&#20316;&#21407;&#29702;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10797v1 Announce Type: cross  Abstract: BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation. It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use. It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>RAG-Fusion&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#24182;&#32467;&#21512;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#25216;&#26415;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#19978;&#19979;&#25991;&#21270;&#21407;&#22987;&#26597;&#35810;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#26377;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.03367</link><description>&lt;p&gt;
RAG-Fusion: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
RAG-Fusion: a New Take on Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03367
&lt;/p&gt;
&lt;p&gt;
RAG-Fusion&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#24182;&#32467;&#21512;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#25216;&#26415;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#19978;&#19979;&#25991;&#21270;&#21407;&#22987;&#26597;&#35810;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#26377;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Infineon&#24050;&#32463;&#30830;&#23450;&#24037;&#31243;&#24072;&#12289;&#23458;&#25143;&#32463;&#29702;&#21644;&#23458;&#25143;&#36805;&#36895;&#33719;&#21462;&#20135;&#21697;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#35299;&#20915;&#65292;&#20294;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#35780;&#20272;&#20102;&#26032;&#36817;&#27969;&#34892;&#30340;RAG-Fusion&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;RAG-Fusion&#23558;RAG&#21644;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#65288;RRF&#65289;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#20351;&#29992;&#20114;&#24800;&#20998;&#25968;&#23545;&#20854;&#36827;&#34892;&#20877;&#25490;&#24207;&#65292;&#24182;&#34701;&#21512;&#25991;&#26723;&#21644;&#20998;&#25968;&#12290;&#36890;&#36807;&#23545;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#20840;&#38754;&#24615;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#21457;&#29616;RAG-Fusion&#33021;&#22815;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#24403;&#29983;&#25104;&#30340;&#26597;&#35810;&#19982;&#21407;&#22987;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#19981;&#36275;&#26102;&#65292;&#26377;&#20123;&#31572;&#26696;&#20559;&#31163;&#20102;&#20027;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02439</link><description>&lt;p&gt;
DiffStitch: &#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02439
&lt;/p&gt;
&lt;p&gt;
DiffStitch&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#31163;&#32447;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#26368;&#20339;&#36712;&#36857;&#65292;&#36825;&#32473;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#24517;&#39035;&#33719;&#24471;&#21040;&#36798;&#39640;&#22870;&#21169;&#21306;&#22495;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36712;&#36857;&#25340;&#25509;&#65288;DiffStitch&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#23427;&#21487;&#20197;&#31995;&#32479;&#22320;&#29983;&#25104;&#36712;&#36857;&#20043;&#38388;&#30340;&#25340;&#25509;&#36716;&#25442;&#12290;DiffStitch&#21487;&#20197;&#26377;&#25928;&#22320;&#36830;&#25509;&#20302;&#22870;&#21169;&#36712;&#36857;&#21644;&#39640;&#22870;&#21169;&#36712;&#36857;&#65292;&#24418;&#25104;&#20840;&#23616;&#26368;&#20248;&#36712;&#36857;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;DiffStitch&#22312;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DiffStitch&#22312;&#19968;&#27493;&#26041;&#27861;&#65288;IQL&#65289;&#12289;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;TD3+BC&#65289;&#21644;&#36712;&#36857;&#26041;&#27861;&#65288;PPO&#65289;&#30340;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.01719</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#36947;&#24503;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Moral Inconsistencies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35821;&#20041;&#31561;&#20215;&#30340;&#25552;&#31034;&#20135;&#29983;&#35821;&#20041;&#31561;&#20215;&#30340;&#21709;&#24212;&#65292;&#37027;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;LLMs&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#26041;&#38754;&#20063;&#23384;&#22312;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#20934;&#30830;&#24230;&#26469;&#34913;&#37327;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#27809;&#26377;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#30340;&#36947;&#24503;&#24773;&#26223;&#65288;&#20363;&#22914;&#65292;&#36947;&#36335;&#20132;&#36816;&#38382;&#39064;&#65289;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#26469;&#34913;&#37327;LLM&#22312;&#36947;&#24503;&#24773;&#26223;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#19982;&#20154;&#31867;&#21028;&#26029;&#22312;&#20116;&#20010;LLMs&#19978;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#22312;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
&lt;/p&gt;</description></item><item><title>HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;</title><link>https://arxiv.org/abs/2402.01696</link><description>&lt;p&gt;
HiGen: &#23618;&#27425;&#24863;&#30693;&#30340;&#23618;&#32423;&#25991;&#26412;&#20998;&#31867;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01696
&lt;/p&gt;
&lt;p&gt;
HiGen&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#65292;&#22312;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#20013;&#32771;&#34385;&#20102;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;ENZYME&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#65288;HTC&#65289;&#26159;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#23376;&#20219;&#21153;&#65292;&#20854;&#29305;&#28857;&#26159;&#20855;&#26377;&#23618;&#32423;&#26631;&#31614;&#20998;&#31867;&#27861;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#21644;&#23618;&#32423;&#26631;&#31614;&#20449;&#24687;&#26469;&#23398;&#20064;&#38745;&#24577;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#21508;&#20010;&#37096;&#20998;&#30340;&#30456;&#20851;&#24615;&#21487;&#33021;&#22240;&#23618;&#32423;&#27700;&#24179;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#65292;&#38656;&#35201;&#21160;&#24577;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiGen&#65292;&#19968;&#20010;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21160;&#24577;&#25991;&#26412;&#34920;&#31034;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#32423;&#24341;&#23548;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#25991;&#26412;&#21644;&#26631;&#31614;&#21517;&#31216;&#35821;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#39046;&#22495;&#30693;&#35782;&#19978;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26679;&#26412;&#26377;&#38480;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21629;&#21517;&#20026;ENZYME&#30340;&#26032;&#39062;&#21644;&#26377;&#20215;&#20540;&#30340;&#29992;&#20110;HTC&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#26469;&#33258;PubMed&#30340;&#25991;&#31456;&#32452;&#25104;&#65292;&#26088;&#22312;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;Vision-Language&#27169;&#22411;&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#23558;&#20854;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20877;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25968;&#23398;&#20219;&#21153;&#21644;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.08025</link><description>&lt;p&gt;
&#33258;&#25105;&#24819;&#35937;&#65306;&#21033;&#29992;&#33258;&#25105;&#24819;&#35937;&#36827;&#34892;&#22810;&#27169;&#22411;&#33258;&#28982;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;Vision-Language&#27169;&#22411;&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#23558;&#20854;&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#20877;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25968;&#23398;&#20219;&#21153;&#21644;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-Language&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#28508;&#21147;&#22312;&#22788;&#29702;&#22797;&#26434;&#22522;&#20110;&#25991;&#26412;&#38382;&#39064;&#26102;&#24448;&#24448;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#23588;&#20854;&#26159;&#24403;&#36825;&#20123;&#38382;&#39064;&#33021;&#22815;&#20174;&#35270;&#35273;&#34920;&#36798;&#20013;&#33719;&#30410;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Self-Imagine&#65292;&#19982;&#20154;&#31867;&#36890;&#36807;&#21019;&#24314;&#38382;&#39064;&#30340;&#35270;&#35273;&#22270;&#24182;&#25512;&#26029;&#35299;&#20915;&#27493;&#39588;&#30340;&#33021;&#21147;&#30456; resonating&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#19968;&#30340;Vision-Language&#27169;&#22411;&#65288;VLM&#65289;&#20351;&#29992;HTML&#29983;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;HTML&#28210;&#26579;&#20026;&#22270;&#20687;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#30456;&#21516;&#30340;VLM&#26681;&#25454;&#38382;&#39064;&#21644;&#22270;&#20687;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#65288;LLAVA-1.5&#21644;GEMINI PRO&#65289;VLMs&#22312;&#19977;&#20010;&#25968;&#23398;&#20219;&#21153;&#21644;&#20061;&#20010;&#36890;&#29992;&#25512;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;LLAVA-1.5&#21644;GEMINI PRO&#22312;&#25152;&#26377;&#25968;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08025v2 Announce Type: replace  Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on aver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#65292;&#22522;&#20110;Doob's h-transform&#65292;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.09236</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#30340;&#24212;&#29992;&#20110;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for conditional diffusion modelling with applications in motif scaffolding for protein design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#65292;&#22522;&#20110;Doob's h-transform&#65292;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#34507;&#30333;&#35774;&#35745;&#24212;&#29992;&#65292;&#22914;&#32467;&#21512;&#29289;&#25110;&#37238;&#30340;&#35774;&#35745;&#65292;&#38656;&#35201;&#20197;&#39640;&#31934;&#24230;&#25645;&#24314;&#20855;&#26377;&#32467;&#26500;&#22522;&#24207;&#30340;&#34507;&#30333;&#36136;&#12290;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;&#30340;&#20027;&#35201;&#20505;&#36873;&#26041;&#26696;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#26089;&#26399;&#23454;&#39564;&#25104;&#21151;&#12290;&#22312;&#25193;&#25955;&#33539;&#24335;&#20013;&#65292;&#22522;&#24207;&#25903;&#26550;&#34987;&#35270;&#20026;&#19968;&#31181;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#26465;&#20214;&#29983;&#25104;&#21327;&#35758;&#25110;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#25991;&#29486;&#20013;&#23548;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21327;&#35758;&#22823;&#22810;&#22522;&#20110;&#21551;&#21457;&#24615;&#21160;&#26426;&#65292;&#20363;&#22914;&#36890;&#36807;&#23545;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30340;&#31867;&#27604;&#65292;&#24182;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#21464;&#24471;&#27169;&#31946;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20110;&#25968;&#23398;&#19978;&#29702;&#35299;&#33391;&#22909;&#30340;Doob's h-transform&#30340;&#20849;&#21516;&#26694;&#26550;&#19979;&#32479;&#19968;&#20102;&#26465;&#20214;&#35757;&#32451;&#21644;&#26465;&#20214;&#25277;&#26679;&#31243;&#24207;&#12290;&#36825;&#31181;&#26032;&#30340;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09236v2 Announce Type: replace  Abstract: Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between ex
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#28304;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;CLIP&#31995;&#21015;&#27169;&#22411;&#27966;&#29983;&#35270;&#35273;&#30446;&#26631;&#23454;&#29616;&#30340;&#22870;&#21169;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#33021;&#22815;&#23454;&#29616;&#22810;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;RL&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2312.09187</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models as a Source of Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09187
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#28304;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;CLIP&#31995;&#21015;&#27169;&#22411;&#27966;&#29983;&#35270;&#35273;&#30446;&#26631;&#23454;&#29616;&#30340;&#22870;&#21169;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#33021;&#22815;&#23454;&#29616;&#22810;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#21487;&#20197;&#22312;&#20016;&#23500;&#22810;&#26679;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#23454;&#29616;&#35768;&#22810;&#30446;&#26631;&#30340;&#36890;&#29992;&#20195;&#29702;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#21069;&#27839;&#20043;&#19968;&#12290;&#24314;&#31435;&#20855;&#26377;RL&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#20851;&#38190;&#38480;&#21046;&#22240;&#32032;&#20043;&#19968;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#23454;&#29616;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#28304;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;CLIP&#31995;&#21015;&#27169;&#22411;&#20013;&#27966;&#29983;&#35270;&#35273;&#23454;&#29616;&#21508;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;&#22870;&#21169;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;RL&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#35268;&#27169;&#21270;&#36235;&#21183;&#65292;&#26174;&#31034;&#26356;&#22823;&#30340;VLM&#20250;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#35270;&#35273;&#30446;&#26631;&#23454;&#29616;&#22870;&#21169;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26377;&#33021;&#21147;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09187v2 Announce Type: replace  Abstract: Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.
&lt;/p&gt;</description></item><item><title>MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.08656</link><description>&lt;p&gt;
MaxK-GNN: &#25506;&#32034;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#29702;&#35770;&#36895;&#24230;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08656
&lt;/p&gt;
&lt;p&gt;
MaxK-GNN&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;MaxK&#38750;&#32447;&#24615;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22402;&#30452;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21152;&#36895;&#26041;&#38754;&#65292;GPU&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#24179;&#21488;&#12290; GPU&#22312;GNN&#19978;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20316;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20869;&#23384;&#35775;&#38382;&#19981;&#35268;&#21017;&#65292;&#23548;&#33268;&#30828;&#20214;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20363;&#22914;PyG&#12289;DGL&#19982;cuSPARSE&#65292;&#20197;&#21450;GNNAdvisor&#26694;&#26550;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#20869;&#23384;&#27969;&#37327;&#20173;&#28982;&#24456;&#26174;&#33879;&#12290; &#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#36890;&#36807;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#22402;&#30452;&#20248;&#21270;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#26159;&#23558;&#21152;&#36895;&#20248;&#21270;&#35270;&#20026;&#8220;&#20107;&#21518;&#24605;&#32771;&#8221;&#65288;&#21363;&#65288;i&#65289;&#32473;&#23450;GNN&#31639;&#27861;&#65292;&#35774;&#35745;&#21152;&#36895;&#22120;&#65292;&#25110;&#65288;ii&#65289;&#32473;&#23450;&#30828;&#20214;&#65292;&#20027;&#35201;&#20248;&#21270;GNN&#31639;&#27861;&#65289;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MaxK-GNN&#65292;&#19968;&#31181;&#38598;&#25104;&#31639;&#27861;&#19982;&#31995;&#32479;&#21019;&#26032;&#30340;&#20808;&#36827;&#39640;&#24615;&#33021;GPU&#35757;&#32451;&#31995;&#32479;&#12290; &#65288;i&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;MaxK&#38750;&#32447;&#24615;&#24182;&#25552;&#20379;&#20102;MaxK&#38750;&#32447;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08656v3 Announce Type: replace-cross  Abstract: In the acceleration of deep neural network training, the GPU has become the mainstream platform. GPUs face substantial challenges on GNNs, such as workload imbalance and memory access irregularities, leading to underutilized hardware. Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks partially address these challenges but memory traffic is still significant.   We argue that drastic performance improvements can only be achieved by the vertical optimization of algorithm and system innovations, rather than treating the speedup optimization as an "after-thought" (i.e., (i) given a GNN algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing the GNN algorithm). In this paper, we present MaxK-GNN, an advanced high-performance GPU training system integrating algorithm and system innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical analysis of MaxK nonlinearity as
&lt;/p&gt;</description></item><item><title>SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04985</link><description>&lt;p&gt;
SparQ&#27880;&#24847;&#21147;&#65306;&#39640;&#25928;&#24102;&#23485;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SparQ Attention: Bandwidth-Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04985
&lt;/p&gt;
&lt;p&gt;
SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21019;&#20102;&#35768;&#22810;&#26032;&#21487;&#33021;&#24615;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#20351;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparQ&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#36890;&#36807;&#36873;&#25321;&#24615;&#33719;&#21462;&#32531;&#23384;&#21382;&#21490;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;LLMs&#30340;&#25512;&#29702;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04985v2 Announce Type: replace  Abstract: Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide ra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Beta Divergence&#36827;&#34892;&#21464;&#20998;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#21644;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#22312;&#20154;&#33080;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2312.00824</link><description>&lt;p&gt;
&#20351;&#29992;Beta Divergence&#36827;&#34892;&#21464;&#20998;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20154;&#33080;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Variational Self-Supervised Contrastive Learning Using Beta Divergence For Face Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00824
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Beta Divergence&#36827;&#34892;&#21464;&#20998;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#26410;&#26631;&#35760;&#21644;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#22312;&#20154;&#33080;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;&#26410;&#26631;&#35760;&#21644;&#22024;&#26434;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#36776;&#21035;&#24615;&#30340;&#35821;&#20041;&#31354;&#38388;&#22312;&#22810;&#26631;&#31614;&#35774;&#32622;&#20013;&#20173;&#26410;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25968;&#25454;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#19988;&#22522;&#20110;&#21464;&#20998;&#26041;&#27861;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#65288;VCL&#65289;&#21033;&#29992;&#21464;&#20998;&#23545;&#27604;&#23398;&#20064;&#19982;beta-divergence&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#31283;&#20581;&#22320;&#23398;&#20064;&#65292;&#21253;&#25324;&#26410;&#21152;&#24037;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#24494;&#35843;&#22330;&#26223;&#20013;&#20351;&#29992;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20154;&#33080;&#29702;&#35299;&#39046;&#22495;&#12290;&#22312;&#20960;&#20046;&#25152;&#26377;&#27979;&#35797;&#22330;&#26223;&#20013;&#65292;VCL&#22343;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00824v2 Announce Type: replace-cross  Abstract: Learning a discriminative semantic space using unlabelled and noisy data remains unaddressed in a multi-label setting. We present a contrastive self-supervised learning method which is robust to data noise, grounded in the domain of variational methods. The method (VCL) utilizes variational contrastive learning with beta-divergence to learn robustly from unlabelled datasets, including uncurated and noisy datasets. We demonstrate the effectiveness of the proposed method through rigorous experiments including linear evaluation and fine-tuning scenarios with multi-label datasets in the face understanding domain. In almost all tested scenarios, VCL surpasses the performance of state-of-the-art self-supervised methods, achieving a noteworthy increase in accuracy.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#20083;&#33146;MRI&#20013;&#21512;&#25104;&#23545;&#27604;&#22686;&#24378;&#65292;&#24341;&#20837;&#20102;Scaled Aggregate Measure (SAMe)&#36827;&#34892;&#37327;&#21270;&#35780;&#20272;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.10879</link><description>&lt;p&gt;
&#25913;&#36827;&#20083;&#33146;MRI&#32959;&#30244;&#20998;&#21106;&#30340;&#21069;&#21518;&#23545;&#27604;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#20083;&#33146;MRI&#20013;&#21512;&#25104;&#23545;&#27604;&#22686;&#24378;&#65292;&#24341;&#20837;&#20102;Scaled Aggregate Measure (SAMe)&#36827;&#34892;&#37327;&#21270;&#35780;&#20272;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21160;&#24577;&#22686;&#24378;MRI&#65288;DCE-MRI&#65289;&#20013;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#23545;&#20110;&#32959;&#30244;&#30340;&#26816;&#27979;&#21644;&#27835;&#30103;&#26377;&#30410;&#22788;&#65292;&#20294;&#23384;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#20405;&#20837;&#24615;&#12289;&#29983;&#29289;&#31215;&#32047;&#24615;&#21450;&#28508;&#22312;&#30340;&#32958;&#28304;&#24615;&#31995;&#32479;&#32420;&#32500;&#21270;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#23558;&#26415;&#21069;&#23545;&#27604;&#21069;T1&#21152;&#26435;&#33026;&#32938;&#39281;&#21644;&#20083;&#33146;MRI&#36716;&#25442;&#20026;&#20854;&#23545;&#24212;&#30340;&#39318;&#27425;DCE-MRI&#24207;&#21015;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#33021;&#21147;&#26469;&#20135;&#29983;&#21512;&#25104;&#23545;&#27604;&#22686;&#24378;&#30340;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#30340;&#27604;&#20363;&#32858;&#21512;&#27979;&#37327;&#65288;SAMe&#65289;&#65292;&#24182;&#20316;&#20026;&#36873;&#25321;&#26368;&#20339;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;DCE-MRI&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;3D&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;p&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10879v2 Announce Type: replace-cross  Abstract: Despite its benefits for tumour detection and treatment, the administration of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated with a range of issues, including their invasiveness, bioaccumulation, and a risk of nephrogenic systemic fibrosis. This study explores the feasibility of producing synthetic contrast enhancements by translating pre-contrast T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI sequence leveraging the capabilities of a generative adversarial network (GAN). Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for quantitatively evaluating the quality of synthetic data in a principled manner and serving as a basis for selecting the optimal generative model. We assess the generated DCE-MRI data using quantitative image quality metrics and apply them to the downstream task of 3D breast tumour segmentation. Our results highlight the potential of p
&lt;/p&gt;</description></item><item><title>EduGym&#26159;&#19968;&#22871;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25945;&#32946;&#30340;&#29615;&#22659;&#21644;&#31508;&#35760;&#26412;&#22871;&#20214;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#36716;&#25442;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2311.10590</link><description>&lt;p&gt;
EduGym: &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25945;&#32946;&#30340;&#29615;&#22659;&#21644;&#31508;&#35760;&#26412;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
EduGym: An Environment and Notebook Suite for Reinforcement Learning Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10590
&lt;/p&gt;
&lt;p&gt;
EduGym&#26159;&#19968;&#22871;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#25945;&#32946;&#30340;&#29615;&#22659;&#21644;&#31508;&#35760;&#26412;&#22871;&#20214;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#29983;&#22312;&#36716;&#25442;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#29983;&#22312;&#23398;&#20064;&#36825;&#20010;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#38469;&#25945;&#23398;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#22312;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#65288;&#26412;&#31185;&#29983;&#12289;&#30805;&#22763;&#29983;&#21644;&#26089;&#26399;&#21338;&#22763;&#29983;&#65289;&#26102;&#24120;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#19968;&#26041;&#38754;&#65292;&#25945;&#31185;&#20070;&#21644;&#65288;&#22312;&#32447;&#65289;&#35762;&#24231;&#25552;&#20379;&#20102;&#22522;&#30784;&#30693;&#35782;&#65292;&#20294;&#23398;&#29983;&#21457;&#29616;&#24456;&#38590;&#22312;&#26041;&#31243;&#24335;&#21644;&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20844;&#20849;&#20195;&#30721;&#24211;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#20363;&#23376;&#65292;&#20294;&#23454;&#29616;&#30340;&#31639;&#27861;&#24448;&#24448;&#22797;&#26434;&#65292;&#24182;&#19988;&#22522;&#30784;&#27979;&#35797;&#29615;&#22659;&#21516;&#26102;&#21253;&#21547;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#25361;&#25112;&#12290;&#23613;&#31649;&#36825;&#22312;&#30740;&#31350;&#35282;&#24230;&#19978;&#26159;&#29616;&#23454;&#30340;&#65292;&#20294;&#23427;&#32463;&#24120;&#38459;&#30861;&#20102;&#25945;&#32946;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;EduGym&#65292;&#36825;&#26159;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#25945;&#32946;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#30456;&#20851;&#20132;&#20114;&#24335;&#31508;&#35760;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10590v2 Announce Type: replace-cross  Abstract: Due to the empirical success of reinforcement learning, an increasing number of students study the subject. However, from our practical teaching experience, we see students entering the field (bachelor, master and early PhD) often struggle. On the one hand, textbooks and (online) lectures provide the fundamentals, but students find it hard to translate between equations and code. On the other hand, public codebases do provide practical examples, but the implemented algorithms tend to be complex, and the underlying test environments contain multiple reinforcement learning challenges at once. Although this is realistic from a research perspective, it often hinders educational conceptual understanding. To solve this issue we introduce EduGym, a set of educational reinforcement learning environments and associated interactive notebooks tailored for education. Each EduGym environment is specifically designed to illustrate a certain 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37197;&#30005;&#31995;&#32479;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#25200;&#21160;&#35270;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#65292;&#24182;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#25552;&#39640;&#38382;&#39064;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#35813;&#26694;&#26550;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.06973</link><description>&lt;p&gt;
&#20998;&#26512;&#39564;&#35777;&#21516;&#27493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37197;&#30005;&#31995;&#32479;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#25200;&#21160;&#35270;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#65292;&#24182;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#25552;&#39640;&#38382;&#39064;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#35813;&#26694;&#26550;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#23454;&#26102;&#19981;&#21487;&#35266;&#27979;&#20998;&#24067;&#31995;&#32479;&#30340;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#20010;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#29366;&#24577;&#20272;&#35745;&#22120;&#22312;&#36755;&#20837;&#27979;&#37327;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#20998;&#26512;&#30028;&#38480;&#12290;&#24050;&#32463;&#26377;&#20154;&#34920;&#26126;&#65292;&#20165;&#22522;&#20110;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24615;&#33021;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#22320;&#35828;&#26126;&#35757;&#32451;&#22909;&#30340;DNN&#22788;&#29702;&#36755;&#20837;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25200;&#21160;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#38382;&#39064;&#20174;&#20998;&#26512;&#19978;&#39564;&#35777;&#20102;DNN&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#35299;&#20915;MILP&#20844;&#24335;&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#19968;&#20010;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#21516;&#27493;&#26102;&#38388;&#30340;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#26469;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#20004;&#20010;&#31995;&#32479;&#37117;&#26159;&#36890;&#36807;&#24494;&#30456;&#20301;&#27979;&#37327;&#19981;&#23436;&#20840;&#35266;&#27979;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, we demonstrated success of a time-synchronized state estimator using deep neural networks (DNNs) for real-time unobservable distribution systems. In this letter, we provide analytical bounds on the performance of that state estimator as a function of perturbations in the input measurements. It has already been shown that evaluating performance based on only the test dataset might not effectively indicate a trained DNN's ability to handle input perturbations. As such, we analytically verify robustness and trustworthiness of DNNs to input perturbations by treating them as mixed-integer linear programming (MILP) problems. The ability of batch normalization in addressing the scalability limitations of the MILP formulation is also highlighted. The framework is validated by performing time-synchronized distribution system state estimation for a modified IEEE 34-node system and a real-world large distribution system, both of which are incompletely observed by micro-phasor measuremen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#24418;&#29366;&#65292;&#33021;&#22815;&#20248;&#21270;&#22320;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#19979;&#36798;&#21040;&#30446;&#26631;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.03233</link><description>&lt;p&gt;
&#23548;&#33322;&#35268;&#27169;&#23450;&#24459;&#65306;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35745;&#31639;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Navigating Scaling Laws: Compute Optimality in Adaptive Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#24418;&#29366;&#65292;&#33021;&#22815;&#20248;&#21270;&#22320;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#19979;&#36798;&#21040;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#20027;&#35201;&#30001;&#32463;&#36807;&#22823;&#37327;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#38750;&#24120;&#24222;&#22823;&#27169;&#22411;&#20027;&#23548;&#12290;&#36825;&#19968;&#33539;&#24335;&#38750;&#24120;&#31616;&#21333;&#65306;&#25237;&#20837;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#65288;&#26368;&#20248;&#22320;&#65289;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19988;&#29978;&#33267;&#33021;&#22815;&#21487;&#39044;&#27979;&#24615;&#22320;&#20570;&#21040;&#65307;&#24050;&#32463;&#25512;&#23548;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#32593;&#32476;&#22312;&#25152;&#38656;&#35745;&#31639;&#27700;&#24179;&#19979;&#30340;&#24615;&#33021;&#12290;&#36825;&#24341;&#20986;&#20102;&#8220;&#35745;&#31639;&#20248;&#21270;&#8221;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20998;&#37197;&#32473;&#23450;&#35745;&#31639;&#27700;&#24179;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20801;&#35768;&#8220;&#33258;&#36866;&#24212;&#8221;&#27169;&#22411;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#25913;&#21464;&#24418;&#29366;&#30340;&#27169;&#22411;&#65292;&#26469;&#25193;&#23637;&#20248;&#21270;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#20986;&#33021;&#22815;&#26368;&#20248;&#22320;&#22312;&#22522;&#26412;&#23450;&#24459;&#20043;&#38388;&#31359;&#34892;&#24182;&#36229;&#36234;&#23427;&#20204;&#30340;&#8220;&#38745;&#24577;&#8221;&#23545;&#24212;&#29289;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#36798;&#21040;&#32473;&#23450;&#30446;&#26631;&#24615;&#33021;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03233v2 Announce Type: replace  Abstract: In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a `compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an `adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their `static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Controllable Machine Unlearning (ConMU)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#24179;&#34913;&#38544;&#31169;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#36816;&#34892;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2310.18574</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25511;&#26426;&#22120;&#36951;&#24536;&#25171;&#30772;&#38544;&#31169;&#12289;&#25928;&#29992;&#12289;&#25928;&#29575;&#19977;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18574
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Controllable Machine Unlearning (ConMU)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#24179;&#34913;&#38544;&#31169;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#36816;&#34892;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#31639;&#27861;&#30001;&#20110;&#23545;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#30340;&#24517;&#35201;&#36981;&#20174;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;MU&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#23545;&#32473;&#23450;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20110;&#26368;&#22823;&#21270;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#29616;&#23454;&#19990;&#30028;&#22522;&#20110;&#32593;&#32476;&#30340;&#24212;&#29992;&#31243;&#24207;&#37117;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#38544;&#31169;&#27861;&#35268;&#12290;&#25506;&#32034;&#38544;&#31169;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#36816;&#34892;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#20840;&#35889;&#23545;&#20110;&#23454;&#38469;&#30340;&#36951;&#24536;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#32780;&#19988;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#65292;&#35774;&#35745;&#20855;&#26377;&#23545;&#19978;&#36848;&#26435;&#34913;&#30340;&#31616;&#21333;&#25511;&#21046;&#30340;MU&#31639;&#27861;&#26159;&#21487;&#21462;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Controllable Machine Unlearning (ConMU)&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;MU&#26657;&#20934;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;ConMU&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18574v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) algorithms have become increasingly critical due to the imperative adherence to data privacy regulations. The primary objective of MU is to erase the influence of specific data samples on a given model without the need to retrain it from scratch. Accordingly, existing methods focus on maximizing user privacy protection. However, there are different degrees of privacy regulations for each real-world web-based application. Exploring the full spectrum of trade-offs between privacy, model utility, and runtime efficiency is critical for practical unlearning scenarios. Furthermore, designing the MU algorithm with simple control of the aforementioned trade-off is desirable but challenging due to the inherent complex interaction. To address the challenges, we present Controllable Machine Unlearning (ConMU), a novel framework designed to facilitate the calibration of MU. The ConMU framework contains three integra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#31526;&#21512;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#38543;&#26426;&#26631;&#31614;&#27745;&#26579;&#65292;&#20135;&#29983;&#26356;&#20855;&#20449;&#24687;&#37327;&#21644;&#26356;&#22362;&#23454;&#35206;&#30422;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;</title><link>https://arxiv.org/abs/2309.05092</link><description>&lt;p&gt;
&#20855;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#33258;&#36866;&#24212;&#31526;&#21512;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive conformal classification with noisy labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#31526;&#21512;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#38543;&#26426;&#26631;&#31614;&#27745;&#26579;&#65292;&#20135;&#29983;&#26356;&#20855;&#20449;&#24687;&#37327;&#21644;&#26356;&#22362;&#23454;&#35206;&#30422;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#22312;&#26657;&#20934;&#26679;&#26412;&#20013;&#30340;&#38543;&#26426;&#26631;&#31614;&#27745;&#26579;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#20016;&#23500;&#30340;&#12289;&#27604;&#36215;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#26377;&#24378;&#22823;&#35206;&#30422;&#20445;&#35777;&#30340;&#39044;&#27979;&#38598;&#12290;&#36825;&#26159;&#36890;&#36807;&#31934;&#30830;&#34920;&#24449;&#26631;&#20934;&#31526;&#21512;&#25512;&#26029;&#22312;&#26631;&#31614;&#27745;&#26579;&#23384;&#22312;&#26102;&#21463;&#21040;&#30340;&#26377;&#25928;&#35206;&#30422;&#33192;&#32960;&#65288;&#25110;&#32039;&#32553;&#65289;&#26469;&#23454;&#29616;&#30340;&#65292;&#28982;&#21518;&#36890;&#36807;&#26032;&#30340;&#26657;&#20934;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#28789;&#27963;&#30340;&#65292;&#21487;&#20197;&#21033;&#29992;&#20851;&#20110;&#26631;&#31614;&#27745;&#26579;&#36807;&#31243;&#30340;&#19981;&#21516;&#24314;&#27169;&#20551;&#35774;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#25110;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#36827;&#34892;&#20102;&#35299;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#21644;&#23545;CIFAR-10H&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#29289;&#20307;&#20998;&#31867;&#24212;&#29992;&#24471;&#21040;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05092v2 Announce Type: replace-cross  Abstract: This paper develops novel conformal prediction methods for classification tasks that can automatically adapt to random label contamination in the calibration sample, leading to more informative prediction sets with stronger coverage guarantees compared to state-of-the-art approaches. This is made possible by a precise characterization of the effective coverage inflation (or deflation) suffered by standard conformal inferences in the presence of label contamination, which is then made actionable through new calibration algorithms. Our solution is flexible and can leverage different modeling assumptions about the label contamination process, while requiring no knowledge of the underlying data distribution or of the inner workings of the machine-learning classifier. The advantages of the proposed methods are demonstrated through extensive simulations and an application to object classification with the CIFAR-10H image data set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stiefel&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20840;&#23616;&#20989;&#25968;&#38750;&#20984;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2308.10547</link><description>&lt;p&gt;
&#22522;&#20110;Stiefel&#27969;&#24418;&#30340;&#20998;&#24067;&#24335;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stiefel&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20840;&#23616;&#20989;&#25968;&#38750;&#20984;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#36717;&#26799;&#24230;&#27861;&#26159;&#19968;&#31181;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#24120;&#27604;&#26368;&#36895;&#19979;&#38477;&#27861;&#25910;&#25947;&#26356;&#24555;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#36828;&#20302;&#20110;&#20108;&#38454;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#21644;&#40654;&#26364;&#27969;&#24418;&#19978;&#24050;&#30740;&#31350;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#22312;&#20998;&#24067;&#24335;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#22312;Stiefel&#27969;&#24418;&#19978;&#26368;&#23567;&#21270;&#20840;&#23616;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#65288;DRCGD&#65289;&#26041;&#27861;&#12290;&#20248;&#21270;&#38382;&#39064;&#22312;&#19968;&#32452;&#20195;&#29702;&#32593;&#32476;&#20013;&#20998;&#24067;&#65292;&#27599;&#20010;&#20195;&#29702;&#19982;&#19968;&#20010;&#23616;&#37096;&#20989;&#25968;&#30456;&#20851;&#32852;&#65292;&#24182;&#19988;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#22312;&#19968;&#20010;&#26080;&#21521;&#36830;&#36890;&#22270;&#19978;&#36827;&#34892;&#12290;&#30001;&#20110;Stiefel&#27969;&#24418;&#26159;&#19968;&#20010;&#38750;&#20984;&#38598;&#65292;&#20840;&#23616;&#20989;&#25968;&#34987;&#34920;&#31034;&#20026;&#21487;&#33021;&#38750;&#20984;&#65288;&#20294;&#24179;&#28369;&#65289;&#23616;&#37096;&#20989;&#25968;&#30340;&#26377;&#38480;&#21644;&#12290;&#35813;&#26041;&#27861;&#19981;&#21463;&#26368;&#20248;&#38750;&#20984;&#24120;&#25968;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10547v2 Announce Type: replace-cross  Abstract: The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#21098;&#20999;&#30340;DP-SGD&#35757;&#32451;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#21033;&#26222;&#24076;&#33576;&#32422;&#26463;&#32593;&#32476;&#25552;&#20379;&#28789;&#25935;&#24230;&#30028;&#38480;&#65292;&#35268;&#36991;&#20102;&#21098;&#20999;&#36807;&#31243;&#30340;&#32570;&#28857;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#27599;&#19968;&#23618;&#30456;&#23545;&#20110;&#20854;&#21442;&#25968;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#26469;&#35757;&#32451;&#36825;&#20123;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2305.16202</link><description>&lt;p&gt;
&#26080;&#21098;&#20999;&#30340;DP-SGD&#65306;&#21033;&#26222;&#24076;&#33576;&#31070;&#32463;&#32593;&#32476;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
DP-SGD Without Clipping: The Lipschitz Neural Network Way
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#21098;&#20999;&#30340;DP-SGD&#35757;&#32451;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#21033;&#26222;&#24076;&#33576;&#32422;&#26463;&#32593;&#32476;&#25552;&#20379;&#28789;&#25935;&#24230;&#30028;&#38480;&#65292;&#35268;&#36991;&#20102;&#21098;&#20999;&#36807;&#31243;&#30340;&#32570;&#28857;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#27599;&#19968;&#23618;&#30456;&#23545;&#20110;&#20854;&#21442;&#25968;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#26469;&#35757;&#32451;&#36825;&#20123;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26041;&#27861;&#22312;&#20272;&#35745;&#32593;&#32476;&#23618;&#30340;&#28789;&#25935;&#24230;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#21098;&#20999;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20381;&#36182;&#20110;&#21033;&#26222;&#24076;&#33576;&#32422;&#26463;&#32593;&#32476;&#26469;&#25552;&#20379;&#28789;&#25935;&#24230;&#30028;&#38480;&#24182;&#35268;&#36991;&#21098;&#20999;&#36807;&#31243;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#19982;&#20854;&#21442;&#25968;&#30456;&#20851;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#19982;&#20854;&#36755;&#20837;&#30456;&#20851;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#20043;&#38388;&#30340;&#26410;&#24320;&#21457;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#38480;&#21046;&#27599;&#19968;&#23618;&#30456;&#23545;&#20110;&#20854;&#21442;&#25968;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#35757;&#32451;&#36825;&#20123;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#20165;&#20351;&#35268;&#27169;&#21270;&#22320;&#35745;&#31639;&#19978;&#36848;&#28789;&#25935;&#24230;&#25104;&#20026;&#21487;&#33021;&#65292;&#36824;&#25552;&#20379;&#20102;&#22914;&#20309;&#36827;&#34892;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16202v2 Announce Type: replace  Abstract: State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, we propose to rely on Lipschitz constrained networks. Our theoretical analysis reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, we prove that we can train these networks with privacy guarantees. Our analysis not only allows the computation of the aforementioned sensitivities at scale, but also provides guidance on how to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>Mobiprox&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25903;&#25345;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21160;&#24577;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#32593;&#32476;&#23618;&#30340;&#36816;&#34892;&#26102;&#21487;&#35843;&#36817;&#20284;&#12290;</title><link>https://arxiv.org/abs/2303.11291</link><description>&lt;p&gt;
Mobiprox&#65306;&#25903;&#25345;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21160;&#24577;&#36817;&#20284;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Mobiprox: Supporting Dynamic Approximate Computing on Mobiles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.11291
&lt;/p&gt;
&lt;p&gt;
Mobiprox&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25903;&#25345;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21160;&#24577;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#32593;&#32476;&#23618;&#30340;&#36816;&#34892;&#26102;&#21487;&#35843;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2303.11291v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;Runtime-tunable context-dependent&#32593;&#32476;&#21387;&#32553;&#23558;&#20351;&#31227;&#21160;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36866;&#24212;&#20110;&#32463;&#24120;&#21464;&#21270;&#30340;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#36755;&#20837;&#8220;&#38590;&#24230;&#8221;&#25110;&#29992;&#25143;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#25216;&#26415;&#26174;&#33879;&#20943;&#23569;&#20102;DL&#30340;&#20869;&#23384;&#12289;&#22788;&#29702;&#21644;&#33021;&#32791;&#65292;&#20294;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#24448;&#24448;&#27704;&#20037;&#21463;&#25439;&#65292;&#29306;&#29298;&#20102;&#25512;&#29702;&#33021;&#21147;&#20197;&#25442;&#21462;&#20943;&#23569;&#36164;&#28304;&#20351;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#21487;&#35843;&#21387;&#32553;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#19981;&#25903;&#25345;&#20219;&#24847;&#21387;&#32553;&#31574;&#30053;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#23454;&#29616;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mobiprox&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#31227;&#21160;&#35774;&#22791;&#20855;&#26377;&#28789;&#27963;&#31934;&#24230;&#30340;&#26694;&#26550;&#12290;Mobiprox&#23454;&#29616;&#20102;&#24352;&#37327;&#25805;&#20316;&#30340;&#21487;&#35843;&#36817;&#20284;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#32593;&#32476;&#23618;&#30340;&#36816;&#34892;&#26102;&#21487;&#35843;&#36817;&#20284;&#12290;Mobiprox&#38468;&#24102;&#30340;&#20998;&#26512;&#22120;&#21644;&#35843;&#25972;&#22120;&#35782;&#21035;&#20102;&#26368;&#26377;&#24076;&#26395;&#30340;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.11291v2 Announce Type: replace  Abstract: Runtime-tunable context-dependent network compression would make mobile deep learning (DL) adaptable to often varying resource availability, input "difficulty", or user needs. The existing compression techniques significantly reduce the memory, processing, and energy tax of DL, yet, the resulting models tend to be permanently impaired, sacrificing the inference power for reduced resource usage. The existing tunable compression approaches, on the other hand, require expensive re-training, do not support arbitrary strategies for adapting the compression and do not provide mobile-ready implementations.   In this paper we present Mobiprox, a framework enabling mobile DL with flexible precision. Mobiprox implements tunable approximations of tensor operations and enables runtime-adaptable approximation of individual network layers. A profiler and a tuner included with Mobiprox identify the most promising neural network approximation config
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#25968;&#25454;&#21644;&#22478;&#24066;&#24433;&#20687;&#25972;&#21512;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#20998;&#26512;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#65292;&#24182;&#22312;&#39044;&#27979;&#32858;&#21512;&#21644;&#32454;&#20998;&#20986;&#34892;&#34892;&#20026;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#38656;&#27714;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2303.04204</link><description>&lt;p&gt;
&#20855;&#26377;&#21355;&#26143;&#22270;&#20687;&#30340;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;&#65306;&#22914;&#20309;&#23558;&#38656;&#27714;&#24314;&#27169;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#32467;&#21512;&#29992;&#20110;&#34892;&#20026;&#20998;&#26512;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep hybrid model with satellite imagery: how to combine demand modeling and computer vision for behavior analysis?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.04204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#25968;&#25454;&#21644;&#22478;&#24066;&#24433;&#20687;&#25972;&#21512;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#20998;&#26512;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#65292;&#24182;&#22312;&#39044;&#27979;&#32858;&#21512;&#21644;&#32454;&#20998;&#20986;&#34892;&#34892;&#20026;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#38656;&#27714;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#38656;&#27714;&#24314;&#27169;&#20165;&#20351;&#29992;&#20302;&#32500;&#25968;&#25968;&#20540;&#25968;&#25454;&#65288;&#21363;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20986;&#34892;&#23646;&#24615;&#65289;&#65292;&#32780;&#19981;&#20351;&#29992;&#39640;&#32500;&#22478;&#24066;&#24433;&#20687;&#12290;&#28982;&#32780;&#65292;&#20986;&#34892;&#34892;&#20026;&#21462;&#20915;&#20110;&#25968;&#20540;&#25968;&#25454;&#21644;&#22478;&#24066;&#24433;&#20687;&#25152;&#20195;&#34920;&#30340;&#22240;&#32032;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#20010;&#21327;&#21516;&#26694;&#26550;&#26469;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20132;&#21449;&#32467;&#26500;&#30340;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#28151;&#21512;&#36816;&#31639;&#22120;&#21644;&#19968;&#20010;&#34892;&#20026;&#39044;&#27979;&#22120;&#65292;&#20174;&#32780;&#23558;&#25968;&#20540;&#21644;&#24433;&#20687;&#25968;&#25454;&#25972;&#21512;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#20174;&#23454;&#35777;&#35282;&#24230;&#30475;&#65292;&#35813;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#20351;&#29992;&#33437;&#21152;&#21733;&#30340;MyDailyTravel&#35843;&#26597;&#20316;&#20026;&#25968;&#20540;&#36755;&#20837;&#21644;&#21355;&#26143;&#22270;&#20687;&#20316;&#20026;&#24433;&#20687;&#36755;&#20837;&#26469;&#20998;&#26512;&#20986;&#34892;&#26041;&#24335;&#36873;&#25321;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28145;&#24230;&#28151;&#21512;&#27169;&#22411;&#22312;&#20351;&#29992;&#25105;&#20204;&#30340;&#30417;&#30563;&#28151;&#21512;&#35774;&#35745;&#26469;&#39044;&#27979;&#32858;&#21512;&#21644;&#32454;&#20998;&#20986;&#34892;&#34892;&#20026;&#26102;&#65292;&#32988;&#36807;&#20256;&#32479;&#30340;&#38656;&#27714;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.04204v2 Announce Type: replace  Abstract: Classical demand modeling analyzes travel behavior using only low-dimensional numeric data (i.e. sociodemographics and travel attributes) but not high-dimensional urban imagery. However, travel behavior depends on the factors represented by both numeric data and urban imagery, thus necessitating a synergetic framework to combine them. This study creates a theoretical framework of deep hybrid models with a crossing structure consisting of a mixing operator and a behavioral predictor, thus integrating the numeric and imagery data into a latent space. Empirically, this framework is applied to analyze travel mode choice using the MyDailyTravel Survey from Chicago as the numeric inputs and the satellite images as the imagery inputs. We found that deep hybrid models outperform both the traditional demand models and the recent deep learning in predicting the aggregate and disaggregate travel behavior with our supervision-as-mixing design. T
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-GNN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20986;&#34892;&#38656;&#27714;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#35777;&#24212;&#29992;&#34920;&#26126;&#27010;&#29575;&#20551;&#35774;&#23545;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#24433;&#21709;&#22823;&#20110;&#30830;&#23450;&#24615;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2303.04040</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#26102;&#31354;&#20986;&#34892;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification of Spatiotemporal Travel Demand with Probabilistic Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.04040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-GNN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20986;&#34892;&#38656;&#27714;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#35777;&#24212;&#29992;&#34920;&#26126;&#27010;&#29575;&#20551;&#35774;&#23545;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#24433;&#21709;&#22823;&#20110;&#30830;&#23450;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#26174;&#33879;&#25552;&#39640;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20986;&#34892;&#38656;&#27714;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20986;&#34892;&#38656;&#27714;&#39044;&#27979;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-GNN&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20986;&#34892;&#38656;&#27714;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20010;Prob-GNN&#26694;&#26550;&#22522;&#20110;&#30830;&#23450;&#24615;&#21644;&#27010;&#29575;&#20551;&#35774;&#65292;&#24182;&#22312;&#33437;&#21152;&#21733;&#24066;&#39044;&#27979;&#20844;&#20849;&#20132;&#36890;&#21644;&#25340;&#36710;&#38656;&#27714;&#30340;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27010;&#29575;&#20551;&#35774;&#65288;&#22914;&#20998;&#24067;&#23614;&#37096;&#12289;&#25903;&#25345;&#65289;&#23545;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;&#24433;&#21709;&#22823;&#20110;&#30830;&#23450;&#24615;&#20551;&#35774;&#65288;&#22914;&#28145;&#24230;&#27169;&#22359;&#12289;&#28145;&#24230;&#65289;&#12290;&#22312;Prob-GNN&#23478;&#26063;&#20013;&#65292;&#37319;&#29992;&#25130;&#26029;&#39640;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;GNN&#22312;&#20844;&#20849;&#20132;&#36890;&#21644;&#25340;&#36710;&#25968;&#25454;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#21363;&#20351;&#22312;&#23384;&#22312;&#26126;&#26174;&#22495;&#20559;&#31227;&#24773;&#20917;&#19979;&#65292;Prob-GNNs
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.04040v2 Announce Type: replace  Abstract: Recent studies have significantly improved the prediction accuracy of travel demand using graph neural networks. However, these studies largely ignored uncertainty that inevitably exists in travel demand prediction. To fill this gap, this study proposes a framework of probabilistic graph neural networks (Prob-GNN) to quantify the spatiotemporal uncertainty of travel demand. This Prob-GNN framework is substantiated by deterministic and probabilistic assumptions, and empirically applied to the task of predicting the transit and ridesharing demand in Chicago. We found that the probabilistic assumptions (e.g. distribution tail, support) have a greater impact on uncertainty prediction than the deterministic ones (e.g. deep modules, depth). Among the family of Prob-GNNs, the GNNs with truncated Gaussian and Laplace distributions achieve the highest performance in transit and ridesharing data. Even under significant domain shifts, Prob-GNNs
&lt;/p&gt;</description></item><item><title>&#21457;&#36865;&#32773;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#20449;&#21495;&#26041;&#26696;&#65292;&#20960;&#20046;&#21487;&#20197;&#20445;&#35777;&#33719;&#24471;&#19982;&#32463;&#20856;&#27169;&#22411;&#20013;&#26368;&#20339;&#25928;&#29992;&#20960;&#20046;&#30456;&#31561;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#32780;&#25509;&#25910;&#32773;&#30340;&#36817;&#20284;&#26368;&#20339;&#21709;&#24212;&#34892;&#20026;&#24182;&#19981;&#20250;&#23545;&#21457;&#36865;&#32773;&#22312;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#30340;&#26368;&#22823;&#21487;&#36798;&#25928;&#29992;&#20135;&#29983;&#24456;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2302.03719</link><description>&lt;p&gt;
&#21149;&#35828;&#34892;&#20026;&#20195;&#29702;&#65306;&#36817;&#20284;&#26368;&#20339;&#21709;&#24212;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Persuading a Behavioral Agent: Approximately Best Responding and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03719
&lt;/p&gt;
&lt;p&gt;
&#21457;&#36865;&#32773;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#20449;&#21495;&#26041;&#26696;&#65292;&#20960;&#20046;&#21487;&#20197;&#20445;&#35777;&#33719;&#24471;&#19982;&#32463;&#20856;&#27169;&#22411;&#20013;&#26368;&#20339;&#25928;&#29992;&#20960;&#20046;&#30456;&#31561;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#32780;&#25509;&#25910;&#32773;&#30340;&#36817;&#20284;&#26368;&#20339;&#21709;&#24212;&#34892;&#20026;&#24182;&#19981;&#20250;&#23545;&#21457;&#36865;&#32773;&#22312;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#30340;&#26368;&#22823;&#21487;&#36798;&#25928;&#29992;&#20135;&#29983;&#24456;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#27169;&#22411;&#20551;&#23450;&#25509;&#25910;&#32773;&#26159;&#19968;&#20010;&#36125;&#21494;&#26031;&#21644;&#26368;&#20339;&#21709;&#24212;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#35828;&#26381;&#27169;&#22411;&#30340;&#25918;&#26494;&#65292;&#20854;&#20013;&#25509;&#25910;&#32773;&#21487;&#20197;&#36817;&#20284;&#26368;&#20339;&#22320;&#21709;&#24212;&#21457;&#36865;&#32773;&#30340;&#20449;&#21495;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#65292;&#65288;1&#65289;&#21457;&#36865;&#32773;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#20449;&#21495;&#26041;&#26696;&#65292;&#20445;&#35777;&#33258;&#24049;&#33719;&#24471;&#30340;&#39044;&#26399;&#25928;&#29992;&#20960;&#20046;&#21644;&#22312;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#25509;&#25910;&#32773;&#37319;&#29992;&#20160;&#20040;&#26679;&#30340;&#36817;&#20284;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65307;&#65288;2&#65289;&#21478;&#19968;&#26041;&#38754;&#65292;&#21363;&#20351;&#25509;&#25910;&#32773;&#20351;&#29992;&#23545;&#21457;&#36865;&#32773;&#26368;&#26377;&#21033;&#30340;&#36817;&#20284;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#20063;&#19981;&#23384;&#22312;&#19968;&#20010;&#20449;&#21495;&#26041;&#26696;&#21487;&#20197;&#32473;&#21457;&#36865;&#32773;&#27604;&#20854;&#22312;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#22810;&#30340;&#25928;&#29992;&#12290;&#36825;&#20004;&#28857;&#35828;&#26126;&#25509;&#25910;&#32773;&#30340;&#36817;&#20284;&#26368;&#20339;&#21709;&#24212;&#34892;&#20026;&#24182;&#19981;&#20250;&#23545;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#21457;&#36865;&#32773;&#33021;&#22815;&#36798;&#21040;&#30340;&#26368;&#22823;&#25928;&#29992;&#20135;&#29983;&#24456;&#22823;&#24433;&#21709;&#12290;&#36825;&#20004;&#20010;&#32467;&#26524;&#30340;&#35777;&#26126;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03719v2 Announce Type: replace-cross  Abstract: The classic Bayesian persuasion model assumes a Bayesian and best-responding receiver. We study a relaxation of the Bayesian persuasion model where the receiver can approximately best respond to the sender's signaling scheme. We show that, under natural assumptions, (1) the sender can find a signaling scheme that guarantees itself an expected utility almost as good as its optimal utility in the classic model, no matter what approximately best-responding strategy the receiver uses; (2) on the other hand, there is no signaling scheme that gives the sender much more utility than its optimal utility in the classic model, even if the receiver uses the approximately best-responding strategy that is best for the sender. Together, (1) and (2) imply that the approximately best-responding behavior of the receiver does not affect the sender's maximal achievable utility a lot in the Bayesian persuasion problem. The proofs of both results r
&lt;/p&gt;</description></item><item><title>TBAL&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#26410;&#26631;&#27880;&#25968;&#25454;&#65292;&#20943;&#23569;&#25163;&#21160;&#26631;&#27880;&#30340;&#20381;&#36182;&#65307;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#21363;&#20351;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#20063;&#21487;&#20197;&#20934;&#30830;&#33258;&#21160;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#25581;&#31034;&#20102;TBAL&#31995;&#32479;&#30340;&#28508;&#22312;&#32570;&#38519;</title><link>https://arxiv.org/abs/2211.12620</link><description>&lt;p&gt;
&#22522;&#20110;&#38408;&#20540;&#30340;&#33258;&#21160;&#26631;&#27880;&#30340;&#20248;&#21183;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Promises and Pitfalls of Threshold-based Auto-labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.12620
&lt;/p&gt;
&lt;p&gt;
TBAL&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#26410;&#26631;&#27880;&#25968;&#25454;&#65292;&#20943;&#23569;&#25163;&#21160;&#26631;&#27880;&#30340;&#20381;&#36182;&#65307;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#21363;&#20351;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#20063;&#21487;&#20197;&#20934;&#30830;&#33258;&#21160;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#25581;&#31034;&#20102;TBAL&#31995;&#32479;&#30340;&#28508;&#22312;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#26159;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#38408;&#20540;&#33258;&#21160;&#26631;&#27880;&#65288;TBAL&#65289;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#33719;&#21462;&#30340;&#39564;&#35777;&#25968;&#25454;&#26469;&#23547;&#25214;&#19968;&#20010;&#32622;&#20449;&#38408;&#20540;&#65292;&#39640;&#20110;&#35813;&#38408;&#20540;&#30340;&#25968;&#25454;&#23558;&#30001;&#26426;&#22120;&#26631;&#35760;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;TBAL&#27491;&#36880;&#28176;&#25104;&#20026;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#37492;&#20110;&#25152;&#24471;&#25968;&#25454;&#30340;&#38271;&#26399;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#21270;&#20351;&#29992;&#65292;&#29702;&#35299;&#36825;&#31181;&#33258;&#21160;&#26631;&#27880;&#31995;&#32479;&#33719;&#21462;&#30340;&#25968;&#25454;&#20309;&#26102;&#21487;&#20197;&#34987;&#20381;&#36182;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#20998;&#26512;TBAL&#31995;&#32479;&#24182;&#25512;&#23548;&#38656;&#35201;&#20445;&#35777;&#26426;&#22120;&#26631;&#35760;&#25968;&#25454;&#36136;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#39564;&#35777;&#25968;&#25454;&#37327;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#34920;&#38754;&#19978;&#31967;&#31957;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#12289;&#20934;&#30830;&#22320;&#26631;&#35760;&#21512;&#29702;&#25968;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;TBAL&#31995;&#32479;&#30340;&#19968;&#20010;&#38544;&#34255;&#30340;&#32570;&#28857;&#26159;&#28508;&#22312;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.12620v2 Announce Type: replace-cross  Abstract: Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. Threshold-based auto-labeling (TBAL), where validation data obtained from humans is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on manual annotation. TBAL is emerging as a widely-used solution in practice. Given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. This is the first work to analyze TBAL systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. Our results provide two crucial insights. First, reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad models. Second, a hidden downside of TBAL systems is potentially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#33021;&#22815;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20809;&#28369;&#24615;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2205.05795</link><description>&lt;p&gt;
&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#21450;&#20854;&#22312;&#21270;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Algebraic Machine Learning with an Application to Chemistry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.05795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#33021;&#22815;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20809;&#28369;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#30740;&#31350;&#25968;&#25454;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#24050;&#25104;&#20026;&#25968;&#25454;&#20998;&#26512;&#36807;&#31243;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#21487;&#20197;&#20174;&#23545;&#25345;&#20037;&#21516;&#35843;&#31561;&#25299;&#25169;&#24037;&#20855;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#20013;&#30475;&#20986;&#12290;&#28982;&#32780;&#65292;&#19968;&#26041;&#38754;&#65292;&#25299;&#25169;&#24037;&#20855;&#26412;&#36136;&#19978;&#20165;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#22522;&#26412;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26356;&#22810;&#20960;&#20309;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#21363;&#22522;&#26412;&#31354;&#38388;&#26159;&#20809;&#28369;&#27969;&#24418;&#12290;&#36825;&#19968;&#20551;&#35774;&#23545;&#20110;&#35768;&#22810;&#21253;&#21547;&#22855;&#28857;&#30340;&#29289;&#29702;&#27169;&#22411;&#26159;&#19981;&#25104;&#31435;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#33021;&#22815;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20219;&#20309;&#20809;&#28369;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20195;&#25968;&#20960;&#20309;&#21644;&#20195;&#25968;&#22810;&#39033;&#24335;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.05795v3 Announce Type: replace-cross  Abstract: As datasets used in scientific applications become more complex, studying the geometry and topology of data has become an increasingly prevalent part of the data analysis process. This can be seen for example with the growing interest in topological tools such as persistent homology. However, on the one hand, topological tools are inherently limited to providing only coarse information about the underlying space of the data. On the other hand, more geometric approaches rely predominately on the manifold hypothesis, which asserts that the underlying space is a smooth manifold. This assumption fails for many physical models where the underlying space contains singularities.   In this paper we develop a machine learning pipeline that captures fine-grain geometric information without having to rely on any smoothness assumptions. Our approach involves working within the scope of algebraic geometry and algebraic varieties instead of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20851;&#20110;&#20851;&#31995;&#25968;&#25454;&#20844;&#24179;&#24615;&#30340;&#35843;&#26597;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#22270;&#19978;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.05396</link><description>&lt;p&gt;
&#23545;&#22270;&#19978;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Fairness for Machine Learning on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.05396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20851;&#20110;&#20851;&#31995;&#25968;&#25454;&#20844;&#24179;&#24615;&#30340;&#35843;&#26597;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#22270;&#19978;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#22270;&#24418;&#24314;&#27169;&#30340;&#22797;&#26434;&#29616;&#35937;&#30340;&#20998;&#26512;&#22312;&#20915;&#31574;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#20123;&#20915;&#31574;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#35768;&#22810;&#30740;&#31350;&#21644;&#35770;&#25991;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#20010;&#20307;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#24046;&#24322;&#23545;&#24453;&#21644;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22270;&#25366;&#25496;&#30340;&#31639;&#27861;&#36129;&#29486;&#20063;&#26080;&#27861;&#25670;&#33073;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#19988;&#23384;&#22312;&#19968;&#20123;&#19982;&#22270;&#30340;&#22266;&#26377;&#24615;&#36136;&#30456;&#20851;&#30340;&#20855;&#20307;&#25361;&#25112;&#65306;(1) &#22270;&#25968;&#25454;&#24182;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#20250;&#20351;&#35768;&#22810;&#29616;&#26377;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21464;&#24471;&#26080;&#25928;&#65292;(2) &#38024;&#23545;&#20851;&#31995;&#25968;&#25454;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#23450;&#20041;&#65292;&#20197;&#21450;(3) &#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#25214;&#21040;&#33391;&#22909;&#24179;&#34913;&#30340;&#31639;&#27861;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20851;&#20110;&#20851;&#31995;&#25968;&#25454;&#20844;&#24179;&#24615;&#30340;&#35843;&#26597;&#12290;&#23427;&#26088;&#22312;&#21576;&#29616;&#19968;&#20010;&#20840;&#38754;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.05396v2 Announce Type: replace  Abstract: Nowadays, the analysis of complex phenomena modeled by graphs plays a crucial role in many real-world application domains where decisions can have a strong societal impact. However, numerous studies and papers have recently revealed that machine learning models could lead to potential disparate treatment between individuals and unfair outcomes. In that context, algorithmic contributions for graph mining are not spared by the problem of fairness and present some specific challenges related to the intrinsic nature of graphs: (1) graph data is non-IID, and this assumption may invalidate many existing studies in fair machine learning, (2) suited metric definitions to assess the different types of fairness with relational data and (3) algorithmic challenge on the difficulty of finding a good trade-off between model accuracy and fairness. This survey is the first one dedicated to fairness for relational data. It aims to present a comprehen
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#26377;&#38480;&#30340;&#22024;&#26434;&#27979;&#37327;&#25968;&#25454;&#20013;&#23454;&#29616;&#27969;&#22330;&#25968;&#25454;&#30340;&#36229;&#20998;&#36776;&#65292;&#26080;&#38656;&#39640;&#20998;&#36776;&#29575;&#21442;&#32771;&#25968;&#25454;&#65292;&#26088;&#22312;&#33719;&#24471;&#36830;&#32493;&#30340;&#26102;&#31354;&#35299;&#12290;</title><link>https://arxiv.org/abs/2203.15402</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed deep-learning applications to experimental fluid mechanics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.15402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#26377;&#38480;&#30340;&#22024;&#26434;&#27979;&#37327;&#25968;&#25454;&#20013;&#23454;&#29616;&#27969;&#22330;&#25968;&#25454;&#30340;&#36229;&#20998;&#36776;&#65292;&#26080;&#38656;&#39640;&#20998;&#36776;&#29575;&#21442;&#32771;&#25968;&#25454;&#65292;&#26088;&#22312;&#33719;&#24471;&#36830;&#32493;&#30340;&#26102;&#31354;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#27979;&#37327;&#25968;&#25454;&#36890;&#24120;&#31232;&#30095;&#12289;&#19981;&#23436;&#25972;&#19988;&#22024;&#26434;&#65292;&#20174;&#20302;&#20998;&#36776;&#29575;&#21644;&#22024;&#26434;&#27979;&#37327;&#25968;&#25454;&#20013;&#39640;&#20998;&#36776;&#29575;&#37325;&#26500;&#27969;&#22330;&#25968;&#25454;&#26159;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#36866;&#29992;&#20110;&#36825;&#26679;&#30340;&#36229;&#20998;&#36776;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#24212;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20174;&#26377;&#38480;&#30340;&#22024;&#26434;&#27979;&#37327;&#20013;&#36229;&#20998;&#36776;&#27969;&#22330;&#25968;&#25454;&#65292;&#26080;&#38656;&#20219;&#20309;&#39640;&#20998;&#36776;&#29575;&#21442;&#32771;&#25968;&#25454;&#65292;&#26088;&#22312;&#33719;&#24471;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#30340;&#36830;&#32493;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.15402v2 Announce Type: replace-cross  Abstract: High-resolution reconstruction of flow-field data from low-resolution and noisy measurements is of interest due to the prevalence of such problems in experimental fluid mechanics, where the measurement data are in general sparse, incomplete and noisy. Deep-learning approaches have been shown suitable for such super-resolution tasks. However, a high number of high-resolution examples is needed, which may not be available for many cases. Moreover, the obtained predictions may lack in complying with the physical principles, e.g. mass and momentum conservation. Physics-informed deep learning provides frameworks for integrating data and physical laws for learning. In this study, we apply physics-informed neural networks (PINNs) for super-resolution of flow-field data both in time and space from a limited set of noisy measurements without having any high-resolution reference data. Our objective is to obtain a continuous solution of t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#31995;&#32479;&#24615;&#20998;&#31867;&#27861;&#20197;&#25351;&#23548;&#29616;&#26377;&#24322;&#36136;&#24615;GNN&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2202.07082</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Graphs with Heterophily: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.07082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#31995;&#32479;&#24615;&#20998;&#31867;&#27861;&#20197;&#25351;&#23548;&#29616;&#26377;&#24322;&#36136;&#24615;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#35768;&#22810;&#22270;&#20998;&#26512;&#20219;&#21153;&#21644;&#24212;&#29992;&#21463;&#30410;&#12290;&#22823;&#22810;&#25968;GNNs&#36890;&#24120;&#20381;&#36182;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#21363;&#23646;&#20110;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#21487;&#33021;&#30456;&#36830;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#22270;&#23646;&#24615;&#65292;&#21363;&#19981;&#21516;&#26631;&#31614;&#30340;&#33410;&#28857;&#24448;&#24448;&#30456;&#36830;&#65292;&#36825;&#26174;&#33879;&#38480;&#21046;&#20102;&#23450;&#21046;&#30340;&#21516;&#36136;&#24615;GNNs&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#24322;&#36136;&#24615;&#22270;&#30340;GNNs&#27491;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20197;&#22686;&#24378;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#23398;&#20064;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;GNNs&#22238;&#39038;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#20998;&#31867;&#27861;&#65292;&#26412;&#36136;&#19978;&#25351;&#23548;&#30528;&#29616;&#26377;&#30340;&#24322;&#36136;&#24615;GNN&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#27010;&#25324;&#24615;&#25688;&#35201;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.07082v2 Announce Type: replace  Abstract: Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph analytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class are more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes with different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a comprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs existing heterophilic GNN models, along with a general summary and detailed analysis. %Furthermore, we summarize the mainstream heterophilic graph benchmarks to facilitate robust and fa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayes&#30028;&#38480;&#65292;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20013;&#27979;&#35797;&#25439;&#22833;&#20998;&#24067;&#25110;&#20998;&#31867;&#20013;&#19981;&#21516;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2202.05560</link><description>&lt;p&gt;
&#20351;&#29992;PAC-Bayes&#30028;&#38480;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.05560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayes&#30028;&#38480;&#65292;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20013;&#27979;&#35797;&#25439;&#22833;&#20998;&#24067;&#25110;&#20998;&#31867;&#20013;&#19981;&#21516;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#20165;&#38480;&#20110;&#24615;&#33021;&#30340;&#26631;&#37327;&#24230;&#37327;&#65292;&#22914;&#25439;&#22833;&#25110;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#20016;&#23500;&#20449;&#24687;&#30340;PAC-Bayes&#30028;&#38480;&#65292;&#36890;&#36807;&#30028;&#23450;&#19968;&#32452;M&#31181;&#38169;&#35823;&#31867;&#22411;&#30340;&#32463;&#39564;&#27010;&#29575;&#19982;&#30495;&#23454;&#27010;&#29575;&#20043;&#38388;&#30340;Kullback-Leibler&#24046;&#24322;&#26469;&#25511;&#21046;&#21487;&#33021;&#32467;&#26524;&#30340;&#25972;&#20010;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.05560v2 Announce Type: replace-cross  Abstract: Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of M error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#25551;&#36848;-&#24207;&#21015;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#22312;&#20840;&#23616;&#27700;&#24179;&#19978;&#23545;&#29983;&#25104;&#38899;&#20048;&#30340;&#32454;&#31890;&#24230;&#21487;&#25511;&#65292;&#36890;&#36807;&#32467;&#21512;&#39640;&#32423;&#29305;&#24449;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;</title><link>https://arxiv.org/abs/2201.10936</link><description>&lt;p&gt;
FIGARO&#65306;&#20855;&#26377;&#32454;&#31890;&#24230;&#33402;&#26415;&#25511;&#21046;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.10936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#25551;&#36848;-&#24207;&#21015;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#22312;&#20840;&#23616;&#27700;&#24179;&#19978;&#23545;&#29983;&#25104;&#38899;&#20048;&#30340;&#32454;&#31890;&#24230;&#21487;&#25511;&#65292;&#36890;&#36807;&#32467;&#21512;&#39640;&#32423;&#29305;&#24449;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#38899;&#20048;&#36817;&#24180;&#26469;&#19968;&#30452;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#34429;&#28982;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#33021;&#23545;&#29983;&#25104;&#30340;&#24207;&#21015;&#26045;&#21152;&#26368;&#23567;&#30340;&#25511;&#21046;&#65292;&#29978;&#33267;&#27809;&#26377;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#30340;&#25551;&#36848;-&#24207;&#21015;&#20219;&#21153;&#65292;&#23427;&#20801;&#35768;&#22312;&#20840;&#23616;&#27700;&#24179;&#19978;&#36827;&#34892;&#32454;&#31890;&#24230;&#21487;&#25511;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#21462;&#26377;&#20851;&#30446;&#26631;&#24207;&#21015;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#20197;&#21450;&#23398;&#20064;&#32473;&#23450;&#30456;&#24212;&#39640;&#32423;&#25551;&#36848;&#26102;&#24207;&#21015;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25551;&#36848;-&#24207;&#21015;&#24314;&#27169;&#24212;&#29992;&#21040;&#31526;&#21495;&#38899;&#20048;&#20013;&#26469;&#35757;&#32451;FIGARO&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#12289;&#40065;&#26834;&#25511;&#21046;&#30340;&#32454;&#31890;&#24230;&#38899;&#20048;&#29983;&#25104;&#65289;&#12290;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#39640;&#32423;&#29305;&#24449;&#19982;&#39046;&#22495;&#30693;&#35782;&#32467;&#21512;&#65292;&#20316;&#20026;&#24378;&#24402;&#32435;&#20559;&#24046;&#65292;&#35813;&#27169;&#22411;&#22312;&#21487;&#25511;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.10936v4 Announce Type: replace-cross  Abstract: Generating music with deep neural networks has been an area of active research in recent years. While the quality of generated samples has been steadily increasing, most methods are only able to exert minimal control over the generated sequence, if any. We propose the self-supervised description-to-sequence task, which allows for fine-grained controllable generation on a global level. We do so by extracting high-level features about the target sequence and learning the conditional distribution of sequences given the corresponding high-level description in a sequence-to-sequence modelling setup. We train FIGARO (FIne-grained music Generation via Attention-based, RObust control) by applying description-to-sequence modelling to symbolic music. By combining learned high level features with domain knowledge, which acts as a strong inductive bias, the model achieves state-of-the-art results in controllable symbolic music generation a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25903;&#25345;&#21521;&#37327;&#26426;&#36229;&#21442;&#25968;&#36827;&#34892;&#25913;&#36827;&#30340;&#26080;&#30417;&#30563;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#32570;&#20047;&#26631;&#35760;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#23545;&#31867;&#26631;&#31614;&#20449;&#24687;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2111.02164</link><description>&lt;p&gt;
&#25968;&#25454;&#32467;&#26500;&gt;&#26631;&#31614;&#65311;&#26080;&#30417;&#30563;&#21551;&#21457;&#24335;SVM&#36229;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Data structure &gt; labels? Unsupervised heuristics for SVM hyperparameter estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.02164
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25903;&#25345;&#21521;&#37327;&#26426;&#36229;&#21442;&#25968;&#36827;&#34892;&#25913;&#36827;&#30340;&#26080;&#30417;&#30563;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#32570;&#20047;&#26631;&#35760;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#23545;&#31867;&#26631;&#31614;&#20449;&#24687;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#26159;&#27169;&#24335;&#35782;&#21035;&#30740;&#31350;&#30340;&#20027;&#35201;&#39046;&#22495;&#20043;&#19968;&#65292;&#22312;&#20854;&#20013;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#38500;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20197;&#22806;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#24182;&#19988;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20107;&#23454;&#21442;&#32771;&#12290;SVM&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#21442;&#25968;&#36873;&#25321;&#65292;&#36890;&#24120;&#36890;&#36807;&#32791;&#26102;&#30340;&#32593;&#26684;&#25628;&#32034;&#20132;&#21449;&#39564;&#35777;&#36807;&#31243;&#65288;GSCV&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#26631;&#35760;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#65292;&#24403;&#36825;&#20123;&#26631;&#35760;&#21463;&#38480;&#26102;&#21487;&#33021;&#20250;&#21463;&#38459;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#23384;&#22312;&#20960;&#31181;&#26080;&#30417;&#30563;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#26469;&#36873;&#25321;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#31867;&#26631;&#31614;&#20449;&#24687;&#12290;&#34429;&#28982;&#36895;&#24230;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#34987;&#20351;&#29992;&#65292;&#22240;&#20026;&#20154;&#20204;&#35748;&#20026;&#23427;&#20204;&#30340;&#32467;&#26524;&#26126;&#26174;&#27604;&#32593;&#26684;&#25628;&#32034;&#24046;&#12290;&#20026;&#20102;&#25361;&#25112;&#36825;&#31181;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;SVM&#36229;&#21442;&#25968;heuristics&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.02164v2 Announce Type: replace  Abstract: Classification is one of the main areas of pattern recognition research, and within it, Support Vector Machine (SVM) is one of the most popular methods outside of field of deep learning -- and a de-facto reference for many Machine Learning approaches. Its performance is determined by parameter selection, which is usually achieved by a time-consuming grid search cross-validation procedure (GSCV). That method, however relies on the availability and quality of labelled examples and thus, when those are limited can be hindered. To address that problem, there exist several unsupervised heuristics that take advantage of the characteristics of the dataset for selecting parameters instead of using class label information. While an order of magnitude faster, they are scarcely used under the assumption that their results are significantly worse than those of grid search. To challenge that assumption, we have proposed improved heuristics for SV
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#21644;&#20219;&#24847;&#38271;&#30340;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#20219;&#21153;&#20013;&#22810;&#28304;&#25968;&#25454;&#27969;&#30340;&#34701;&#21512;&#21644;&#36328;&#26102;&#38388;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2110.08021</link><description>&lt;p&gt;
StreaMulT: &#27969;&#24335;&#22810;&#27169;&#24577;Transformer&#29992;&#20110;&#24322;&#26500;&#21644;&#20219;&#24847;&#38271;&#30340;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#21644;&#20219;&#24847;&#38271;&#30340;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#20219;&#21153;&#20013;&#22810;&#28304;&#25968;&#25454;&#27969;&#30340;&#34701;&#21512;&#21644;&#36328;&#26102;&#38388;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24037;&#19994;4.0&#31995;&#32479;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#24615;&#32500;&#25252;&#20219;&#21153;&#65288;&#22914;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65289;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#30456;&#24212;&#32780;&#23454;&#38469;&#30340;&#24773;&#26223;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#22810;&#28304;&#25968;&#25454;&#27969;&#65292;&#22914;&#20256;&#24863;&#22120;&#27979;&#37327;&#26102;&#38388;&#24207;&#21015;&#12289;&#26426;&#22120;&#22270;&#20687;&#12289;&#25991;&#26412;&#32500;&#25252;&#25253;&#21578;&#31561;&#12290;&#36825;&#20123;&#24322;&#26500;&#22810;&#27169;&#24577;&#27969;&#22312;&#20854;&#37319;&#38598;&#39057;&#29575;&#19978;&#20063;&#19981;&#21516;&#65292;&#21487;&#33021;&#21253;&#21547;&#26102;&#38388;&#19978;&#19981;&#23545;&#40784;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#20219;&#24847;&#38271;&#65292;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#31995;&#32479;&#21644;&#20219;&#21153;&#12290;&#34429;&#28982;&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#20013;&#27809;&#26377;&#32771;&#34385;&#36807;&#19982;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#36328;&#26102;&#38388;&#39044;&#27979;&#65289;&#19968;&#36215;&#32771;&#34385;&#20219;&#24847;&#38271;&#30340;&#22810;&#27169;&#24577;&#27969;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#31181;&#24322;&#26500;&#22810;&#27169;&#24577;&#23398;&#20064;&#33539;&#24335;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#24335;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08021v2 Announce Type: replace-cross  Abstract: The increasing complexity of Industry 4.0 systems brings new challenges regarding predictive maintenance tasks such as fault detection and diagnosis. A corresponding and realistic setting includes multi-source data streams from different modalities, such as sensors measurements time series, machine images, textual maintenance reports, etc. These heterogeneous multimodal streams also differ in their acquisition frequency, may embed temporally unaligned information and can be arbitrarily long, depending on the considered system and task. Whereas multimodal fusion has been largely studied in a static setting, to the best of our knowledge, there exists no previous work considering arbitrarily long multimodal streams alongside with related tasks such as prediction across time. Thus, in this paper, we first formalize this paradigm of heterogeneous multimodal learning in a streaming setting as a new one. To tackle this challenge, we p
&lt;/p&gt;</description></item><item><title>AML&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20813;&#21463;&#23433;&#20840;&#23041;&#32961;&#65292;&#36125;&#21494;&#26031;&#35270;&#35282;&#20026;&#38450;&#24481;&#25552;&#20379;&#20102;&#26032;&#30340;&#22909;&#22788;</title><link>https://arxiv.org/abs/2003.03546</link><description>&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65306;&#36125;&#21494;&#26031;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning: Bayesian Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2003.03546
&lt;/p&gt;
&lt;p&gt;
AML&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20813;&#21463;&#23433;&#20840;&#23041;&#32961;&#65292;&#36125;&#21494;&#26031;&#35270;&#35282;&#20026;&#38450;&#24481;&#25552;&#20379;&#20102;&#26032;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;(AML)&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#20813;&#21463;&#23433;&#20840;&#23041;&#32961;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#23384;&#22312;&#25932;&#23545;&#26041;&#31215;&#26497;&#25805;&#32437;&#36755;&#20837;&#25968;&#25454;&#20197;&#27450;&#39575;&#23398;&#20064;&#31995;&#32479;&#12290; &#36825;&#21019;&#36896;&#20102;&#19968;&#31867;&#26032;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;ML&#31995;&#32479;&#21487;&#33021;&#20250;&#38754;&#20020;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#34987;&#31216;&#20026;&#25932;&#23545;&#31283;&#20581;&#24615;&#30340;&#21487;&#20449;&#25805;&#20316;&#25152;&#24517;&#38656;&#30340;&#24615;&#36136;&#12290; &#22823;&#37096;&#20998;AML&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#23545;&#25239;&#23398;&#20064;&#31995;&#32479;&#21644;&#20934;&#22791;&#25805;&#32437;&#36755;&#20837;&#25968;&#25454;&#30340;&#23545;&#25163;&#20043;&#38388;&#20914;&#31361;&#30340;&#21338;&#24328;&#35770;&#24314;&#27169;&#20043;&#19978;&#12290; &#36825;&#20551;&#35774;&#27599;&#20010;&#20195;&#29702;&#37117;&#20102;&#35299;&#23545;&#25163;&#30340;&#20852;&#36259;&#21644;&#19981;&#30830;&#23450;&#24615;&#21028;&#26029;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#20110;Nash&#22343;&#34913;&#30340;&#25512;&#29702;&#12290; &#28982;&#32780;&#65292;&#22312;AML&#20856;&#22411;&#30340;&#23433;&#20840;&#26041;&#26696;&#20013;&#65292;&#36825;&#31181;&#20849;&#21516;&#30693;&#35782;&#20551;&#35774;&#24182;&#19981;&#29616;&#23454;&#12290; &#22312;&#22238;&#39038;&#20102;&#36825;&#31181;&#21338;&#24328;&#35770;&#26041;&#27861;&#20043;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#35270;&#35282;&#22312;&#38450;&#24481;&#20013;&#25552;&#20379;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
arXiv:2003.03546v2 Announce Type: replace  Abstract: Adversarial Machine Learning (AML) is emerging as a major field aimed at protecting machine learning (ML) systems against security threats: in certain scenarios there may be adversaries that actively manipulate input data to fool learning systems. This creates a new class of security vulnerabilities that ML systems may face, and a new desirable property called adversarial robustness essential to trust operations based on ML outputs. Most work in AML is built upon a game-theoretic modelling of the conflict between a learning system and an adversary, ready to manipulate input data. This assumes that each agent knows their opponent's interests and uncertainty judgments, facilitating inferences based on Nash equilibria. However, such common knowledge assumption is not realistic in the security scenarios typical of AML. After reviewing such game-theoretic approaches, we discuss the benefits that Bayesian perspectives provide when defendin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12999</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Machine Learning for Molecular Docking. (arXiv:2401.12999v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12999
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#23376;&#23545;&#25509;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#29305;&#24615;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#30450;&#30446;&#23545;&#25509;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#26500;&#24314;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#24555;&#33647;&#29289;&#24320;&#21457;&#30340;&#25928;&#29575;&#12290;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#32467;&#21512;&#36807;&#31243;&#38656;&#35201;&#22312;&#24191;&#27867;&#30340;&#31354;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25628;&#32034;&#21644;&#37319;&#26679;&#12290;&#20256;&#32479;&#30340;&#23545;&#25509;&#26041;&#27861;&#36890;&#36807;&#25628;&#32034;&#21487;&#33021;&#30340;&#32467;&#21512;&#20301;&#28857;&#21644;&#26500;&#35937;&#26469;&#23454;&#29616;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#21463;&#21040;&#36825;&#19968;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#37327;&#23376;&#21551;&#21457;&#31639;&#27861;&#19982;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;&#32534;&#30721;&#30340;&#20998;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#26799;&#24230;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#30450;&#30446;&#23545;&#25509;&#20013;&#30340;&#25913;&#36827;&#12290;&#25968;&#20540;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#23545;&#25509;&#31639;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#19978;&#34920;&#29616;&#20986;&#20102;&#36229;&#36807;10%&#30340;&#25552;&#21319;&#12290;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#25509;&#31639;&#27861;DiffDock&#30456;&#27604;&#65292;Top-1&#65288;RMSD&lt;2&#65289;&#30340;&#25104;&#21151;&#29575;&#20174;33%&#25552;&#39640;&#21040;35%&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular docking is an important tool for structure-based drug design, accelerating the efficiency of drug development. Complex and dynamic binding processes between proteins and small molecules require searching and sampling over a wide spatial range. Traditional docking by searching for possible binding sites and conformations is computationally complex and results poorly under blind docking. Quantum-inspired algorithms combining quantum properties and annealing show great advantages in solving combinatorial optimization problems. Inspired by this, we achieve an improved in blind docking by using quantum-inspired combined with gradients learned by deep learning in the encoded molecular space. Numerical simulation shows that our method outperforms traditional docking algorithms and deep learning-based algorithms over 10\%. Compared to the current state-of-the-art deep learning-based docking algorithm DiffDock, the success rate of Top-1 (RMSD&lt;2) achieves an improvement from 33\% to 35
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#26684;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#27169;&#22411;&#36716;&#25442;&#20026;&#23618;&#27425;&#32467;&#26500;&#30340;&#34920;&#26684;&#26597;&#25214;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#21462;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;DART&#39044;&#21462;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#24773;&#20917;&#19979;&#21482;&#26377;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.06362</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#12289;&#33976;&#39311;&#21644;&#34920;&#26684;&#21270;&#65306;&#36208;&#21521;&#23454;&#29992;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching. (arXiv:2401.06362v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06362
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#26684;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#27169;&#22411;&#36716;&#25442;&#20026;&#23618;&#27425;&#32467;&#26500;&#30340;&#34920;&#26684;&#26597;&#25214;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#21462;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;DART&#39044;&#21462;&#27169;&#22411;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#37327;&#30340;&#24773;&#20917;&#19979;&#21482;&#26377;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#20934;&#30830;&#30340;&#20869;&#23384;&#35775;&#38382;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20102;&#39640;&#25928;&#24615;&#65292;&#36825;&#26159;&#25968;&#25454;&#39044;&#21462;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#24320;&#38144;&#36896;&#25104;&#20102;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#23454;&#38469;&#39044;&#21462;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#26684;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#65292;&#21516;&#26102;&#21448;&#19981;&#29306;&#29298;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#34920;&#26684;&#21270;&#26041;&#27861;&#23558;&#19968;&#20010;&#32463;&#36807;&#33976;&#39311;&#30340;&#20855;&#26377;&#39640;&#31934;&#30830;&#24230;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20316;&#20026;&#36755;&#20837;&#65292;&#23558;&#20854;&#26114;&#36149;&#30340;&#30697;&#38453;&#20056;&#27861;&#36716;&#25442;&#25104;&#24555;&#36895;&#34920;&#26684;&#26597;&#25214;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#20316;&#20026;&#19978;&#36848;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;DART&#65292;&#19968;&#20010;&#30001;&#31616;&#21333;&#34920;&#26684;&#23618;&#27425;&#32467;&#26500;&#32452;&#25104;&#30340;&#39044;&#21462;&#27169;&#22411;&#12290;&#22312;F1&#24471;&#20998;&#19979;&#38477;&#20102;0.09&#30340;&#24773;&#20917;&#19979;&#65292;DART&#20174;&#22823;&#22411;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#20943;&#23569;&#20102;99.99%&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#20174;&#33976;&#39311;&#27169;&#22411;&#20013;&#20943;&#23569;&#20102;91.83%&#30340;&#36816;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based Neural Networks (NN) have demonstrated their effectiveness in accurate memory access prediction, an essential step in data prefetching. However, the substantial computational overheads associated with these models result in high inference latency, limiting their feasibility as practical prefetchers. To close the gap, we propose a new approach based on tabularization that significantly reduces model complexity and inference latency without sacrificing prediction accuracy. Our novel tabularization methodology takes as input a distilled, yet highly accurate attention-based model for memory access prediction and efficiently converts its expensive matrix multiplications into a hierarchy of fast table lookups. As an exemplar of the above approach, we develop DART, a prefetcher comprised of a simple hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99% of arithmetic operations from the large attention-based model and 91.83% from the distilled model. DAR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20998;&#31867;&#22120;&#26657;&#20934;&#26469;&#23454;&#29616;&#21453;&#27450;&#35784;&#39044;&#38450;&#20013;&#30340;&#20915;&#31574;&#35299;&#32806;&#12290;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#31574;&#30053;&#65292;&#20182;&#20204;&#21457;&#29616;&#31561;&#36317;&#21644;&#36125;&#22612;&#26657;&#20934;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#31361;&#20986;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#20248;&#21270;&#35299;&#32806;&#21162;&#21147;&#30340;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.05240</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#31867;&#22120;&#26657;&#20934;&#23454;&#29616;&#21453;&#27450;&#35784;&#39044;&#38450;&#30340;&#20915;&#31574;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Decoupling Decision-Making in Fraud Prevention through Classifier Calibration for Business Logic Action. (arXiv:2401.05240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20998;&#31867;&#22120;&#26657;&#20934;&#26469;&#23454;&#29616;&#21453;&#27450;&#35784;&#39044;&#38450;&#20013;&#30340;&#20915;&#31574;&#35299;&#32806;&#12290;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#31574;&#30053;&#65292;&#20182;&#20204;&#21457;&#29616;&#31561;&#36317;&#21644;&#36125;&#22612;&#26657;&#20934;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#31361;&#20986;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#20248;&#21270;&#35299;&#32806;&#21162;&#21147;&#30340;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#19987;&#27880;&#20110;&#29305;&#23450;&#30446;&#26631;&#65292;&#27604;&#22914;&#21019;&#24314;&#20998;&#31867;&#22120;&#65292;&#36890;&#24120;&#22522;&#20110;&#21830;&#19994;&#29615;&#22659;&#20013;&#24050;&#30693;&#30340;&#20154;&#32676;&#29305;&#24449;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#20010;&#20307;&#29305;&#24449;&#30340;&#27169;&#22411;&#38543;&#26102;&#38388;&#32780;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#31934;&#24230;&#65292;&#24341;&#20837;&#35299;&#32806;&#30340;&#27010;&#24565;&#65306;&#20174;&#28857;&#35780;&#20272;&#36716;&#21521;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#26657;&#20934;&#31574;&#30053;&#20316;&#20026;&#35299;&#32806;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#19994;&#21153;&#36923;&#36753;&#26694;&#26550;&#20013;&#30340;&#34892;&#21160;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30495;&#23454;&#30340;&#21830;&#19994;&#22330;&#26223;&#21644;&#22810;&#20010;ML&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26435;&#34913;&#21644;&#24615;&#33021;&#24433;&#21709;&#65292;&#24182;&#20026;&#23547;&#27714;&#20248;&#21270;&#35299;&#32806;&#21162;&#21147;&#30340;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#36716;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#31561;&#36317;&#21644;&#36125;&#22612;&#26657;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models typically focus on specific targets like creating classifiers, often based on known population feature distributions in a business context. However, models calculating individual features adapt over time to improve precision, introducing the concept of decoupling: shifting from point evaluation to data distribution. We use calibration strategies as strategy for decoupling machine learning (ML) classifiers from score-based actions within business logic frameworks. To evaluate these strategies, we perform a comparative analysis using a real-world business scenario and multiple ML models. Our findings highlight the trade-offs and performance implications of the approach, offering valuable insights for practitioners seeking to optimize their decoupling efforts. In particular, the Isotonic and Beta calibration methods stand out for scenarios in which there is shift between training and testing data.
&lt;/p&gt;</description></item><item><title>&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2312.15591</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15591
&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#32467;&#21512;&#20102;&#22270;&#25968;&#25454;&#24211;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#20449;&#24687;&#31995;&#32479;&#26102;&#20195;&#65292;&#39640;&#25928;&#20934;&#30830;&#22320;&#26816;&#32034;&#25968;&#25454;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#65288;NGDB&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#23558;&#22270;&#25968;&#25454;&#24211;&#65288;&#22270;&#24418;&#25968;&#25454;&#24211;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#39640;&#25928;&#23384;&#20648;&#12289;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;&#31070;&#32463;&#23884;&#20837;&#23384;&#20648;&#21644;&#22797;&#26434;&#31070;&#32463;&#36923;&#36753;&#26597;&#35810;&#22238;&#31572;&#20026;NGDB&#25552;&#20379;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#24403;&#22270;&#24418;&#19981;&#23436;&#25972;&#26102;&#65292;&#31070;&#32463;&#22270;&#25968;&#25454;&#24211;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#28508;&#22312;&#27169;&#24335;&#21644;&#34920;&#31034;&#26469;&#22635;&#34917;&#22270;&#32467;&#26500;&#20013;&#30340;&#31354;&#32570;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#20851;&#31995;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#32452;&#21512;&#26597;&#35810;&#25512;&#26029;&#20986;&#26356;&#22810;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#36890;&#36807;&#27604;&#36739;&#22270;&#25968;&#25454;&#24211;&#20013;Turing&#22870;&#24471;&#20027;&#30340;&#31572;&#26696;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36716;&#21270;&#20026;&#27169;&#25311;&#36153;&#26364;-&#21345;&#20811;&#27169;&#22411;&#30340;&#39640;&#25928;&#37319;&#26679;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.19608</link><description>&lt;p&gt;
&#35770;&#36153;&#26364;-&#21345;&#20811;&#35757;&#32451;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On Feynman--Kac training of partial Bayesian neural networks. (arXiv:2310.19608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36716;&#21270;&#20026;&#27169;&#25311;&#36153;&#26364;-&#21345;&#20811;&#27169;&#22411;&#30340;&#39640;&#25928;&#37319;&#26679;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#37096;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(pBNNs)&#34987;&#35777;&#26126;&#19982;&#20840;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;pBNNs&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#20013;&#24448;&#24448;&#26159;&#22810;&#23792;&#30340;&#65292;&#22240;&#27492;&#29992;&#21442;&#25968;&#27169;&#22411;&#26469;&#36817;&#20284;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#23558;pBNN&#30340;&#35757;&#32451;&#36716;&#21270;&#20026;&#27169;&#25311;&#36153;&#26364;-&#21345;&#20811;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#30340;&#21464;&#31181;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#21487;&#34892;&#30340;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20272;&#35745;&#21442;&#25968;&#21644;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#26696;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19253</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flow-based Distributionally Robust Optimization. (arXiv:2310.19253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#35201;&#27714;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#26368;&#19981;&#21033;&#20998;&#24067;&#65292;LFD&#65289;&#26159;&#36830;&#32493;&#30340;&#65292;&#20174;&#32780;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#23545;&#35825;&#23548;&#30340;&#40065;&#26834;&#31639;&#27861;&#30340;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26080;&#38480;&#32500;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36880;&#27493;&#35757;&#32451;&#22359;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#26469;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#26694;&#26550;&#36890;&#29992;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#26679;&#26412;&#22823;&#23567;&#65292;&#24182;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.18285</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#19982;&#32676;&#20307;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning with Group-Aware Prompt Tuning. (arXiv:2310.18285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#23454;&#29616;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#20197;&#35757;&#32451;&#36866;&#24212;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20419;&#20351;&#23427;&#20204;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20855;&#26377;&#19981;&#21516;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#22330;&#26223;&#12290;&#20026;&#20102;&#28385;&#36275;FL&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#38656;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#24341;&#20837;&#20102;&#21516;&#26102;&#23398;&#20064;&#20849;&#20139;&#21644;&#32676;&#20307;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#33021;&#22815;&#21516;&#26102;&#33719;&#21462;&#36890;&#29992;&#30693;&#35782;&#21644;&#32676;&#20307;&#29305;&#23450;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#36873;&#25321;&#27169;&#22359;&#20026;&#27599;&#20010;&#36755;&#20837;&#20998;&#37197;&#20010;&#24615;&#21270;&#30340;&#32676;&#20307;&#25552;&#31034;&#65292;&#20351;&#20840;&#23616;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#19981;&#21516;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25645;&#24314;&#20102;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable success in various machine-learning tasks, prompting their widespread adoption. In this paper, we explore their application in the context of federated learning (FL), with a particular focus on heterogeneous scenarios where individual clients possess diverse local datasets. To meet the computational and communication demands of FL, we leverage pre-trained Transformers and use an efficient prompt-tuning strategy. Our strategy introduces the concept of learning both shared and group prompts, enabling the acquisition of universal knowledge and group-specific knowledge simultaneously. Additionally, a prompt selection module assigns personalized group prompts to each input, aligning the global model with the data distribution of each client. This approach allows us to train a single global model that can automatically adapt to various local client data distributions without requiring local fine-tuning. In this way, our proposed method effectively bridge
&lt;/p&gt;</description></item><item><title>BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.16183</link><description>&lt;p&gt;
BLP 2023&#20219;&#21153;2&#65306;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16183
&lt;/p&gt;
&lt;p&gt;
BLP 2023&#20219;&#21153;2&#26159;&#20851;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20219;&#21153;&#30340;&#35814;&#32454;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#25552;&#20132;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24635;&#32467;&#20102;&#20316;&#20026;BLP 2023&#21019;&#26032;&#24037;&#20316;&#22346;&#30340;&#19968;&#37096;&#20998;&#20030;&#21150;&#30340;BLP&#24773;&#24863;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#30340;&#23450;&#20041;&#26159;&#22312;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#12290;&#35813;&#20219;&#21153;&#21560;&#24341;&#20102;71&#20010;&#21442;&#19982;&#32773;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#38454;&#27573;&#20998;&#21035;&#26377;29&#20010;&#21644;30&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#12290;&#24635;&#20849;&#65292;&#21442;&#19982;&#32773;&#25552;&#20132;&#20102;597&#20010;&#36816;&#34892;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24635;&#20849;&#26377;15&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#12290;&#25552;&#20132;&#30340;&#31995;&#32479;&#28085;&#30422;&#20102;&#20174;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#35813;&#20219;&#21153;&#30340;&#35774;&#32622;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#21442;&#19982;&#32773;&#25552;&#20132;&#30340;&#31995;&#32479;&#12290;&#20849;&#20139;&#20219;&#21153;&#30340;&#25152;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an overview of the BLP Sentiment Shared Task, organized as part of the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is defined as the detection of sentiment in a given piece of social media text. This task attracted interest from 71 participants, among whom 29 and 30 teams submitted systems during the development and evaluation phases, respectively. In total, participants submitted 597 runs. However, a total of 15 teams submitted system description papers. The range of approaches in the submitted systems spans from classical machine learning models, fine-tuning pre-trained models, to leveraging Large Language Model (LLMs) in zero- and few-shot settings. In this paper, we provide a detailed account of the task setup, including dataset development and evaluation setup. Additionally, we provide a brief overview of the systems submitted by the participants. All datasets and evaluation scripts from the shared task have been made publicly available for the res
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35797;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#22806;&#37096;&#26377;&#25928;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#35797;&#39564;&#25968;&#25454;&#23545;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#25919;&#31574;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14763</link><description>&lt;p&gt;
&#22806;&#37096;&#39564;&#35777;&#31574;&#30053;&#35780;&#20272;&#32467;&#21512;&#35797;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Externally Valid Policy Evaluation Combining Trial and Observational Data. (arXiv:2310.14763v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14763
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35797;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#22806;&#37096;&#26377;&#25928;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#35797;&#39564;&#25968;&#25454;&#23545;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#25919;&#31574;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35797;&#39564;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#35780;&#20272;&#20915;&#31574;&#31574;&#30053;&#24433;&#21709;&#30340;&#37329; standard&#12290;&#28982;&#32780;&#65292;&#35797;&#39564;&#25968;&#25454;&#26469;&#33258;&#21487;&#33021;&#19982;&#30446;&#26631;&#20154;&#32676;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#36825;&#24341;&#21457;&#20102;&#22806;&#37096;&#25928;&#24230;&#65288;&#20063;&#31216;&#20026;&#27867;&#21270;&#33021;&#21147;&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#35797;&#39564;&#25968;&#25454;&#23545;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#25919;&#31574;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#30446;&#26631;&#20154;&#32676;&#30340;&#39069;&#22806;&#21327;&#21464;&#37327;&#25968;&#25454;&#29992;&#20110;&#27169;&#25311;&#35797;&#39564;&#30740;&#31350;&#20013;&#20010;&#20307;&#30340;&#25277;&#26679;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#25351;&#23450;&#30340;&#27169;&#22411;&#26410;&#26657;&#20934;&#33539;&#22260;&#20869;&#20135;&#29983;&#21487;&#39564;&#35777;&#30340;&#22522;&#20110;&#35797;&#39564;&#30340;&#25919;&#31574;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#21363;&#20351;&#26679;&#26412;&#26159;&#26377;&#38480;&#30340;&#65292;&#26377;&#25928;&#24615;&#20063;&#24471;&#21040;&#20445;&#35777;&#12290;&#20351;&#29992;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#35828;&#26126;&#20102;&#35748;&#35777;&#30340;&#25919;&#31574;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. Trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external validity (aka. generalizability). In this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population. Additional covariate data from the target population is used to model the sampling of individuals in the trial study. We develop a method that yields certifiably valid trial-based policy evaluations under any specified range of model miscalibrations. The method is nonparametric and the validity is assured even with finite samples. The certified policy evaluations are illustrated using both simulated and real data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2310.09358</link><description>&lt;p&gt;
&#20309;&#26102;&#25165;&#33021;&#20351;&#21095;&#26412;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20445;&#25345;&#31283;&#23450;? (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
When are Bandits Robust to Misspecification?. (arXiv:2310.09358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#22312;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#20381;&#36182;&#20110;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#22870;&#21169;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#24378;&#30423;&#31639;&#27861;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#31639;&#27861;&#12290;&#36890;&#24120;&#30340;&#20551;&#35774;&#26159;&#21487;&#34892;&#24615;&#65292;&#21363;&#34892;&#20026;&#30340;&#30495;&#23454;&#22870;&#21169;&#23436;&#20840;&#30001;&#26576;&#20010;&#21442;&#25968;&#21270;&#27169;&#22411;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#30495;&#23454;&#22870;&#21169;&#19982;&#27169;&#22411;&#31867;&#20043;&#38388;&#23384;&#22312;&#65288;&#21487;&#33021;&#26174;&#33879;&#65289;&#30340;&#35823;&#24046;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#21442;&#25968;&#21270;&#30340;&#24378;&#30423;&#21644;&#24773;&#22659;&#21270;&#30340;&#24378;&#30423;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20381;&#36182;&#38382;&#39064;&#23454;&#20363;&#21644;&#27169;&#22411;&#31867;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20351;&#24471;&#32463;&#20856;&#31639;&#27861;&#22914;&#949;-&#36138;&#24515;&#21644;LinUCB&#22312;&#21363;&#20351;&#22870;&#21169;&#23384;&#22312;&#20005;&#37325;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22815;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#35777;&#27425;&#32447;&#24615;&#65288;&#27425;&#20110;&#26102;&#38388;&#33539;&#22260;&#65289;&#30340;&#36951;&#25022;&#20445;&#38556;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#38024;&#23545;&#38169;&#35823;&#35268;&#33539;&#30340;&#26368;&#22351;&#24773;&#20917;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26174;&#31034;&#36951;&#25022;&#36793;&#30028;&#38543;&#26102;&#38388;&#25104;&#32447;&#24615;&#27604;&#20363;&#22686;&#38271;&#65292;&#24182;&#19988;&#35828;&#26126;&#23384;&#22312;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#24378;&#30423;&#38382;&#39064;&#23454;&#20363;&#38598;&#21512;&#22312;&#38169;&#35823;&#35268;&#33539;&#19979;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parametric feature-based reward models are widely employed by algorithms for decision making settings such as bandits and contextual bandits. The typical assumption under which they are analysed is realizability, i.e., that the true rewards of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true rewards are (potentially significantly) misspecified with respect to the model class. For parameterized bandits and contextual bandits, we identify sufficient conditions, depending on the problem instance and model class, under which classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the time horizon) regret guarantees under even grossly misspecified rewards. This is in contrast to existing worst-case results for misspecified bandits which show regret bounds that scale linearly with time, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03032</link><description>&lt;p&gt;
&#22270;&#22686;&#24378;&#20248;&#21270;&#22120;&#29992;&#20110;&#32467;&#26500;&#24863;&#30693;&#25512;&#33616;&#23884;&#20837;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#34394;&#25311;&#34920;&#31034;&#65292;&#24182;&#19988;&#26159;&#21518;&#32493;&#20915;&#31574;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#26356;&#26032;&#26426;&#21046;&#65292;&#31216;&#20026;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#65292;&#20197;&#40723;&#21169;&#30456;&#20851;&#33410;&#28857;&#22312;&#27599;&#19968;&#27493;&#20013;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28436;&#21270;&#12290;&#19982;&#36890;&#24120;&#20316;&#20026;&#20013;&#38388;&#37096;&#20998;&#30340;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#19981;&#21516;&#65292;SEvo&#33021;&#22815;&#30452;&#25509;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35745;&#31639;&#24320;&#38144;&#21487;&#24573;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;SEvo&#30340;&#25910;&#25947;&#24615;&#36136;&#21450;&#20854;&#21487;&#33021;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#20197;&#35777;&#26126;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;SEvo&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#30697;&#20272;&#35745;&#26657;&#27491;&#30340;SEvo&#22686;&#24378;AdamW&#20013;&#65292;&#35777;&#26126;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#25928;&#26524;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#20026;&#26377;&#25928;&#25512;&#33616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#23558;&#26799;&#24230;&#26041;&#24046;&#30340;&#20381;&#36182;&#24615;&#20174;&#27425;&#20248;&#30340;$O(\sqrt{V_T\log V_T})$&#25913;&#36827;&#21040;&#26368;&#20248;&#36895;&#29575;$O(\sqrt{V_T})$&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#26410;&#30693;Lipschitz&#24120;&#25968;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.16044</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#31934;&#32454;&#31163;&#25955;&#21270;&#26041;&#27861;&#25552;&#39640;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Adaptive Online Learning Using Refined Discretization. (arXiv:2309.16044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16044
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#23558;&#26799;&#24230;&#26041;&#24046;&#30340;&#20381;&#36182;&#24615;&#20174;&#27425;&#20248;&#30340;$O(\sqrt{V_T\log V_T})$&#25913;&#36827;&#21040;&#26368;&#20248;&#36895;&#29575;$O(\sqrt{V_T})$&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#26410;&#30693;Lipschitz&#24120;&#25968;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;Lipschitz&#25439;&#22833;&#30340;&#38750;&#32422;&#26463;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#21516;&#26102;&#36798;&#21040;&#65288;i&#65289;&#20108;&#38454;&#26799;&#24230;&#33258;&#36866;&#24212;&#24615;&#65307;&#21644;&#65288;ii&#65289;&#27604;&#36739;&#22120;&#33539;&#25968;&#33258;&#36866;&#24212;&#24615;&#65292;&#20063;&#34987;&#31216;&#20026;&#25991;&#29486;&#20013;&#30340;&#8220;&#21442;&#25968;&#33258;&#30001;&#24615;&#8221;&#12290;&#29616;&#26377;&#30340;&#36951;&#25022;&#30028;&#65288;Cutkosky&#21644;Orabona&#65292;2018&#65307;Mhammedi&#21644;Koolen&#65292;2020&#65307;Jacobsen&#21644;Cutkosky&#65292;2022&#65289;&#23545;&#20110;&#26799;&#24230;&#26041;&#24046;$V_T$&#26377;&#27425;&#20248;&#30340;$O(\sqrt{V_T\log V_T})$&#20381;&#36182;&#24615;&#65292;&#32780;&#26412;&#24037;&#20316;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#23558;&#20854;&#25913;&#36827;&#20026;&#26368;&#20248;&#36895;&#29575;$O(\sqrt{V_T})$&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#19981;&#20999;&#23454;&#38469;&#30340;&#21152;&#20493;&#25216;&#24039;&#12290;&#36825;&#19968;&#32467;&#26524;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;Lipschitz&#24120;&#25968;&#30340;&#24773;&#20917;&#65292;&#28040;&#38500;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#33539;&#22260;&#27604;&#29575;&#38382;&#39064;&#65288;Mhammedi&#21644;Koolen&#65292;2020&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#38382;&#39064;&#30340;&#36830;&#32493;&#26102;&#38388;&#31867;&#27604;&#20013;&#21487;&#20197;&#30456;&#24403;&#23481;&#26131;&#22320;&#23454;&#29616;&#30446;&#26631;&#30340;&#21516;&#26102;&#36866;&#24212;&#24615;&#65292;&#20854;&#20013;&#29615;&#22659;&#30001;&#20219;&#24847;&#36830;&#32493;&#21322;&#38808;&#24335;&#24314;&#27169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We study unconstrained Online Linear Optimization with Lipschitz losses. The goal is to simultaneously achieve ($i$) second order gradient adaptivity; and ($ii$) comparator norm adaptivity also known as "parameter freeness" in the literature. Existing regret bounds (Cutkosky and Orabona, 2018; Mhammedi and Koolen, 2020; Jacobsen and Cutkosky, 2022) have the suboptimal $O(\sqrt{V_T\log V_T})$ dependence on the gradient variance $V_T$, while the present work improves it to the optimal rate $O(\sqrt{V_T})$ using a novel continuous-time-inspired algorithm, without any impractical doubling trick. This result can be extended to the setting with unknown Lipschitz constant, eliminating the range ratio problem from prior works (Mhammedi and Koolen, 2020).  Concretely, we first show that the aimed simultaneous adaptivity can be achieved fairly easily in a continuous time analogue of the problem, where the environment is modeled by an arbitrary continuous semimartingale. Then, our key innovation 
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29420;&#31435;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27491;&#30830;&#26041;&#31243;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#21457;&#29616;&#26041;&#31243;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#22312;&#27809;&#26377;&#20808;&#39564;&#26041;&#31243;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#21457;&#29616;&#26041;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04901</link><description>&lt;p&gt;
&#36808;&#21521;&#30495;&#27491;&#30340;&#24494;&#20998;&#26041;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards true discovery of the differential equations. (arXiv:2308.04901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29420;&#31435;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#27491;&#30830;&#26041;&#31243;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#21457;&#29616;&#26041;&#31243;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#22312;&#27809;&#26377;&#20808;&#39564;&#26041;&#31243;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#21457;&#29616;&#26041;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#20998;&#26041;&#31243;&#21457;&#29616;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#29992;&#20110;&#24320;&#21457;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#33258;&#28982;&#30456;&#20851;&#30340;&#24212;&#29992;&#20013;&#12290;&#36890;&#36807;&#24039;&#22937;&#22320;&#32467;&#21512;&#36816;&#21160;&#26041;&#31243;&#30340;&#19968;&#33324;&#21442;&#25968;&#24418;&#24335;&#21644;&#21512;&#36866;&#30340;&#24494;&#20998;&#39033;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#26041;&#31243;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29420;&#31435;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24037;&#20855;&#65292;&#28040;&#38500;&#20102;&#23545;&#26041;&#31243;&#24418;&#24335;&#20551;&#35774;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#37325;&#28857;&#35299;&#20915;&#20102;&#22312;&#27491;&#30830;&#26041;&#31243;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#21457;&#29616;&#26041;&#31243;&#30340;&#36866;&#24403;&#24615;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#22312;&#27809;&#26377;&#20808;&#39564;&#26041;&#31243;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#21457;&#29616;&#26041;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential equation discovery, a machine learning subfield, is used to develop interpretable models, particularly in nature-related applications. By expertly incorporating the general parametric form of the equation of motion and appropriate differential terms, algorithms can autonomously uncover equations from data. This paper explores the prerequisites and tools for independent equation discovery without expert input, eliminating the need for equation form assumptions. We focus on addressing the challenge of assessing the adequacy of discovered equations when the correct equation is unknown, with the aim of providing insights for reliable equation discovery without prior knowledge of the equation form.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.14642</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65306;&#25105;&#20204;&#24212;&#35813;&#22362;&#25345;&#21040;&#24213;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#65292;&#29305;&#21035;&#26159;&#30528;&#38470;&#31283;&#23450;&#65288;STL&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#25910;&#25947;&#20110;&#20960;&#20309;&#65288;&#20256;&#32479;&#19978;&#31216;&#20026;&#8220;&#32447;&#24615;&#8221;&#65289;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;STL&#20272;&#35745;&#22120;&#30340;&#26799;&#24230;&#26041;&#24046;&#30340;&#20108;&#27425;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#21253;&#25324;&#20102;&#35823;&#25351;&#23450;&#30340;&#21464;&#20998;&#26063;&#12290;&#32467;&#21512;&#20808;&#21069;&#20851;&#20110;&#20108;&#27425;&#26041;&#24046;&#26465;&#20214;&#30340;&#24037;&#20316;&#65292;&#36825;&#30452;&#25509;&#26263;&#31034;&#20102;&#22312;&#20351;&#29992;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;BBVI&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29616;&#26377;&#23545;&#20110;&#27491;&#24120;&#23553;&#38381;&#24418;&#24335;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#19982;STL&#20272;&#35745;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20026;&#20004;&#32773;&#25552;&#20379;&#26126;&#30830;&#30340;&#38750;&#28176;&#36827;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09222</link><description>&lt;p&gt;
&#38543;&#26426;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09222
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;f-&#25955;&#24230;&#30340;&#21551;&#21457;&#65292;&#24050;&#30693;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#25913;&#36827;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;DomainBed&#21644;Tabular&#20998;&#31867;&#22522;&#20934;&#19978;&#20998;&#21035;&#27604;&#29616;&#26377;&#26368;&#20339;&#32467;&#26524;&#25552;&#21319;&#20102;0.7%&#21644;1.44%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;BERT&#22312;GLUE&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.94%&#65292;&#23558;ViT&#22312;ImageNet-1K&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.01%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#39044;&#31034;&#30528;&#23427;&#22312;&#25913;&#21892;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AS-GAN&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#20013;AS-GAN&#26377;&#25928;&#22320;&#23398;&#20064;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;&#22312;&#32447;&#21046;&#36896;&#31995;&#32479;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.06268</link><description>&lt;p&gt;
AS-GAN&#22686;&#24378;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#21046;&#36896;&#31995;&#32479;&#22312;&#32447;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System. (arXiv:2306.06268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AS-GAN&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#20013;AS-GAN&#26377;&#25928;&#22320;&#23398;&#20064;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#29992;&#20110;&#22312;&#32447;&#21046;&#36896;&#31995;&#32479;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20808;&#36827;&#21046;&#36896;&#31995;&#32479;&#30340;&#22312;&#32447;&#24863;&#30693;&#30417;&#27979;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#24120;&#26465;&#20214;&#19979;&#25910;&#38598;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#36825;&#20250;&#23548;&#33268;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20005;&#37325;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#26469;&#22686;&#21152;&#21487;&#29992;&#30340;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#65288;&#21363;&#23569;&#25968;&#26679;&#26412;&#65289;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23569;&#25968;&#26679;&#26412;&#65292;&#23398;&#20064;&#24322;&#24120;&#29366;&#24577;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#25104;&#20026;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#20197;&#21450;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#20449;&#21495;&#26159;&#25353;&#26102;&#38388;&#20174;&#21046;&#36896;&#31995;&#32479;&#20013;&#39034;&#24207;&#25910;&#38598;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#32771;&#34385;&#21040;&#26102;&#38388;&#30340;&#39034;&#24207;&#24615;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has been extensively adopted for the online sensing-based monitoring in advanced manufacturing systems. However, the sensor data collected under abnormal states are usually insufficient, leading to significant data imbalanced issue for supervised machine learning. A common solution for this issue is to incorporate data augmentation technique, i.e., augmenting the available abnormal states data (i.e., minority samples) via synthetic generation. To generate the high-quality minority samples effectively, it is vital to learn the underlying distribution of the abnormal states data. In recent years, the generative adversarial network (GAN)-based approaches become popular to learn data distribution as well as perform data augmentation. However, in practice, the quality of generated samples from GAN-based data augmentation may vary drastically. In addition, the sensor signals are collected sequentially by time from the manufacturing systems, which means the consideration
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06210</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#32456;&#23618;&#21453;&#28436;&#36827;&#34892;&#21333;&#27169;&#22411;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23454;&#29992;&#21333;&#27169;&#22411;&#24402;&#22240;&#30340;&#20852;&#36259;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#19968;&#20010;&#26679;&#26412;&#26159;&#30001;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#19981;&#26159;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#35777;&#26126;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FLIPAD&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#21033;&#29992;&#30340;&#26368;&#32456;&#23618;&#21453;&#28436;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#30340; Lasso &#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#29702;&#35770;&#32467;&#26524;&#36824;&#24471;&#21040;&#20102;&#23454;&#39564;&#30740;&#31350;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#26412;&#25991;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05843</link><description>&lt;p&gt;
&#26080;&#39046;&#22495;&#20559;&#35265;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#31215;&#20998;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via Bayesian Quadrature. (arXiv:2306.05843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#26410;&#30693;&#32422;&#26463;&#20197;&#21450;&#26597;&#35810;&#25298;&#32477;&#38382;&#39064;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#20855;&#26377;&#22810;&#26679;&#30340;&#32422;&#26463;&#26465;&#20214;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#12289;&#39640;&#24230;&#21487;&#24182;&#34892;&#21270;&#31561;&#29305;&#28857;&#12290;&#21516;&#26102;&#65292;&#24403;&#23384;&#22312;&#26410;&#30693;&#32422;&#26463;&#26102;&#65292;&#20363;&#22914;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#21160;&#29289;&#23454;&#39564;&#23433;&#20840;&#24615;&#31561;&#39046;&#22495;&#65292;&#24517;&#39035;&#30830;&#31435;&#26410;&#30693;&#32422;&#26463;&#20043;&#21518;&#25165;&#33021;&#26597;&#35810;&#30446;&#26631;&#20989;&#25968;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20165;&#38024;&#23545;&#19978;&#36848;&#26576;&#20123;&#29305;&#24449;&#32780;&#24182;&#38750;&#32508;&#21512;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;cSOBER&#65292;&#19968;&#31181;&#22522;&#20110;SOBER&#31639;&#27861;&#30340;&#39046;&#22495;&#26080;&#20851;&#22411;&#35880;&#24910;&#24182;&#34892;&#20027;&#21160;&#37319;&#26679;&#22120;&#65292;&#32771;&#34385;&#21040;&#20102;&#26410;&#30693;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#38598;&#25104;&#35823;&#24046;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#26041;&#27861;&#65292;&#22788;&#29702;&#22810;&#31181;&#32422;&#26463;&#26465;&#20214;&#21644;&#26410;&#30693;&#32422;&#26463;&#26597;&#35810;&#25298;&#32477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world optimisation problems often feature complex combinations of (1) diverse constraints, (2) discrete and mixed spaces, and are (3) highly parallelisable. (4) There are also cases where the objective function cannot be queried if unknown constraints are not satisfied, e.g. in drug discovery, safety on animal experiments (unknown constraints) must be established before human clinical trials (querying objective function) may proceed. However, most existing works target each of the above three problems in isolation and do not consider (4) unknown constraints with query rejection. For problems with diverse constraints and/or unconventional input spaces, it is difficult to apply these techniques as they are often mutually incompatible. We propose cSOBER, a domain-agnostic prudent parallel active sampler for Bayesian optimisation, based on SOBER of Adachi et al. (2023). We consider infeasibility under unknown constraints as a type of integration error that we can estimate. We propose 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05587</link><description>&lt;p&gt;
MC-NN&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types. (arXiv:2306.05587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#30340;&#26041;&#27861;&#65292;&#25968;&#25454;&#26174;&#31034;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#23545;&#20844;&#20849;&#21355;&#29983;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#23545;&#32769;&#24180;&#20154;&#12289;&#20799;&#31461;&#21644;&#24739;&#26377;&#28508;&#22312;&#30142;&#30149;&#30340;&#20154;&#26469;&#35828;&#26356;&#20026;&#20005;&#37325;&#12290;&#20005;&#37325;&#30149;&#20917;&#30340;&#21457;&#29983;&#65292;&#22914;&#32954;&#28814;&#65292;&#20984;&#26174;&#20102;&#39044;&#38450;&#27969;&#24863;&#20256;&#25773;&#30340;&#37325;&#35201;&#24615;&#12290;&#20934;&#30830;&#32780;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#23545;&#20110;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#34880;&#20957;&#32032;&#21644;&#31070;&#32463;&#27688;&#37240;&#37238;&#34507;&#30333;&#24207;&#21015;&#39044;&#27979;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22312;&#19968;&#20010;&#23436;&#25972;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#22312;&#21508;&#31181;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#24207;&#21015;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26469;&#33258;&#23436;&#25972;&#21644;&#37096;&#20998;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#27969;&#24863;A&#30149;&#27602;&#30340;&#23487;&#20027;&#21644;&#25239;&#21407;&#20122;&#22411;&#20855;&#26377;&#28508;&#21147;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>V2Meow&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;O(100K)&#38899;&#39057;&#29255;&#27573;&#37197;&#23545;&#30340;&#35270;&#39057;&#24103;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#36755;&#20837;&#30340;&#35270;&#35273;&#35821;&#20041;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#39057;&#65292;&#26080;&#38656;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06594</link><description>&lt;p&gt;
V2Meow: &#36890;&#36807;&#38899;&#20048;&#29983;&#25104;&#22120;&#36319;&#38543;&#35270;&#35273;&#33410;&#25293;&#36827;&#34892;&#8220;&#21941;&#21483;&#8221;(arXiv:2305.06594v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
V2Meow: Meowing to the Visual Beat via Music Generation. (arXiv:2305.06594v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06594
&lt;/p&gt;
&lt;p&gt;
V2Meow&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;O(100K)&#38899;&#39057;&#29255;&#27573;&#37197;&#23545;&#30340;&#35270;&#39057;&#24103;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#36755;&#20837;&#30340;&#35270;&#35273;&#35821;&#20041;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#39057;&#65292;&#26080;&#38656;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#19982;&#35270;&#39057;&#35270;&#35273;&#20869;&#23481;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#20048;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35270;&#35273;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#65292;&#22914;MIDI&#25991;&#20214;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;&#38899;&#39057;&#27874;&#24418;&#12290;&#32771;&#34385;&#21040;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#20026;&#23569;&#25968;&#20048;&#22120;&#25110;&#29305;&#23450;&#31867;&#22411;&#30340;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#38899;&#20048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;V2Meow&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#39057;&#36755;&#20837;&#30340;&#35270;&#35273;&#35821;&#20041;&#30456;&#21305;&#37197;&#30340;&#39640;&#36136;&#37327;&#38899;&#39057;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23427;&#26159;&#36890;&#36807;&#19982;&#20174;&#37326;&#29983;&#38899;&#20048;&#35270;&#39057;&#20013;&#25366;&#25496;&#30340;O(100K)&#38899;&#20048;&#38899;&#39057;&#29255;&#27573;&#37197;&#23545;&#30340;&#35270;&#39057;&#24103;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#27809;&#26377;&#28041;&#21450;&#20219;&#20309;&#24182;&#34892;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#12290;V2Meow&#33021;&#22815;&#20165;&#22312;&#20808;&#21069;&#35757;&#32451;&#30340;&#20174;&#20219;&#24847;&#38745;&#24577;&#35270;&#39057;&#25552;&#21462;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#26465;&#20214;&#19979;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#38899;&#39057;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating high quality music that complements the visual content of a video is a challenging task. Most existing visual conditioned music generation systems generate symbolic music data, such as MIDI files, instead of raw audio waveform. Given the limited availability of symbolic music data, such methods can only generate music for a few instruments or for specific types of visual input. In this paper, we propose a novel approach called V2Meow that can generate high-quality music audio that aligns well with the visual semantics of a diverse range of video input types. Specifically, the proposed music generation system is a multi-stage autoregressive model which is trained with a number of O(100K) music audio clips paired with video frames, which are mined from in-the-wild music videos, and no parallel symbolic music data is involved. V2Meow is able to synthesize high-fidelity music audio waveform solely conditioned on pre-trained visual features extracted from an arbitrary silent vide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.02299</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Training with Structured Sparsity. (arXiv:2305.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31232;&#30095;&#21160;&#24577;&#35757;&#32451;&#65288;DST&#65289;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#65292;&#21487;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#20855;&#26377;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#22312;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#21305;&#37197;&#20102;&#23494;&#38598;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#65292;&#21516;&#26102;&#20351;&#24471;&#31232;&#30095;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#24471;&#21040;&#30340;&#27169;&#22411;&#39640;&#24230;&#31232;&#30095;&#65292;&#29702;&#35770;&#19978;&#35757;&#32451;&#26356;&#20415;&#23452;&#65292;&#20294;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#65292;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#21152;&#36895;&#20381;&#28982;&#20855;&#26377;&#20154;&#20204;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181; DST &#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#31181;&#21464;&#20307;&#30340;&#32467;&#26500;&#21270; N:M &#31232;&#30095;&#24615;&#65292;&#20854;&#21152;&#36895;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36890;&#24120;&#34987;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450; N:M &#31232;&#30095;&#26041;&#27861;&#65288;&#24120;&#25968;&#25159;&#20837;&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#32553;&#20943;&#21442;&#25968;&#21644;&#20869;&#23384;&#21344;&#29992;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#32463;&#36807;&#23545; PyTorch CPU &#23454;&#29616;&#30340;&#31616;&#21333;&#34920;&#31034;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#36739;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20943;&#23569;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/calgaryml/condensed-sparsity &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
DST methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at https://github.com/calgaryml/condensed-sparsity
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#20102;&#20174;&#22797;&#26434;&#31995;&#32479;&#30456;&#20851;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#20840;&#38754;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#30340;&#31526;&#21495;&#21644;&#26415;&#35821;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.10031</link><description>&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26550;&#26500;&#65306;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Architectures of Topological Deep Learning: A Survey on Topological Neural Networks. (arXiv:2304.10031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10031
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#20102;&#20174;&#22797;&#26434;&#31995;&#32479;&#30456;&#20851;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#20840;&#38754;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#30340;&#31526;&#21495;&#21644;&#26415;&#35821;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#20013;&#20805;&#28385;&#20102;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#20854;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65306;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#21040;&#34507;&#30333;&#36136;&#20013;&#21407;&#23376;&#20043;&#38388;&#30340;&#38745;&#30005;&#30456;&#20114;&#20316;&#29992;&#12290;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21644;&#20174;&#36825;&#20123;&#31995;&#32479;&#30456;&#20851;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#22914;&#39044;&#27979;&#19968;&#20010;&#20154;&#23646;&#20110;&#21738;&#20010;&#31038;&#21306;&#25110;&#39044;&#27979;&#19968;&#20010;&#34507;&#30333;&#36136;&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#21512;&#29702;&#30340;&#33647;&#29289;&#24320;&#21457;&#38774;&#28857;&#12290;TDL&#24050;&#32463;&#35777;&#26126;&#25317;&#26377;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#30340;&#20248;&#21183;&#65292;&#36825;&#20026;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;TDL&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#20063;&#23548;&#33268;&#20102;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#20307;&#31995;&#32467;&#26500;&#31526;&#21495;&#21644;&#26415;&#35821;&#19978;&#30340;&#19981;&#19968;&#33268;&#12290;&#36825;&#23545;&#20110;&#24314;&#31435;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#21644;&#23558;TNN&#37096;&#32626;&#21040;&#26032;&#30340;&#29616;&#23454;&#38382;&#39064;&#20013;&#37117;&#26159;&#19968;&#20010;&#30495;&#27491;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#29702;&#35299;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16133</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#35299;&#20915;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#20219;&#21153;&#19981;&#19968;&#33268;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#25928;&#65292;&#20445;&#35777;&#23427;&#20204;&#22312;&#21508;&#33258;&#25903;&#25345;&#30340;&#20219;&#21153;&#20013;&#30340;&#19968;&#33268;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#35748;&#20026;&#19981;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#36755;&#20986;&#30340;&#22823;&#22411;&#31995;&#32479;&#26469;&#35828;&#26159;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#19968;&#33268;&#65292;&#22240;&#27492;&#65292;&#35780;&#20272;&#21487;&#33021;&#21253;&#25324;&#19981;&#21516;&#27169;&#24577;&#36755;&#20986;&#30340;&#38750;&#24120;&#24322;&#26500;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#23567;&#22411;&#20294;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#26469;&#21019;&#24314;&#23545;&#27604;&#38598;&#65292;&#20197;&#26356;&#25913;&#37329;&#26631;&#31614;&#65292;&#24182;&#27010;&#36848;&#20102;&#29992;&#20110;&#36890;&#36807;&#23545;&#27604;&#25509;&#36817;&#21407;&#22987;&#21644;&#20462;&#25913;&#21518;&#30340;&#23454;&#20363;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2302.12906</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Invertible Quantum Neural Networks. (arXiv:2302.12906v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#27169;&#25311;&#21644;&#29983;&#25104;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#38376;&#31639;&#27861;&#29992;&#20110;&#37327;&#23376;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;QINN&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#34928;&#21464;&#20026;&#36731;&#23376;&#30340;Z&#29627;&#33394;&#23376;&#30340;&#21943;&#27880;&#30456;&#20851;&#20135;&#29983;&#30340;LHC&#25968;&#25454;&#65292;&#36825;&#26159;&#31890;&#23376;&#23545;&#25758;&#26426;&#31934;&#23494;&#27979;&#37327;&#30340;&#26631;&#20934;&#36807;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;QINN&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#28151;&#21512;&#30340;QINN&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#19968;&#20010;&#26174;&#33879;&#26356;&#22823;&#30340;&#23436;&#20840;&#32463;&#20856;&#30340;INN&#30340;&#34920;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invertible Neural Networks (INN) have become established tools for the simulation and generation of highly complex data. We propose a quantum-gate algorithm for a Quantum Invertible Neural Network (QINN) and apply it to the LHC data of jet-associated production of a Z-boson that decays into leptons, a standard candle process for particle collider precision measurements. We compare the QINN's performance for different loss functions and training scenarios. For this task, we find that a hybrid QINN matches the performance of a significantly larger purely classical INN in learning and generating complex data.
&lt;/p&gt;</description></item><item><title>&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2302.09656</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09656
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;, &#30830;&#23450;&#19981;&#30830;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#26159;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20351;&#24471;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#33021;&#22815;&#34987;&#35780;&#20272;&#65292;&#19981;&#21516;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;IBNNs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#27010;&#25324;&#21644;&#20811;&#26381;&#26631;&#20934;BNNs&#30340;&#26576;&#20123;&#32570;&#28857;&#12290;&#26631;&#20934;BNNs&#20351;&#29992;&#21333;&#19968;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;IBNNs&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#20204;&#20801;&#35768;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#27604;&#26631;&#20934;BNNs&#26356;&#21152;&#40065;&#26834;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;PAC&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#38598;&#12290;&#25105;&#20204;&#23558;IBNNs&#24212;&#29992;&#20110;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#19968;&#20010;&#26159;&#20026;&#20102;&#20154;&#24037;&#33008;&#33146;&#25511;&#21046;&#27169;&#25311;&#34880;&#31958;&#21644;&#33008;&#23707;&#32032;&#21160;&#21147;&#23398;&#65292;&#21478;&#19968;&#20010;&#26159;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
&lt;/p&gt;</description></item></channel></rss>