<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.01771</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#19968;&#31181;&#26032;&#39062;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#22266;&#26377;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20379;&#39640;&#21487;&#29992;&#21644;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#26381;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#21160;&#20316;&#30001;&#19977;&#20010;&#29305;&#24449;&#21521;&#37327;&#32452;&#25104;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#24352;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#22870;&#21169;&#26159;&#36890;&#36807;&#23558;&#21160;&#20316;&#30340;&#29305;&#24449;&#24352;&#37327;&#19982;&#19968;&#20010;&#22266;&#23450;&#20294;&#26410;&#30693;&#30340;&#21442;&#25968;&#24352;&#37327;&#30340;&#20869;&#31215;&#24212;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#26469;&#30830;&#23450;&#30340;&#65292;&#32780;&#36825;&#20010;&#21442;&#25968;&#24352;&#37327;&#20855;&#26377;&#36739;&#20302;&#30340;&#31649;&#29366;&#31209;&#12290;&#20026;&#20102;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#25506;&#32034;&#23376;&#31354;&#38388;&#28982;&#21518;&#32454;&#21270;&#8221;&#30340;&#26032;&#31639;&#27861;&#65288;G-LowTESTR&#65289;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#65292;&#20197;&#25506;&#32034;&#23884;&#20837;&#22312;&#20915;&#31574;&#24773;&#22659;&#20013;&#30340;&#26412;&#36136;&#20302;&#31209;&#24352;&#37327;&#23376;&#31354;&#38388;&#20449;&#24687;&#65292;&#28982;&#21518;&#23558;&#21407;&#22987;&#27010;&#29575;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
&lt;/p&gt;</description></item><item><title>Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.00136</link><description>&lt;p&gt;
Neuroformer&#65306;&#29992;&#20110;&#33041;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00136
&lt;/p&gt;
&lt;p&gt;
Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#20135;&#29983;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#38656;&#35201;&#26032;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;&#21463;&#21040;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22823;&#35268;&#27169;&#30340;&#32454;&#32990;&#20998;&#36776;&#29575;&#31070;&#32463;&#20803;&#23574;&#23792;&#25968;&#25454;&#30340;&#20998;&#26512;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#26102;&#31354;&#29983;&#25104;&#38382;&#39064;&#12290;Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;transformer&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#19987;&#20026;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#12290;&#23427;&#19982;&#29305;&#24449;&#22823;&#23567;&#21576;&#32447;&#24615;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#65292;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#39044;&#27979;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;Neuroformer&#65292;&#24182;&#21457;&#29616;&#23427;&#26082;&#33021;&#20934;&#30830;&#39044;&#27979;&#27169;&#25311;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#65292;&#20063;&#33021;&#20869;&#22312;&#22320;&#25512;&#26029;&#20986;&#24213;&#23618;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21253;&#25324;&#26041;&#21521;&#12290;&#24403;&#39044;&#35757;&#32451;&#29992;&#20110;&#35299;&#30721;&#31070;&#32463;&#21709;&#24212;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#39044;&#27979;&#23567;&#40736;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20302;&#20809;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#21033;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#21644;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#36716;&#25442;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#23433;&#38450;&#21644;&#30417;&#25511;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.20190</link><description>&lt;p&gt;
&#20302;&#20809;&#26465;&#20214;&#19979;&#25913;&#21892;&#35270;&#35273;&#20219;&#21153;&#30340;&#21487;&#35265;&#20809;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Visible to Thermal image Translation for improving visual task in low light conditions. (arXiv:2310.20190v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#20302;&#20809;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#21033;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#21644;&#26816;&#27979;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21487;&#35265;&#20809;&#22270;&#20687;&#21040;&#28909;&#32418;&#22806;&#22270;&#20687;&#30340;&#36716;&#25442;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#23433;&#38450;&#21644;&#30417;&#25511;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#20809;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35265;&#20809;&#22270;&#20687;&#24456;&#38590;&#23436;&#25104;&#19968;&#20123;&#35270;&#35273;&#20219;&#21153;&#65292;&#22914;&#34892;&#20154;&#26816;&#27979;&#21644;&#22270;&#20687;&#32763;&#35793;&#12290;&#28909;&#32418;&#22806;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#28909;&#21464;&#21270;&#21487;&#20197;&#29992;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#21644;&#19968;&#20010;&#26816;&#27979;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#21487;&#35265;&#20809;&#22270;&#20687;&#32763;&#35793;&#25104;&#28909;&#32418;&#22806;&#22270;&#20687;&#65292;&#24182;&#23558;&#29983;&#25104;&#30340;&#28909;&#32418;&#22806;&#22270;&#20687;&#19982;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;Parrot Anafi Thermal&#26080;&#20154;&#26426;&#22312;&#20004;&#20010;&#19981;&#21516;&#20301;&#32622;&#25910;&#38598;&#20102;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;&#32593;&#32476;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20102;&#39044;&#22788;&#29702;&#12289;&#22686;&#24378;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GAN&#23558;&#21487;&#35265;&#20809;&#35757;&#32451;&#25968;&#25454;&#36716;&#25442;&#20026;&#28909;&#32418;&#22806;&#25968;&#25454;&#26159;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#29616;&#22312;&#21487;&#20197;&#26356;&#24555;&#36895;&#12289;&#26356;&#32463;&#27982;&#22320;&#29983;&#25104;&#28909;&#32418;&#22806;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#23433;&#38450;&#21644;&#30417;&#25511;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several visual tasks, such as pedestrian detection and image-to-image translation, are challenging to accomplish in low light using RGB images. Heat variation of objects in thermal images can be used to overcome this. In this work, an end-to-end framework, which consists of a generative network and a detector network, is proposed to translate RGB image into Thermal ones and compare generated thermal images with real data. We have collected images from two different locations using the Parrot Anafi Thermal drone. After that, we created a two-stream network, preprocessed, augmented, the image data, and trained the generator and discriminator models from scratch. The findings demonstrate that it is feasible to translate RGB training data to thermal data using GAN. As a result, thermal data can now be produced more quickly and affordably, which is useful for security and surveillance applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#25552;&#39640;SR&#24615;&#33021;&#65292;&#25913;&#21892;MRI&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15767</link><description>&lt;p&gt;
&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#19982;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning. (arXiv:2310.15767v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#25552;&#39640;SR&#24615;&#33021;&#65292;&#25913;&#21892;MRI&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;MRI&#20998;&#36776;&#29575;&#30340;&#22266;&#26377;&#38480;&#21046;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#26041;&#27861;&#23637;&#29616;&#20102;&#25552;&#21319;MRI&#20998;&#36776;&#29575;&#30340;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;HR MRI&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#21487;&#33021;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#37197;&#23545;MRI SR&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19979;&#30340;SR&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#30495;&#23454;&#30340;HR&#22270;&#20687;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;SR&#22270;&#20687;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#20419;&#36827;&#36776;&#21035;&#24615;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#21576;&#29616;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#21363;&#20351;&#32570;&#20047;HR&#22270;&#20687;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution (HR) magnetic resonance imaging (MRI) is crucial for enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent limitation of MRI resolution restricts its widespread applicability. Deep learning-based image super-resolution (SR) methods exhibit promise in improving MRI resolution without additional cost. However, these methods frequently require a substantial number of HR MRI images for training, which can be challenging to acquire. In this paper, we propose an unpaired MRI SR approach that employs self-supervised contrastive learning to enhance SR performance with limited training data. Our approach leverages both authentic HR images and synthetically generated SR images to construct positive and negative sample pairs, thus facilitating the learning of discriminative features. Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR image
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#26041;&#24335;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#39640;&#20102;&#32593;&#32476;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13236</link><description>&lt;p&gt;
&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Training A Semantic Communication System with Federated Learning. (arXiv:2310.13236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#26041;&#24335;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#39640;&#20102;&#32593;&#32476;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#22240;&#20854;&#20943;&#23569;&#25968;&#25454;&#20887;&#20313;&#30340;&#21151;&#33021;&#25104;&#20026;&#19979;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#30340;&#37325;&#35201;&#25903;&#26609;&#12290;&#22823;&#22810;&#25968;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#20551;&#35774;&#26377;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#25968;&#25454;&#20027;&#35201;&#26159;&#30001;&#29992;&#25143;&#29983;&#25104;&#30340;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#25968;&#25454;&#20256;&#36755;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#23545;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#26696;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#20013;&#25506;&#32034;&#35821;&#20041;&#36890;&#20449;&#65292;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#35299;&#20915;&#36890;&#20449;&#24320;&#38144;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#20256;&#36865;&#30340;&#20449;&#24687;&#37327;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#33410;&#30465;&#22823;&#37327;&#24102;&#23485;&#65292;&#20943;&#23569;&#25972;&#20307;&#32593;&#32476;&#27969;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Semantic communication has emerged as a pillar for the next generation of communication systems due to its capabilities in alleviating data redundancy. Most semantic communication systems are built using advanced deep learning models whose performance heavily depends on data availability. These studies assume that an abundance of training data is available, which is unrealistic. In practice, data is mainly created on the user side. Due to privacy and security concerns, the transmission of data is restricted, which is necessary for conventional centralized training schemes. To address this challenge, we explore semantic communication in federated learning (FL) setting that utilizes user data without leaking privacy. Additionally, we design our system to tackle the communication overhead by reducing the quantity of information delivered in each global round. In this way, we can save significant bandwidth for resource-limited devices and reduce overall network traffic. Finally, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.12802</link><description>&lt;p&gt;
&#19968;&#31181;&#38598;&#20307;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
An effective theory of collective deep learning. (arXiv:2310.12802v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12802
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#32806;&#21512;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#38598;&#20307;&#23398;&#20064;&#30340;&#20986;&#29616;&#26159;&#23545;&#29289;&#29702;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#31038;&#20250;&#23398;&#30340;&#24191;&#27867;&#24433;&#21709;&#30340;&#19968;&#39033;&#21162;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#20010;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#21160;&#24577;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#20043;&#38388;&#30340;&#31454;&#20105;&#65292;&#23558;&#20960;&#20010;&#26368;&#36817;&#30340;&#20998;&#25955;&#31639;&#27861;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#19982;&#20855;&#26377;&#28140;&#28781;&#38543;&#26426;&#24615;&#30340;Ginzburg-Landau&#27169;&#22411;&#31867;&#20284;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#26377;&#25928;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#31895;&#31890;&#21270;&#34892;&#20026;&#12290;&#36825;&#20010;&#26694;&#26550;&#39044;&#27979;&#20102;&#21442;&#25968;&#35299;&#30340;&#65288;&#28145;&#24230;&#20381;&#36182;&#30340;&#65289;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#25581;&#31034;&#20102;&#38598;&#20307;&#23398;&#20064;&#30456;&#30340;&#24320;&#22987;&#65292;&#20197;&#21450;&#28145;&#24230;&#24341;&#36215;&#30340;&#20020;&#30028;&#28857;&#24310;&#36831;&#21644;&#24494;&#35266;&#23398;&#20064;&#36335;&#24452;&#30340;&#40065;&#26834;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unraveling the emergence of collective learning in systems of coupled artificial neural networks is an endeavor with broader implications for physics, machine learning, neuroscience and society. Here we introduce a minimal model that condenses several recent decentralized algorithms by considering a competition between two terms: the local learning dynamics in the parameters of each neural network unit, and a diffusive coupling among units that tends to homogenize the parameters of the ensemble. We derive the coarse-grained behavior of our model via an effective theory for linear networks that we show is analogous to a deformed Ginzburg-Landau model with quenched disorder. This framework predicts (depth-dependent) disorder-order-disorder phase transitions in the parameters' solutions that reveal the onset of a collective learning phase, along with a depth-induced delay of the critical point and a robust shape of the microscopic learning path. We validate our theory in realistic ensembl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#22240;&#26524;&#27169;&#22411;&#22312;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36731;&#24494;&#25552;&#39640;&#20102;&#25972;&#20307;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12421</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26816;&#27979;&#21644;&#20943;&#36731;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#31639;&#27861;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling. (arXiv:2310.12421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#22240;&#26524;&#27169;&#22411;&#22312;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36731;&#24494;&#25552;&#39640;&#20102;&#25972;&#20307;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#12290;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#22240;&#26524;&#24314;&#27169;&#30340;&#27010;&#24565;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;UC Irvine&#26426;&#22120;&#23398;&#20064;&#24211;&#20013;&#21487;&#19979;&#36733;&#30340;&#25104;&#24180;&#20154;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#24320;&#21457;&#20102;&#65288;1&#65289;&#19968;&#20010;&#34987;&#35270;&#20026;&#40657;&#31665;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#65288;2&#65289;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#20559;&#35265;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#21644;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#27979;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#65288;p&lt;0.05&#65289;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23637;&#31034;&#20102;&#22240;&#26524;&#27169;&#22411;&#22312;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25972;&#20307;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#36731;&#24494;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#30452;&#35266;&#26131;&#25026;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#32479;&#35745;&#36719;&#20214;&#24037;&#20855;&#65288;&#22914;R&#20013;&#30340;&#8220;lavaan&#8221;&#65289;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#24182;&#20419;&#36827;&#20102;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as "lavaan" in R. Hence, it enhances explainability and promotes trust.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#37319;&#26679;&#22120;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36817;&#20284;&#22797;&#26434;&#30340;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#32447;&#24615;&#21270;&#30340;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39069;&#22806;&#30340;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#23545;&#25239;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.06643</link><description>&lt;p&gt;
&#39640;&#32500;&#21518;&#39564;&#25512;&#26029;&#30340;&#38544;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Implicit Variational Inference for High-Dimensional Posteriors. (arXiv:2310.06643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#37319;&#26679;&#22120;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36817;&#20284;&#22797;&#26434;&#30340;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#32447;&#24615;&#21270;&#30340;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39069;&#22806;&#30340;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#23545;&#25239;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#65292;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#22909;&#22788;&#22312;&#20110;&#20934;&#30830;&#25429;&#25417;&#30495;&#23454;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#30340;&#31070;&#32463;&#37319;&#26679;&#22120;&#65292;&#36825;&#23545;&#20110;&#36817;&#20284;&#39640;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#38750;&#24120;&#36866;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#21270;&#31070;&#32463;&#37319;&#26679;&#22120;&#24341;&#20837;&#26032;&#30340;&#32422;&#26463;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#37492;&#21035;&#22120;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#35299;&#20915;&#35745;&#31639;&#19978;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#24674;&#22797;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#32593;&#32476;&#24615;&#33021;&#20851;&#38190;&#20294;&#33261;&#21517;&#26157;&#33879;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notorious
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#65288;ARC&#65289;&#26469;&#23450;&#20041;&#36866;&#24403;&#30340;&#23545;&#20934;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#22312;WiFi&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;CSI&#25968;&#25454;&#19978;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06328</link><description>&lt;p&gt;
&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#23450;&#20041;CSI&#25968;&#25454;&#30340;&#23545;&#20934;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Exploit the antenna response consistency to define the alignment criteria for CSI data. (arXiv:2310.06328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22825;&#32447;&#21709;&#24212;&#19968;&#33268;&#24615;&#65288;ARC&#65289;&#26469;&#23450;&#20041;&#36866;&#24403;&#30340;&#23545;&#20934;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#22312;WiFi&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;CSI&#25968;&#25454;&#19978;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#29992;&#20110;&#22522;&#20110;WiFi&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30001;&#20110;&#33021;&#22815;&#35299;&#20915;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#32780;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;SSL&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#27604;&#23398;&#20064;&#65292;&#31227;&#26893;&#21040;CSI&#25968;&#25454;&#19978;&#24448;&#24448;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#23545;&#20934;&#26631;&#20934;&#19981;&#24403;&#65292;&#36825;&#30772;&#22351;&#20102;&#29305;&#24449;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;``Anetenna Response Consistency (ARC)''&#20316;&#20026;&#23450;&#20041;&#21512;&#36866;&#23545;&#20934;&#26631;&#20934;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;ARC&#30340;&#35774;&#35745;&#22312;&#20445;&#30041;&#36755;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#23545;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20174;CSI&#25968;&#25454;&#32467;&#26500;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;ARC&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26368;&#20248;&#35299;&#23548;&#33268;&#20102;&#20174;&#36755;&#20837;CSI&#25968;&#25454;&#21040;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#21160;&#20316;&#21521;&#37327;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \textbf{A}netenna \textbf{R}esponse \textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;I&#178;F&#65292;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#65292;&#24182;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.13016</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#28436;&#24433;&#21709;&#20989;&#25968;&#29702;&#35299;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Gradient Leakage via Inversion Influence Functions. (arXiv:2309.13016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;I&#178;F&#65292;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#65292;&#24182;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#65288;DGL&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26799;&#24230;&#21521;&#37327;&#20013;&#24674;&#22797;&#31169;&#26377;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20855;&#26377;&#25935;&#24863;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#20998;&#24067;&#24335;&#23398;&#20064;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#25361;&#25112;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#38656;&#35201;&#20849;&#20139;&#26799;&#24230;&#12290;&#38450;&#24481;&#27492;&#31867;&#25915;&#20987;&#38656;&#35201;&#20294;&#32570;&#20047;&#23545;&#38544;&#31169;&#27844;&#38706;&#21457;&#29983;&#30340;&#26102;&#38388;&#21644;&#26041;&#24335;&#30340;&#29702;&#35299;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#28436;&#24433;&#21709;&#20989;&#25968;&#65288;I&#178;F&#65289;&#65292;&#36890;&#36807;&#38544;&#24335;&#35299;&#20915;DGL&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#38381;&#24335;&#36830;&#25509;&#12290;&#19982;&#30452;&#25509;&#35299;&#20915;DGL&#30456;&#27604;&#65292;I&#178;F&#22312;&#20998;&#26512;&#28145;&#24230;&#32593;&#32476;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20165;&#38656;&#35201;&#26799;&#24230;&#21644;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#30340;&#39044;&#35328;&#35775;&#38382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;I&#178;F&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#12289;&#25915;&#20987;&#23454;&#29616;&#21644;&#22522;&#20110;&#22122;&#22768;&#30340;&#38450;&#24481;&#20013;&#37117;&#33021;&#26377;&#25928;&#36817;&#20284;DGL&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20102;&#35299;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#30340;&#26426;&#29702;&#21644;&#24212;&#23545;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;(ATMS)&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20132;&#26131;&#27963;&#21160;&#30340;&#24207;&#21015;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21322;&#30495;&#23454;&#24066;&#22330;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01784</link><description>&lt;p&gt;
ATMS: &#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
ATMS: Algorithmic Trading-Guided Market Simulation. (arXiv:2309.01784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;(ATMS)&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20132;&#26131;&#27963;&#21160;&#30340;&#24207;&#21015;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21322;&#30495;&#23454;&#24066;&#22330;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#26500;&#24314;&#31639;&#27861;&#20132;&#26131;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#20132;&#26131;&#27963;&#21160;&#30340;&#24207;&#21015;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#34913;&#37327;&#24066;&#22330;&#24046;&#24322;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#36890;&#36807;&#31639;&#27861;&#20132;&#26131;&#20195;&#29702;&#21644;&#24066;&#22330;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35780;&#20272;&#24213;&#23618;&#24066;&#22330;&#30340;&#22240;&#26524;&#25928;&#24212;&#24046;&#24322;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;(ATMS)&#65292;&#36890;&#36807;&#20248;&#21270;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#21463;SeqGAN&#30340;&#21551;&#21457;&#65292;ATMS&#23558;&#27169;&#25311;&#22120;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20197;&#32771;&#34385;&#20132;&#26131;&#30340;&#24207;&#21015;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;ATMS&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#26469;&#32469;&#36807;&#23545;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#24494;&#20998;&#65292;&#36825;&#28041;&#21450;&#21040;&#38750;&#21487;&#24494;&#20998;&#25805;&#20316;&#65292;&#22914;&#20174;&#24066;&#22330;&#20013;&#21024;&#38500;&#35746;&#21333;&#12290;&#36890;&#36807;&#22312;&#21322;&#30495;&#23454;&#24066;&#22330;&#19978;&#36827;&#34892;&#22823;&#37327;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
The effective construction of an Algorithmic Trading (AT) strategy often relies on market simulators, which remains challenging due to existing methods' inability to adapt to the sequential and dynamic nature of trading activities. This work fills this gap by proposing a metric to quantify market discrepancy. This metric measures the difference between a causal effect from underlying market unique characteristics and it is evaluated through the interaction between the AT agent and the market. Most importantly, we introduce Algorithmic Trading-guided Market Simulation (ATMS) by optimizing our proposed metric. Inspired by SeqGAN, ATMS formulates the simulator as a stochastic policy in reinforcement learning (RL) to account for the sequential nature of trading. Moreover, ATMS utilizes the policy gradient update to bypass differentiating the proposed metric, which involves non-differentiable operations such as order deletion from the market. Through extensive experiments on semi-real marke
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.00608</link><description>&lt;p&gt;
Copiloting the Copilots: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23436;&#25104;&#24341;&#25806;&#34701;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00608
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#65292;&#23545;&#20110;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#23454;&#38469;&#31995;&#32479;&#21512;&#25104;&#27491;&#30830;&#30340;&#20462;&#34917;&#31243;&#24207;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#24320;&#21457;&#20154;&#21592;&#22312;&#21508;&#31181;&#32534;&#30721;&#20219;&#21153;&#20013;&#20855;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#24050;&#30452;&#25509;&#24212;&#29992;&#20110;&#20462;&#34917;&#31243;&#24207;&#30340;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#23558;&#31243;&#24207;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#30446;&#26631;&#32534;&#31243;&#35821;&#35328;&#30340;&#24213;&#23618;&#35821;&#20041;&#32422;&#26463;&#19968;&#26080;&#25152;&#30693;&#12290;&#36825;&#23548;&#33268;&#29983;&#25104;&#20102;&#22823;&#37327;&#38745;&#24577;&#26080;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#65292;&#38459;&#30861;&#20102;&#35813;&#25216;&#26415;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Repilot&#65292;&#19968;&#31181;&#22312;&#20462;&#22797;&#36807;&#31243;&#20013;&#36890;&#36807;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#20462;&#34917;&#31243;&#24207;&#20174;&#32780;&#36827;&#19968;&#27493;&#25903;&#25345;AI&#8220;&#21103;&#39550;&#39542;&#21592;&#8221;&#65288;&#21363;LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#35768;&#22810;LLMs&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#36755;&#20986;&#65288;&#21363;&#36880;&#20010;&#20196;&#29260;&#29983;&#25104;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#32534;&#20889;&#31243;&#24207;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23436;&#25104;&#24341;&#25806;&#26174;&#33879;&#25552;&#21319;&#21644;&#24341;&#23548;&#12290;Repilot&#21327;&#21516;&#21512;&#25104;&#20102;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14991</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24341;&#20837;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#36171;&#20104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#24378;&#22823;&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#19968;&#20010;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#22312;&#35760;&#24518;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21487;&#22609;&#24615;&#20043;&#38388;&#20445;&#25345;&#36866;&#24403;&#24179;&#34913;&#65292;&#24182;&#33719;&#24471;&#36275;&#22815;&#30340;&#20860;&#23481;&#24615;&#26469;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20445;&#25345;&#35760;&#24518;&#31283;&#23450;&#24615;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20173;&#38590;&#20197;&#20687;&#29983;&#29289;&#26234;&#33021;&#65288;BI&#65289;&#37027;&#26679;&#28789;&#27963;&#22320;&#36866;&#24212;&#22686;&#37327;&#21464;&#21270;&#12290;&#36890;&#36807;&#24314;&#27169;&#19968;&#20010;&#33021;&#22815;&#20027;&#21160;&#35843;&#33410;&#36951;&#24536;&#30340;&#31283;&#20581;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#23398;&#20064;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#20998;&#24067;&#20013;&#36866;&#24403;&#34928;&#20943;&#26087;&#35760;&#24518;&#26469;&#25913;&#21892;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#21327;&#35843;&#22810;&#23398;&#20064;&#32773;&#26550;&#26500;&#26469;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#25552;&#39640;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31361;&#35302;&#35843;&#33410;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning aims to empower artificial intelligence (AI) with strong adaptability to the real world. For this purpose, a desirable solution should properly balance memory stability with learning plasticity, and acquire sufficient compatibility to capture the observed distributions. Existing advances mainly focus on preserving memory stability to overcome catastrophic forgetting, but remain difficult to flexibly accommodate incremental changes as biological intelligence (BI) does. By modeling a robust Drosophila learning system that actively regulates forgetting with multiple learning modules, here we propose a generic approach that appropriately attenuates old memories in parameter distributions to improve learning plasticity, and accordingly coordinates a multi-learner architecture to ensure solution compatibility. Through extensive theoretical and empirical validation, our approach not only clearly enhances the performance of continual learning, especially over synaptic regula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05061</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#26041;&#31243;&#21644;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#21644;&#35821;&#35328;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#32780;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#21644;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#22312;&#25512;&#29702;&#38454;&#27573;&#20174;&#25552;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#36816;&#31639;&#31526;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27169;&#22411;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#36816;&#31639;&#31526;&#30340;&#23453;&#36149;&#30340;&#20154;&#31867;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31181;&#22810;&#27169;&#24335;&#33539;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#8220;&#26631;&#39064;&#8221;&#26469;&#25972;&#21512;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#26041;&#31243;&#24335;&#34920;&#36798;&#30340;&#36816;&#31639;&#31526;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25193;&#23637;&#20102;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#26222;&#36941;&#24615;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#19978;&#19979;&#25991;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;ICON-LM&#8221;&#65292;&#22522;&#20110;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#24179;&#22343;&#30340;&#21333;&#24490;&#29615;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#22120;&#65288;SOBOW&#65289;&#65292;&#23427;&#33021;&#22312;&#22788;&#29702;&#20989;&#25968;&#21464;&#21270;&#21644;&#30495;&#23454;&#36229;&#26799;&#24230;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.03811</link><description>&lt;p&gt;
&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#21644;&#26102;&#21464;&#30446;&#26631;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Bilevel Optimization with Time-Varying Objective Functions. (arXiv:2308.03811v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#24179;&#22343;&#30340;&#21333;&#24490;&#29615;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#22120;&#65288;SOBOW&#65289;&#65292;&#23427;&#33021;&#22312;&#22788;&#29702;&#20989;&#25968;&#21464;&#21270;&#21644;&#30495;&#23454;&#36229;&#26799;&#24230;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#26356;&#26032;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#24378;&#22823;&#24037;&#20855;&#65292;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#32771;&#34385;&#30340;&#26159;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#38745;&#24577;&#20989;&#25968;&#65292;&#22312;&#26032;&#20852;&#30340;&#22312;&#32447;&#24212;&#29992;&#20013;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#21644;&#26102;&#21464;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#65288;OBO&#65289;&#65292;&#20854;&#20013;&#20989;&#25968;&#21487;&#20197;&#26102;&#21464;&#65292;&#24182;&#19988;&#20195;&#29702;&#19981;&#26029;&#26681;&#25454;&#22312;&#32447;&#27969;&#25968;&#25454;&#26356;&#26032;&#20915;&#31574;&#12290;&#20026;&#20102;&#22788;&#29702;OBO&#20013;&#30340;&#20989;&#25968;&#21464;&#21270;&#21644;&#30495;&#23454;&#36229;&#26799;&#24230;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#24179;&#22343;&#30340;&#21333;&#24490;&#29615;&#22312;&#32447;&#21452;&#23618;&#20248;&#21270;&#22120;&#65288;SOBOW&#65289;&#65292;&#23427;&#26681;&#25454;&#20869;&#23618;&#31383;&#21475;&#24179;&#22343;&#30340;&#26368;&#36817;&#36229;&#26799;&#24230;&#20272;&#35745;&#20540;&#26469;&#26356;&#26032;&#22806;&#23618;&#30340;&#20915;&#31574;&#12290;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#65292;SOBOW&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#20808;&#21069;&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#22788;&#29702;&#21333;&#24490;&#29615;&#26356;&#26032;&#21644;&#20989;&#25968;&#21464;&#21270;&#24102;&#26469;&#30340;&#29420;&#29305;&#25216;&#26415;&#22256;&#38590;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#22686;&#21152;&#31232;&#30095;&#24615;&#20197;&#21450;&#32467;&#26500;&#21644;&#20840;&#23616;&#20449;&#24687;&#26469;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27867;&#21270;&#24615;&#30340;&#22330;&#35770;&#24418;&#24335;&#20307;&#31995;&#65292;&#29992;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#38544;&#34255;&#23618;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#38416;&#26126;&#20102;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23545;&#32593;&#32476;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.16695</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36125;&#21494;&#26031;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#21464;&#24322;&#24615;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A theory of data variability in Neural Network Bayesian inference. (arXiv:2307.16695v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27867;&#21270;&#24615;&#30340;&#22330;&#35770;&#24418;&#24335;&#20307;&#31995;&#65292;&#29992;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#38544;&#34255;&#23618;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#38416;&#26126;&#20102;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23545;&#32593;&#32476;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#26680;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#36890;&#36807;&#20351;&#29992;&#26680;&#21644;&#25512;&#29702;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#38544;&#34255;&#23618;&#30340;&#26497;&#38480;&#24773;&#20917;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#22312;&#36825;&#20010;&#26497;&#38480;&#30340;&#22522;&#30784;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#22330;&#35770;&#24418;&#24335;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#26080;&#38480;&#23485;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35745;&#31639;&#20102;&#20855;&#26377;&#24322;&#36136;&#26465;&#30446;&#30340;&#26680;&#30697;&#38453;&#30340;&#32447;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#19982;&#30446;&#21069;&#20351;&#29992;&#30340;&#35889;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#25512;&#23548;&#20986;&#27867;&#21270;&#29305;&#24615;&#65292;&#38416;&#26126;&#20102;&#36755;&#20837;&#32500;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23548;&#33268;&#20102;&#19968;&#31181;&#38750;&#39640;&#26031;&#20316;&#29992;&#65292;&#31867;&#20284;&#20110;($\varphi^3+\varphi^4$)-&#29702;&#35770;&#12290;&#22312;&#19968;&#20010;&#21512;&#25104;&#20219;&#21153;&#21644;MNIST&#19978;&#20351;&#29992;&#25105;&#20204;&#30340;&#24418;&#24335;&#20307;&#31995;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference and kernel methods are well established in machine learning. The neural network Gaussian process in particular provides a concept to investigate neural networks in the limit of infinitely wide hidden layers by using kernel and inference methods. Here we build upon this limit and provide a field-theoretic formalism which covers the generalization properties of infinitely wide networks. We systematically compute generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries. In contrast to currently employed spectral methods we derive the generalization properties from the statistical properties of the input, elucidating the interplay of input dimensionality, size of the training data set, and variability of the data. We show that data variability leads to a non-Gaussian action reminiscent of a ($\varphi^3+\varphi^4$)-theory. Using our formalism on a synthetic task and on MNIST we obtain a homogeneous kernel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;Dikin&#27493;&#34892;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#24182;&#36866;&#24212;&#19968;&#33324;&#24230;&#37327;&#65292;&#20026;&#24102;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#25277;&#26679;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.12943</link><description>&lt;p&gt;
&#29992;&#24230;&#37327;&#30340;Dikin&#27493;&#39588;&#26377;&#25928;&#22320;&#25277;&#26679;PSD&#38181;&#20307;
&lt;/p&gt;
&lt;p&gt;
Efficiently Sampling the PSD Cone with the Metric Dikin Walk. (arXiv:2307.12943v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;Dikin&#27493;&#34892;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#24182;&#36866;&#24212;&#19968;&#33324;&#24230;&#37327;&#65292;&#20026;&#24102;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#25277;&#26679;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#23450;&#35268;&#21010;&#20195;&#34920;&#20102;&#39640;&#25928;&#35745;&#31639;&#30340;&#21069;&#27839;&#12290;&#23613;&#31649;&#22312;&#21322;&#23450;&#26368;&#20248;&#21270;&#19978;&#24050;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#22914;&#20170;&#20869;&#28857;&#27861;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#35299;&#20915;&#20013;&#31561;&#35268;&#27169;&#30340;&#38382;&#39064;&#65292;&#20294;&#26159;&#25277;&#26679;&#21322;&#23450;&#35299;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#30452;&#25509;&#24212;&#29992;&#24050;&#30693;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#25277;&#26679;&#19968;&#33324;&#20984;&#20307;&#30340;&#26041;&#27861;&#23548;&#33268;&#36816;&#34892;&#26102;&#38388;&#36807;&#38271;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#30340;&#36890;&#29992;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#33293;&#20837;&#38454;&#27573;&#20316;&#20026;&#39044;&#22788;&#29702;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;Dikin&#27493;&#34892;&#65292;&#24182;&#39318;&#20808;&#23558;&#20854;&#36866;&#24212;&#20110;&#19968;&#33324;&#24230;&#37327;&#65292;&#28982;&#21518;&#20026;&#24102;&#26377;&#20223;&#23556;&#32422;&#26463;&#30340;PSD&#38181;&#20307;&#35774;&#35745;&#21512;&#36866;&#30340;&#24230;&#37327;&#12290;&#25152;&#24471;&#21040;&#30340;&#28151;&#21512;&#26102;&#38388;&#21644;&#27599;&#27493;&#22797;&#26434;&#24230;&#30456;&#24403;&#23567;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#24230;&#37327;&#65292;&#21487;&#20197;&#20351;&#20854;&#23545;&#32422;&#26463;&#30340;&#20381;&#36182;&#20851;&#31995;&#21464;&#20026;&#22810;&#23545;&#25968;&#32423;&#30340;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#30697;&#38453;&#20989;&#25968;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#32452;&#21512;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#23545;&#25163;&#34892;&#20026;&#12290;&#22312;&#25308;&#21344;&#24237;&#21644;&#24694;&#24847;&#23545;&#25163;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21487;&#20197;&#25511;&#21046;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#65292;&#20294;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#29615;&#22659;&#19981;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2307.07980</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#65306;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#24212;&#23545;&#23545;&#25163;&#30340;&#24694;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment. (arXiv:2307.07980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#23545;&#25163;&#34892;&#20026;&#12290;&#22312;&#25308;&#21344;&#24237;&#21644;&#24694;&#24847;&#23545;&#25163;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21487;&#20197;&#25511;&#21046;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#65292;&#20294;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#29615;&#22659;&#19981;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25308;&#21344;&#24237;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#12290;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#36890;&#36807;&#65288;&#23545;&#25163;&#30340;&#65289;&#36951;&#25022;&#26469;&#35780;&#20272;&#65292;&#22312;&#29615;&#22659;&#25552;&#20379;&#23545;&#25163;&#25439;&#22833;&#26102;&#35780;&#20272;&#19968;&#27493;&#20915;&#31574;&#30340;&#36136;&#37327;&#65292;&#32780;&#26399;&#26395;&#24471;&#21040;&#19968;&#20010;&#27425;&#32447;&#24615;&#30340;&#19978;&#30028;&#12290;&#20294;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#19968;&#31867;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#65292;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#21644;&#23384;&#22312;&#25308;&#21344;&#24237;&#21442;&#19982;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#24067;&#24335;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21482;&#33021;&#23454;&#29616;&#32447;&#24615;&#30340;&#23545;&#25163;&#36951;&#25022;&#19978;&#30028;&#65292;&#36825;&#26159;&#32039;&#23494;&#30340;&#12290;&#36825;&#26159;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#24517;&#28982;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#21487;&#20197;&#23558;&#32447;&#24615;&#23545;&#25163;&#36951;&#25022;&#30340;&#24120;&#25968;&#25511;&#21046;&#22312;&#21512;&#29702;&#30340;&#27700;&#24179;&#19978;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#29615;&#22659;&#19981;&#26159;&#23436;&#20840;&#23545;&#25239;&#24615;&#30340;&#65292;&#21363;&#35802;&#23454;&#21442;&#19982;&#32773;&#30340;&#25439;&#22833;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;i.i.d.&#65289;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#21069;&#38754;&#25552;&#21040;&#30340;&#23545;&#25163;&#36951;&#25022;&#30456;&#21453;&#65292;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#30340;&#38543;&#26426;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies distributed online learning under Byzantine attacks. The performance of an online learning algorithm is often characterized by (adversarial) regret, which evaluates the quality of one-step-ahead decision-making when an environment provides adversarial losses, and a sublinear bound is preferred. But we prove that, even with a class of state-of-the-art robust aggregation rules, in an adversarial environment and in the presence of Byzantine participants, distributed online gradient descent can only achieve a linear adversarial regret bound, which is tight. This is the inevitable consequence of Byzantine attacks, even though we can control the constant of the linear adversarial regret to a reasonable level. Interestingly, when the environment is not fully adversarial so that the losses of the honest participants are i.i.d. (independent and identically distributed), we show that sublinear stochastic regret, in contrast to the aforementioned adversarial regret, is possible
&lt;/p&gt;</description></item><item><title>FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.06933</link><description>&lt;p&gt;
FDAPT: &#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06933
&lt;/p&gt;
&lt;p&gt;
FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;DAPT&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26356;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FDAPT&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FDAPT&#22312;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#37117;&#33021;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20923;&#32467;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FFDAPT&#65289;&#12290;FFDAPT&#24179;&#22343;&#25552;&#39640;&#20102;12.1%&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;FDAPT&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#19968;&#33324;&#24615;&#33021;&#27874;&#21160;&#20445;&#25345;&#22312;1%&#20197;&#19979;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#24037;&#20316;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL) can enhance model adaptation by leveraging more sensitive and distributed data while preserving data privacy. However, few studies have focused on this method. Therefore, we conduct the first comprehensive empirical study to evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We demonstrate that FDAPT can maintain competitive downstream task performance to the centralized baseline in both IID and non-IID situations. Furthermore, we propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and exhibits similar downstream task performance to standard FDAPT, with general performance fluctuations remaining less than 1%. Finally, through a critical evaluation of our work, we identify promising future research directions for this new research area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06250</link><description>&lt;p&gt;
&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20998;&#35299;&#26088;&#22312;&#36890;&#36807;&#28508;&#22312;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#25581;&#31034;&#25968;&#25454;&#30340;&#34920;&#24449;&#65292;&#20854;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#30456;&#20114;&#20851;&#32852;&#12290;&#22914;&#26524;&#35299;&#37322;&#25968;&#25454;&#30340;&#28508;&#22312;&#27169;&#22411;&#26159;&#21807;&#19968;&#30340;&#65292;&#37027;&#20040;&#36825;&#31181;&#34920;&#31034;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#24403;&#23384;&#22312;&#19981;&#37197;&#23545;&#30340;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#26102;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#24178;&#39044;&#37117;&#20250;&#25913;&#21464;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26426;&#21046;&#12290;&#24403;&#22240;&#26524;&#21464;&#37327;&#23436;&#20840;&#35266;&#27979;&#21040;&#26102;&#65292;&#22312;&#35802;&#23454;&#24615;&#20551;&#35774;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#32479;&#35745;&#19968;&#33268;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20445;&#35777;&#20102;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#21644;ap&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#22240;&#26524;&#20998;&#35299;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22478;&#24066;&#37319;&#26679;&#30340;&#31616;&#21333;&#21152;&#22122;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#37319;&#26679;&#22120;&#65292;&#35813;&#26041;&#26696;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23454;&#35777;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26126;&#26174;&#25552;&#21319;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26377;&#25928;&#31163;&#25955;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05439</link><description>&lt;p&gt;
&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#30340;&#22478;&#24066;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Metropolis Sampling for Constrained Diffusion Models. (arXiv:2307.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22478;&#24066;&#37319;&#26679;&#30340;&#31616;&#21333;&#21152;&#22122;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#37319;&#26679;&#22120;&#65292;&#35813;&#26041;&#26696;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23454;&#35777;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26126;&#26174;&#25552;&#21319;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#26159;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26377;&#25928;&#31163;&#25955;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#30340;&#20027;&#35201;&#33539;&#24335;&#12290;&#23427;&#20204;&#23545;&#40654;&#26364;&#27969;&#24418;&#30340;&#25193;&#23637;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#19978;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#36825;&#20123;&#27969;&#24418;&#30001;&#19968;&#32452;&#32422;&#26463;&#23450;&#20041;&#65292;&#24182;&#19988;&#19981;&#34987;&#29616;&#26377;&#30340;&#65288;&#40654;&#26364;&#65289;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#25152;&#35206;&#30422;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#25968;&#38556;&#30861;&#26041;&#27861;&#25110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26032;&#22411;&#21152;&#22122;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32422;&#26463;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#30340;&#37319;&#26679;&#22120;&#35745;&#31639;&#36127;&#25285;&#36739;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22478;&#24066;&#37319;&#26679;&#30340;&#26367;&#20195;&#31616;&#21333;&#21152;&#22122;&#26041;&#26696;&#65292;&#19982;&#26089;&#26399;&#30340;&#37319;&#26679;&#22120;&#30456;&#27604;&#65292;&#35745;&#31639;&#25928;&#29575;&#21644;&#23454;&#35777;&#24615;&#33021;&#37117;&#26377;&#24456;&#22823;&#25552;&#21319;&#12290;&#22312;&#29420;&#31435;&#30340;&#20852;&#36259;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#26032;&#36807;&#31243;&#23545;&#24212;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#26377;&#25928;&#31163;&#25955;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have recently emerged as the predominant paradigm for generative modelling. Their extension to Riemannian manifolds has facilitated their application to an array of problems in the natural sciences. Yet, in many practical settings, such manifolds are defined by a set of constraints and are not covered by the existing (Riemannian) diffusion model methodology. Recent work has attempted to address this issue by employing novel noising processes based on logarithmic barrier methods or reflected Brownian motions. However, the associated samplers are computationally burdensome as the complexity of the constraints increases. In this paper, we introduce an alternative simple noising scheme based on Metropolis sampling that affords substantial gains in computational efficiency and empirical performance compared to the earlier samplers. Of independent interest, we prove that this new process corresponds to a valid discretisation of the reflected Brownian motion. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolGroup&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#27599;&#20010;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20197;&#35299;&#20915;&#21512;&#20316;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#26102;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04052</link><description>&lt;p&gt;
&#23398;&#20064;&#23558;&#36741;&#21161;&#25968;&#25454;&#38598;&#20998;&#32452;&#29992;&#20110;&#20998;&#23376;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Group Auxiliary Datasets for Molecule. (arXiv:2307.04052v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MolGroup&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#27599;&#20010;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20197;&#35299;&#20915;&#21512;&#20316;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#38598;&#26102;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#20998;&#23376;&#25968;&#25454;&#38598;&#20013;&#26377;&#38480;&#30340;&#27880;&#37322;&#21487;&#29992;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#26159;&#19982;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#25317;&#26377;&#26356;&#22810;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#33021;&#20445;&#35777;&#25913;&#36827;&#12290;&#24403;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#19982;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#19981;&#21516;&#25110;&#30456;&#20114;&#30683;&#30462;&#26102;&#65292;&#36127;&#36801;&#31227;&#21487;&#33021;&#20250;&#21457;&#29983;&#12290;&#37492;&#20110;&#27492;&#65292;&#24403;&#20849;&#21516;&#35757;&#32451;&#26102;&#65292;&#30830;&#23450;&#21487;&#20197;&#20351;&#30446;&#26631;&#25968;&#25454;&#38598;&#21463;&#30410;&#30340;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#32780;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23558;&#22270;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#32467;&#21512;&#21487;&#20197;&#20316;&#20026;&#30830;&#23450;&#39640;&#20146;&#21644;&#24615;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#26356;&#21487;&#38752;&#25351;&#26631;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolGroup&#65292;&#23427;&#23558;&#25968;&#25454;&#38598;&#20146;&#21644;&#24615;&#20998;&#20026;&#20219;&#21153;&#20146;&#21644;&#24615;&#21644;&#32467;&#26500;&#20146;&#21644;&#24615;&#65292;&#20197;&#39044;&#27979;&#27599;&#20010;&#36741;&#21161;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;MolGroup&#36890;&#36807;&#21033;&#29992;&#36335;&#30001;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited availability of annotations in small molecule datasets presents a challenge to machine learning models. To address this, one common strategy is to collaborate with additional auxiliary datasets. However, having more data does not always guarantee improvements. Negative transfer can occur when the knowledge in the target dataset differs or contradicts that of the auxiliary molecule datasets. In light of this, identifying the auxiliary molecule datasets that can benefit the target dataset when jointly trained remains a critical and unresolved problem. Through an empirical analysis, we observe that combining graph structure similarity and task similarity can serve as a more reliable indicator for identifying high-affinity auxiliary datasets. Motivated by this insight, we propose MolGroup, which separates the dataset affinity into task and structure affinity to predict the potential benefits of each auxiliary molecule dataset. MolGroup achieves this by utilizing a routing mecha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20248;&#21270;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26102;&#24046;(TD)&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#32463;&#20856;&#21453;&#20363;&#20013;&#30830;&#23450;&#20102;&#24433;&#21709;&#31639;&#27861;&#25910;&#25947;&#25110;&#21457;&#25955;&#30340;&#20004;&#20010;&#21147;&#37327;&#65292;&#24182;&#22312;&#32447;&#24615;&#36924;&#36817;&#21644;&#24179;&#26041;&#25439;&#22833;&#20197;&#22806;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;TD&#30340;&#25910;&#25947;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;TD&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25104;&#21151;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.17750</link><description>&lt;p&gt;
TD&#25910;&#25947;&#24615;&#65306;&#19968;&#20010;&#20248;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
TD Convergence: An Optimization Perspective. (arXiv:2306.17750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20248;&#21270;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26102;&#24046;(TD)&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#22312;&#32463;&#20856;&#21453;&#20363;&#20013;&#30830;&#23450;&#20102;&#24433;&#21709;&#31639;&#27861;&#25910;&#25947;&#25110;&#21457;&#25955;&#30340;&#20004;&#20010;&#21147;&#37327;&#65292;&#24182;&#22312;&#32447;&#24615;&#36924;&#36817;&#21644;&#24179;&#26041;&#25439;&#22833;&#20197;&#22806;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;TD&#30340;&#25910;&#25947;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;TD&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25104;&#21151;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33879;&#21517;&#30340;&#26102;&#24046;(TD)&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#36890;&#36807;&#20248;&#21270;&#30340;&#35270;&#35282;&#26469;&#30475;&#24453;&#31639;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#35770;&#35777;&#20102;TD&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#27599;&#27425;&#36845;&#20195;&#26102;&#35201;&#26368;&#23567;&#21270;&#30340;&#20989;&#25968;&#37117;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;TD&#22312;&#32463;&#20856;&#21453;&#20363;&#20013;&#30340;&#21457;&#25955;&#34892;&#20026;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20915;&#23450;&#31639;&#27861;&#25910;&#25947;&#25110;&#21457;&#25955;&#34892;&#20026;&#30340;&#20004;&#20010;&#21147;&#37327;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#19968;&#20248;&#21270;&#35270;&#35282;&#25512;&#24191;&#21040;&#20102;&#27604;&#32447;&#24615;&#36924;&#36817;&#21644;&#24179;&#26041;&#25439;&#22833;&#26356;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#65292;&#35777;&#26126;&#20102;TD&#30340;&#25910;&#25947;&#24615;&#21462;&#20915;&#20110;&#36825;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;TD&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12929</link><description>&lt;p&gt;
&#21487;&#37327;&#21270;Transformer&#65306;&#36890;&#36807;&#24110;&#21161;&#27880;&#24847;&#21147;&#22836;&#8220;&#20160;&#20040;&#20063;&#19981;&#20570;&#8221;&#21435;&#38500;&#31163;&#32676;&#20540;
&lt;/p&gt;
&lt;p&gt;
Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;Transformer&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#65292;&#20294;&#36825;&#26159;&#20197;&#26497;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;&#37327;&#21270;&#26159;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26102;&#38388;&#21644;&#23384;&#20648;&#22120;&#28040;&#32791;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;transformer&#27169;&#22411;&#24448;&#24448;&#23398;&#20064;&#21040;&#20854;&#28608;&#27963;&#20013;&#30340;&#24378;&#31163;&#32676;&#20540;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37327;&#21270;&#12290;&#20026;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#30340;&#23384;&#22312;&#38656;&#35201;&#23558;&#28608;&#27963;&#32622;&#20110;&#26356;&#39640;&#30340;&#27604;&#29305;&#23485;&#24230;&#25110;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#23383;&#26684;&#24335;&#65292;&#36827;&#34892;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#20854;&#20182;&#21464;&#36890;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24378;&#31163;&#32676;&#20540;&#19982;&#29305;&#23450;&#27880;&#24847;&#22836;&#34892;&#20026;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#22836;&#35797;&#22270;&#23398;&#20064;&#8220;&#26080;&#25805;&#20316;&#8221;&#25110;&#20165;&#20165;&#26159;&#37096;&#20998;&#27531;&#24046;&#26356;&#26032;&#12290;&#20026;&#20102;&#23454;&#29616;&#27880;&#24847;&#21147;&#22836;&#20013;&#38656;&#35201;&#30340;&#31934;&#30830;&#38646;&#20301;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#25945;&#25480;&#27880;&#24847;&#21147;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#30340;&#37327;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#37327;&#21270;&#29978;&#33267;&#26159;&#24378;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.11167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35823;&#23548;&#65306;&#20351;&#29992;Only Connect Wall&#25968;&#25454;&#38598;&#25506;&#32034;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#21644;Einstellung&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20197;&#26469;&#65292;&#23545;&#20154;&#31867;&#20223;&#30495;&#26234;&#33021;&#30340;&#36861;&#27714;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25345;&#20037;&#35805;&#39064;&#12290;&#26368;&#26032;&#19968;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#28436;&#36827;&#21644;&#26032;&#20852;&#33021;&#21147;&#23558;&#36825;&#20010;&#20027;&#39064;&#20174;&#23398;&#26415;&#30028;&#24102;&#21040;&#20102;&#25991;&#21270;&#26102;&#20195;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;NLP&#35780;&#20272;&#22522;&#20934;&#20219;&#21153;&#27979;&#35797;&#20102;&#20154;&#31867;&#20223;&#30495;&#34892;&#20026;&#30340;&#19968;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;BIG-bench&#30340;&#8220;&#31867;&#20154;&#34892;&#20026;&#8221;&#20219;&#21153;&#65289;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#19968;&#20010;&#20219;&#21153;&#32771;&#23519;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#26159;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#30740;&#31350;&#36739;&#20026;&#28145;&#20837;&#30340;&#20027;&#39064;&#65292;&#26631;&#20934;&#21270;&#27979;&#35797;&#20027;&#35201;&#20351;&#29992;&#23558;&#32447;&#32034;&#35789;&#20043;&#38388;&#30340;&#65288;&#24322;&#26500;&#65289;&#36830;&#25509;&#33021;&#21147;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#26263;&#31034;&#24615;&#30340;&#35823;&#23548;&#24615;&#21050;&#28608;-&#34987;&#31216;&#20026;&#8220;&#35825;&#23548;&#35823;&#35299;&#8221;&#30340;&#24178;&#25200;&#22240;&#32032;-&#36890;&#36807;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20107;&#20808;&#35753;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#26377;&#30456;&#20284;&#25340;&#20889;&#30340;&#38169;&#35823;&#22240;&#32032;&#26469;&#23454;&#39564;&#24615;&#22320;&#35825;&#23548;&#36825;&#26679;&#30340;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#21512;&#39029;&#25439;&#22833;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#20108;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#36890;&#36807;&#23545;&#24178;&#20928;&#25968;&#25454;&#20313;&#37327;&#30340;&#26465;&#20214;&#30340;&#30830;&#23450;&#65292;&#24471;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#32467;&#26524;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31070;&#32463;&#20803;&#21160;&#24577;&#21464;&#21270;&#20570;&#20986;&#31934;&#32454;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2306.09955</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#39029;&#25439;&#22833;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#27973;&#23618;ReLU&#32593;&#32476;&#65306;&#25105;&#20204;&#20309;&#26102;&#36807;&#24230;&#25311;&#21512;&#19988;&#20854;&#26159;&#21542;&#33391;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?. (arXiv:2306.09955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#21512;&#39029;&#25439;&#22833;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#20108;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#36890;&#36807;&#23545;&#24178;&#20928;&#25968;&#25454;&#20313;&#37327;&#30340;&#26465;&#20214;&#30340;&#30830;&#23450;&#65292;&#24471;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#32467;&#26524;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31070;&#32463;&#20803;&#21160;&#24577;&#21464;&#21270;&#20570;&#20986;&#31934;&#32454;&#25551;&#36848;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644;&#21512;&#39029;&#25439;&#22833;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#20108;&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#20102;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#20854;&#20013;&#30456;&#23545;&#36739;&#23567;&#27604;&#20363;&#30340;&#26631;&#31614;&#34987;&#25439;&#22351;&#25110;&#32763;&#36716;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24178;&#20928;&#25968;&#25454;&#20313;&#37327;&#30340;&#26465;&#20214;&#65292;&#20135;&#29983;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#32467;&#26524;&#65306;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23558;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#27979;&#35797;&#25968;&#25454;&#34987;&#27491;&#30830;&#20998;&#31867;&#65307;&#36807;&#25311;&#21512;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23558;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#20294;&#27979;&#35797;&#25968;&#25454;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#21463;&#21040;&#24120;&#25968;&#19979;&#38480;&#30340;&#32422;&#26463;&#65307;&#20197;&#21450;&#19981;&#36807;&#25311;&#21512;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24178;&#20928;&#30340;&#28857;&#21487;&#20197;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#27979;&#35797;&#25968;&#25454;&#34987;&#27491;&#30830;&#20998;&#31867;&#65292;&#20294;&#26159;&#19981;&#24178;&#20928;&#30340;&#28857;&#26080;&#27861;&#20570;&#20986;&#21516;&#26679;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#31070;&#32463;&#20803;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#21464;&#21270;&#30340;&#19968;&#31181;&#31934;&#32454;&#25551;&#36848;&#65292;&#24182;&#25581;&#31034;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#20013;&#65292;&#24178;&#20928;&#28857;&#25509;&#36817;&#36798;&#21040;&#38646;&#25439;&#22833;&#65292;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#20013;&#65292;&#24178;&#20928;&#28857;&#20250;&#25391;&#33633;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;</title><link>http://arxiv.org/abs/2306.04027</link><description>&lt;p&gt;
&#22240;&#23376;&#22270;&#27169;&#22411;&#35270;&#35282;&#19979;&#30340;&#24178;&#39044;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Intervention Generalization: A View from Factor Graph Models. (arXiv:2306.04027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;(IFM)&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#20197;&#23454;&#29616;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#25512;&#24191;&#21040;&#26032;&#30340;&#26465;&#20214;&#12290;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25552;&#20379;&#36275;&#22815;&#22810;&#30340;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#21487;&#33021;&#26368;&#32456;&#23398;&#20064;&#20174;&#26032;&#30340;&#23454;&#39564;&#26465;&#20214;&#21040;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#26144;&#23556;&#65292;&#20294;&#26159;&#22788;&#29702;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#32452;&#21512;&#31354;&#38388;&#24456;&#22256;&#38590;&#12290;&#22312;&#20856;&#22411;&#30340;&#31232;&#30095;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#22914;&#26524;&#19981;&#20381;&#36182;&#20110;&#37325;&#30340;&#35268;&#21017;&#21270;&#25110;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#31181;&#26144;&#23556;&#26159;&#19981;&#36866;&#24403;&#30340;&#12290;&#36825;&#26679;&#30340;&#20551;&#35774;&#21487;&#33021;&#26159;&#21487;&#38752;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24456;&#38590;&#36777;&#25252;&#25110;&#27979;&#35797;&#12290;&#26412;&#25991;&#20174;&#22240;&#23376;&#22270;&#27169;&#22411;&#30340;&#35821;&#35328;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#22914;&#20309;&#20445;&#35777;&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#21040;&#26032;&#30340;&#26465;&#20214;&#30340;&#36291;&#36801;&#65292;&#20165;&#22522;&#20110;&#23545;&#25805;&#32437;&#31995;&#32479;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#26368;&#23567;&#20551;&#35774;&#12290;&#20551;&#35774;&#30340;&#8220;&#24178;&#39044;&#22240;&#23376;&#27169;&#22411;&#8221;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#23427;&#24456;&#26041;&#20415;&#22320;&#22788;&#29702;&#20102;&#22823;&#37327;&#21487;&#33021;&#30340;&#24178;&#39044;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we take a close look at how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated $\textit{interventional factor model}$ (IFM) may not always be informative, but it conveniently abs
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>http://arxiv.org/abs/2306.02913</link><description>&lt;p&gt;
&#20998;&#25955;&#21270;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;
&lt;/p&gt;
&lt;p&gt;
Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02913
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;D-SGD&#65289;&#20801;&#35768;&#22312;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#25511;&#21046;&#19979;&#65292;&#22823;&#37327;&#35774;&#22791;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#29702;&#35770;&#35748;&#20026;&#65292;&#20998;&#25955;&#21270;&#19981;&#21487;&#36991;&#20813;&#22320;&#21066;&#24369;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25361;&#25112;&#20256;&#32479;&#20449;&#24565;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#26032;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#20998;&#25955;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#38750;&#20984;&#38750;-$\beta$-&#24179;&#28369;&#35774;&#32622;&#19979;&#65292;D-SGD&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#24179;&#22343;&#26041;&#21521;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#31181;&#24778;&#20154;&#30340;&#28176;&#36817;&#31561;&#20215;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#27491;&#21017;&#21270;-&#20248;&#21270;&#26435;&#34913;&#20197;&#21450;&#20998;&#25955;&#21270;&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;D-SGD&#20013;&#23384;&#22312;&#19968;&#20010;&#33258;&#30001;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#20272;&#35745;&#65307;&#65288;2&#65289;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#65307;&#65288;3&#65289;D-SGD&#30340;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#19981;&#20250;&#38543;&#30528;&#24635;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#36825;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#25110;KSD&#25439;&#22833;&#26469;&#30830;&#20445;&#31070;&#32463;&#31639;&#23376;&#33021;&#22815;&#22312;&#28151;&#27788;&#31995;&#32479;&#19978;&#22797;&#29616;&#20854;&#32479;&#35745;&#25110;&#32467;&#26500;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01187</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#20197;&#20445;&#25345;&#28151;&#27788;&#21560;&#24341;&#23376;&#30340;&#19981;&#21464;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
Training neural operators to preserve invariant measures of chaotic attractors. (arXiv:2306.01187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#25110;KSD&#25439;&#22833;&#26469;&#30830;&#20445;&#31070;&#32463;&#31639;&#23376;&#33021;&#22815;&#22312;&#28151;&#27788;&#31995;&#32479;&#19978;&#22797;&#29616;&#20854;&#32479;&#35745;&#25110;&#32467;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#27788;&#31995;&#32479;&#20351;&#24471;&#38271;&#26102;&#38388;&#39044;&#27979;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#21021;&#22987;&#26465;&#20214;&#30340;&#24494;&#23567;&#25200;&#21160;&#20250;&#23548;&#33268;&#36712;&#36857;&#20197;&#25351;&#25968;&#36895;&#24230;&#21457;&#25955;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#20026;&#26368;&#23567;&#21270;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#65292;&#34429;&#28982;&#33021;&#22815;&#20934;&#30830;&#22320;&#36827;&#34892;&#30701;&#26399;&#39044;&#27979;&#65292;&#20294;&#24120;&#24120;&#26080;&#27861;&#20877;&#38271;&#26102;&#38388;&#20869;&#22797;&#21046;&#21160;&#21147;&#23398;&#30340;&#32479;&#35745;&#25110;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#19988;&#21487;&#33021;&#20135;&#29983;&#36864;&#21270;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26694;&#26550;&#65292;&#26088;&#22312;&#20445;&#25345;&#34920;&#24449;&#21160;&#24577;&#19981;&#21464;&#32479;&#35745;&#23646;&#24615;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#30340;&#19981;&#21464;&#27979;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#22810;&#29615;&#22659;&#35774;&#32622;&#20013;&#65288;&#27599;&#20010;&#26679;&#26412;&#36712;&#36857;&#37117;&#21463;&#30053;&#24494;&#19981;&#21516;&#21160;&#24577;&#30340;&#25511;&#21046;&#65289;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26032;&#30340;&#22788;&#29702;&#22024;&#26434;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#21040;&#30340;&#21160;&#24577;&#19982;&#31070;&#32463;&#31639;&#23376;&#36755;&#20986;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#30340;&#25439;&#22833;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19987;&#23478;&#23545;&#22522;&#30784;&#29289;&#29702;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#30830;&#23450;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chaotic systems make long-horizon forecasts difficult because small perturbations in initial conditions cause trajectories to diverge at an exponential rate. In this setting, neural operators trained to minimize squared error losses, while capable of accurate short-term forecasts, often fail to reproduce statistical or structural properties of the dynamics over longer time horizons and can yield degenerate results. In this paper, we propose an alternative framework designed to preserve invariant measures of chaotic attractors that characterize the time-invariant statistical properties of the dynamics. Specifically, in the multi-environment setting (where each sample trajectory is governed by slightly different dynamics), we consider two novel approaches to training with noisy data. First, we propose a loss based on the optimal transport distance between the observed dynamics and the neural operator outputs. This approach requires expert knowledge of the underlying physics to determine 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.15001</link><description>&lt;p&gt;
&#22797;&#25968;&#20540;&#33258;&#32534;&#30721;&#22120;&#23545;&#29289;&#20307;&#21457;&#29616;&#30340;&#23545;&#27604;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15001
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#20013;&#24515;&#27169;&#22411;&#20351;&#29992;&#25554;&#27133;&#21644;&#27880;&#24847;&#21147;&#36335;&#30001;&#36827;&#34892;&#32465;&#23450;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#26377;&#20960;&#20010;&#27010;&#24565;&#24615;&#30340;&#23616;&#38480;&#24615;&#65306;&#25554;&#27133;&#30340;&#25968;&#37327;&#26159;&#30828;&#32534;&#30721;&#30340;&#65307;&#25152;&#26377;&#25554;&#27133;&#30340;&#23481;&#37327;&#30456;&#31561;&#65307;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65307;&#25554;&#27133;&#20869;&#27809;&#26377;&#30446;&#26631;&#32423;&#21035;&#30340;&#20851;&#31995;&#22240;&#32032;&#12290;&#21407;&#21017;&#19978;&#65292;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22797;&#25968;&#20540;&#28608;&#27963;&#22312;&#20854;&#30456;&#20301;&#20998;&#37327;&#20013;&#23384;&#20648;&#32465;&#23450;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#30340;&#24037;&#20316;&#31034;&#20363;&#21482;&#26159;&#26368;&#36817;&#25165;&#26377;&#65292;&#32780;&#19988;&#23454;&#38469;&#19978;&#20173;&#28982;&#38480;&#20110;&#29609;&#20855;&#28784;&#24230;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#23384;&#20648;&#19981;&#21040;&#19977;&#20010;&#29289;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26550;&#26500;&#20462;&#25913;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2305.10564</link><description>&lt;p&gt;
&#23545;&#25918;&#24323;&#20998;&#31867;&#22120;&#36827;&#34892;&#21453;&#20107;&#23454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#24323;&#20998;&#31867;&#22120;&#21487;&#20197;&#36873;&#25321;&#22312;&#19981;&#30830;&#23450;&#26102;&#25918;&#24323;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#38382;&#39064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20445;&#30041;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#40657;&#30418;&#25918;&#24323;&#20998;&#31867;&#22120;&#26102;&#65292;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#32771;&#34385;&#20998;&#31867;&#22120;&#22312;&#23427;&#30340;&#25918;&#24323;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#24403;&#25918;&#23556;&#31185;&#21307;&#29983;&#19981;&#30830;&#23450;&#20854;&#35786;&#26029;&#25110;&#24403;&#39550;&#39542;&#21592;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#19981;&#27880;&#24847;&#26102;&#65292;&#36825;&#20123;&#32570;&#22833;&#30340;&#39044;&#27979;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#22260;&#32469;&#30528;&#23450;&#20041;&#19968;&#20010;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#21363;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#30340;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#25351;&#23450;&#20102;&#26465;&#20214;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions are crucial when, e.g., a radiologist is unsure of their diagnosis or when a driver is inattentive in a self-driving car. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01588</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26799;&#24230;&#21098;&#20999;&#65306;&#38543;&#26426;&#20559;&#24046;&#21644;&#32039;&#23494;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees. (arXiv:2305.01588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21098;&#20999;&#26159;&#26631;&#20934;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#27969;&#34892;&#20462;&#25913;&#26041;&#27861;&#65292;&#27599;&#27425;&#36845;&#20195;&#23558;&#26799;&#24230;&#33539;&#25968;&#38480;&#21046;&#22312;&#26576;&#20010;&#20540;c&gt;0&#12290;&#23427;&#34987;&#24191;&#27867;&#29992;&#20110;&#31283;&#23450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;( Goodfellow et al., 2016 )&#25110;&#24378;&#21046;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;( Abadi et al., 2016 )&#12290;&#23613;&#31649;&#21098;&#20999;&#26426;&#21046;&#21463;&#27426;&#36814;&#19988;&#31616;&#21333;&#65292;&#20294;&#20854;&#25910;&#25947;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#30340;$c$&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#26174;&#31034;&#20102;&#23545;&#20219;&#24847;&#21098;&#36753;&#38408;&#20540;&#30340;&#31934;&#30830;&#20381;&#36182;&#65292;&#24182;&#19988;&#34920;&#26126;&#25105;&#20204;&#30340;&#20445;&#35777;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#37117;&#26159;&#32039;&#23494;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;(i)&#23545;&#20110;&#30830;&#23450;&#24615;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#21098;&#36753;&#38408;&#20540;&#20165;&#24433;&#21709;&#25910;&#25947;&#30340;&#39640;&#38454;&#39033;&#65292;(ii)&#22312;&#38543;&#26426;&#35774;&#32622;&#20013;&#65292;&#21363;&#20351;&#23545;&#20110;&#20219;&#24847;&#23567;&#30340;&#27493;&#38271;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#30495;&#27491;&#30340;&#26368;&#20248;&#35299;&#22312;&#26631;&#20934;&#30340;&#22122;&#22768;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#23450;&#30340;&#38543;&#26426;&#22122;&#22768;&#20551;&#35774;&#65292;&#22312;&#27492;&#20551;&#35774;&#19979;&#65292;&#25910;&#25947;&#26159;&#20445;&#35777;&#30340;&#65292;&#21098;&#20999;&#38408;&#20540;$c$&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient clipping is a popular modification to standard (stochastic) gradient descent, at every iteration limiting the gradient norm to a certain value $c &gt;0$. It is widely used for example for stabilizing the training of deep learning models (Goodfellow et al., 2016), or for enforcing differential privacy (Abadi et al., 2016). Despite popularity and simplicity of the clipping mechanism, its convergence guarantees often require specific values of $c$ and strong noise assumptions.  In this paper, we give convergence guarantees that show precise dependence on arbitrary clipping thresholds $c$ and show that our guarantees are tight with both deterministic and stochastic gradients. In particular, we show that (i) for deterministic gradient descent, the clipping threshold only affects the higher-order terms of convergence, (ii) in the stochastic setting convergence to the true optimum cannot be guaranteed under the standard noise assumption, even under arbitrary small step-sizes. We give ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07687</link><description>&lt;p&gt;
MLRegTest&#65306;&#26426;&#22120;&#23398;&#20064;&#27491;&#21017;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#24050;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#32454;&#33268;&#22320;&#26816;&#26597;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#27169;&#24335;&#65292;&#24182;&#22312;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#26410;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#26102;&#24314;&#31435;&#20449;&#24515;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#24418;&#24335;&#35821;&#35328;&#20195;&#34920;&#30528;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#24182;&#27491;&#30830;&#22320;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25104;&#21151;&#27867;&#21270;&#30340;&#24050;&#30693;&#25361;&#25112;&#12290;MLRegTest&#26681;&#25454;&#23427;&#20204;&#30340;&#36923;&#36753;&#22797;&#26434;&#24230;&#65288;&#21333;&#35843;&#20108;&#38454;&#65292;&#19968;&#38454;&#65292;&#21629;&#39064;&#25110;&#21333;&#39033;&#24335;&#34920;&#36798;&#24335;&#65289;&#21644;&#36923;&#36753;&#25991;&#23383;&#30340;&#31181;&#31867;&#65288;&#23383;&#31526;&#20018;&#65292;&#23450;&#32423;&#23383;&#31526;&#20018;&#65292;&#23376;&#24207;&#21015;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65289;&#32452;&#32455;&#20854;&#35821;&#35328;&#12290;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#25991;&#23383;&#30340;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#29702;&#35299;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#23427;&#20204;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#30340;&#33539;&#24335;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#20351;&#29992;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#21644;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#21464;&#20998;&#25805;&#20316;&#65292;&#36890;&#36807;&#26368;&#36895;&#19979;&#38477;&#27861;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#36827;&#34892;&#31070;&#32463;&#31639;&#23376;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#38750;&#24120;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.04234</link><description>&lt;p&gt;
&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65306;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations. (arXiv:2304.04234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#30340;&#33539;&#24335;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#20351;&#29992;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#21644;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#21464;&#20998;&#25805;&#20316;&#65292;&#36890;&#36807;&#26368;&#36895;&#19979;&#38477;&#27861;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#36827;&#34892;&#31070;&#32463;&#31639;&#23376;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#38750;&#24120;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#26041;&#27861;&#30340;&#26032;&#33539;&#24335;&#65292;&#20026;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#29992;&#21464;&#20998;&#24418;&#24335;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#31070;&#32463;&#31639;&#23376;&#32473;&#20986;&#30340;&#33410;&#28857;&#35299;&#39044;&#27979;&#20013;&#25512;&#23548;&#20986;&#31995;&#32479;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#36827;&#34892;&#21464;&#20998;&#25805;&#20316;&#65292;&#26500;&#24314;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#26469;&#25512;&#23548;&#32447;&#24615;&#31995;&#32479;&#30340;&#27531;&#24046;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#26368;&#36895;&#19979;&#38477;&#27861;&#65288;SD&#65289;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#65288;CG&#65289;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#26356;&#26032;&#27493;&#39588;&#65292;&#20316;&#20026;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26356;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;VOL&#21487;&#20197;&#23398;&#20064;&#21040;&#22312;&#31283;&#23450;&#20256;&#28909;&#21644;&#21464;&#21018;&#24230;&#24377;&#24615;PDE&#20013;&#21508;&#31181;&#35299;&#31639;&#23376;&#65292;&#32467;&#26524;&#20196;&#20154;&#28385;&#24847;&#65292;&#35823;&#24046;&#36739;&#23567;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#23454;&#29616;&#26080;&#26631;&#31614;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the variational method, we propose a novel paradigm that provides a unified framework of training neural operators and solving partial differential equations (PDEs) with the variational form, which we refer to as the variational operator learning (VOL). We first derive the functional approximation of the system from the node solution prediction given by neural operators, and then conduct the variational operation by automatic differentiation, constructing a forward-backward propagation loop to derive the residual of the linear system. One or several update steps of the steepest decent method (SD) and the conjugate gradient method (CG) are provided in every iteration as a cheap yet effective update for training the neural operators. Experimental results show the proposed VOL can learn a variety of solution operators in PDEs of the steady heat transfer and the variable stiffness elasticity with satisfactory results and small error. The proposed VOL achieves nearly label-free tra
&lt;/p&gt;</description></item><item><title>PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08789</link><description>&lt;p&gt;
PLEX&#65306;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining. (arXiv:2303.08789v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08789
&lt;/p&gt;
&lt;p&gt;
PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20016;&#23500;&#30340;&#34920;&#24449;&#26159;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#20851;&#38190;&#65292;&#20294;&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#23398;&#20064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29702;&#24819;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21508;&#31181;&#24050;&#27880;&#37322;&#20219;&#21153;&#30340;&#19987;&#23478;&#35270;&#35273;-&#21160;&#20316;&#28436;&#31034;&#65292;&#26159;&#31232;&#32570;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;PLEX&#65292;&#23427;&#26159;&#20174;&#20219;&#21153;&#19981;&#21487;&#30693;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#20013;&#23398;&#20064;&#30340;&#65292;&#20276;&#38543;&#30528;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#8212;&#8212;&#36825;&#26159;&#19968;&#31181;&#25968;&#37327;&#21487;&#35266;&#30340;&#19982;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;PLEX&#32972;&#21518;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#35266;&#23519;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36712;&#36857;&#19979;&#65292;&#26377;&#21161;&#20110;&#35825;&#23548;&#28508;&#22312;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#35757;&#32451;&#26426;&#22120;&#20154;&#25191;&#34892;&#19982;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#25805;&#20316;&#20363;&#31243;&#65292;&#32780;&#22810;&#26679;&#21270;&#30340;&#20165;&#20026;&#35270;&#39057;&#28436;&#31034;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#25945;&#20250;&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#22521;&#35757;&#20316;&#21697;&#19981;&#21516;&#65292;PLEX&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#30340;&#24863;&#35273;&#36816;&#21160;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task polic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.01353</link><description>&lt;p&gt;
&#23545;&#27491;&#21017;&#21270;&#20013;&#30340;&#20559;&#24046;&#36827;&#34892;&#24809;&#32602;&#23558;&#20351;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Penalising the biases in norm regularisation enforces sparsity. (arXiv:2303.01353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36890;&#36807;&#25511;&#21046;&#21442;&#25968;&#30340;&#33539;&#25968;&#24448;&#24448;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#33539;&#25968;&#21644;&#25152;&#24471;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#19968;&#38544;&#34255;&#23618;&#21644;&#19968;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#20989;&#25968;&#25152;&#38656;&#30340;&#21442;&#25968;&#33539;&#25968;&#30001;&#20854;&#20108;&#38454;&#23548;&#25968;&#30340;&#24635;&#21464;&#24046;&#21152;&#26435;&#24471;&#21040;&#65292;&#20854;&#20013;&#25152;&#21152;&#26435;&#30340;&#22240;&#23376;&#20026;$\sqrt{1+x^2}$&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#19981;&#23545;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#26102;&#65292;&#36825;&#20010;&#21152;&#26435;&#22240;&#23376;&#20250;&#28040;&#22833;&#12290;&#36825;&#20010;&#39069;&#22806;&#30340;&#21152;&#26435;&#22240;&#23376;&#30340;&#23384;&#22312;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#34987;&#35777;&#26126;&#21487;&#20197;&#24378;&#21046;&#23454;&#29616;&#26368;&#23567;&#33539;&#25968;&#20869;&#25554;&#22120;&#30340;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#65288;&#22312;&#25296;&#28857;&#25968;&#37327;&#19978;&#65289;&#12290;&#30456;&#21453;&#65292;&#30465;&#30053;&#20559;&#24046;&#30340;&#33539;&#25968;&#21017;&#20250;&#23548;&#33268;&#38750;&#31232;&#30095;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#27491;&#21017;&#21270;&#20013;&#23545;&#20559;&#24046;&#39033;&#36827;&#34892;&#24809;&#32602;&#65292;&#26080;&#35770;&#26159;&#26174;&#24335;&#36824;&#26159;&#38544;&#24335;&#22320;&#65292;&#37117;&#20250;&#23548;&#33268;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a $\sqrt{1+x^2}$ factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#30740;&#31350;&#20102;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23457;&#26680;&#21592;&#20043;&#38388;&#23384;&#22312;&#24191;&#27867;&#30340;&#20998;&#27495;&#65292;&#24182;&#19988;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#30340;&#22238;&#24212;&#12290;&#36825;&#23545;&#20110;&#20869;&#23481;&#23457;&#26680;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2301.12534</link><description>&lt;p&gt;
&#20154;&#24037;&#21644;&#26426;&#22120;&#20851;&#20110;&#20160;&#20040;&#26159;&#20882;&#29359;&#23384;&#22312;&#36739;&#22823;&#20998;&#27495;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#65306;&#32479;&#19968;&#20027;&#35266;&#20882;&#29359;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#30740;&#31350;&#20102;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23457;&#26680;&#21592;&#20043;&#38388;&#23384;&#22312;&#24191;&#27867;&#30340;&#20998;&#27495;&#65292;&#24182;&#19988;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#30340;&#22238;&#24212;&#12290;&#36825;&#23545;&#20110;&#20869;&#23481;&#23457;&#26680;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#26159;&#20869;&#23481;&#23457;&#26680;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20160;&#20040;&#26159;&#20882;&#29359;&#24615;&#30340;&#21487;&#20197;&#26159;&#39640;&#24230;&#20027;&#35266;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#28041;&#21450;&#21040;&#29616;&#23454;&#19990;&#30028;&#31038;&#20132;&#32593;&#31449;&#25919;&#27835;&#35328;&#35770;&#26102;&#65292;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#23545;&#20110;&#20160;&#20040;&#26159;&#20882;&#29359;&#24615;&#30340;&#23384;&#22312;&#20998;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#23457;&#26680;&#21592;&#20043;&#38388;&#65288;&#21253;&#25324;&#20154;&#24037;&#21644;&#26426;&#22120;&#65289;&#23384;&#22312;&#24191;&#27867;&#20998;&#27495;&#65307;&#21644;&#65288;2&#65289;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#22522;&#20110;&#20182;&#20204;&#30340;&#25919;&#27835;&#20542;&#21521;&#22914;&#20309;&#22238;&#24212;&#12290;&#23545;&#20110;&#65288;1&#65289;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#35268;&#27169;&#30340;&#22122;&#22768;&#23457;&#35745;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#21644;&#20154;&#24037;&#22238;&#31572;&#12290;&#23545;&#20110;&#65288;2&#65289;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#39318;&#21019;&#30340;&#20849;&#24773;&#20882;&#29359;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22122;&#22768;&#23457;&#35745;&#25581;&#31034;&#20102;&#19981;&#21516;&#26426;&#22120;&#23457;&#26680;&#21592;&#20043;&#38388;&#30340;&#23457;&#26680;&#32467;&#26524;&#24046;&#24322;&#24456;&#22823;&#12290;&#25105;&#20204;&#19982;&#20154;&#24037;&#23457;&#26680;&#21592;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25919;&#27835;&#20542;&#21521;&#32467;&#21512;&#25935;&#24863;&#38382;&#39064;&#20250;&#24433;&#21709;&#21040;&#19968;&#23545;&#19968;&#30340;&#20882;&#29359;&#65292;&#20197;&#21450;&#20849;&#24773;&#20882;&#29359;&#12290;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;https://github.com/Homan-Lab/voic&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a noise audit at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05785</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#23454;&#29616;&#39640;&#25928;&#30340;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#35774;&#35745;&#30340;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24456;&#38590;&#26500;&#24314;&#26368;&#20248;&#28608;&#27963;&#20989;&#25968;&#65292;&#32780;&#24403;&#21069;&#30340;&#28608;&#27963;&#20989;&#25968;&#25628;&#32034;&#31639;&#27861;&#36807;&#20110;&#26114;&#36149;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;2,913&#20010;&#31995;&#32479;&#29983;&#25104;&#30340;&#28608;&#27963;&#20989;&#25968;&#20174;&#22836;&#35757;&#32451;&#21367;&#31215;&#12289;&#27531;&#24046;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#21019;&#24314; Act-Bench-CNN&#12289;Act-Bench-ResNet &#21644; Act-Bench-ViT &#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;&#22522;&#20934;&#31354;&#38388;&#65292;&#21457;&#29616;&#19982;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#21644;&#28608;&#27963;&#20989;&#25968;&#36755;&#20986;&#20998;&#24067;&#30456;&#20851;&#32852;&#30340; Fisher &#20449;&#24687;&#30697;&#38453;&#30340;&#39057;&#35889;&#23545;&#24615;&#33021;&#30340;&#39044;&#27979;&#24615;&#24456;&#39640;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#20195;&#29702;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#30340;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#21516;&#26102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21457;&#29616;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#65292;&#22312;&#25209;&#24402;&#19968;&#21270;&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#32479;&#35745;&#21442;&#25968;&#19981;&#21305;&#37197;&#23548;&#33268;&#20102;&#26799;&#24230;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.02982</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25209;&#24402;&#19968;&#21270;&#20250;&#25439;&#23475;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Batch Normalization Damage Federated Learning on Non-IID Data?. (arXiv:2301.02982v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21457;&#29616;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20013;&#65292;&#22312;&#25209;&#24402;&#19968;&#21270;&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#32479;&#35745;&#21442;&#25968;&#19981;&#21305;&#37197;&#23548;&#33268;&#20102;&#26799;&#24230;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#28041;&#21450;&#22312;&#32593;&#32476;&#36793;&#32536;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#36793;&#32536;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;&#20026;&#20102;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;DNN&#27169;&#22411;&#65292;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#36895;&#35757;&#32451;&#21644;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;BN&#20250;&#26174;&#33879;&#25439;&#23475;FL&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;FL&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#20302;&#20110;&#38598;&#20013;&#24335;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#27809;&#26377;&#25552;&#20379;&#20851;&#20110;BN&#22914;&#20309;&#25439;&#23475;FL&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#65292;BN&#20013;&#23616;&#37096;&#21644;&#20840;&#23616;&#32479;&#35745;&#21442;&#25968;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#26799;&#24230;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a promising distributed learning paradigm, federated learning (FL) involves training deep neural network (DNN) models at the network edge while protecting the privacy of the edge clients. To train a large-scale DNN model, batch normalization (BN) has been regarded as a simple and effective means to accelerate the training and improve the generalization capability. However, recent findings indicate that BN can significantly impair the performance of FL in the presence of non-i.i.d. data. While several FL algorithms have been proposed to address this issue, their performance still falls significantly when compared to the centralized scheme. Furthermore, none of them have provided a theoretical explanation of how the BN damages the FL convergence. In this paper, we present the first convergence analysis to show that under the non-i.i.d. data, the mismatch between the local and global statistical parameters in BN causes the gradient deviation between the local and global models, which, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#27969;&#24418;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#23450;&#20041;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26597;&#35810;&#28857;&#22312;&#27969;&#24418;&#19978;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10962</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#39640;&#26031;&#36807;&#31243;&#30340;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization on Manifolds via Graph Gaussian Processes. (arXiv:2210.10962v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#27969;&#24418;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#23450;&#20041;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26597;&#35810;&#28857;&#22312;&#27969;&#24418;&#19978;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#19982;&#39640;&#26031;&#36807;&#31243;&#19978;&#38480;&#32622;&#20449;&#24230;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#27969;&#24418;&#19978;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#22312;&#26080;&#27861;&#33719;&#24471;&#23436;&#25972;&#27969;&#24418;&#34920;&#31034;&#19988;&#26597;&#35810;&#30446;&#26631;&#26114;&#36149;&#30340;&#24212;&#29992;&#22330;&#26223;&#32780;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#20381;&#38752;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#26469;&#23450;&#20041;&#29992;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#12290;&#20351;&#29992;&#20808;&#21069;&#25152;&#26377;&#26597;&#35810;&#30340;&#21518;&#39564;&#20998;&#24067;&#36880;&#27493;&#36873;&#25321;&#26597;&#35810;&#28857;&#12290;&#25105;&#20204;&#22312;&#26597;&#35810;&#27425;&#25968;&#21644;&#28857;&#20113;&#22823;&#23567;&#26041;&#38754;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;&#25968;&#20540;&#23454;&#39564;&#34917;&#20805;&#20102;&#29702;&#35770;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper integrates manifold learning techniques within a \emph{Gaussian process upper confidence bound} algorithm to optimize an objective function on a manifold. Our approach is motivated by applications where a full representation of the manifold is not available and querying the objective is expensive. We rely on a point cloud of manifold samples to define a graph Gaussian process surrogate model for the objective. Query points are sequentially chosen using the posterior distribution of the surrogate model given all previous queries. We establish regret bounds in terms of the number of queries and the size of the point cloud. Several numerical examples complement the theory and illustrate the performance of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;&#12290;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#21098;&#26525;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#30340;&#36817;&#20284;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.09134</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization. (arXiv:2210.09134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;&#12290;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#21098;&#26525;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#30340;&#36817;&#20284;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#25152;&#26377;&#23884;&#22871;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35780;&#20272;&#36825;&#20123;&#23376;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#20027;&#35201;&#24212;&#29992;&#20110;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#31038;&#21306;&#30340;&#31616;&#21333;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21407;&#21017;&#24615;&#21098;&#26525;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#20250;&#20135;&#29983;&#36817;&#20284;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#21098;&#26525;&#31639;&#27861;&#65292;&#20197;&#32531;&#35299;&#30452;&#25509;&#24212;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#31616;&#21270;&#25152;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;UCI&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#23545;&#19981;&#21516;&#25512;&#29702;&#31639;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#21098;&#26525;&#26041;&#26696;&#35299;&#20915;&#20102;&#20449;&#21495;&#22788;&#29702;&#31038;&#21306;&#20351;&#29992;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;m.
&lt;/p&gt;
&lt;p&gt;
Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community on simple models. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. Direct application of Bayesian model reduction, however, gives rise to approximation errors. Therefore, a novel iterative pruning algorithm is presented to alleviate the problems arising with naive Bayesian model reduction, as supported experimentally on the publicly available UCI datasets for different inference algorithms. This novel parameter pruning scheme solves the shortcomings of current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and m
&lt;/p&gt;</description></item><item><title>&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#36801;&#31227;&#23398;&#20064;&#20174;&#22810;&#20010;&#23567;&#36741;&#21161;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20174;&#20855;&#26377;&#8220;&#30456;&#20284;&#8221;&#29305;&#24449;&#30340;&#32452;&#32455;&#22270;&#20687;&#20013;&#23398;&#20064;&#26579;&#33394;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#20687;&#30149;&#29702;&#23398;&#23478;&#19968;&#26679;&#33258;&#21160;&#35780;&#20998;&#32452;&#32455;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2209.05954</link><description>&lt;p&gt;
&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20687;&#30149;&#29702;&#23398;&#23478;&#19968;&#26679;&#33258;&#21160;&#35780;&#20998;&#32452;&#32455;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Automatically Score Tissue Images Like a Pathologist by Transfer Learning. (arXiv:2209.05954v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#36801;&#31227;&#23398;&#20064;&#20174;&#22810;&#20010;&#23567;&#36741;&#21161;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20174;&#20855;&#26377;&#8220;&#30456;&#20284;&#8221;&#29305;&#24449;&#30340;&#32452;&#32455;&#22270;&#20687;&#20013;&#23398;&#20064;&#26579;&#33394;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#20687;&#30149;&#29702;&#23398;&#23478;&#19968;&#26679;&#33258;&#21160;&#35780;&#20998;&#32452;&#32455;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#20129;&#21407;&#22240;&#12290;&#26089;&#26399;&#35786;&#26029;&#30284;&#30151;&#21487;&#20197;&#25405;&#25937;&#24456;&#22810;&#29983;&#21629;&#12290;&#30149;&#29702;&#23398;&#23478;&#24517;&#39035;&#25163;&#21160;&#26597;&#30475;&#32452;&#32455;&#24494;&#38453;&#21015; (TMA) &#22270;&#20687;&#20197;&#35782;&#21035;&#32959;&#30244;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#26102;&#38388;&#12289;&#19981;&#19968;&#33268;&#19988;&#20027;&#35266;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#26816;&#27979;&#32959;&#30244;&#30340;&#31639;&#27861;&#35201;&#20040;&#27809;&#26377;&#36798;&#21040;&#30149;&#29702;&#23398;&#23478;&#30340;&#20934;&#30830;&#27700;&#24179;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#20855;&#26377;&#19981;&#21516;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#30340; TMA &#22270;&#20687;&#21487;&#33021;&#20855;&#26377;&#30456;&#21516;&#30340;&#24471;&#20998;&#12290;&#30001;&#20110;&#21307;&#30103;&#32452;&#32455;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#38480;&#21046;&#65292;&#23398;&#20064; TMA &#22270;&#20687;&#30340;&#26579;&#33394;&#27169;&#24335;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#26469;&#33258;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#30340; TMA &#22270;&#20687;&#21487;&#33021;&#20855;&#26377;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20294;&#30452;&#25509;&#20351;&#29992;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36873;&#25321;&#24615;&#36801;&#31227;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#23567;&#36741;&#21161;&#38598;&#30340;&#30693;&#35782;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#21462;&#26174;&#31034;&#8220;&#31867;&#20284;&#8221;&#30340;&#32452;&#32455;&#22270;&#20687;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;TMA&#22270;&#20687;&#35780;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is the second leading cause of death in the world. Diagnosing cancer early on can save many lives. Pathologists have to look at tissue microarray (TMA) images manually to identify tumors, which can be time-consuming, inconsistent and subjective. Existing algorithms that automatically detect tumors have either not achieved the accuracy level of a pathologist or require substantial human involvements. A major challenge is that TMA images with different shapes, sizes, and locations can have the same score. Learning staining patterns in TMA images requires a huge number of images, which are severely limited due to privacy concerns and regulations in medical organizations. TMA images from different cancer types may have common characteristics that could provide valuable information, but using them directly harms the accuracy. By selective transfer learning from multiple small auxiliary sets, the proposed algorithm is able to extract knowledge from tissue images showing a ``similar" s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.04798</link><description>&lt;p&gt;
A*Net&#65306;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. (arXiv:2206.04798v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#37325;&#35201;&#33410;&#28857;&#21644;&#36793;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;A*Net&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#23545;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#29702;&#19968;&#30452;&#30001;&#23884;&#20837;&#26041;&#27861;&#20027;&#23548;&#12290;&#34429;&#28982;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#20855;&#26377;&#23884;&#20837;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#24402;&#32435;&#33021;&#21147;&#65292;&#20294;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#25351;&#25968;&#32423;&#36335;&#24452;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A*Net&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#36335;&#24452;&#26041;&#27861;&#12290;&#21463;&#21040;A*&#31639;&#27861;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;A*Net&#23398;&#20064;&#20102;&#19968;&#20010;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#36873;&#25321;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#27604;&#20363;&#21487;&#20197;&#25351;&#23450;&#65292;&#20197;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#20256;&#23548;&#24615;&#21644;&#24402;&#32435;&#24615;&#30693;&#35782;&#22270;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;A*Net&#22312;&#20165;&#35775;&#38382;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;10%&#33410;&#28857;&#21644;10%&#36793;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#22522;&#20110;&#36335;&#24452;&#26041;&#27861;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#30334;&#19975;&#32423;&#25968;&#25454;&#38598;ogbl-wikikg2&#19978;&#65292;A*Net&#19981;&#20165;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#36824;&#23454;&#29616;&#20102;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A* algorithm for shortest path problems, our A*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A*Net not only achieves a new state-of-the-art result, but also converges 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07028</link><description>&lt;p&gt;
&#25506;&#32034;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#22312;&#32852;&#37030;&#33976;&#39311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#30693;&#35782;&#19968;&#33268;&#24615;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#32852;&#37030;&#33976;&#39311;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#24322;&#36136;&#24615;&#24341;&#36215;&#30340;&#30693;&#35782;&#24046;&#24322;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm, &#22312;&#27492;&#26381;&#21153;&#22120;&#21608;&#26399;&#24615;&#22320;&#25910;&#38598;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;, &#32780;&#19981;&#32452;&#35013;&#20854;&#31169;&#26377;&#25968;&#25454;. &#26377;&#38480;&#30340;&#36890;&#35759;&#21644;&#20010;&#24615;&#21270;&#38656;&#27714;&#23545;FL&#25552;&#20986;&#20102;&#20005;&#23803;&#25361;&#25112;. &#32852;&#37030;&#33976;&#39311; (FD) &#34987;&#25552;&#20986;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;, &#19982;&#27492;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;, &#25903;&#25345;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#35759;&#24320;&#38144;. &#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FD&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;. &#19968;&#20123;&#26368;&#36817;&#30340;&#26080;&#20195;&#29702;&#25968;&#25454;&#30340;FD&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#39069;&#22806;&#30340;&#20844;&#20849;&#25968;&#25454;&#30340;&#38656;&#27714;, &#20294;&#30001;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#32780;&#20135;&#29983;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;, &#23548;&#33268;&#26381;&#21153;&#22120;&#19978;&#30340;&#27169;&#22411;&#34920;&#31034;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;.
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving machine learning paradigm in which the server periodically aggregates local model parameters from clients without assembling their private data.  Constrained communication and personalization requirements pose severe challenges to FL. Federated distillation (FD) is proposed to simultaneously address the above two problems, which exchanges knowledge between the server and clients, supporting heterogeneous local models while significantly reducing communication overhead. However, most existing FD methods require a proxy dataset, which is often unavailable in reality.  A few recent proxy-data-free FD approaches can eliminate the need for additional public data, but suffer from remarkable discrepancy among local knowledge due to client-side model heterogeneity, leading to ambiguous representation on the server and inevitable accuracy degradation.  To tackle this issue, we propose a proxy-data-free FD algorithm based on distributed knowledge c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#26469;&#20272;&#35745;&#39044;&#27979;&#24471;&#20998;&#30340;&#26102;&#21464;&#24046;&#24322;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#19981;&#21487;&#39564;&#35777;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2110.00115</link><description>&lt;p&gt;
&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Comparing Sequential Forecasters. (arXiv:2110.00115v5 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#26469;&#20272;&#35745;&#39044;&#27979;&#24471;&#20998;&#30340;&#26102;&#21464;&#24046;&#24322;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#19981;&#21487;&#39564;&#35777;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20004;&#20010;&#39044;&#27979;&#22120;&#65292;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#23545;&#19968;&#31995;&#21015;&#20107;&#20214;&#36827;&#34892;&#21333;&#27425;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#30456;&#23545;&#22522;&#30784;&#30340;&#38382;&#39064;&#65306;&#22312;&#19981;&#20551;&#35774;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#27604;&#36739;&#36825;&#20123;&#39044;&#27979;&#22120;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#36824;&#26159;&#20107;&#21518;&#27604;&#36739;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#29992;&#20110;&#20272;&#35745;&#26102;&#21464;&#39044;&#27979;&#24471;&#20998;&#24046;&#24322;&#30340;&#26032;&#22411;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#32473;&#20986;&#20102;&#20005;&#26684;&#30340;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#32622;&#20449;&#24207;&#21015;&#65288;CS&#65289;&#65292;&#23427;&#26159;&#19968;&#31995;&#21015;&#32622;&#20449;&#21306;&#38388;&#65292;&#21487;&#20197;&#36830;&#32493;&#30417;&#27979;&#24182;&#22312;&#20219;&#24847;&#25968;&#25454;&#20381;&#36182;&#20572;&#26102;&#65288;&#8220;anytime-valid&#8221;&#65289;&#19979;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32622;&#20449;&#24207;&#21015;&#30340;&#23485;&#24230;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#36866;&#24212;&#20102;&#24471;&#20998;&#24046;&#24322;&#30340;&#24213;&#23618;&#26041;&#24046;&#12290;&#23427;&#20204;&#30340;&#26500;&#24314;&#22522;&#20110;&#21338;&#24328;&#35770;&#32479;&#35745;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29992;&#20110;&#39034;&#24207;&#26816;&#39564;&#24369;&#38646;&#20551;&#35774;&#30340;e&#36807;&#31243;&#21644;p&#36807;&#31243;&#65292;&#21363;&#19968;&#20010;&#39044;&#27979;&#22120;&#24179;&#22343;&#34920;&#29616;&#26159;&#21542;&#20248;&#20110;&#21478;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider two forecasters, each making a single prediction for a sequence of events over time. We ask a relatively basic question: how might we compare these forecasters, either online or post-hoc, while avoiding unverifiable assumptions on how the forecasts and outcomes were generated? In this paper, we present a rigorous answer to this question by designing novel sequential inference procedures for estimating the time-varying difference in forecast scores. To do this, we employ confidence sequences (CS), which are sequences of confidence intervals that can be continuously monitored and are valid at arbitrary data-dependent stopping times ("anytime-valid"). The widths of our CSs are adaptive to the underlying variance of the score differences. Underlying their construction is a game-theoretic statistical framework, in which we further identify e-processes and p-processes for sequentially testing a weak null hypothesis -- whether one forecaster outperforms another on average (rather tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;GNN&#22312;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#27010;&#29575;&#19978;GNN&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#22495;&#19978;&#30340;&#21487;&#27979;&#20989;&#25968;&#37117;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20854;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#21464;&#21270;&#26377;&#20851;&#65292;&#24182;&#19988;&#32467;&#26524;&#21487;&#25193;&#23637;&#29992;&#20110;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2106.08992</link><description>&lt;p&gt;
GNN&#22312;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the approximation capability of GNNs in node classification/regression tasks. (arXiv:2106.08992v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;GNN&#22312;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#27010;&#29575;&#19978;GNN&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#22495;&#19978;&#30340;&#21487;&#27979;&#20989;&#25968;&#37117;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20854;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#21464;&#21270;&#26377;&#20851;&#65292;&#24182;&#19988;&#32467;&#26524;&#21487;&#25193;&#23637;&#29992;&#20110;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26159;&#19968;&#31181;&#24191;&#27867;&#30340;&#29992;&#20110;&#22270;&#22788;&#29702;&#30340;&#36830;&#25509;&#24335;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#36880;&#27493;&#36924;&#36817;&#20851;&#20110;&#22270;&#30340;&#20219;&#24847;&#20989;&#25968;&#65292;&#36825;&#21462;&#20915;&#20110;&#30001;Weisfeiler--Lehman(WL)&#27979;&#35797;&#23450;&#20041;&#30340;&#22270;&#31561;&#20215;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#19968;&#26041;&#38754;&#22240;&#20026;&#23427;&#20204;&#26159;&#29992;Stone-Weierstrass&#23450;&#29702;&#26469;&#25512;&#23548;&#24471;&#20986;&#30340;&#65292;&#36825;&#31181;&#26041;&#27861;&#26412;&#36136;&#19978;&#21482;&#26159;&#19968;&#31181;&#23384;&#22312;&#24615;&#35777;&#26126;&#65292;&#21478;&#19968;&#26041;&#38754;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#24403;&#21069;&#30340;&#32467;&#26524;&#37117;&#26159;&#19987;&#38376;&#38024;&#23545;&#22270;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#30340;&#65292;&#32780;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#38382;&#39064;&#20063;&#38750;&#24120;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;GNN&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#65292;&#26080;&#35770;&#30446;&#26631;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#36824;&#26159;&#31163;&#25955;&#30340;&#65292;GNN&#22312;&#27010;&#29575;&#19978;&#37117;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#20219;&#20309;&#26377;&#38480;&#22495;&#19978;&#30340;&#21487;&#27979;&#20989;&#25968;&#37117;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#21482;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#12289;ReLU&#28608;&#27963;&#20989;&#25968;&#21644;&#20849;&#20139;&#25152;&#26377;&#33410;&#28857;&#30340;&#36793;&#26435;&#20540;&#30340;GNN&#36827;&#34892;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20063;&#25581;&#31034;&#20102;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#38543;&#20854;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#21464;&#21270;&#21450;&#20854;&#32467;&#26524;&#38024;&#23545;&#22823;&#22411;&#22270;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a broad class of connectionist models for graph processing. Recent studies have shown that GNNs can approximate any function on graphs, modulo the equivalence relation on graphs defined by the Weisfeiler--Lehman (WL) test. However, these results suffer from some limitations, both because they were derived using the Stone--Weierstrass theorem -- which is existential in nature, -- and because they assume that the target function to be approximated must be continuous. Furthermore, all current results are dedicated to graph classification/regression tasks, where the GNN must produce a single output for the whole graph, while also node classification/regression problems, in which an output is returned for each node, are very common. In this paper, we propose an alternative way to demonstrate the approximation capability of GNNs that overcomes these limitations. Indeed, we show that GNNs are universal approximators in probability for node classification/regre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;Frank-Wolfe&#31639;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#20272;&#35745;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#20102;&#20248;&#21270;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2105.13913</link><description>&lt;p&gt;
&#21482;&#38656;&#31616;&#21333;&#27493;&#39588;&#65306;Frank-Wolfe&#31639;&#27861;&#21644;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions. (arXiv:2105.13913v6 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.13913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;Frank-Wolfe&#31639;&#27861;&#21464;&#20307;&#65292;&#21033;&#29992;&#24191;&#20041;&#33258;&#21327;&#35843;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#22312;&#19981;&#38656;&#35201;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#20272;&#35745;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;&#20102;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#33258;&#21327;&#35843;&#26159;&#35768;&#22810;&#37325;&#35201;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Frank-Wolfe&#21464;&#20307;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#35813;&#21464;&#20307;&#20351;&#29992;&#20102;&#24320;&#29615;&#27493;&#38271;&#31574;&#30053;$\gamma_t=2/(t+2)$&#65292;&#23545;&#20110;&#36825;&#31867;&#20989;&#25968;&#22312;&#21407;&#22987;&#38388;&#38553;&#21644;Frank-Wolfe&#38388;&#38553;&#26041;&#38754;&#33719;&#24471;&#20102;$\mathcal{O}(1/t)$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$t$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#36825;&#36991;&#20813;&#20102;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#25110;&#38656;&#35201;&#20272;&#35745;&#20808;&#21069;&#24037;&#20316;&#30340;&#23616;&#37096;&#24179;&#28369;&#24230;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#21516;&#24120;&#35265;&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#65292;&#20363;&#22914;&#65292;&#24403;&#25152;&#32771;&#34385;&#30340;&#21487;&#34892;&#22495;&#26159;&#22343;&#21248;&#20984;&#30340;&#25110;&#32773;&#26159;&#22810;&#38754;&#20307;&#30340;&#26102;&#20505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy $\gamma_t = 2/(t+2)$, obtaining a $\mathcal{O}(1/t)$ convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where $t$ is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.
&lt;/p&gt;</description></item></channel></rss>