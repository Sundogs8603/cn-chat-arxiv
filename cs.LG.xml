<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01344</link><description>&lt;p&gt;
&#21333;&#35843;&#12289;Bi-Lipschitz&#21644;Polyak-\L{}ojasiewicz&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Monotone, Bi-Lipschitz, and Polyak-\L{}ojasiewicz Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01344
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BiLipNet&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#36870;&#30340;\emph{Bi-Lipschitz}&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#25511;&#21046;&#20854;\emph{Lipschitzness}&#65288;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#36755;&#20986;&#25935;&#24863;&#24615;&#65289;&#21644;\emph{inverse Lipschitzness}&#65288;&#19981;&#21516;&#36755;&#20986;&#30340;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#65289;&#30340;&#33021;&#21147;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#20855;&#26377;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#20197;&#26500;&#24314;Bi-Lipschitz&#32593;&#32476;&#12290;&#35748;&#35777;&#26159;&#22522;&#20110;&#22686;&#37327;&#20108;&#27425;&#32422;&#26463;&#30340;&#65292;&#19982;&#35889;&#24402;&#19968;&#21270;&#30456;&#27604;&#65292;&#23427;&#33021;&#23454;&#29616;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#21453;&#21521;&#35745;&#31639;&#24418;&#24335;&#21270;&#20026;&#19977;&#31639;&#23376;&#20998;&#35010;&#38382;&#39064;&#65292;&#24050;&#30693;&#23384;&#22312;&#24555;&#36895;&#31639;&#27861;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;Bi-Lipschitz&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#37327;&#36755;&#20986;&#32593;&#32476;&#65292;&#21363;PLNet&#65292;&#23427;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#26377;&#21033;&#29305;&#24615;&#30340;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#65292;&#20363;&#22914;&#29420;&#29305;&#24615;&#21644;&#39640;&#25928;&#35745;&#31639;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new \emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \emph{Lipschitzness} (output sensitivity to input perturbations) and \emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01274</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#29305;&#24449;&#34920;&#31034;&#32780;&#34920;&#29616;&#20986;&#33394;&#12290;&#32463;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#23569;&#26679;&#26412;&#23398;&#20064;&#65289;&#20013;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#23613;&#31649;&#23545;&#20110;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#35780;&#20272;&#24050;&#32463;&#26377;&#20102;&#33391;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#22312;&#22768;&#23398;&#39046;&#22495;&#21364;&#26126;&#26174;&#32570;&#22833;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#19982;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#22522;&#20934;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19968;&#20123;&#23569;&#26679;&#26412;&#38382;&#39064;&#65288;&#22914;SpeechCommandsv2&#65289;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#35821;&#38899;&#20026;&#22522;&#30784;&#30340;&#23569;&#26679;&#26412;&#38382;&#39064;&#19982;&#22810;&#20010;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#30528;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.
&lt;/p&gt;</description></item><item><title>&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06627</link><description>&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#25512;&#21160;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Feedback Loops With Language Models Drive In-Context Reward Hacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06627
&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22806;&#37096;&#19990;&#30028;&#20135;&#29983;&#24433;&#21709;&#65306;&#23427;&#20204;&#26597;&#35810;&#21487;&#20197;&#35835;&#20889;&#32593;&#39029;&#30340;API&#65292;&#29983;&#25104;&#33021;&#22815;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#36816;&#34892;&#31995;&#32479;&#21629;&#20196;&#12290;&#36825;&#20123;&#20114;&#21160;&#24418;&#25104;&#20102;&#21453;&#39304;&#24490;&#29615;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#24433;&#21709;&#19990;&#30028;&#65292;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#21518;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;(ICRH)&#65292;&#21363;&#27979;&#35797;&#26102;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20248;&#21270;&#65288;&#21487;&#33021;&#38544;&#21547;&#30340;&#65289;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#34987;&#37096;&#32626;&#29992;&#20110;&#22686;&#21152;Twitter&#21442;&#19982;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65307;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#26816;&#32034;&#20854;&#20197;&#21069;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#25512;&#25991;&#26356;&#20855;&#20105;&#35758;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#21442;&#19982;&#24230;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#23548;&#33268;ICRH&#30340;&#20004;&#20010;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#23545;&#20110;&#36825;&#20123;&#36807;&#31243;&#65292;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26159;&#19981;&#36275;&#22815;&#30340;-&#20182;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#21453;&#39304;&#25928;&#24212;&#65292;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#26368;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#21442;&#25968;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#22312;&#24213;&#23618;&#28436;&#21270;&#20989;&#25968;&#26410;&#30693;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#23398;&#20064;&#39044;&#27979;&#19979;&#19968;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#24230;&#37327;&#21644;&#32500;&#24230;&#26469;&#37327;&#21270;&#22312;&#21487;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#38169;&#35823;&#21644;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06614</link><description>&lt;p&gt;
&#21160;&#21147;&#31995;&#32479;&#20013;&#39034;&#24207;&#39044;&#27979;&#30340;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Complexity of Sequential Prediction in Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06614
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#21442;&#25968;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#22312;&#24213;&#23618;&#28436;&#21270;&#20989;&#25968;&#26410;&#30693;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#23398;&#20064;&#39044;&#27979;&#19979;&#19968;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#32452;&#21512;&#24230;&#37327;&#21644;&#32500;&#24230;&#26469;&#37327;&#21270;&#22312;&#21487;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#38169;&#35823;&#21644;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24213;&#23618;&#28436;&#21270;&#20989;&#25968;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#19979;&#19968;&#29366;&#24577;&#30340;&#38382;&#39064;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#23545;&#21160;&#21147;&#31995;&#32479;&#27809;&#26377;&#21442;&#25968;&#20551;&#35774;&#65292;&#24182;&#20174;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#26032;&#30340;&#32452;&#21512;&#24230;&#37327;&#21644;&#32500;&#24230;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#37327;&#21270;&#20102;&#22312;&#21487;&#23454;&#29616;&#21644;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#38169;&#35823;&#21644;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning to predict the next state of a dynamical system when the underlying evolution function is unknown. Unlike previous work, we place no parametric assumptions on the dynamical system, and study the problem from a learning theory perspective. We define new combinatorial measures and dimensions and show that they quantify the optimal mistake and regret bounds in the realizable and agnostic setting respectively.
&lt;/p&gt;</description></item><item><title>RQP-SGD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#37096;&#32626;&#30340;&#20302;&#20869;&#23384;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#22312;&#20855;&#26377;&#20984;&#30446;&#26631;&#21644;&#37327;&#21270;&#32422;&#26463;&#30340;ML&#20219;&#21153;&#19978;&#30340;&#25928;&#29992;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06606</link><description>&lt;p&gt;
RQP-SGD&#65306;&#36890;&#36807;&#22024;&#26434;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#37327;&#21270;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06606
&lt;/p&gt;
&lt;p&gt;
RQP-SGD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#37096;&#32626;&#30340;&#20302;&#20869;&#23384;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#22312;&#20855;&#26377;&#20984;&#30446;&#26631;&#21644;&#37327;&#21270;&#32422;&#26463;&#30340;ML&#20219;&#21153;&#19978;&#30340;&#25928;&#29992;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20852;&#36215;&#20419;&#20351;&#20102;&#23545;&#22312;&#36793;&#32536;&#37096;&#32626;&#23454;&#26102;&#12289;&#39640;&#25928;&#12289;&#23433;&#20840;&#25968;&#25454;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#23454;&#20540;&#26435;&#37325;&#21442;&#25968;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#22312;&#22823;&#22411;&#27169;&#22411;&#19978;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20351;&#29992;&#37327;&#21270;&#31163;&#25955;&#26435;&#37325;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#20302;&#32500;&#27169;&#22411;&#20063;&#38656;&#35201;&#20445;&#25252;&#24213;&#23618;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RQP-SGD&#65292;&#19968;&#31181;&#29992;&#20110;&#20302;&#20869;&#23384;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#38544;&#31169;&#20445;&#25252;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#19982;&#38543;&#26426;&#37327;&#21270;&#30456;&#32467;&#21512;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34913;&#37327;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20984;&#30446;&#26631;&#21644;&#37327;&#21270;&#32422;&#26463;&#30340;ML&#20219;&#21153;&#19978;&#23454;&#26045;RQP-SGD&#30340;&#25928;&#29992;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of IoT devices has prompted the demand for deploying machine learning at-the-edge with real-time, efficient, and secure data processing. In this context, implementing machine learning (ML) models with real-valued weight parameters can prove to be impractical particularly for large models, and there is a need to train models with quantized discrete weights. At the same time, these low-dimensional models also need to preserve privacy of the underlying dataset. In this work, we present RQP-SGD, a new approach for privacy-preserving quantization to train machine learning models for low-memory ML-at-the-edge. This approach combines differentially private stochastic gradient descent (DP-SGD) with randomized quantization, providing a measurable privacy guarantee in machine learning. In particular, we study the utility convergence of implementing RQP-SGD on ML tasks with convex objectives and quantization constraints and demonstrate its efficacy over deterministic quantization. Throug
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;</title><link>https://arxiv.org/abs/2402.06590</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#65306;&#26234;&#33021;&#30340;&#22522;&#30707;
&lt;/p&gt;
&lt;p&gt;
Predictive representations: building blocks of intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06590
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#34892;&#20026;&#36890;&#24120;&#38656;&#35201;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#35268;&#23450;&#20102;&#20160;&#20040;&#26679;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#26159;&#26377;&#29992;&#30340;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#23427;&#20204;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#29702;&#35770;&#35266;&#28857;&#19982;&#35748;&#30693;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#32487;&#20219;&#32773;&#34920;&#24449;&#65288;SR&#65289;&#21450;&#20854;&#24191;&#20041;&#24418;&#24335;&#65292;&#23427;&#20204;&#19981;&#20165;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#24037;&#20855;&#65292;&#20063;&#20316;&#20026;&#22823;&#33041;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#34701;&#21512;&#34920;&#26126;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#20998;&#21106;&#20013;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#20027;&#24178;&#32593;&#32476;&#30340;&#29305;&#24449;&#33021;&#21542;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#36890;&#36807;&#25552;&#20986;&#29420;&#31435;&#25237;&#31080;&#21644;&#29305;&#24449;&#34701;&#21512;&#20004;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;PANet&#19978;&#23454;&#29616;&#20102;&#36825;&#20123;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#38598;&#25104;&#20027;&#24178;&#32593;&#32476;&#21487;&#20197;&#25429;&#25417;&#26356;&#20016;&#23500;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#21319;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06581</link><description>&lt;p&gt;
&#36229;&#36234;&#38646;&#20214;&#20043;&#21644;&#65306;&#38598;&#25104;&#20027;&#24178;&#32593;&#32476;&#36827;&#34892;&#23569;&#26679;&#26412;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
More than the Sum of Its Parts: Ensembling Backbone Networks for Few-Shot Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23569;&#26679;&#26412;&#20998;&#21106;&#20013;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#20027;&#24178;&#32593;&#32476;&#30340;&#29305;&#24449;&#33021;&#21542;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#36890;&#36807;&#25552;&#20986;&#29420;&#31435;&#25237;&#31080;&#21644;&#29305;&#24449;&#34701;&#21512;&#20004;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;PANet&#19978;&#23454;&#29616;&#20102;&#36825;&#20123;&#25216;&#26415;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#38598;&#25104;&#20027;&#24178;&#32593;&#32476;&#21487;&#20197;&#25429;&#25417;&#26356;&#20016;&#23500;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#21319;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#23454;&#29616;&#40065;&#26834;&#22270;&#20687;&#29702;&#35299;&#30340;&#20851;&#38190;&#21069;&#25552;&#12290;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#65292;&#23569;&#26679;&#26412;&#20998;&#21106;&#26159;&#20256;&#32479;&#20998;&#21106;&#26041;&#27861;&#30340;&#25193;&#23637;&#21644;&#20248;&#21270;&#12290;&#22312;&#23569;&#26679;&#26412;&#20998;&#21106;&#20013;&#65292;&#20027;&#35201;&#30340;&#26041;&#27861;&#26159;&#20381;&#38752;&#21333;&#19968;&#30340;&#20027;&#24178;&#32593;&#32476;&#36827;&#34892;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#12290;&#36873;&#25321;&#20351;&#29992;&#21738;&#20010;&#20027;&#24178;&#32593;&#32476;&#26159;&#24433;&#21709;&#25972;&#20307;&#24615;&#33021;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#20174;&#19981;&#21516;&#20027;&#24178;&#32593;&#32476;&#34701;&#21512;&#29305;&#24449;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#20998;&#21106;&#27169;&#22411;&#25429;&#25417;&#26356;&#20016;&#23500;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#38598;&#25104;&#25216;&#26415;&#8212;&#8212;&#29420;&#31435;&#25237;&#31080;&#21644;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#20998;&#21106;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#22312;PANet&#19978;&#23454;&#29616;&#20102;&#25552;&#20986;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#22312;PANet&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#21106;&#25513;&#30721;&#30340;&#27169;&#22359;&#36991;&#20813;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a key prerequisite to robust image understanding for applications in \acrlong{ai} and Robotics. \acrlong{fss}, in particular, concerns the extension and optimization of traditional segmentation methods in challenging conditions where limited training examples are available. A predominant approach in \acrlong{fss} is to rely on a single backbone for visual feature extraction. Choosing which backbone to leverage is a deciding factor contributing to the overall performance. In this work, we interrogate on whether fusing features from different backbones can improve the ability of \acrlong{fss} models to capture richer visual features. To tackle this question, we propose and compare two ensembling techniques-Independent Voting and Feature Fusion. Among the available \acrlong{fss} methods, we implement the proposed ensembling techniques on PANet. The module dedicated to predicting segmentation masks from the backbone embeddings in PANet avoids trainable parameters, 
&lt;/p&gt;</description></item><item><title>SAE&#26159;&#19968;&#31181;&#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38598;&#21512;&#36755;&#20837;&#30340;&#26368;&#20339;&#36864;&#20986;&#25968;&#37327;&#21644;&#28145;&#24230;&#65292;&#22312;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#26550;&#26500;&#25110;&#24212;&#29992;&#31243;&#24207;&#28789;&#27963;&#22320;&#23450;&#21046;&#20854;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.06580</link><description>&lt;p&gt;
SAE: &#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SAE: Single Architecture Ensemble Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06580
&lt;/p&gt;
&lt;p&gt;
SAE&#26159;&#19968;&#31181;&#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38598;&#21512;&#36755;&#20837;&#30340;&#26368;&#20339;&#36864;&#20986;&#25968;&#37327;&#21644;&#28145;&#24230;&#65292;&#22312;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#26550;&#26500;&#25110;&#24212;&#29992;&#31243;&#24207;&#28789;&#27963;&#22320;&#23450;&#21046;&#20854;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#19968;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38598;&#21512;&#33021;&#22815;&#22312;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#25552;&#21069;&#36864;&#20986;&#25110;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#26694;&#26550;&#23558;&#38598;&#21512;&#21387;&#32553;&#21040;&#21333;&#19968;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26223;&#35266;&#36804;&#20170;&#20026;&#27490;&#26159;&#38646;&#25955;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#31639;&#27861;&#24615;&#33021;&#33853;&#21518;&#20110;&#29420;&#31435;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#65292;&#24182;&#38656;&#35201;&#24191;&#27867;&#30340;&#26550;&#26500;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#21040;&#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#19968;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#38598;&#21512;&#36755;&#20837;&#30340;&#26368;&#20339;&#36864;&#20986;&#25968;&#37327;&#21644;&#28145;&#24230;&#12290;&#36825;&#20351;&#24471;SAE&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#26550;&#26500;&#25110;&#24212;&#29992;&#31243;&#24207;&#28789;&#27963;&#22320;&#23450;&#21046;&#20854;&#37197;&#32622;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#31867;&#22411;&#21644;&#22823;&#23567;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;SAE&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#25110;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembles of separate neural networks (NNs) have shown superior accuracy and confidence calibration over single NN across tasks. Recent methods compress ensembles within a single network via early exits or multi-input multi-output frameworks. However, the landscape of these methods is fragmented thus far, making it difficult to choose the right approach for a given task. Furthermore, the algorithmic performance of these methods is behind the ensemble of separate NNs and requires extensive architecture tuning. We propose a novel methodology unifying these approaches into a Single Architecture Ensemble (SAE). Our method learns the optimal number and depth of exits per ensemble input in a single NN. This enables the SAE framework to flexibly tailor its configuration for a given architecture or application. We evaluate SAEs on image classification and regression across various network architecture types and sizes. We demonstrate competitive accuracy or confidence calibration to baselines w
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.06578</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Universality of Coupling-based Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#65288;&#22914;RealNVP&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23613;&#31649;&#32806;&#21512;&#27969;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#24456;&#26222;&#36941;&#65292;&#20294;&#30001;&#20110;&#20854;&#21463;&#38480;&#30340;&#26550;&#26500;&#65292;&#23545;&#20110;&#32806;&#21512;&#27969;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#23450;&#29702;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20351;&#29992;&#20219;&#24847;&#30149;&#24577;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#32467;&#26500;&#26412;&#36136;&#19978;&#23548;&#33268;&#20307;&#31215;&#20445;&#25345;&#27969;&#65292;&#36825;&#26159;&#19968;&#20010;&#38480;&#21046;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#26412;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#26222;&#36866;&#24615;&#23450;&#29702;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#24037;&#20316;&#30340;&#20960;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#26222;&#36941;&#32463;&#39564;&#65292;&#24182;&#20026;&#36873;&#25321;&#32806;&#21512;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#32454;&#33268;&#20837;&#24494;&#30340;&#35266;&#28857;&#65292;&#22635;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel theoretical framework for understanding the expressive power of coupling-based normalizing flows such as RealNVP. Despite their prevalence in scientific applications, a comprehensive understanding of coupling flows remains elusive due to their restricted architectures. Existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. Additionally, we demonstrate that these constructions inherently lead to volume-preserving flows, a property which we show to be a fundamental constraint for expressivity. We propose a new distributional universality theorem for coupling-based normalizing flows, which overcomes several limitations of prior work. Our results support the general wisdom that the coupling architecture is expressive and provide a nuanced view for choosing the expressivity of coupling functions, bridging a gap between empirical results and theoretical understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperDistill&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#31616;&#24418;&#24577;&#26465;&#20214;&#36229;&#32593;&#32476;&#65292;&#21487;&#22312;&#35757;&#32451;&#21644;&#26410;&#30693;&#27979;&#35797;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#19982;&#36890;&#29992;&#30340;transformers&#31574;&#30053;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.06570</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#31616;&#24418;&#24577;&#26465;&#20214;&#36229;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#29992;&#24418;&#24577;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperDistill&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#31616;&#24418;&#24577;&#26465;&#20214;&#36229;&#32593;&#32476;&#65292;&#21487;&#22312;&#35757;&#32451;&#21644;&#26410;&#30693;&#27979;&#35797;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#19982;&#36890;&#29992;&#30340;transformers&#31574;&#30053;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#20043;&#38388;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#23545;&#26410;&#30693;&#24418;&#24577;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#36890;&#29992;&#31574;&#30053;&#38656;&#35201;&#20687;transformers&#65288;TF&#65289;&#36825;&#26679;&#20855;&#26377;&#36739;&#22823;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#22797;&#26434;&#26550;&#26500;&#65292;&#32780;&#27604;&#36739;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21017;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#22312;&#25512;&#29702;&#26102;&#26082;&#33021;&#36798;&#21040;&#20687;TF&#19968;&#26679;&#22909;&#30340;&#24615;&#33021;&#65292;&#21448;&#33021;&#20855;&#26377;&#20687;MLP&#19968;&#26679;&#30340;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDistill&#12290;&#23427;&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#20010;&#24418;&#24577;&#26465;&#20214;&#30340;&#36229;&#32593;&#32476;&#65288;HN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#26426;&#22120;&#20154;&#29305;&#23450;&#30340;MLP&#31574;&#30053;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#23545;&#20110;&#25104;&#21151;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#30340;&#31574;&#30053;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;UNIMAL&#19978;&#65292;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#31181;&#19981;&#21516;&#24418;&#24577;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;HyperDistill&#22312;&#35757;&#32451;&#21644;&#26410;&#30693;&#27979;&#35797;&#26426;&#22120;&#20154;&#19978;&#37117;&#33021;&#21644;&#36890;&#29992;&#30340;TF&#25945;&#24072;&#31574;&#30053;&#19968;&#26679;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#23558;&#27169;&#22411;&#23610;&#23544;&#20943;&#23567;&#20102;6-14&#20493;&#65292;&#35745;&#31639;&#25104;&#26412;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20943;&#23567;&#20102;67-160&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a universal policy across different robot morphologies can significantly improve learning efficiency and enable zero-shot generalization to unseen morphologies. However, learning a highly performant universal policy requires sophisticated architectures like transformers (TF) that have larger memory and computational cost than simpler multi-layer perceptrons (MLP). To achieve both good performance like TF and high efficiency like MLP at inference time, we propose HyperDistill, which consists of: (1) A morphology-conditioned hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy distillation approach that is essential for successful training. We show that on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill performs as well as a universal TF teacher policy on both training and unseen test robots, but reduces model size by 6-14 times, and computational cost by 67-160 times in different environments. Our analysis attributes the efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#21644;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65292;&#25581;&#31034;&#20102;&#21307;&#30103;&#23454;&#36341;&#27169;&#24335;&#19982;&#20002;&#22833;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;&#20998;&#26512;&#20559;&#35265;&#12289;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.06563</link><description>&lt;p&gt;
&#21307;&#23398;&#30340;&#26263;&#29289;&#36136;&#20013;&#38544;&#34255;&#30528;&#20160;&#20040;&#65311;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#22788;&#29702;&#20002;&#22833;&#25968;&#25454;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#21644;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65292;&#25581;&#31034;&#20102;&#21307;&#30103;&#23454;&#36341;&#27169;&#24335;&#19982;&#20002;&#22833;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;&#20998;&#26512;&#20559;&#35265;&#12289;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#20154;&#35760;&#24405;&#65288;EPR&#65289;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#20854;&#20013;&#21253;&#21547;&#37325;&#35201;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#29702;&#35299;&#21644;&#22788;&#29702;&#36825;&#20123;&#32570;&#22833;&#25968;&#25454;&#26159;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#26524;&#19981;&#21152;&#20197;&#35299;&#20915;&#65292;&#21487;&#33021;&#23548;&#33268;&#20998;&#26512;&#20013;&#30340;&#20559;&#35265;&#21644;&#20851;&#38190;&#32467;&#35770;&#30340;&#25197;&#26354;&#12290;&#32570;&#22833;&#25968;&#25454;&#21487;&#33021;&#19982;&#21307;&#30103;&#19987;&#19994;&#20154;&#22763;&#30340;&#23454;&#36341;&#27169;&#24335;&#26377;&#20851;&#65292;&#23545;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#34917;&#21487;&#20197;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#32479;&#35745;&#26041;&#27861;&#26469;&#29702;&#35299;&#21644;&#35299;&#37322;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#20013;&#24515;&#30340;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#20197;&#21450;&#33521;&#22269;&#26368;&#22823;&#30340;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65288;TARN&#65289;&#20013;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#12290;&#22312;&#23545;56,961&#20010;&#19982;&#20799;&#31461;&#24613;&#35786;&#37096;&#23601;&#35786;&#30456;&#20851;&#30340;&#21021;&#27493;&#29983;&#21629;&#20307;&#24449;&#21644;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20002;&#22833;&#25968;&#25454;&#24456;&#21487;&#33021;&#26159;&#38750;&#38543;&#26426;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#25968;&#25454;&#19982;&#21307;&#30103;&#19987;&#19994;&#20154;&#22763;&#30340;&#23454;&#36341;&#27169;&#24335;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic patient records (EPRs) produce a wealth of data but contain significant missing information. Understanding and handling this missing data is an important part of clinical data analysis and if left unaddressed could result in bias in analysis and distortion in critical conclusions. Missing data may be linked to health care professional practice patterns and imputation of missing data can increase the validity of clinical decisions. This study focuses on statistical approaches for understanding and interpreting the missing data and machine learning based clinical data imputation using a single centre's paediatric emergency data and the data from UK's largest clinical audit for traumatic injury database (TARN). In the study of 56,961 data points related to initial vital signs and observations taken on children presenting to an Emergency Department, we have shown that missing data are likely to be non-random and how these are linked to health care professional practice patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23433;&#20840;&#20445;&#35777;&#25506;&#32034;&#26694;&#26550;&#65292;&#20351;&#29992;&#26368;&#20248;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26377;&#38480;&#26102;&#38388;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#30340;&#20445;&#35777;&#25506;&#32034;&#65292;&#21516;&#26102;&#20855;&#26377;&#35777;&#26126;&#30340;&#23433;&#20840;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06562</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#23433;&#20840;&#20445;&#35777;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Safe Guaranteed Exploration for Non-linear Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23433;&#20840;&#20445;&#35777;&#25506;&#32034;&#26694;&#26550;&#65292;&#20351;&#29992;&#26368;&#20248;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26377;&#38480;&#26102;&#38388;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#30340;&#20445;&#35777;&#25506;&#32034;&#65292;&#21516;&#26102;&#20855;&#26377;&#35777;&#26126;&#30340;&#23433;&#20840;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#26159;&#38480;&#21046;&#26426;&#22120;&#20154;&#33258;&#20027;&#24615;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#34429;&#28982;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#36275;&#22815;&#25506;&#32034;&#30340;&#20445;&#35777;&#23545;&#20110;&#30830;&#20445;&#33258;&#20027;&#20219;&#21153;&#23436;&#25104;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23433;&#20840;&#20445;&#35777;&#25506;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20248;&#25511;&#21046;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#65306;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20445;&#35777;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#20219;&#24847;&#39640;&#27010;&#29575;&#19979;&#34987;&#35777;&#26126;&#26159;&#23433;&#20840;&#30340;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21487;&#36866;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#26410;&#30693;&#39046;&#22495;&#30340;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;SageMPC&#65292;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36827;&#34892;&#23433;&#20840;&#20445;&#35777;&#25506;&#32034;&#12290;SageMPC&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#25928;&#29575;&#65306;i) &#21033;&#29992;Lipschitz&#36793;&#30028;&#65292;ii) &#30446;&#26631;&#23548;&#21521;&#25506;&#32034;&#65292;&#21644;iii) &#36880;&#27493;&#35843;&#25972;&#39118;&#26684;&#30340;&#37325;&#26032;&#35268;&#21010;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safely exploring environments with a-priori unknown constraints is a fundamental challenge that restricts the autonomy of robots. While safety is paramount, guarantees on sufficient exploration are also crucial for ensuring autonomous task completion. To address these challenges, we propose a novel safe guaranteed exploration framework using optimal control, which achieves first-of-its-kind results: guaranteed exploration for non-linear systems with finite time sample complexity bounds, while being provably safe with arbitrarily high probability. The framework is general and applicable to many real-world scenarios with complex non-linear dynamics and unknown domains. Based on this framework we propose an efficient algorithm, SageMPC, SAfe Guaranteed Exploration using Model Predictive Control. SageMPC improves efficiency by incorporating three techniques: i) exploiting a Lipschitz bound, ii) goal-directed exploration, and iii) receding horizon style re-planning, all while maintaining th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35270;&#39057;&#26631;&#27880;&#22120;&#65288;VA&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#26500;&#24314;&#35270;&#39057;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#31995;&#32479;&#23454;&#29616;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#30452;&#25509;&#21442;&#19982;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#27880;&#37322;&#26041;&#27861;&#30340;&#36164;&#28304;&#28040;&#32791;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06560</link><description>&lt;p&gt;
&#35270;&#39057;&#26631;&#27880;&#22120;&#65306;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#26500;&#24314;&#35270;&#39057;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35270;&#39057;&#26631;&#27880;&#22120;&#65288;VA&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#23398;&#20064;&#26500;&#24314;&#35270;&#39057;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#31995;&#32479;&#23454;&#29616;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#30452;&#25509;&#21442;&#19982;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#27880;&#37322;&#26041;&#27861;&#30340;&#36164;&#28304;&#28040;&#32791;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#21644;&#19968;&#33268;&#30340;&#27880;&#37322;&#23545;&#20110;&#25104;&#21151;&#24320;&#21457;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#27880;&#37322;&#26041;&#27861;&#32791;&#36153;&#36164;&#28304;&#19988;&#25928;&#29575;&#20302;&#19979;&#65292;&#24120;&#24120;&#20381;&#36182;&#20110;&#38750;&#39046;&#22495;&#19987;&#23478;&#30340;&#31532;&#19977;&#26041;&#27880;&#37322;&#32773;&#12290;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#32780;&#35328;&#65292;&#36890;&#24120;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#22256;&#38590;&#26679;&#26412;&#24448;&#24448;&#24456;&#38590;&#22312;&#27809;&#26377;&#19994;&#21153;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#26631;&#27880;&#12290;&#36825;&#20123;&#22256;&#38590;&#26679;&#26412;&#22312;&#27880;&#37322;&#36807;&#31243;&#20013;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#22320;&#20986;&#29616;&#65292;&#38656;&#35201;&#36827;&#34892;&#21487;&#21464;&#27425;&#25968;&#30340;&#36845;&#20195;&#21644;&#21453;&#39304;&#24490;&#29615;&#65292;&#20174;&#32780;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#36153;&#29992;&#21644;&#26102;&#38388;&#25104;&#26412;&#20197;&#20445;&#35777;&#36136;&#37327;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26356;&#30452;&#25509;&#22320;&#36890;&#36807;&#39046;&#22495;&#19987;&#23478;&#30340;&#21442;&#19982;&#65292;&#20351;&#29992;&#20154;&#22312;&#24490;&#29615;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#35270;&#39057;&#26631;&#27880;&#22120;&#65288;VA&#65289;&#65292;&#29992;&#20110;&#27880;&#37322;&#12289;&#31649;&#29702;&#21644;&#36845;&#20195;&#35270;&#39057;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20197;&#26368;&#32456;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Traditional data annotation methods are resource-intensive and inefficient, often leading to a reliance on third-party annotators who are not the domain experts. Hard samples, which are usually the most informative for model training, tend to be difficult to label accurately and consistently without business context. These can arise unpredictably during the annotation process, requiring a variable number of iterations and rounds of feedback, leading to unforeseen expenses and time commitments to guarantee quality.   We posit that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We propose a novel framework we call Video Annotator (VA) for annotating, managing, and iterating on video classification datasets. Our approach offers a new paradigm for an end-user-centered model development process,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06559</link><description>&lt;p&gt;
Diffusion-ES:&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26799;&#24230;&#35268;&#21010;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#38646;&#38454;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#20013;&#23545;&#22797;&#26434;&#21644;&#22810;&#27169;&#24577;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#26377;&#24456;&#24378;&#20248;&#21183;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#22312;&#25193;&#25955;&#27169;&#22411;&#25152;&#25429;&#33719;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#21644;&#20284;&#28982;&#24615;&#30340;&#36712;&#36857;&#12290;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#38656;&#35201;&#19968;&#20010;&#36866;&#21512;&#20110;&#28165;&#27905;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#36712;&#36857;&#20248;&#21270;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionES&#65292;&#19968;&#31181;&#23558;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;Diffusion-ES&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#12290;&#23427;&#36890;&#36807;&#25130;&#26029;&#25193;&#25955;&#36807;&#31243;&#23545;&#24471;&#20998;&#39640;&#30340;&#36712;&#36857;&#36827;&#34892;&#21464;&#24322;&#65292;&#35813;&#36807;&#31243;&#24212;&#29992;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#21435;&#22122;&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#27450;&#39575;&#24615;&#36335;&#24452;&#35268;&#21010;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#23454;&#26102;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06552</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#27450;&#39575;&#24615;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Deceptive Path Planning via Reinforcement Learning with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#27450;&#39575;&#24615;&#36335;&#24452;&#35268;&#21010;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#23454;&#26102;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#39575;&#24615;&#36335;&#24452;&#35268;&#21010;(DPP)&#26159;&#25351;&#35774;&#35745;&#19968;&#26465;&#36335;&#24452;&#65292;&#20197;&#38544;&#34255;&#20854;&#30495;&#23454;&#30446;&#26631;&#20197;&#20813;&#34987;&#22806;&#37096;&#35266;&#23519;&#32773;&#21457;&#29616;&#12290;&#29616;&#26377;&#30340;DPP&#26041;&#27861;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#22914;&#20840;&#23616;&#29366;&#24577;&#21487;&#35266;&#23519;&#24615;&#21644;&#23436;&#32654;&#30340;&#27169;&#22411;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#21363;&#20351;&#23545;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#36827;&#34892;&#24494;&#23567;&#26356;&#25913;&#20063;&#20250;&#36843;&#20351;&#37325;&#26032;&#35745;&#31639;&#25972;&#20010;&#26032;&#35299;&#12290;&#37492;&#20110;&#36825;&#20123;&#32570;&#28857;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#32570;&#20047;&#36866;&#24212;&#29616;&#23454;&#38382;&#39064;&#35268;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#21450;&#26080;&#27861;&#35843;&#25972;&#27450;&#39575;&#31243;&#24230;&#21644;&#23454;&#26102;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;(RL)&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#35757;&#32451;&#31574;&#30053;&#20197;&#22312;&#20219;&#24847;&#21152;&#26435;&#22270;&#19978;&#25191;&#34892;DPP&#65292;&#24182;&#20811;&#26381;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#20102;&#19968;&#20010;&#23616;&#37096;&#24863;&#30693;&#27169;&#22411;&#65292;&#19968;&#31181;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#27010;&#25324;&#20102;DPP&#38382;&#39064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deceptive path planning (DPP) is the problem of designing a path that hides its true goal from an outside observer. Existing methods for DPP rely on unrealistic assumptions, such as global state observability and perfect model knowledge, and are typically problem-specific, meaning that even minor changes to a previously solved problem can force expensive computation of an entirely new solution. Given these drawbacks, such methods do not generalize to unseen problem instances, lack scalability to realistic problem sizes, and preclude both on-the-fly tunability of deception levels and real-time adaptivity to changing environments. In this paper, we propose a reinforcement learning (RL)-based scheme for training policies to perform DPP over arbitrary weighted graphs that overcomes these issues. The core of our approach is the introduction of a local perception model for the agent, a new state space representation distilling the key components of the DPP problem, the use of graph neural ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.06544</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26657;&#20934;&#38271;&#31687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Calibrating Long-form Generations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#65292;&#26657;&#20934;&#26159;&#24517;&#35201;&#30340; - &#27169;&#22411;&#30340;&#35780;&#20272;&#32622;&#20449;&#24230;&#24212;&#35813;&#19982;&#20854;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#23454;&#38469;&#21487;&#33021;&#24615;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#21644;&#26657;&#20934;&#25351;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#20108;&#20803;&#30495;/&#20551;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#31687;&#29983;&#25104;&#20013;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#31572;&#26696;&#21487;&#33021;&#37096;&#20998;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#30340;&#21709;&#24212;&#27491;&#30830;&#24615;&#21644;&#20851;&#32852;&#30340;&#32622;&#20449;&#27700;&#24179;&#37117;&#34987;&#35270;&#20026;&#19968;&#31995;&#21015;&#20998;&#25968;&#30340;&#20998;&#24067;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#26469;&#31934;&#30830;&#35780;&#20272;LLM&#30340;&#26657;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#19968;&#33268;&#24615;&#21644;&#33258;&#35780;&#20272;&#30340;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06535</link><description>&lt;p&gt;
Bandit Convex Optimisation&#65288;&#24378;&#30423;&#20984;&#20248;&#21270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Bandit Convex Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#30423;&#20984;&#20248;&#21270;&#26159;&#30740;&#31350;&#38646;&#38454;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35768;&#22810;&#24037;&#20855;&#65292;&#21253;&#25324;&#20999;&#24179;&#38754;&#26041;&#27861;&#12289;&#20869;&#28857;&#26041;&#27861;&#12289;&#36830;&#32493;&#25351;&#25968;&#26435;&#37325;&#12289;&#26799;&#24230;&#19979;&#38477;&#21644;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;&#12290;&#35299;&#37322;&#20102;&#35768;&#22810;&#20551;&#35774;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#23613;&#31649;&#22312;&#36825;&#37324;&#27809;&#26377;&#22826;&#22810;&#30495;&#27491;&#26032;&#30340;&#19996;&#35199;&#65292;&#20294;&#19968;&#20123;&#29616;&#26377;&#24037;&#20855;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#33719;&#24471;&#26032;&#31639;&#27861;&#12290;&#19968;&#20123;&#30028;&#38480;&#31245;&#24494;&#25913;&#36827;&#20102;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. These notes cover the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online Newton step. The nuances between the many assumptions and setups are explained. Although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. A few bounds are improved in minor ways.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#65292;&#23558;&#20248;&#21270;&#36712;&#36857;&#38480;&#21046;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#27169;&#22411;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06532</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#29992;&#20110;&#20195;&#29702;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Bayesian Optimization for Surrogate Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#65292;&#23558;&#20248;&#21270;&#36712;&#36857;&#38480;&#21046;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#27169;&#22411;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#19981;&#26597;&#35810;&#30495;&#23454;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#20248;&#21270;&#23398;&#20064;&#21040;&#30340;&#20195;&#29702;&#30446;&#26631;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32463;&#24120;&#36935;&#21040;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#30340;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;Lipschitz&#26377;&#30028;&#28304;&#25209;&#35780;&#23478;&#27169;&#22411;&#26469;&#32422;&#26463;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#20808;&#39564;&#30340;&#19968;&#23450;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21160;&#24577;&#35843;&#25972;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#30340;&#24378;&#24230;&#12290;&#22312;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/michael-s-yao/gabo &#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline model-based policy optimization seeks to optimize a learned surrogate objective function without querying the true oracle objective during optimization. However, inaccurate surrogate model predictions are frequently encountered along the optimization trajectory. To address this limitation, we propose generative adversarial Bayesian optimization (GABO) using adaptive source critic regularization, a task-agnostic framework for Bayesian optimization that employs a Lipschitz-bounded source critic model to constrain the optimization trajectory to regions where the surrogate function is reliable. We show that under certain assumptions for the continuous input space prior, our algorithm dynamically adjusts the strength of the source critic regularization. GABO outperforms existing baselines on a number of different offline optimization tasks across a variety of scientific domains. Our code is available at https://github.com/michael-s-yao/gabo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20843;&#21449;&#26641;&#32467;&#26500;&#23558;&#26631;&#31614;&#20174;&#19968;&#20010;&#24050;&#26631;&#27880;&#30340;&#28857;&#20113;&#36716;&#31227;&#21040;&#19968;&#20010;&#26410;&#26631;&#27880;&#30340;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28857;&#20113;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#33258;&#21160;&#22312;&#34920;&#31034;&#21516;&#19968;&#30495;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#28857;&#20113;&#20043;&#38388;&#36827;&#34892;&#26631;&#31614;&#36716;&#31227;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06531</link><description>&lt;p&gt;
&#32771;&#34385;&#21464;&#21270;&#26816;&#27979;&#30340;&#35821;&#20041;&#20843;&#21449;&#26641;&#20043;&#38388;&#28857;&#20113;&#31435;&#38754;&#26631;&#31614;&#30340;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Transferring facade labels between point clouds with semantic octrees while considering change detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20843;&#21449;&#26641;&#32467;&#26500;&#23558;&#26631;&#31614;&#20174;&#19968;&#20010;&#24050;&#26631;&#27880;&#30340;&#28857;&#20113;&#36716;&#31227;&#21040;&#19968;&#20010;&#26410;&#26631;&#27880;&#30340;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28857;&#20113;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#33258;&#21160;&#22312;&#34920;&#31034;&#21516;&#19968;&#30495;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#28857;&#20113;&#20043;&#38388;&#36827;&#34892;&#26631;&#31614;&#36716;&#31227;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#19977;&#32500;&#25968;&#25454;&#22312;&#27979;&#32472;&#12289;&#24314;&#31569;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#36825;&#20123;&#25968;&#25454;&#26159;&#19981;&#22815;&#30340;&#65292;&#35821;&#20041;&#26631;&#31614;&#23545;&#20110;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20843;&#21449;&#26641;&#32467;&#26500;&#23558;&#26631;&#31614;&#20174;&#19968;&#20010;&#24050;&#26631;&#27880;&#30340;&#28857;&#20113;&#36716;&#31227;&#21040;&#19968;&#20010;&#26410;&#26631;&#27880;&#30340;&#28857;&#20113;&#30340;&#26041;&#27861;&#12290;&#35813;&#32467;&#26500;&#36824;&#20998;&#26512;&#20102;&#28857;&#20113;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#21464;&#21270;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#36716;&#31227;&#26631;&#31614;&#12290;&#26412;&#39033;&#30446;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34920;&#31034;&#21516;&#19968;&#30495;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#28857;&#20113;&#20043;&#38388;&#36827;&#34892;&#33258;&#21160;&#26631;&#31614;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#36824;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#26631;&#31614;&#36716;&#31227;&#26469;&#35268;&#36991;&#38543;&#26426;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point clouds and high-resolution 3D data have become increasingly important in various fields, including surveying, construction, and virtual reality. However, simply having this data is not enough; to extract useful information, semantic labeling is crucial. In this context, we propose a method to transfer annotations from a labeled to an unlabeled point cloud using an octree structure. The structure also analyses changes between the point clouds. Our experiments confirm that our method effectively transfers annotations while addressing changes. The primary contribution of this project is the development of the method for automatic label transfer between two different point clouds that represent the same real-world object. The proposed method can be of great importance for data-driven deep learning algorithms as it can also allow circumventing stochastic transfer learning by deterministic label transfer between datasets depicting the same objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06530</link><description>&lt;p&gt;
&#25913;&#36827;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#22312;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#36827;&#19968;&#27493;&#24515;&#32908;&#25439;&#20260;&#38750;&#24120;&#37325;&#35201;&#65292;MI&#26159;&#30001;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#24341;&#36215;&#30340;&#19968;&#31181;&#20005;&#37325;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#65288;OCC&#65289;&#31639;&#27861;&#36827;&#34892;&#26089;&#26399;MI&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#22810;&#27169;&#24577;&#23376;&#31354;&#38388;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#20811;&#26381;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;&#25216;&#26415;&#28041;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;MI&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22797;&#21512;&#26680;&#22312;&#38750;&#32447;&#24615;&#25237;&#24433;&#25216;&#24039;&#20013;&#34701;&#21512;&#39640;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;sigmoid&#20989;&#25968;&#65292;&#23558;&#22810;&#35270;&#22270;&#36229;&#22768;&#24515;&#21160;&#22270;&#32467;&#21512;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#35843;&#25972;&#25237;&#24433;&#30697;&#38453;&#30340;&#26368;&#22823;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25237;&#24433;&#30697;&#38453;&#26356;&#26032;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#20248;&#21270;&#30340;&#20302;&#32500;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;MI&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#26469;&#30740;&#31350;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06525</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#26080;&#38480;&#23485;&#22270;&#21367;&#31215;&#32593;&#32476;&#21450;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Flexible infinite-width graph convolutional networks and the importance of representation learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#26469;&#30740;&#31350;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#24120;&#35265;&#29702;&#35770;&#26041;&#27861;&#26159;&#36827;&#34892;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#65292;&#27492;&#26102;&#36755;&#20986;&#25104;&#20026;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#24067;&#12290;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#12290;&#28982;&#32780;&#65292;NNGP&#20869;&#26680;&#26159;&#22266;&#23450;&#30340;&#65292;&#21482;&#33021;&#36890;&#36807;&#23569;&#37327;&#36229;&#21442;&#25968;&#36827;&#34892;&#35843;&#33410;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#19982;&#26377;&#38480;&#23485;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#36890;&#24120;&#34987;&#35748;&#20026;&#33021;&#22815;&#34920;&#29616;&#33391;&#22909;&#65292;&#27491;&#26159;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20351;&#20854;&#22312;&#29702;&#35770;&#19978;&#21487;&#22788;&#29702;&#30340;&#21516;&#26102;&#65292;NNGP&#21487;&#33021;&#20250;&#28040;&#38500;&#20351;&#20854;&#24037;&#20316;&#33391;&#22909;&#30340;&#22240;&#32032;&#65288;&#34920;&#31034;&#23398;&#20064;&#65289;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#31034;&#23398;&#20064;&#26159;&#21542;&#24517;&#35201;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#24037;&#20855;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#12290;&#36825;&#19982;NNGP&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#20026;&#23427;&#26159;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#24182;&#20351;&#29992;&#20869;&#26680;&#65292;&#20294;&#23427;&#24102;&#26377;&#19968;&#20010;&#8220;&#26059;&#38062;&#8221;&#26469;&#25511;&#21046;&#34920;&#31034;&#23398;&#20064;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become Gaussian process (GP) distributed. This is known as a neural network Gaussian process (NNGP). However, the NNGP kernel is fixed, and tunable only through a small number of hyperparameters, eliminating any possibility of representation learning. This contrasts with finite-width NNs, which are often believed to perform well precisely because they are able to learn representations. Thus in simplifying NNs to make them theoretically tractable, NNGPs may eliminate precisely what makes them work well (representation learning). This motivated us to understand whether representation learning is necessary in a range of graph classification tasks. We develop a precise tool for this task, the graph convolutional deep kernel machine. This is very similar to an NNGP, in that it is an infinite width limit and uses kernels, but comes with a `knob' to control the amount 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;MLS&#28857;&#20113;&#21644;&#35789;&#34955;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#31435;&#38754;&#32454;&#33410;&#12290;&#36890;&#36807;&#32467;&#21512;&#39044;&#23450;&#20041;&#30340;3D&#27169;&#22411;&#24211;&#21644;&#21322;&#20840;&#23616;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#35789;&#34955;&#27861;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#31435;&#38754;&#37325;&#24314;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#25110;&#20272;&#31639;&#31435;&#38754;&#22826;&#38451;&#33021;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06521</link><description>&lt;p&gt;
&#20351;&#29992;MLS&#28857;&#20113;&#21644;&#35789;&#34955;&#27861;&#37325;&#24314;&#31435;&#38754;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
Reconstructing facade details using MLS point clouds and Bag-of-Words approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;MLS&#28857;&#20113;&#21644;&#35789;&#34955;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#31435;&#38754;&#32454;&#33410;&#12290;&#36890;&#36807;&#32467;&#21512;&#39044;&#23450;&#20041;&#30340;3D&#27169;&#22411;&#24211;&#21644;&#21322;&#20840;&#23616;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#35789;&#34955;&#27861;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#31435;&#38754;&#37325;&#24314;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#25110;&#20272;&#31639;&#31435;&#38754;&#22826;&#38451;&#33021;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31435;&#38754;&#20803;&#32032;&#37325;&#24314;&#20013;&#65292;&#35782;&#21035;&#29305;&#23450;&#23545;&#35937;&#31867;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#24120;&#24120;&#36890;&#36807;&#30697;&#24418;&#20551;&#35774;&#25110;&#36793;&#30028;&#26694;&#26469;&#35268;&#36991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37325;&#24314;3D&#31435;&#38754;&#32454;&#33410;&#12290;&#25105;&#20204;&#23558;MLS&#28857;&#20113;&#21644;&#39044;&#23450;&#20041;&#30340;3D&#27169;&#22411;&#24211;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;&#35789;&#34955;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#21152;&#20837;&#21322;&#20840;&#23616;&#29305;&#24449;&#26469;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#21472;&#21152;&#20102;&#38543;&#26426;&#22122;&#22768;&#30340;&#27169;&#22411;&#21644;TUM-FA\c{C}ADE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#35789;&#34955;&#27861;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#22312;&#19981;&#36827;&#34892;&#30697;&#24418;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#31435;&#38754;&#37325;&#24314;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#25110;&#20272;&#31639;&#31435;&#38754;&#22826;&#38451;&#33021;&#28508;&#21147;&#31561;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the reconstruction of fa\c{c}ade elements, the identification of specific object types remains challenging and is often circumvented by rectangularity assumptions or the use of bounding boxes. We propose a new approach for the reconstruction of 3D fa\c{c}ade details. We combine MLS point clouds and a pre-defined 3D model library using a BoW concept, which we augment by incorporating semi-global features. We conduct experiments on the models superimposed with random noise and on the TUM-FA\c{C}ADE dataset. Our method demonstrates promising results, improving the conventional BoW approach. It holds the potential to be utilized for more realistic facade reconstruction without rectangularity assumptions, which can be used in applications such as testing automated driving functions or estimating fa\c{c}ade solar potential.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.06512</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Trial Outcome Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#24180;&#26102;&#38388;&#21644;&#22823;&#37327;&#36130;&#21147;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#25490;&#38500;&#21487;&#33021;&#22833;&#36133;&#30340;&#33647;&#29289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#25104;&#26412;&#33410;&#32422;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#36866;&#24212;&#26032;&#27169;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20284;&#20449;&#24687;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#65288;LIFTED&#65289;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIFTED&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;LIFTED&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#65292;&#20174;&#27169;&#24577;&#29305;&#23450;&#30340;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34701;&#21512;&#20960;&#20309;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#20419;&#36827;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.06506</link><description>&lt;p&gt;
&#20351;&#29992;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying point clouds at the facade-level using geometric features and deep learning networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34701;&#21512;&#20960;&#20309;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#20419;&#36827;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31435;&#38754;&#32454;&#33410;&#30340;&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#31435;&#38754;&#32423;&#21035;&#19978;&#23545;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#26159;&#21019;&#24314;&#36825;&#26679;&#30340;&#25968;&#23383;&#21103;&#26412;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36825;&#31181;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26089;&#26399;&#34701;&#21512;&#30340;&#29305;&#24449;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#34917;&#20607;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#22312;&#25429;&#25417;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20419;&#36827;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D building models with facade details are playing an important role in many applications now. Classifying point clouds at facade-level is key to create such digital replicas of the real world. However, few studies have focused on such detailed classification with deep neural networks. We propose a method fusing geometric features with deep learning networks for point cloud classification at facade-level. Our experiments conclude that such early-fused features improve deep learning methods' performance. This method can be applied for compensating deep learning networks' ability in capturing local geometric information and promoting the advancement of semantic segmentation.
&lt;/p&gt;</description></item><item><title>ACTER&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;RL&#31574;&#30053;&#22833;&#36133;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.06503</link><description>&lt;p&gt;
ACTER: &#29992;&#20110;&#35299;&#37322;&#21644;&#35786;&#26029;RL&#31574;&#30053;&#30340;&#22810;&#26679;&#19988;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06503
&lt;/p&gt;
&lt;p&gt;
ACTER&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;RL&#31574;&#30053;&#22833;&#36133;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#22833;&#36133;&#22914;&#20309;&#21457;&#29983;&#20197;&#21450;&#22914;&#20309;&#38450;&#27490;&#26159;&#20026;&#20102;&#23454;&#29616;&#35843;&#35797;&#12289;&#32500;&#25252;&#29992;&#25143;&#20449;&#20219;&#21644;&#24320;&#21457;&#20010;&#24615;&#21270;&#31574;&#30053;&#32780;&#24517;&#35201;&#30340;&#12290;&#21453;&#20107;&#23454;&#25512;&#29702;&#32463;&#24120;&#34987;&#29992;&#26469;&#24402;&#21646;&#21644;&#29702;&#35299;&#22833;&#36133;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#25509;&#36817;&#30340;&#21487;&#33021;&#19990;&#30028;&#20197;&#36991;&#20813;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;RL&#20013;&#30340;&#21453;&#20107;&#23454;&#29366;&#24577;&#35299;&#37322;&#21482;&#33021;&#20351;&#29992;&#24403;&#21069;&#29366;&#24577;&#29305;&#24449;&#26469;&#35299;&#37322;&#32467;&#26524;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#39044;&#38450;&#36127;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#25514;&#26045;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTER&#65288;&#29992;&#20110;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#32467;&#26524;&#30340;&#21487;&#34892;&#21453;&#20107;&#23454;&#24207;&#21015;&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#29983;&#25104;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;&#22833;&#36133;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;ACTER&#30740;&#31350;&#23548;&#33268;&#22833;&#36133;&#30340;&#21160;&#20316;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;NSGA-II&#29983;&#25104;&#21487;&#20197;&#26368;&#23567;&#21270;&#25913;&#21464;&#19988;&#20855;&#26377;&#39640;&#30830;&#23450;&#24615;&#30340;&#21453;&#20107;&#23454;&#21160;&#20316;&#24207;&#21015;&#65292;&#20197;&#38450;&#27490;&#22833;&#36133;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how failure occurs and how it can be prevented in reinforcement learning (RL) is necessary to enable debugging, maintain user trust, and develop personalized policies. Counterfactual reasoning has often been used to assign blame and understand failure by searching for the closest possible world in which the failure is avoided. However, current counterfactual state explanations in RL can only explain an outcome using just the current state features and offer no actionable recourse on how a negative outcome could have been prevented. In this work, we propose ACTER (Actionable Counterfactual Sequences for Explaining Reinforcement Learning Outcomes), an algorithm for generating counterfactual sequences that provides actionable advice on how failure can be avoided. ACTER investigates actions leading to a failure and uses the evolutionary algorithm NSGA-II to generate counterfactual sequences of actions that prevent it with minimal changes and high certainty even in stochastic 
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06501</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scalable Interactive Machine Learning for Future Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06501
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#37492;&#20110;&#38656;&#35201;&#24378;&#22823;&#30340;&#20915;&#31574;&#36807;&#31243;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#38598;&#25104;&#20855;&#26377;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;C2&#36816;&#20316;&#27969;&#31243;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#26368;&#36817;&#22312;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#65292;&#20154;&#31867;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21512;&#20316;&#20197;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#30446;&#21069;&#31185;&#25216;&#21457;&#23637;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#24046;&#36317;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;C2&#29615;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#20010;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20849;&#21516;&#26088;&#22312;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SIML&#65289;&#65306;1&#65289;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#31639;&#27861;&#20197;&#23454;&#29616;&#21327;&#21516;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25506;&#35752;&#20102;&#22312;&#20840;&#39592;&#39635;&#21644;&#28107;&#24052;&#32467;&#29031;&#23556;&#27835;&#30103;&#20013;&#65292;&#20351;&#29992;2D&#21644;3D U-Net&#27169;&#22411;&#20197;&#21450;nnU-Net&#26694;&#26550;&#33258;&#21160;&#20998;&#21106;&#29031;&#23556;&#35745;&#21010;&#38774;&#20307;&#31215;(PTV)&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;nnU-Net&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06494</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#39592;&#39635;&#21644;&#28107;&#24052;&#32467;&#29031;&#23556;&#35745;&#21010;&#38774;&#20307;&#31215;&#33258;&#21160;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Auto-Segmentation of Planning Target Volume for Total Marrow and Lymph Node Irradiation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25506;&#35752;&#20102;&#22312;&#20840;&#39592;&#39635;&#21644;&#28107;&#24052;&#32467;&#29031;&#23556;&#27835;&#30103;&#20013;&#65292;&#20351;&#29992;2D&#21644;3D U-Net&#27169;&#22411;&#20197;&#21450;nnU-Net&#26694;&#26550;&#33258;&#21160;&#20998;&#21106;&#29031;&#23556;&#35745;&#21010;&#38774;&#20307;&#31215;(PTV)&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;nnU-Net&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20248;&#21270;&#30284;&#30151;&#27835;&#30103;&#20013;&#30340;&#25918;&#30103;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#29031;&#23556;&#22914;&#20840;&#39592;&#39635;&#21644;&#28107;&#24052;&#32467;&#29031;&#23556;(TMLI)&#26102;&#65292;&#20934;&#30830;&#21246;&#30011;&#29031;&#23556;&#35745;&#21010;&#38774;&#20307;&#31215;(PTV)&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#36825;&#31181;&#27835;&#30103;&#65292;&#20381;&#36182;&#25163;&#24037;&#21246;&#30011;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#28145;&#24230;&#23398;&#20064;(DL)&#24212;&#29992;&#20110;&#33258;&#21160;&#20998;&#21106;TMLI&#27835;&#30103;&#20013;&#30340;PTV&#65292;&#24314;&#31435;&#22312;&#20808;&#21069;&#22522;&#20110;2D U-Net&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#19978;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;(i)&#36890;&#36807;&#20351;&#29992;nnU-Net&#26694;&#26550;&#24320;&#21457;&#20102;2D&#21644;3D U-Net&#27169;&#22411;&#65292;(ii)&#36890;&#36807;&#23558;&#39592;&#39612;&#25490;&#38500;&#22312;&#22806;&#35780;&#20272;&#20102;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#22312;PTV&#19978;&#30340;&#34920;&#29616;&#65292;&#39592;&#39612;&#20027;&#35201;&#21253;&#21547;&#28107;&#24052;&#32467;&#65292;&#26159;&#26368;&#20855;&#25361;&#25112;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;nnU-Net&#26694;&#26550;&#30340;&#24341;&#20837;&#22312;&#20998;&#21106;&#24615;&#33021;&#19978;&#26377;&#32479;&#35745;&#19978;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to optimize the radiotherapy delivery for cancer treatment, especially when dealing with complex treatments such as Total Marrow and Lymph Node Irradiation (TMLI), the accurate contouring of the Planning Target Volume (PTV) is crucial. Unfortunately, relying on manual contouring for such treatments is time-consuming and prone to errors. In this paper, we investigate the application of Deep Learning (DL) to automate the segmentation of the PTV in TMLI treatment, building upon previous work that introduced a solution to this problem based on a 2D U-Net model. We extend the previous research (i) by employing the nnU-Net framework to develop both 2D and 3D U-Net models and (ii) by evaluating the trained models on the PTV with the exclusion of bones, which consist mainly of lymp-nodes and represent the most challenging region of the target volume to segment. Our result show that the introduction of nnU-NET framework led to statistically significant improvement in the segmentation p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SQ-Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#24341;&#20837;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#35757;&#32451;&#38598;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#65292;&#37117;&#33021;&#22815;&#26126;&#30830;&#22320;&#40723;&#21169;&#27169;&#22411;&#22312;&#32534;&#30721;&#21477;&#23376;&#26102;&#20445;&#25345;&#31995;&#32479;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06492</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#23884;&#20837;&#22312;Transformer&#20013;&#24341;&#23548;&#31995;&#32479;&#24615;
&lt;/p&gt;
&lt;p&gt;
Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SQ-Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#24341;&#20837;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#35757;&#32451;&#38598;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#65292;&#37117;&#33021;&#22815;&#26126;&#30830;&#22320;&#40723;&#21169;&#27169;&#22411;&#22312;&#32534;&#30721;&#21477;&#23376;&#26102;&#20445;&#25345;&#31995;&#32479;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35757;&#32451;&#36807;&#22797;&#26434;&#25968;&#25454;&#38598;&#21518;&#33021;&#22815;&#25512;&#24191;&#21040;&#32467;&#26500;&#21644;&#23454;&#20307;&#30340;&#26032;&#32452;&#21512;&#65292;&#20294;&#22312;&#22797;&#26434;&#24230;&#19981;&#36275;&#30340;&#25968;&#25454;&#38598;&#19978;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#35757;&#32451;&#38598;&#36275;&#22815;&#22797;&#26434;&#26102;&#65292;&#27169;&#22411;&#20351;&#29992;&#31995;&#32479;&#24615;&#30340;&#27880;&#24847;&#27169;&#24335;&#23545;&#20855;&#26377;&#20849;&#21516;&#21477;&#27861;&#32467;&#26500;&#30340;&#21477;&#23376;&#36827;&#34892;&#32534;&#30721;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SQ-Transformer&#65288;&#32467;&#26500;&#21270;&#37327;&#21270;&#65289;&#65292;&#21363;&#20351;&#20351;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;&#35757;&#32451;&#38598;&#65292;&#20063;&#33021;&#26126;&#30830;&#22320;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#40723;&#21169;&#31995;&#32479;&#24615;&#12290;&#22312;&#23884;&#20837;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26500;&#23548;&#21521;&#30340;&#21521;&#37327;&#37327;&#21270;&#65288;SoVQ&#65289;&#65292;&#23558;&#21333;&#35789;&#23884;&#20837;&#32858;&#31867;&#25104;&#33509;&#24178;&#31867;&#20855;&#26377;&#32467;&#26500;&#31561;&#20215;&#30340;&#23454;&#20307;&#12290;&#22312;&#27880;&#24847;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31995;&#32479;&#24615;&#27880;&#24847;&#23618;&#65288;SAL&#65289;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#31995;&#32479;&#24615;&#27491;&#21017;&#21270;&#23618;&#65288;SRL&#65289;&#65292;&#23427;&#20204;&#37117;&#22312;&#37327;&#21270;&#30340;&#35789;&#23884;&#20837;&#19978;&#25805;&#20316;&#65292;&#20197;&#20415;&#20197;&#19981;&#21464;&#25110;&#31867;&#20284;&#30340;&#27880;&#24847;&#27169;&#24335;&#32534;&#30721;&#20855;&#26377;&#30456;&#21516;&#32467;&#26500;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empiricall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.06465</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On Differentially Private Subspace Estimation Without Distributional Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#38754;&#20020;&#30528;&#19968;&#20010;&#34987;&#31216;&#20026;&#32500;&#25968;&#35781;&#21650;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;&#25104;&#26412;&#30340;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#20855;&#26377;&#22266;&#26377;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#26799;&#24230;&#32463;&#24120;&#20301;&#20110;&#19968;&#20010;&#20302;&#32500;&#23376;&#31354;&#38388;&#38468;&#36817;&#12290;&#22914;&#26524;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#28857;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#36825;&#31181;&#20302;&#32500;&#32467;&#26500;&#65292;&#23601;&#21487;&#20197;&#36991;&#20813;&#22240;&#39640;&#32500;&#24230;&#32780;&#25903;&#20184;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying (in terms of privacy and accuracy) for the high ambient dimension.   On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed this limitation by considering points that are i.i.d. samples from a Gaussian distribution whose covariance matrix has a certain eigenvalue gap. Yet, it was still left unclear whether we could provide similar upper bounds without distributional assumptions and whether we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#36229;&#22768;&#23548;&#33322;&#30340;&#24515;&#33039;&#36229;&#22768;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#20998;&#21106;&#12289;&#20248;&#21270;&#30340;&#20307;&#31215;&#25968;&#25454;&#34920;&#31034;&#21644;GPU&#21152;&#36895;&#30340;&#33945;&#29305;&#21345;&#27931;&#36335;&#24452;&#36861;&#36394;&#65292;&#29983;&#25104;&#22823;&#37327;&#35270;&#35282;&#30456;&#20851;&#21644;&#20855;&#26377;&#24739;&#32773;&#29305;&#24322;&#24615;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.06463</link><description>&lt;p&gt;
&#33258;&#20027;&#36229;&#22768;&#23548;&#33322;&#30340;&#24515;&#33039;&#36229;&#22768;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Cardiac ultrasound simulation for autonomous ultrasound navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06463
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#36229;&#22768;&#23548;&#33322;&#30340;&#24515;&#33039;&#36229;&#22768;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#20998;&#21106;&#12289;&#20248;&#21270;&#30340;&#20307;&#31215;&#25968;&#25454;&#34920;&#31034;&#21644;GPU&#21152;&#36895;&#30340;&#33945;&#29305;&#21345;&#27931;&#36335;&#24452;&#36861;&#36394;&#65292;&#29983;&#25104;&#22823;&#37327;&#35270;&#35282;&#30456;&#20851;&#21644;&#20855;&#26377;&#24739;&#32773;&#29305;&#24322;&#24615;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#25104;&#20687;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35786;&#26029;&#21644;&#20171;&#20837;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#20687;&#20266;&#24433;&#12289;&#33719;&#21462;&#21442;&#25968;&#33539;&#22260;&#21644;&#24739;&#32773;&#35299;&#21078;&#21464;&#24322;&#31561;&#21407;&#22240;&#65292;&#25805;&#20316;&#21592;&#25216;&#33021;&#30340;&#24046;&#24322;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#30340;&#19981;&#31283;&#23450;&#12290;&#33258;&#21160;&#21270;&#22270;&#20687;&#33719;&#21462;&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#33719;&#21462;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#20294;&#35757;&#32451;&#27492;&#31867;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#23548;&#33322;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#22312;&#24120;&#35268;&#26816;&#26597;&#20013;&#27809;&#26377;&#20445;&#23384;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20854;&#20182;&#27169;&#24577;&#21644;&#20219;&#24847;&#20301;&#32622;&#29983;&#25104;&#22823;&#37327;&#36229;&#22768;&#22270;&#20687;&#65292;&#20197;&#20415;&#21518;&#32493;&#36890;&#36807;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#25311;&#27969;&#31243;&#65292;&#20351;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#20998;&#21106;&#12289;&#20248;&#21270;&#30340;&#20307;&#31215;&#25968;&#25454;&#34920;&#31034;&#21644;GPU&#21152;&#36895;&#30340;&#33945;&#29305;&#21345;&#27931;&#36335;&#24452;&#36861;&#36394;&#26469;&#29983;&#25104;&#35270;&#35282;&#30456;&#20851;&#21644;&#20855;&#26377;&#24739;&#32773;&#29305;&#24322;&#24615;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound is well-established as an imaging modality for diagnostic and interventional purposes. However, the image quality varies with operator skills as acquiring and interpreting ultrasound images requires extensive training due to the imaging artefacts, the range of acquisition parameters and the variability of patient anatomies. Automating the image acquisition task could improve acquisition reproducibility and quality but training such an algorithm requires large amounts of navigation data, not saved in routine examinations. Thus, we propose a method to generate large amounts of ultrasound images from other modalities and from arbitrary positions, such that this pipeline can later be used by learning algorithms for navigation. We present a novel simulation pipeline which uses segmentations from other modalities, an optimized volumetric data representation and GPU-accelerated Monte Carlo path tracing to generate view-dependent and patient-specific ultrasound images. We extensivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SeqRF&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#30452;&#32447;&#21270;&#27010;&#29575;&#27969;&#26469;&#20943;&#23567;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#24182;&#20197;&#27492;&#21152;&#36895;&#21462;&#26679;&#21644;&#25552;&#39640;&#32508;&#21512;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.06461</link><description>&lt;p&gt;
&#39034;&#24207;&#27969;&#21305;&#37197;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sequential Flow Matching for Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SeqRF&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#30452;&#32447;&#21270;&#27010;&#29575;&#27969;&#26469;&#20943;&#23567;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#24182;&#20197;&#27492;&#21152;&#36895;&#21462;&#26679;&#21644;&#25552;&#39640;&#32508;&#21512;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#24341;&#23548;&#36830;&#32493;&#26102;&#38388;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#25193;&#25955;&#27169;&#22411;&#25110;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#65289;&#30340;&#27010;&#29575;&#27969;&#26159;&#36890;&#36807;&#25968;&#20540;&#35299;&#31639;&#22120;&#24555;&#36895;&#21462;&#26679;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#22122;&#22768;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#27010;&#29575;&#36335;&#24452;&#26469;&#23398;&#20064;&#32447;&#24615;&#36335;&#24452;&#12290;ODE&#27169;&#22411;&#30340;&#20223;&#30495;&#36895;&#24230;&#24930;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;ODE&#36712;&#36857;&#30340;&#39640;&#26354;&#29575;&#23548;&#33268;&#30340;ODE&#27714;&#35299;&#22120;&#30340;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#36825;&#20250;&#22312;&#20302;NFE&#33539;&#22260;&#20869;&#25918;&#22823;&#25968;&#20540;&#35299;&#31639;&#22120;&#30340;&#25130;&#26029;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SeqRF&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#30452;&#32447;&#21270;&#27010;&#29575;&#27969;&#20197;&#20943;&#23567;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#21152;&#36895;&#21462;&#26679;&#24182;&#25552;&#39640;&#32508;&#21512;&#36136;&#37327;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#20102;SeqRF&#30340;&#30452;&#32447;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Straightening the probability flow of the continuous-time generative models, such as diffusion models or flow-based models, is the key to fast sampling through the numerical solvers, existing methods learn a linear path by directly generating the probability path the joint distribution between the noise and data distribution. One key reason for the slow sampling speed of the ODE-based solvers that simulate these generative models is the global truncation error of the ODE solver, caused by the high curvature of the ODE trajectory, which explodes the truncation error of the numerical solvers in the low-NFE regime. To address this challenge, We propose a novel method called SeqRF, a learning technique that straightens the probability flow to reduce the global truncation error and hence enable acceleration of sampling and improve the synthesis quality. In both theoretical and empirical studies, we first observe the straightening property of our SeqRF. Through empirical evaluations via SeqR
&lt;/p&gt;</description></item><item><title>V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06457</link><description>&lt;p&gt;
V-STaR: &#33258;&#23398;&#25512;&#29702;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
V-STaR: Training Verifiers for Self-Taught Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06457
&lt;/p&gt;
&lt;p&gt;
V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20363;&#22914;STaR&#65288;Zelikman&#31561;&#20154;&#65292;2022&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36845;&#20195;&#24494;&#35843;LLM&#20197;&#25552;&#39640;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#20002;&#24323;&#20102;&#22823;&#37327;&#30340;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-STaR&#65292;&#23427;&#21033;&#29992;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20351;&#29992;DPO&#35757;&#32451;&#19968;&#20010;&#21028;&#26029;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#30340;&#39564;&#35777;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#29992;&#26469;&#22312;&#20247;&#22810;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#27425;&#36816;&#34892;V-STaR&#20250;&#36880;&#27493;&#20135;&#29983;&#26356;&#22909;&#30340;&#25512;&#29702;&#22120;&#21644;&#39564;&#35777;&#22120;&#65292;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;LLaMA2&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#26500;&#24314;&#22810;&#20010;&#20915;&#31574;&#26641;&#65292;&#24182;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#32452;&#21512;&#24615;&#33021;&#65292;&#20197;&#25351;&#23548;&#26368;&#32456;&#39044;&#27979;&#30340;&#20915;&#31574;&#26641;&#32452;&#21512;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06452</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20272;&#26500;&#24314;&#36807;&#31243;&#20013;&#32452;&#21512;&#24615;&#33021;&#30340;&#31639;&#27861;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#22810;&#20010;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
An Algorithmic Framework for Constructing Multiple Decision Trees by Evaluating Their Combination Performance Throughout the Construction Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#26500;&#24314;&#22810;&#20010;&#20915;&#31574;&#26641;&#65292;&#24182;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#32452;&#21512;&#24615;&#33021;&#65292;&#20197;&#25351;&#23548;&#26368;&#32456;&#39044;&#27979;&#30340;&#20915;&#31574;&#26641;&#32452;&#21512;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#20915;&#31574;&#26641;&#32452;&#21512;&#36827;&#34892;&#39044;&#27979;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#30446;&#21069;&#26500;&#24314;&#20915;&#31574;&#26641;&#32452;&#21512;&#36827;&#34892;&#39044;&#27979;&#30340;&#20856;&#22411;&#26041;&#27861;&#26377;bagging&#21644;boosting&#12290;bagging&#26041;&#27861;&#29420;&#31435;&#26500;&#24314;&#20915;&#31574;&#26641;&#32780;&#19981;&#35780;&#20272;&#23427;&#20204;&#30340;&#32452;&#21512;&#24615;&#33021;&#65292;&#24182;&#22312;&#20043;&#21518;&#36827;&#34892;&#24179;&#22343;&#12290;boosting&#26041;&#27861;&#39034;&#24207;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#21482;&#22312;&#27599;&#19968;&#27493;&#35780;&#20272;&#26032;&#20915;&#31574;&#26641;&#19982;&#22266;&#23450;&#36807;&#21435;&#20915;&#31574;&#26641;&#32452;&#21512;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#19981;&#30452;&#25509;&#26500;&#24314;&#20063;&#19981;&#35780;&#20272;&#26368;&#32456;&#39044;&#27979;&#30340;&#20915;&#31574;&#26641;&#32452;&#21512;&#30340;&#36866;&#29992;&#24615;&#12290;&#24403;&#26368;&#32456;&#39044;&#27979;&#22522;&#20110;&#22810;&#20010;&#20915;&#31574;&#26641;&#32452;&#21512;&#26102;&#65292;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#35780;&#20272;&#32452;&#21512;&#30340;&#36866;&#29992;&#24615;&#26159;&#24456;&#33258;&#28982;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#26500;&#24314;&#20915;&#31574;&#26641;&#24182;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#32452;&#21512;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37325;&#22797;&#20004;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#21516;&#26102;&#26500;&#24314;&#22810;&#20010;&#20915;&#31574;&#26641;&#65292;&#31532;&#20108;&#27493;&#26159;&#26681;&#25454;&#32452;&#21512;&#24615;&#33021;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#26641;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#25351;&#23548;&#21518;&#32493;&#30340;&#20915;&#31574;&#26641;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions using a combination of decision trees are known to be effective in machine learning. Typical ideas for constructing a combination of decision trees for prediction are bagging and boosting. Bagging independently constructs decision trees without evaluating their combination performance and averages them afterward. Boosting constructs decision trees sequentially, only evaluating a combination performance of a new decision tree and the fixed past decision trees at each step. Therefore, neither method directly constructs nor evaluates a combination of decision trees for the final prediction. When the final prediction is based on a combination of decision trees, it is natural to evaluate the appropriateness of the combination when constructing them. In this study, we propose a new algorithmic framework that constructs decision trees simultaneously and evaluates their combination performance throughout the construction process. Our framework repeats two procedures. In the first p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#24179;&#34913;&#31639;&#27861;&#25512;&#29702;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25214;&#21040;&#31639;&#27861;&#30340;&#24179;&#34913;&#28857;&#26469;&#35757;&#32451;&#32593;&#32476;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06445</link><description>&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;&#31639;&#27861;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Deep Equilibrium Algorithmic Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#24179;&#34913;&#31639;&#27861;&#25512;&#29702;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25214;&#21040;&#31639;&#27861;&#30340;&#24179;&#34913;&#28857;&#26469;&#35757;&#32451;&#32593;&#32476;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#23398;&#20064;&#25191;&#34892;&#32463;&#20856;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#19968;&#30452;&#20351;&#29992;&#30340;&#26159;&#36882;&#24402;&#26550;&#26500;&#65292;&#20854;&#20013;&#27599;&#20010;GNN&#30340;&#36845;&#20195;&#19982;&#31639;&#27861;&#30340;&#36845;&#20195;&#19968;&#33268;&#12290;&#30001;&#20110;&#31639;&#27861;&#30340;&#35299;&#36890;&#24120;&#26159;&#19968;&#20010;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#29468;&#27979;&#24182;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#25214;&#21040;&#24179;&#34913;&#28857;&#26469;&#35757;&#32451;&#32593;&#32476;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#12290;&#27880;&#24847;&#65292;&#36825;&#19981;&#38656;&#35201;&#23558;&#27599;&#20010;GNN&#30340;&#36845;&#20195;&#19982;&#31639;&#27861;&#30340;&#27493;&#39588;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on neural algorithmic reasoning has demonstrated that graph neural networks (GNNs) could learn to execute classical algorithms. Doing so, however, has always used a recurrent architecture, where each iteration of the GNN aligns with an algorithm's iteration. Since an algorithm's solution is often an equilibrium, we conjecture and empirically validate that one can train a network to solve algorithmic problems by directly finding the equilibrium. Note that this does not require matching each GNN iteration with a step of the algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;ResNet&#32467;&#26500;&#21644;&#27888;&#21202;&#32423;&#25968;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#36882;&#24402;&#32467;&#26500;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#23398;&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#28508;&#21147;&#24040;&#22823;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.06441</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#27888;&#21202;&#32423;&#25968;&#21644;&#36882;&#24402;&#32467;&#26500;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Incorporating Taylor Series and Recursive Structure in Neural Networks for Time Series Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;ResNet&#32467;&#26500;&#21644;&#27888;&#21202;&#32423;&#25968;&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#36882;&#24402;&#32467;&#26500;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#23398;&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#28508;&#21147;&#24040;&#22823;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;&#29289;&#29702;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#21644;&#37329;&#34701;&#31561;&#22810;&#20010;&#39046;&#22495;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;ResNet&#32467;&#26500;&#30340;&#35201;&#32032;&#19982;&#27888;&#21202;&#32423;&#25968;&#26694;&#26550;&#30340;&#21019;&#26032;&#32467;&#21512;&#36215;&#26469;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#24341;&#20837;&#36882;&#24402;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25512;&#36827;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#23398;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is relevant in various disciplines such as physics, biology, chemistry, and finance. In this paper, we present a novel neural network architecture that integrates elements from ResNet structures, while introducing the innovative incorporation of the Taylor series framework. This approach demonstrates notable enhancements in test accuracy across many of the baseline datasets investigated. Furthermore, we extend our method to incorporate a recursive step, which leads to even further improvements in test accuracy. Our findings underscore the potential of our proposed model to significantly advance time series analysis methodologies, offering promising avenues for future research and application.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#36973;&#36935;&#28151;&#28102;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#24573;&#30053;&#28151;&#28102;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06434</link><description>&lt;p&gt;
&#30495;&#30456;&#22312;&#21738;&#37324;&#65311;&#22312;&#36830;&#32493;&#30340;&#19990;&#30028;&#20013;&#36973;&#36935;&#28151;&#28102;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Where is the Truth? The Risk of Getting Confounded in a Continual World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06434
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#36973;&#36935;&#28151;&#28102;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#24573;&#30053;&#28151;&#28102;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#19968;&#20010;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#65292;&#32780;&#36825;&#31181;&#30456;&#20851;&#24615;&#26080;&#27861;&#27867;&#21270;&#21040;&#26032;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#38598;&#23601;&#26159;&#28151;&#28102;&#30340;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;&#65292;&#22312;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#65292;&#28151;&#28102;&#22240;&#32032;&#21487;&#33021;&#38543;&#30528;&#20219;&#21153;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#23548;&#33268;&#30340;&#25361;&#25112;&#36828;&#36828;&#36229;&#36807;&#36890;&#24120;&#32771;&#34385;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#19978;&#25512;&#23548;&#20102;&#36825;&#31181;&#28151;&#28102;&#22240;&#32032;&#23545;&#19968;&#32452;&#28151;&#28102;&#20219;&#21153;&#30340;&#26377;&#25928;&#32852;&#21512;&#35299;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#36830;&#32493;&#25968;&#25454;&#38598;&#20013;&#65292;&#24403;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#26102;&#65292;&#34394;&#20551;&#30456;&#20851;&#24615;&#24456;&#23481;&#26131;&#34987;&#24573;&#30053;&#65292;&#20294;&#26159;&#22312;&#39034;&#24207;&#32771;&#34385;&#20219;&#21153;&#26102;&#65292;&#36991;&#20813;&#28151;&#28102;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#26631;&#20934;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#24573;&#30053;&#28151;&#28102;&#65292;&#32780;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21017;&#26159;&#25104;&#21151;&#30340;&#12290;&#25105;&#20204;&#30340;&#36830;&#32493;&#28151;&#28102;&#25968;&#25454;&#38598;ConCon&#22522;&#20110;CLEVR&#22270;&#20687;&#65292;&#35777;&#26126;&#20102;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A dataset is confounded if it is most easily solved via a spurious correlation which fails to generalize to new data. We will show that, in a continual learning setting where confounders may vary in time across tasks, the resulting challenge far exceeds the standard forgetting problem normally considered. In particular, we derive mathematically the effect of such confounders on the space of valid joint solutions to sets of confounded tasks. Interestingly, our theory predicts that for many such continual datasets, spurious correlations are easily ignored when the tasks are trained on jointly, but it is far harder to avoid confounding when they are considered sequentially. We construct such a dataset and demonstrate empirically that standard continual learning methods fail to ignore confounders, while training jointly on all tasks is successful. Our continually confounded dataset, ConCon, is based on CLEVR images and demonstrates the need for continual learning methods with more robust b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#25216;&#26415;&#26469;&#35299;&#20915;&#29983;&#25104;&#24335;AI&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;ZKML&#23454;&#29616;&#65292;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#23545;&#29983;&#25104;&#24335;AI&#36755;&#20986;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06414</link><description>&lt;p&gt;
&#30456;&#20449;&#36807;&#31243;&#65306;&#38646;&#30693;&#35782;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#29983;&#25104;&#24335;AI&#20132;&#20114;&#20013;&#30340;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#25216;&#26415;&#26469;&#35299;&#20915;&#29983;&#25104;&#24335;AI&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;ZKML&#23454;&#29616;&#65292;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#23545;&#29983;&#25104;&#24335;AI&#36755;&#20986;&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#65288;&#22914;transformers&#27169;&#22411;&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20844;&#24179;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#20851;&#20999;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#24378;&#35843;&#36890;&#36807;&#29983;&#25104;&#24335;AI&#30830;&#20445;&#36825;&#20123;&#39046;&#22495;&#30340;&#20844;&#24179;&#24615;&#21644;&#36136;&#37327;&#30340;&#32039;&#36843;&#24615;&#65292;&#24182;&#25506;&#32034;&#20351;&#29992;&#23494;&#30721;&#23398;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#38646;&#30693;&#35782;&#35777;&#26126;&#65288;ZKP&#65289;&#65292;&#26469;&#35299;&#20915;&#24615;&#33021;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25252;&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;&#23558;ZKP&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#38646;&#30693;&#35782;&#26426;&#22120;&#23398;&#20064;&#65288;ZKML&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#25935;&#24863;&#27169;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#29420;&#31435;&#39564;&#35777;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#12290;ZKML&#36890;&#36807;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#23494;&#30721;&#23398;&#23457;&#35745;&#30165;&#36857;&#65292;&#24182;&#30830;&#20445;&#29992;&#25143;&#20043;&#38388;&#30340;&#32479;&#19968;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;snarkGPT&#65292;&#19968;&#20010;&#23454;&#38469;&#30340;transformers&#22411;ZKML&#23454;&#29616;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#39564;&#35777;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI, exemplified by models like transformers, has opened up new possibilities in various domains but also raised concerns about fairness, transparency and reliability, especially in fields like medicine and law. This paper emphasizes the urgency of ensuring fairness and quality in these domains through generative AI. It explores using cryptographic techniques, particularly Zero-Knowledge Proofs (ZKPs), to address concerns regarding performance fairness and accuracy while protecting model privacy. Applying ZKPs to Machine Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables independent validation of AI-generated content without revealing sensitive model information, promoting transparency and trust. ZKML enhances AI fairness by providing cryptographic audit trails for model predictions and ensuring uniform performance across users. We introduce snarkGPT, a practical ZKML implementation for transformers, to empower users to verify output accuracy and qualit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARINA-P&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31995;&#21015;&#30456;&#20851;&#21387;&#32553;&#22120;&#65292;&#20248;&#21270;&#20102;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;MARINA-P&#22312;&#31639;&#27861;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#25903;&#25345;&#21452;&#21521;&#21387;&#32553;&#30340;&#36215;&#28857;&#12290;&#36890;&#36807;&#19982;&#19978;&#34892;&#21387;&#32553;&#21644;&#21160;&#37327;&#27493;&#39588;&#30340;&#32467;&#21512;&#65292;M3&#26041;&#27861;&#23454;&#29616;&#20102;&#21452;&#21521;&#21387;&#32553;&#65292;&#24182;&#22312;&#24635;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.06412</link><description>&lt;p&gt;
&#25552;&#39640;&#38750;&#20984;&#20998;&#24067;&#24335;&#20248;&#21270;&#22312;&#20989;&#25968;&#30456;&#20284;&#24615;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#21452;&#21521;&#36890;&#20449;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARINA-P&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31995;&#21015;&#30456;&#20851;&#21387;&#32553;&#22120;&#65292;&#20248;&#21270;&#20102;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;MARINA-P&#22312;&#31639;&#27861;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#25903;&#25345;&#21452;&#21521;&#21387;&#32553;&#30340;&#36215;&#28857;&#12290;&#36890;&#36807;&#19982;&#19978;&#34892;&#21387;&#32553;&#21644;&#21160;&#37327;&#27493;&#39588;&#30340;&#32467;&#21512;&#65292;M3&#26041;&#27861;&#23454;&#29616;&#20102;&#21452;&#21521;&#21387;&#32553;&#65292;&#24182;&#22312;&#24635;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#21153;&#22120;&#21644;&#24037;&#20316;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#25928;&#36890;&#20449;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20248;&#21270;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#65292;&#24182;&#25581;&#31034;&#20102;&#24403;&#21069;&#27969;&#34892;&#30340;&#19979;&#34892;&#21387;&#32553;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12290;&#39318;&#20808;&#32771;&#34385;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#21487;&#24573;&#30053;&#30340;&#32431;&#31929;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;MARINA-P&#65292;&#19968;&#31181;&#20351;&#29992;&#19968;&#31995;&#21015;&#30456;&#20851;&#21387;&#32553;&#22120;&#30340;&#26032;&#22411;&#19979;&#34892;&#21387;&#32553;&#26041;&#27861;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#20351;&#29992;&#25490;&#21015;&#21387;&#32553;&#22120;&#30340;MARINA-P&#21487;&#20197;&#23454;&#29616;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38543;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#25552;&#39640;&#65292;&#22240;&#27492;&#22312;&#31639;&#27861;&#19978;&#21487;&#35777;&#26126;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;MARINA-P&#21487;&#20197;&#20316;&#20026;&#25903;&#25345;&#21452;&#21521;&#21387;&#32553;&#30340;&#26041;&#27861;&#30340;&#36215;&#28857;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;M3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;MARINA-P&#19982;&#19978;&#34892;&#21387;&#32553;&#21644;&#21160;&#37327;&#27493;&#39588;&#32452;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#21452;&#21521;&#21387;&#32553;&#65292;&#24182;&#22312;&#24635;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#35777;&#26126;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing the server-to-worker communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analyses demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression. We introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number
&lt;/p&gt;</description></item><item><title>&#23618;&#27425;&#21270;Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25552;&#21462;&#36807;&#21435;&#32463;&#39564;&#30340;&#20449;&#24687;&#20016;&#23500;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20803;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06402</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;Transformer&#26159;&#39640;&#25928;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Transformers are Efficient Meta-Reinforcement Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06402
&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25552;&#21462;&#36807;&#21435;&#32463;&#39564;&#30340;&#20449;&#24687;&#20016;&#23500;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20803;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22312;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#23618;&#27425;&#21270;Transformer&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;HTrMRL&#65289;&#12290;HTrMRL&#26088;&#22312;&#35299;&#20915;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#20197;&#21069;&#26410;&#35265;&#20219;&#21153;&#20013;&#26377;&#25928;&#25191;&#34892;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#21435;&#30340;&#32463;&#39564;&#20316;&#20026;&#20449;&#24687;&#20016;&#23500;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25552;&#28860;&#21644;&#24212;&#29992;&#21040;&#26032;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#33021;&#22815;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#20803;&#35757;&#32451;&#65292;&#21516;&#26102;&#26174;&#33879;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;Meta-World&#22522;&#20934;&#30340;&#21508;&#31181;&#27169;&#25311;&#20219;&#21153;&#19978;&#33719;&#24471;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22686;&#24378;&#20102;&#20195;&#29702;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#20026;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;AI&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Hierarchical Transformers for Meta-Reinforcement Learning (HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims to address the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of information, which our model effectively distills and applies to new contexts. Our learned algorithm is capable of outperforming the previous state-of-the-art and provides more efficient meta-training while significantly improving generalization capabilities. Experimental results, obtained across various simulated tasks of the Meta-World Benchmark, indicate a significant improvement in learning efficiency and adaptability compared to the state-of-the-art on a variety of tasks. Our approach not only enhances the agent's ability to generalize from limited data but also paves the way for more robust and versatile AI systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25552;&#21319;&#26041;&#27861;&#26500;&#24314;&#22810;&#20010;&#20803;&#26641;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#20915;&#31574;&#26641;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06386</link><description>&lt;p&gt;
&#22522;&#20110;Boosting&#30340;&#39034;&#24207;&#20803;&#26641;&#38598;&#25104;&#26500;&#24314;&#20197;&#25913;&#36827;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Boosting-Based Sequential Meta-Tree Ensemble Construction for Improved Decision Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25552;&#21319;&#26041;&#27861;&#26500;&#24314;&#22810;&#20010;&#20803;&#26641;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#20915;&#31574;&#26641;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#36807;&#24230;&#21152;&#28145;&#26641;&#24418;&#32467;&#26500;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#36817;&#26399;&#26377;&#20154;&#25552;&#20986;&#20102;&#20803;&#26641;&#26469;&#35299;&#20915;&#36807;&#24230;&#21152;&#28145;&#26641;&#24418;&#32467;&#26500;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#65292;&#20803;&#26641;&#33021;&#22815;&#20445;&#35777;&#32479;&#35745;&#19978;&#30340;&#26368;&#20248;&#24615;&#12290;&#22240;&#27492;&#65292;&#30456;&#36739;&#20110;&#20915;&#31574;&#26641;&#65292;&#25105;&#20204;&#26399;&#26395;&#20803;&#26641;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#21333;&#20010;&#20915;&#31574;&#26641;&#30456;&#27604;&#65292;&#24050;&#30693;&#30001;&#25552;&#21319;&#31639;&#27861;&#26500;&#36896;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#22312;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26399;&#26395;&#30001;&#20803;&#26641;&#38598;&#25104;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#27604;&#21333;&#20010;&#20803;&#26641;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20197;&#21069;&#27809;&#26377;&#30740;&#31350;&#20351;&#29992;&#25552;&#21319;&#26041;&#27861;&#26500;&#24314;&#22810;&#20010;&#20803;&#26641;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25552;&#21319;&#26041;&#27861;&#26500;&#24314;&#22810;&#20010;&#20803;&#26641;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A decision tree is one of the most popular approaches in machine learning fields. However, it suffers from the problem of overfitting caused by overly deepened trees. Then, a meta-tree is recently proposed. It solves the problem of overfitting caused by overly deepened trees. Moreover, the meta-tree guarantees statistical optimality based on Bayes decision theory. Therefore, the meta-tree is expected to perform better than the decision tree. In contrast to a single decision tree, it is known that ensembles of decision trees, which are typically constructed boosting algorithms, are more effective in improving predictive performance. Thus, it is expected that ensembles of meta-trees are more effective in improving predictive performance than a single meta-tree, and there are no previous studies that construct multiple meta-trees in boosting. Therefore, in this study, we propose a method to construct multiple meta-trees using a boosting approach. Through experiments with synthetic and ben
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#26368;&#20248;&#31639;&#27861;&#65292;&#22312;&#23398;&#20064;&#39640;&#26031;&#26641;&#21644;&#39640;&#26031;&#22810;&#39033;&#24335;&#26641;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06380</link><description>&lt;p&gt;
&#39640;&#26031;&#65288;&#22810;&#39033;&#24335;&#65289;&#26641;&#30340;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal estimation of Gaussian (poly)trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#26368;&#20248;&#31639;&#27861;&#65292;&#22312;&#23398;&#20064;&#39640;&#26031;&#26641;&#21644;&#39640;&#26031;&#22810;&#39033;&#24335;&#26641;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#21521;&#39640;&#26031;&#26641;&#21644;&#26377;&#21521;&#39640;&#26031;&#22810;&#39033;&#24335;&#26641;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20998;&#24067;&#23398;&#20064;&#65288;&#21363;KL&#36317;&#31163;&#65289;&#21644;&#32467;&#26500;&#23398;&#20064;&#65288;&#21363;&#31934;&#30830;&#24674;&#22797;&#65289;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22522;&#20110;Chow-Liu&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#26368;&#20248;&#30340;&#26641;&#29366;&#20998;&#24067;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#23545;&#29992;&#20110;&#22810;&#39033;&#24335;&#26641;&#30340;PC&#31639;&#27861;&#30340;&#20462;&#25913;&#65292;&#23427;&#20351;&#29992;&#20559;&#30456;&#20851;&#20316;&#20026;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#22120;&#36827;&#34892;&#22522;&#20110;&#32422;&#26463;&#30340;&#32467;&#26500;&#23398;&#20064;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#26174;&#24335;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#25512;&#23548;&#21305;&#37197;&#30340;&#19979;&#30028;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#21508;&#31181;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#27934;&#23519;&#21644;&#32463;&#39564;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop optimal algorithms for learning undirected Gaussian trees and directed Gaussian polytrees from data. We consider both problems of distribution learning (i.e. in KL distance) and structure learning (i.e. exact recovery). The first approach is based on the Chow-Liu algorithm, and learns an optimal tree-structured distribution efficiently. The second approach is a modification of the PC algorithm for polytrees that uses partial correlation as a conditional independence tester for constraint-based structure learning. We derive explicit finite-sample guarantees for both approaches, and show that both approaches are optimal by deriving matching lower bounds. Additionally, we conduct numerical experiments to compare the performance of various algorithms, providing further insights and empirical evidence.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#22320;&#36136;&#23450;&#21521;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#23454;&#29616;&#39640;&#31934;&#24230;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;</title><link>https://arxiv.org/abs/2402.06377</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#39640;&#31934;&#24230;&#22320;&#36136;&#23450;&#21521;
&lt;/p&gt;
&lt;p&gt;
High-Precision Geosteering via Reinforcement Learning and Particle Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06377
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#22320;&#36136;&#23450;&#21521;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#23454;&#29616;&#39640;&#31934;&#24230;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#36136;&#23450;&#21521;&#26159;&#38075;&#20117;&#20316;&#19994;&#20013;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#20256;&#32479;&#19978;&#28041;&#21450;&#23545;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#22914;&#20117;&#27979;&#25968;&#25454;&#65289;&#30340;&#25163;&#21160;&#35299;&#35835;&#12290;&#36825;&#24341;&#20837;&#20102;&#20027;&#35266;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#30340;&#31243;&#24207;&#12290;&#23398;&#26415;&#30028;&#23581;&#35797;&#36890;&#36807;&#36138;&#23146;&#20248;&#21270;&#21644;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#65288;ADP&#65289;&#26469;&#35299;&#20915;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;&#20248;&#21270;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#20102;&#19968;&#23450;&#30340;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#36866;&#24212;&#29616;&#23454;&#22810;&#26679;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#22870;&#21169;&#30340;&#36845;&#20195;&#23398;&#20064;&#26469;&#20419;&#36827;&#26368;&#20248;&#20915;&#31574;&#12290;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#20363;&#22914;&#31890;&#23376;&#28388;&#27874;&#65288;PF&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#20449;&#24687;&#30340;&#34917;&#20805;&#31574;&#30053;&#65292;&#29992;&#20110;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;RL&#30340;&#22320;&#36136;&#23450;&#21521;&#19982;PF&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#29616;&#23454;&#30340;&#22320;&#36136;&#23450;&#21521;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;PF&#22788;&#29702;&#23454;&#26102;&#20117;&#27979;&#25968;&#25454;&#65292;&#20272;&#35745;&#20117;&#30340;&#20301;&#32622;&#30456;&#23545;&#20110;&#22320;&#23618;&#65292;&#28982;&#21518;&#23558;&#20854;&#20449;&#24687;&#29992;&#20110;&#22522;&#20110;RL&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geosteering, a key component of drilling operations, traditionally involves manual interpretation of various data sources such as well-log data. This introduces subjective biases and inconsistent procedures. Academic attempts to solve geosteering decision optimization with greedy optimization and Approximate Dynamic Programming (ADP) showed promise but lacked adaptivity to realistic diverse scenarios. Reinforcement learning (RL) offers a solution to these challenges, facilitating optimal decision-making through reward-based iterative learning. State estimation methods, e.g., particle filter (PF), provide a complementary strategy for geosteering decision-making based on online information. We integrate an RL-based geosteering with PF to address realistic geosteering scenarios. Our framework deploys PF to process real-time well-log data to estimate the location of the well relative to the stratigraphic layers, which then informs the RL-based decision-making process. We compare our method
&lt;/p&gt;</description></item><item><title>TEE4EHR&#26159;&#19968;&#20010;&#20351;&#29992;&#28857;&#36807;&#31243;&#25439;&#22833;&#20989;&#25968;&#30340;Transformer&#20107;&#20214;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#32534;&#30721;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#23454;&#39564;&#23460;&#27979;&#35797;&#30340;&#27169;&#24335;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;EHR&#20013;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#24211;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06367</link><description>&lt;p&gt;
TEE4EHR&#65306;&#29992;&#20110;&#26356;&#22909;&#22320;&#23398;&#20064;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#34920;&#31034;&#30340;Transformer&#20107;&#20214;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
TEE4EHR: Transformer Event Encoder for Better Representation Learning in Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06367
&lt;/p&gt;
&lt;p&gt;
TEE4EHR&#26159;&#19968;&#20010;&#20351;&#29992;&#28857;&#36807;&#31243;&#25439;&#22833;&#20989;&#25968;&#30340;Transformer&#20107;&#20214;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#32534;&#30721;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#23454;&#39564;&#23460;&#27979;&#35797;&#30340;&#27169;&#24335;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;EHR&#20013;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#24211;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26159;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#20020;&#24202;&#21464;&#37327;&#30340;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#24182;&#38750;&#38543;&#26426;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#21644;&#24739;&#32773;&#30340;&#29366;&#24577;&#12290;&#28857;&#36807;&#31243;&#26159;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#19982;&#19981;&#35268;&#21017;&#37319;&#26679;&#27169;&#24335;&#19968;&#33268;&#30340;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;TEE4EHR&#26159;&#19968;&#20010;&#20855;&#26377;&#28857;&#36807;&#31243;&#25439;&#22833;&#20989;&#25968;&#30340;Transformer&#20107;&#20214;&#32534;&#30721;&#22120;&#65288;TEE&#65289;&#65292;&#23427;&#23545;EHR&#20013;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#27169;&#24335;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;TEE&#30340;&#25928;&#29992;&#24050;&#22312;&#21508;&#31181;&#22522;&#20934;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#30340;EHR&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;TEE&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#65292;&#36825;&#22312;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#26410;&#26469;&#20107;&#20214;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregular sampling of time series in electronic health records (EHRs) is one of the main challenges for developing machine learning models. Additionally, the pattern of missing data in certain clinical variables is not at random but depends on the decisions of clinicians and the state of the patient. Point process is a mathematical framework for analyzing event sequence data that is consistent with irregular sampling patterns. Our model, TEE4EHR, is a transformer event encoder (TEE) with point process loss that encodes the pattern of laboratory tests in EHRs. The utility of our TEE has been investigated in a variety of benchmark event sequence datasets. Additionally, we conduct experiments on two real-world EHR databases to provide a more comprehensive evaluation of our model. Firstly, in a self-supervised learning approach, the TEE is jointly learned with an existing attention-based deep neural network which gives superior performance in negative log-likelihood and future event predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.06357</link><description>&lt;p&gt;
SpongeNet &#25915;&#20987;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28023;&#32501;&#26435;&#37325;&#20013;&#27602;
&lt;/p&gt;
&lt;p&gt;
The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#25915;&#20987;&#26088;&#22312;&#22686;&#21152;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#29616;&#26377;&#30340;&#28023;&#32501;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#20013;&#27602;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#28023;&#32501;&#31034;&#20363;&#21033;&#29992;&#28155;&#21152;&#21040;&#27169;&#22411;&#36755;&#20837;&#30340;&#25200;&#21160;&#26469;&#22686;&#21152;&#33021;&#37327;&#21644;&#24310;&#36831;&#65292;&#32780;&#28023;&#32501;&#20013;&#27602;&#21017;&#25913;&#21464;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#24341;&#21457;&#25512;&#29702;&#26102;&#30340;&#33021;&#37327;/&#24310;&#36831;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28023;&#32501;&#25915;&#20987;&#65292;&#31216;&#20026; SpongeNet&#12290;SpongeNet &#26159;&#31532;&#19968;&#20010;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#28023;&#32501;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#28023;&#32501;&#20013;&#27602;&#65292;SpongeNet &#21487;&#20197;&#25104;&#21151;&#22686;&#21152;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#19987;&#38376;&#38024;&#23545;&#28023;&#32501;&#20013;&#27602;&#36827;&#34892;&#35843;&#25972;&#65288;&#21363;&#20943;&#23567;&#25209;&#24402;&#19968;&#21270;&#20559;&#24046;&#20540;&#65289;&#65292;&#21017;&#27602;&#23475;&#38450;&#24481;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26174;&#31034;&#20986;&#28023;&#32501;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy/latency effects.   In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is the first sponge attack that is performed directly on the parameters of a pre-trained model. Our experiments show that SpongeNet can successfully increase the energy consumption of vision models with fewer samples required than Sponge Poisoning. Our experiments indicate that poisoning defenses are ineffective if not adjusted specifically for the defense against Sponge Poisoning (i.e., they decrease batch normalization bias values). Our work shows that Spong
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32447;&#30340;&#20844;&#24179;RMAB&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#33218;&#30340;&#25289;&#21462;&#19982;&#20854;&#20248;&#21183;&#25104;&#27604;&#20363;&#65292;&#23454;&#29616;&#20102;&#20844;&#24179;&#30340;&#26333;&#20809;&#12290;&#31639;&#27861;&#22312;&#21333;&#27425;&#25289;&#21462;&#30340;&#20844;&#24179;&#24615;&#36951;&#25022;&#26041;&#38754;&#21462;&#24471;&#20102;&#27425;&#32447;&#24615;&#30340;&#32467;&#26524;$O(\sqrt{T\ln T})$&#12290;</title><link>https://arxiv.org/abs/2402.06348</link><description>&lt;p&gt;
&#22312;&#32447;&#19981;&#24179;&#34913;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#26333;&#20809;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness of Exposure in Online Restless Multi-armed Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32447;&#30340;&#20844;&#24179;RMAB&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#33218;&#30340;&#25289;&#21462;&#19982;&#20854;&#20248;&#21183;&#25104;&#27604;&#20363;&#65292;&#23454;&#29616;&#20102;&#20844;&#24179;&#30340;&#26333;&#20809;&#12290;&#31639;&#27861;&#22312;&#21333;&#27425;&#25289;&#21462;&#30340;&#20844;&#24179;&#24615;&#36951;&#25022;&#26041;&#38754;&#21462;&#24471;&#20102;&#27425;&#32447;&#24615;&#30340;&#32467;&#26524;$O(\sqrt{T\ln T})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMAB&#65289;&#25512;&#24191;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#20854;&#20013;&#27599;&#20010;&#33218;&#23637;&#31034;&#39532;&#23572;&#21487;&#22827;&#34892;&#20026;&#65292;&#24182;&#26681;&#25454;&#20854;&#36807;&#28193;&#21160;&#24577;&#36827;&#34892;&#36716;&#25442;&#12290;&#38024;&#23545;RMAB&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20110;&#31163;&#32447;&#21644;&#22312;&#32447;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#33218;&#20043;&#38388;&#30340;&#25289;&#21462;&#20998;&#24067;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20248;&#31574;&#30053;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#65292;&#20854;&#20013;&#19968;&#20123;&#33218;&#19981;&#22815;&#26292;&#38706;&#12290;&#29616;&#26377;&#30340;RMAB&#20844;&#24179;&#24615;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31163;&#32447;&#26696;&#20363;&#20013;&#65292;&#36825;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#29615;&#22659;&#22823;&#37096;&#20998;&#19981;&#30693;&#36947;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22312;&#32447;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20844;&#24179;&#30340;RMAB&#26694;&#26550;&#65292;&#20854;&#20013;&#27599;&#20010;&#33218;&#25509;&#25910;&#30340;&#25289;&#21462;&#19982;&#20854;&#20248;&#21183;&#25104;&#27604;&#20363;&#12290;&#25105;&#20204;&#23558;&#33218;&#30340;&#20248;&#21183;&#23450;&#20041;&#20026;&#20854;&#31283;&#24577;&#22870;&#21169;&#20998;&#24067;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21333;&#27425;&#25289;&#21462;&#30340;&#20844;&#24179;&#24615;&#36951;&#25022;&#26041;&#38754;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#32467;&#26524;$O(\sqrt{T\ln T})$&#65292;&#20854;&#20013;$T$&#26159;&#24635;&#30340;&#23581;&#35797;&#27425;&#25968;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#27425;&#25289;&#21462;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-armed bandits (RMABs) generalize the multi-armed bandits where each arm exhibits Markovian behavior and transitions according to their transition dynamics. Solutions to RMAB exist for both offline and online cases. However, they do not consider the distribution of pulls among the arms. Studies have shown that optimal policies lead to unfairness, where some arms are not exposed enough. Existing works in fairness in RMABs focus heavily on the offline case, which diminishes their application in real-world scenarios where the environment is largely unknown. In the online scenario, we propose the first fair RMAB framework, where each arm receives pulls in proportion to its merit. We define the merit of an arm as a function of its stationary reward distribution. We prove that our algorithm achieves sublinear fairness regret in the single pull case $O(\sqrt{T\ln T})$, with $T$ being the total number of episodes. Empirically, we show that our algorithm performs well in the multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#38598;&#35782;&#21035;&#35780;&#20272;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>https://arxiv.org/abs/2402.06331</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#38598;&#35782;&#21035;&#35780;&#20272;&#20013;&#32771;&#34385;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Taking Class Imbalance Into Account in Open Set Recognition Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#38598;&#35782;&#21035;&#35780;&#20272;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31995;&#32479;&#19981;&#20165;&#22312;&#27969;&#34892;&#24230;&#19978;&#36880;&#28176;&#22686;&#21152;&#65292;&#32780;&#19988;&#36824;&#21463;&#21040;&#29992;&#25143;&#20449;&#20219;&#30340;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#31995;&#32479;&#23545;&#20110;&#26410;&#30693;&#31867;&#21035;&#26679;&#26412;&#30340;&#35782;&#21035;&#20173;&#26381;&#20174;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#30340;&#38169;&#35823;&#26631;&#31614;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#38598;&#35782;&#21035;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#31867;&#21035;&#19981;&#24179;&#34913;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#26679;&#26412;&#20043;&#38388;&#30340;&#20108;&#20998;&#30340;&#24433;&#21709;&#12290;&#20316;&#20026;&#38382;&#39064;&#20998;&#26512;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#35780;&#20272;&#26041;&#27861;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years Deep Neural Network-based systems are not only increasing in popularity but also receive growing user trust. However, due to the closed-world assumption of such systems, they cannot recognize samples from unknown classes and often induce an incorrect label with high confidence. Presented work looks at the evaluation of methods for Open Set Recognition, focusing on the impact of class imbalance, especially in the dichotomy between known and unknown samples. As an outcome of problem analysis, we present a set of guidelines for evaluation of methods in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#39033;&#20851;&#20110;&#25345;&#32493;&#22270;&#23398;&#20064;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#25345;&#32493;&#24615;&#33021;&#25913;&#36827;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06330</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Continual Learning on Graphs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#39033;&#20851;&#20110;&#25345;&#32493;&#22270;&#23398;&#20064;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#25345;&#32493;&#24615;&#33021;&#25913;&#36827;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#65292;&#25345;&#32493;&#22270;&#23398;&#20064;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#23427;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#25345;&#32493;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#32780;&#24573;&#30053;&#20102;&#25345;&#32493;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#26088;&#22312;&#20174;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35282;&#24230;&#25552;&#20379;&#23545;&#26368;&#36817;&#20851;&#20110;&#25345;&#32493;&#22270;&#23398;&#20064;&#30340;&#21162;&#21147;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35282;&#24230;&#65292;&#24341;&#20837;&#20102;&#25345;&#32493;&#22270;&#23398;&#20064;&#30340;&#26032;&#20998;&#31867;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#23558;&#36825;&#20123;&#25345;&#32493;&#22270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#25345;&#32493;&#24615;&#33021;&#25913;&#36827;&#30340;&#25361;&#25112;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#25345;&#32493;&#22270;&#23398;&#20064;&#21457;&#23637;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#25345;&#32493;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, continual graph learning has been increasingly adopted for diverse graph-structured data processing tasks in non-stationary environments. Despite its promising learning capability, current studies on continual graph learning mainly focus on mitigating the catastrophic forgetting problem while ignoring continuous performance improvement. To bridge this gap, this article aims to provide a comprehensive survey of recent efforts on continual graph learning. Specifically, we introduce a new taxonomy of continual graph learning from the perspective of overcoming catastrophic forgetting. Moreover, we systematically analyze the challenges of applying these continual graph learning methods in improving performance continuously and then discuss the possible solutions. Finally, we present open issues and future directions pertaining to the development of continual graph learning and discuss how they impact continuous performance improvement.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06326</link><description>&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning on Temporal Interaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;(TIGs)&#34987;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;TIGs&#19978;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;TIG&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#8220;&#39044;&#35757;&#32451;&#65292;&#39044;&#27979;&#8221;&#35757;&#32451;&#33539;&#24335;&#20013;&#20381;&#28982;&#38754;&#20020;&#30528;&#20004;&#20010;&#38590;&#39064;&#12290;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#24046;&#24322;&#20005;&#37325;&#21066;&#24369;&#20102;&#27169;&#22411;&#22312;&#21160;&#24577;&#28436;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#36965;&#36828;&#26410;&#26469;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#12290;&#20854;&#27425;&#65292;&#39044;&#25991;&#26412;&#20219;&#21153;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24212;&#29992;&#22330;&#26223;&#20013;&#24456;&#38590;&#23545;&#40784;&#20854;&#23398;&#20064;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.   Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straig
&lt;/p&gt;</description></item><item><title>&#22312;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#21487;&#20197;&#20135;&#29983;&#38750;&#22343;&#21248;&#20559;&#24046;&#65292;&#22240;&#27492;&#36890;&#24120;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20250;&#19982;&#31364;&#25945;&#24072;NN&#19968;&#26679;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06323</link><description>&lt;p&gt;
&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#22914;&#20309;&#24341;&#36215;&#19981;&#22343;&#21248;&#20559;&#24046;&#65306;&#20856;&#22411;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#19982;&#31364;&#25945;&#24072;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06323
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#21487;&#20197;&#20135;&#29983;&#38750;&#22343;&#21248;&#20559;&#24046;&#65292;&#22240;&#27492;&#36890;&#24120;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20250;&#19982;&#31364;&#25945;&#24072;NN&#19968;&#26679;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#29702;&#35770;&#38590;&#39064;&#26159;&#24403;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#21040;&#38646;&#35823;&#24046;&#65288;&#21363;&#25554;&#20540;&#25968;&#25454;&#65289;&#26102;&#65292;&#20026;&#20160;&#20040;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36890;&#24120;&#65292;NN&#26159;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25110;&#20854;&#21464;&#31181;&#20043;&#19968;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#23454;&#35777;&#30740;&#31350;&#26816;&#39564;&#20102;&#20174;&#30475;&#20284;&#22343;&#21248;&#30340;&#21442;&#25968;&#20808;&#39564;&#20013;&#37319;&#26679;&#30340;&#38543;&#26426;NN&#23545;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#35813;NN&#23545;&#35757;&#32451;&#38598;&#36827;&#34892;&#20102;&#23436;&#32654;&#20998;&#31867;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;NN&#26679;&#26412;&#36890;&#24120;&#20687;SGD&#35757;&#32451;&#30340;NN&#19968;&#26679;&#27867;&#21270;&#33391;&#22909;&#12290;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#23384;&#22312;&#19982;&#26631;&#31614;&#19968;&#33268;&#30340;&#31364;&#8220;&#25945;&#24072;NN&#8221;&#65292;&#37027;&#20040;&#36825;&#26679;&#30340;&#38543;&#26426;NN&#25554;&#20540;&#22120;&#36890;&#24120;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;NN&#21442;&#25968;&#21270;&#20013;&#30340;&#8220;&#24179;&#22374;&#8221;&#20808;&#39564;&#36890;&#36807;NN&#32467;&#26500;&#20013;&#30340;&#20887;&#20313;&#24341;&#20837;&#20102;&#20016;&#23500;&#30340;NN&#20989;&#25968;&#20808;&#39564;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20250;&#23545;&#36739;&#31616;&#21333;&#30340;&#20989;&#25968;&#20135;&#29983;&#20559;&#21521;&#65292;&#36825;&#20123;&#20989;&#25968;&#38656;&#35201;&#36739;&#23569;&#30340;&#30456;&#20851;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background. A main theoretical puzzle is why over-parameterized Neural Networks (NNs) generalize well when trained to zero loss (i.e., so they interpolate the data). Usually, the NN is trained with Stochastic Gradient Descent (SGD) or one of its variants. However, recent empirical work examined the generalization of a random NN that interpolates the data: the NN was sampled from a seemingly uniform prior over the parameters, conditioned on that the NN perfectly classifying the training set. Interestingly, such a NN sample typically generalized as well as SGD-trained NNs.   Contributions. We prove that such a random NN interpolator typically generalizes well if there exists an underlying narrow ``teacher NN" that agrees with the labels. Specifically, we show that such a `flat' prior over the NN parametrization induces a rich prior over the NN functions, due to the redundancy in the NN structure. In particular, this creates a bias towards simpler functions, which require less relevant pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120;&#65288;PDDS&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#36845;&#20195;&#31890;&#23376;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#23545;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#37319;&#26679;&#21644;&#35745;&#31639;&#35268;&#33539;&#21270;&#24120;&#25968;&#12290;&#19982;&#26631;&#20934;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;PDDS &#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#28176;&#36817;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.06320</link><description>&lt;p&gt;
&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Particle Denoising Diffusion Sampler
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120;&#65288;PDDS&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#36845;&#20195;&#31890;&#23376;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#23545;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#37319;&#26679;&#21644;&#35745;&#31639;&#35268;&#33539;&#21270;&#24120;&#25968;&#12290;&#19982;&#26631;&#20934;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;PDDS &#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#28176;&#36817;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#23558;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#24471;&#20998;&#21305;&#37197;&#24605;&#24819;&#20272;&#35745;&#36825;&#31181;&#25193;&#25955;&#30340;&#26102;&#38388;&#21453;&#28436;&#26469;&#33719;&#24471;&#26469;&#33258;&#25968;&#25454;&#20998;&#24067;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#37319;&#29992;&#31867;&#20284;&#30340;&#31574;&#30053;&#26469;&#20174;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#35268;&#33539;&#21270;&#24120;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#37324;&#65292;&#26102;&#38388;&#21453;&#28436;&#25193;&#25955;&#26159;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26032;&#39062;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#21407;&#22987;&#36845;&#20195;&#31890;&#23376;&#26041;&#26696;&#26469;&#27169;&#25311;&#30340;&#12290;&#19982;&#26631;&#20934;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#32467;&#26524;&#30340;&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120; (PDDS) &#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#28176;&#36817;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#21644;&#39640;&#32500;&#37319;&#26679;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102; PDDS&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have become ubiquitous for generative modeling. The core idea is to transport the data distribution to a Gaussian by using a diffusion. Approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. We follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. However, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. Contrary to standard denoising diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS) provides asymptotically consistent estimates under mild assumptions. We demonstrate PDDS on multimodal and high dimensional sampling tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;TimEHR&#65292;&#29992;&#20110;&#20174;EHR&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#26465;&#20214;GAN&#65292;TimEHR&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#12289;&#32570;&#22833;&#20540;&#21644;&#39640;&#32500;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06318</link><description>&lt;p&gt;
TimEHR&#65306;&#29992;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TimEHR: Image-based Time Series Generation for Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;TimEHR&#65292;&#29992;&#20110;&#20174;EHR&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#26465;&#20214;GAN&#65292;TimEHR&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#12289;&#32570;&#22833;&#20540;&#21644;&#39640;&#32500;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22914;&#19981;&#35268;&#21017;&#37319;&#26679;&#65292;&#32570;&#22833;&#20540;&#21644;&#39640;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#27169;&#22411;TimEHR&#65292;&#29992;&#20110;&#20174;EHR&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TimEHR&#23558;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#22270;&#20687;&#65292;&#22522;&#20110;&#20004;&#20010;&#26465;&#20214;GAN&#26500;&#24314;&#12290;&#31532;&#19968;&#20010;GAN&#29983;&#25104;&#32570;&#22833;&#27169;&#24335;&#65292;&#31532;&#20108;&#20010;GAN&#26681;&#25454;&#32570;&#22833;&#27169;&#24335;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#20540;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;EHR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TimEHR&#22312;&#20445;&#30495;&#24230;&#65292;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series in Electronic Health Records (EHRs) present unique challenges for generative models, such as irregular sampling, missing values, and high dimensionality. In this paper, we propose a novel generative adversarial network (GAN) model, TimEHR, to generate time series data from EHRs. In particular, TimEHR treats time series as images and is based on two conditional GANs. The first GAN generates missingness patterns, and the second GAN generates time series values based on the missingness pattern. Experimental results on three real-world EHR datasets show that TimEHR outperforms state-of-the-art methods in terms of fidelity, utility, and privacy metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#25968;&#25454;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#21644;&#29702;&#35299;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#25239;&#33740;&#33647;&#29289;&#22810;&#37325;&#32784;&#33647;&#24615;&#32454;&#33740;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06295</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#22810;&#37325;&#25239;&#33740;&#33647;&#29289;&#32784;&#33647;&#24615;&#30340;&#26089;&#26399;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Interpretable Data-Driven Models for Early Prediction of Antimicrobial Multidrug Resistance Using Multivariate Time-Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#25968;&#25454;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#21644;&#29702;&#35299;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#25239;&#33740;&#33647;&#29289;&#22810;&#37325;&#32784;&#33647;&#24615;&#32454;&#33740;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#22810;&#27169;&#24577;&#27880;&#20876;&#65292;&#21253;&#25324;&#38745;&#24577;&#25968;&#25454;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#12290;&#34429;&#28982;MTS&#26159;&#20020;&#24202;&#39044;&#27979;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#20294;&#23558;&#20854;&#19982;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#34701;&#21512;&#21487;&#33021;&#20250;&#24102;&#26469;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#25104;&#20026;&#35782;&#21035;&#21644;&#23450;&#20041;&#21307;&#30103;&#39046;&#22495;&#28508;&#22312;&#27169;&#24335;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;DNN&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#36824;&#38656;&#35201;&#22522;&#26412;&#25913;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#22312;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#38598;&#21512;&#19978;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#21644;&#29702;&#35299;&#39532;&#24503;&#37324;&#35757;&#25289;&#24067;&#25289;&#36798;&#22823;&#23398;&#21307;&#38498;&#65288;&#35199;&#29677;&#29273;&#65289;&#30340;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#25239;&#33740;&#33647;&#29289;&#22810;&#37325;&#32784;&#33647;&#24615;&#65288;AMR&#65289;&#32454;&#33740;&#30340;&#20986;&#29616;&#12290;&#24739;&#32773;&#30340;&#20010;&#20154;&#36164;&#26009;&#21644;&#21021;&#22987;&#20581;&#24247;&#29366;&#20917;&#20351;&#29992;&#38745;&#24577;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#28436;&#21464;&#36807;&#31243;&#20351;&#29992;MTS&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHR) is an inherently multimodal register of the patient's health status characterized by static data and multivariate time series (MTS). While MTS are a valuable tool for clinical prediction, their fusion with other data modalities can possibly result in more thorough insights and more accurate results. Deep neural networks (DNNs) have emerged as fundamental tools for identifying and defining underlying patterns in the healthcare domain. However, fundamental improvements in interpretability are needed for DNN models to be widely used in the clinical setting. In this study, we present an approach built on a collection of interpretable multimodal data-driven models that may anticipate and understand the emergence of antimicrobial multidrug resistance (AMR) germs in the intensive care unit (ICU) of the University Hospital of Fuenlabrada (Madrid, Spain). The profile and initial health status of the patient are modeled using static variables, while the evolution 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06293</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting of Irregular Time Series via Conditional Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#22825;&#25991;&#23398;&#21644;&#27668;&#20505;&#23398;&#12290;&#30446;&#21069;&#35813;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20165;&#20272;&#35745;&#21333;&#20010;&#36890;&#36947;&#21644;&#21333;&#20010;&#26102;&#38388;&#28857;&#19978;&#35266;&#27979;&#20540;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20551;&#35774;&#20102;&#19968;&#20010;&#22266;&#23450;&#24418;&#29366;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;ProFITi&#65292;&#29992;&#20110;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20102;&#22312;&#36807;&#21435;&#35266;&#27979;&#21644;&#26597;&#35810;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#19978;&#26465;&#20214;&#19979;&#26102;&#38388;&#24207;&#21015;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#20316;&#20026;&#27169;&#22411;&#32452;&#20214;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#19968;&#20010;&#21487;&#36870;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#25972;&#20010;&#23454;&#25968;&#32447;&#19978;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting of irregularly sampled multivariate time series with missing values is an important problem in many fields, including health care, astronomy, and climate. State-of-the-art methods for the task estimate only marginal distributions of observations in single channels and at single timepoints, assuming a fixed-shape parametric distribution. In this work, we propose a novel model, ProFITi, for probabilistic forecasting of irregularly sampled time series with missing values using conditional normalizing flows. The model learns joint distributions over the future values of the time series conditioned on past observations and queried channels and times, without assuming any fixed shape of the underlying distribution. As model components, we introduce a novel invertible triangular attention layer and an invertible non-linear activation function on and onto the whole real line. We conduct extensive experiments on four datasets and demonstrate that the proposed model pro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24773;&#20917;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#22810;&#26102;&#24207;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#22810;&#31354;&#38388;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#36825;&#31687;&#35770;&#25991;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06289</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Evaluating Membership Inference Attacks and Defenses in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06289
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24773;&#20917;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#22810;&#26102;&#24207;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#22810;&#31354;&#38388;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#36825;&#31687;&#35770;&#25991;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#23041;&#32961;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26085;&#30410;&#22686;&#38271;&#12290;&#21322;&#35802;&#23454;&#30340;&#25915;&#20987;&#32773;&#65292;&#20363;&#22914;&#26381;&#21153;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#20449;&#24687;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;MIAs&#21644;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;MIAs&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#21512;&#22810;&#20010;&#36890;&#20449;&#36718;&#27425;&#30340;&#27169;&#22411;&#20449;&#24687;(&#22810;&#26102;&#24207;)&#30456;&#27604;&#20110;&#21033;&#29992;&#21333;&#20010;&#26102;&#26399;&#30340;&#27169;&#22411;&#20449;&#24687;&#25552;&#39640;&#20102;MIAs&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#22312;&#38750;&#30446;&#26631;&#23458;&#25143;&#31471;(Multi-spatial)&#20013;&#34701;&#20837;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;MIAs&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#24403;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#21516;&#36136;&#30340;&#26102;&#20505;&#12290;&#36825;&#20984;&#26174;&#20102;&#22312;MIAs&#20013;&#32771;&#34385;&#26102;&#24207;&#21644;&#31354;&#38388;&#27169;&#22411;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#35780;&#20272;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#26426;&#21046;&#23545;MIAs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks (MIAs) pose a growing threat to privacy preservation in federated learning. The semi-honest attacker, e.g., the server, may determine whether a particular sample belongs to a target client according to the observed model information. This paper conducts an evaluation of existing MIAs and corresponding defense strategies. Our evaluation on MIAs reveals two important findings about the trend of MIAs. Firstly, combining model information from multiple communication rounds (Multi-temporal) enhances the overall effectiveness of MIAs compared to utilizing model information from a single epoch. Secondly, incorporating models from non-target clients (Multi-spatial) significantly improves the effectiveness of MIAs, particularly when the clients' data is homogeneous. This highlights the importance of considering the temporal and spatial model information in MIAs. Next, we assess the effectiveness via privacy-utility tradeoff for two type defense mechanisms against MI
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06287</link><description>&lt;p&gt;
AI&#65292;&#19982;&#20154;&#30456;&#36935;&#65306;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#39640;&#39118;&#38505;&#20219;&#21153;&#21644;&#20915;&#31574;&#12290;&#36825;&#31181;&#26085;&#30410;&#22686;&#38271;&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#20154;&#31867;&#29616;&#22312;&#19981;&#26029;&#19982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#27599;&#22825;&#36827;&#34892;&#27169;&#22411;&#30340;&#22521;&#35757;&#21644;&#20351;&#29992;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#32771;&#34385;&#20154;&#19982;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20132;&#20114;&#65292;&#20294;&#20854;&#20998;&#31867;&#31232;&#30095;&#19988;&#30446;&#26631;&#21508;&#24322;&#12290;&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#24403;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06282</link><description>&lt;p&gt;
&#33719;&#21462;&#12289;&#21512;&#24182;&#12289;&#39044;&#27979;&#65306;&#36890;&#36807;&#25968;&#25454;&#28246;&#22686;&#24378;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
Retrieve, Merge, Predict: Augmenting Tables with Data Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#26159;&#32473;&#23450;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#26684;&#22686;&#24378;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26816;&#32034;&#21487;&#36830;&#25509;&#30340;&#34920;&#26684;&#12289;&#21512;&#24182;&#20449;&#24687;&#21644;&#39044;&#27979;&#32467;&#26524;&#34920;&#26684;&#12290;&#20316;&#20026;&#25968;&#25454;&#28246;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;YADL&#65288;&#21478;&#19968;&#20010;&#25968;&#25454;&#28246;&#65289;-&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#27492;&#25968;&#25454;&#21457;&#29616;&#20219;&#21153;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;-&#21644;Open Data US&#65292;&#19968;&#20010;&#34987;&#24341;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#28246;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#25968;&#25454;&#28246;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27010;&#36848;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#65292;&#26088;&#22312;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an in-depth analysis of data discovery in data lakes, focusing on table augmentation for given machine learning tasks. We analyze alternative methods used in the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. As data lakes, the paper uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for benchmarking this data discovery task -- and Open Data US, a well-referenced real data lake. Through systematic exploration on both lakes, our study outlines the importance of accurately retrieving join candidates and the efficiency of simple merging methods. We report new insights on the benefits of existing solutions and on their limitations, aiming at guiding future research in this space.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;&#65292;&#36890;&#36807;&#32435;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#39564;&#25968;&#25454;&#23494;&#20999;&#21305;&#37197;&#30340;&#22320;&#38663;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06277</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#21487;&#25511;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable seismic velocity synthesis using generative diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;&#65292;&#36890;&#36807;&#32435;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#39564;&#25968;&#25454;&#23494;&#20999;&#21305;&#37197;&#30340;&#22320;&#38663;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22320;&#38663;&#36895;&#24230;&#20272;&#35745;&#23545;&#20110;&#29702;&#35299;&#22320;&#29699;&#30340;&#22320;&#19979;&#32467;&#26500;&#12289;&#35780;&#20272;&#33258;&#28982;&#36164;&#28304;&#21644;&#35780;&#20272;&#22320;&#38663;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21453;&#28436;&#31639;&#27861;&#22312;&#21306;&#22495;&#65288;&#20363;&#22914;&#21208;&#25506;&#65289;&#21644;&#20840;&#29699;&#36895;&#24230;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#65292;&#20197;&#35206;&#30422;&#30446;&#26631;&#35299;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25552;&#39640;&#36895;&#24230;&#20272;&#35745;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#36824;&#38656;&#35201;&#32435;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#20363;&#22914;&#22320;&#36136;&#31867;&#21035;&#12289;&#38075;&#20117;&#35760;&#24405;&#21644;&#22320;&#19979;&#32467;&#26500;&#65292;&#20294;&#30446;&#21069;&#30340;&#32479;&#35745;&#25110;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#19981;&#22815;&#28789;&#27963;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21487;&#23481;&#26131;&#22320;&#32435;&#20837;&#36825;&#20123;&#20808;&#39564;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#39564;&#25968;&#25454;&#23494;&#20999;&#21305;&#37197;&#30340;&#22320;&#38663;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate seismic velocity estimations are vital to understanding Earth's subsurface structures, assessing natural resources, and evaluating seismic hazards. Machine learning-based inversion algorithms have shown promising performance in regional (i.e., for exploration) and global velocity estimation, while their effectiveness hinges on access to large and diverse training datasets whose distributions generally cover the target solutions. Additionally, enhancing the precision and reliability of velocity estimation also requires incorporating prior information, e.g., geological classes, well logs, and subsurface structures, but current statistical or neural network-based methods are not flexible enough to handle such multi-modal information. To address both challenges, we propose to use conditional generative diffusion models for seismic velocity synthesis, in which we readily incorporate those priors. This approach enables the generation of seismic velocities that closely match the expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#36890;&#36807;&#21160;&#24577;&#25506;&#32034;&#36755;&#20837;&#31354;&#38388;&#24182;&#26681;&#25454;&#23433;&#20840;&#35201;&#27714;&#21644;&#36807;&#21435;&#35266;&#23519;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36712;&#36857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#25216;&#26415;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06276</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#19979;&#23433;&#20840;&#30340;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Active Learning for Time-Series Modeling with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#36890;&#36807;&#21160;&#24577;&#25506;&#32034;&#36755;&#20837;&#31354;&#38388;&#24182;&#26681;&#25454;&#23433;&#20840;&#35201;&#27714;&#21644;&#36807;&#21435;&#35266;&#23519;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36712;&#36857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#25216;&#26415;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22914;&#27169;&#25311;&#21644;&#39044;&#27979;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#32771;&#34385;&#32473;&#23450;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#38750;&#32447;&#24615;&#22806;&#37096;&#36755;&#20837;&#32467;&#26500;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#22320;&#25506;&#32034;&#36755;&#20837;&#31354;&#38388;&#26469;&#29983;&#25104;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#36712;&#36857;&#12290;&#35813;&#26041;&#27861;&#23558;&#36755;&#20837;&#36712;&#36857;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#30340;&#36712;&#36857;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#26159;&#26681;&#25454;&#23433;&#20840;&#35201;&#27714;&#21644;&#36807;&#21435;&#30340;&#35266;&#23519;&#36880;&#27493;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#22312;&#25216;&#26415;&#24212;&#29992;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#25216;&#26415;&#20351;&#29992;&#26696;&#20363;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning time-series models is useful for many applications, such as simulation and forecasting. In this study, we consider the problem of actively learning time-series models while taking given safety constraints into account. For time-series modeling we employ a Gaussian process with a nonlinear exogenous input structure. The proposed approach generates data appropriate for time series model learning, i.e. input and output trajectories, by dynamically exploring the input space. The approach parametrizes the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations. We analyze the proposed algorithm and evaluate it empirically on a technical application. The results show the effectiveness of our approach in a realistic technical use case.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#21483;&#20570;&#31070;&#32463;SPH&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26631;&#20934;SPH&#27714;&#35299;&#22120;&#30340;&#32452;&#21512;&#26469;&#25913;&#36827;GNN&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#20934;&#30830;&#24314;&#27169;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06275</link><description>&lt;p&gt;
&#31070;&#32463;SPH: &#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#21160;&#21147;&#23398;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#21483;&#20570;&#31070;&#32463;SPH&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26631;&#20934;SPH&#27714;&#35299;&#22120;&#30340;&#32452;&#21512;&#26469;&#25913;&#36827;GNN&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#20934;&#30830;&#24314;&#27169;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;SPH&#65289;&#22312;&#29616;&#20195;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;SPH&#26159;&#19968;&#31867;&#36890;&#36807;&#26377;&#38480;&#26448;&#26009;&#28857;&#23545;&#27969;&#20307;&#21160;&#21147;&#23398;&#36827;&#34892;&#31163;&#25955;&#21270;&#22788;&#29702;&#30340;&#25289;&#26684;&#26391;&#26085;&#26041;&#26696;&#65292;&#36890;&#36807;&#36319;&#36394;&#36825;&#20123;&#26448;&#26009;&#28857;&#26469;&#36861;&#36394;&#20854;&#28436;&#21464;&#30340;&#36895;&#24230;&#22330;&#12290;&#30001;&#20110;&#20223;&#30495;&#30340;&#31890;&#23376;&#29305;&#24615;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25104;&#21151;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;GNN&#30340;&#27169;&#25311;&#22120;&#30340;&#23454;&#38469;&#23454;&#29992;&#24615;&#20381;&#36182;&#20110;&#20854;&#23545;&#29289;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38590;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24352;&#21147;&#19981;&#31283;&#23450;&#24615;&#23548;&#33268;&#30340;&#31890;&#23376;&#32858;&#31867;&#29616;&#35937;&#26159;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#29992;&#26631;&#20934;SPH&#27714;&#35299;&#22120;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#65288;&#21253;&#25324;&#21387;&#21147;&#12289;&#31896;&#24615;&#21644;&#22806;&#21147;&#37096;&#20998;&#65289;&#22686;&#24378;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GNN&#30340;&#27169;&#25311;&#22120;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#25152;&#26377;&#32463;&#36807;&#31070;&#32463;SPH&#22686;&#24378;&#30340;&#27169;&#25311;&#22120;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smoothed particle hydrodynamics (SPH) is omnipresent in modern engineering and scientific disciplines. SPH is a class of Lagrangian schemes that discretize fluid dynamics via finite material points that are tracked through the evolving velocity field. Due to the particle-like nature of the simulation, graph neural networks (GNNs) have emerged as appealing and successful surrogates. However, the practical utility of such GNN-based simulators relies on their ability to faithfully model physics, providing accurate and stable predictions over long time horizons - which is a notoriously hard problem. In this work, we identify particle clustering originating from tensile instabilities as one of the primary pitfalls. Based on these insights, we enhance both training and rollout inference of state-of-the-art GNN-based simulators with varying components from standard SPH solvers, including pressure, viscous, and external force components. All neural SPH-enhanced simulators achieve better perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#23545;&#20110;&#20984;&#38382;&#39064;&#19981;&#21463;&#20256;&#32479;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23616;&#37096;&#26799;&#24230;H\"older&#36830;&#32493;&#24615;&#26465;&#20214;&#19979;&#25910;&#25947;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32447;&#25628;&#32034;&#27493;&#39588;&#21644;&#36817;&#20284;&#30340;&#20351;&#29992;&#12290;&#23545;&#23616;&#37096;H\"older&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#24615;&#39034;&#24207;&#30340;&#20808;&#39564;&#30693;&#35782;&#20063;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616; H\"older &#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.06271</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#22312;&#27809;&#26377;&#36817;&#20284;&#30340;&#24773;&#20917;&#19979;&#26159;&#36890;&#29992;&#30340;
&lt;/p&gt;
&lt;p&gt;
Adaptive proximal gradient methods are universal without approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06271
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#23545;&#20110;&#20984;&#38382;&#39064;&#19981;&#21463;&#20256;&#32479;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23616;&#37096;&#26799;&#24230;H\"older&#36830;&#32493;&#24615;&#26465;&#20214;&#19979;&#25910;&#25947;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32447;&#25628;&#32034;&#27493;&#39588;&#21644;&#36817;&#20284;&#30340;&#20351;&#29992;&#12290;&#23545;&#23616;&#37096;H\"older&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#24615;&#39034;&#24207;&#30340;&#20808;&#39564;&#30693;&#35782;&#20063;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616; H\"older &#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20984;&#38382;&#39064;&#65292;&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#19981;&#21463;&#20256;&#32479;&#21033;&#26222;&#24076;&#20857;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#31867;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#26041;&#27861;&#22312;&#20165;&#20855;&#26377;&#23616;&#37096;H\"older&#26799;&#24230;&#36830;&#32493;&#24615;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#25910;&#25947;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36830;&#32493;&#21487;&#24494;&#30340;&#21322;&#20195;&#25968;&#20989;&#25968;&#12290;&#20026;&#20102;&#24357;&#34917;&#32570;&#20047;&#23616;&#37096;&#21033;&#26222;&#24076;&#20857;&#36830;&#32493;&#24615;&#30340;&#38382;&#39064;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;$\varepsilon$-oracle&#21644;/&#25110;&#32447;&#25628;&#32034;&#27493;&#39588;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#26222;&#36890;&#30340;H\"older&#19981;&#31561;&#24335;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#36817;&#20284;&#65292;&#21516;&#26102;&#20445;&#25345;&#36866;&#24212;&#24615;&#26041;&#26696;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#23616;&#37096;H\"older&#24120;&#25968;&#25110;H\"older&#36830;&#32493;&#24615;&#30340;&#39034;&#24207;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#23436;&#20840;&#30340;&#24207;&#21015;&#25910;&#25947;&#24615;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20102;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#28085;&#30422;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616; H\"older &#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that adaptive proximal gradient methods for convex problems are not restricted to traditional Lipschitzian assumptions. Our analysis reveals that a class of linesearch-free methods is still convergent under mere local H\"older gradient continuity, covering in particular continuously differentiable semi-algebraic functions. To mitigate the lack of local Lipschitz continuity, popular approaches revolve around $\varepsilon$-oracles and/or linesearch procedures. In contrast, we exploit plain H\"older inequalities not entailing any approximation, all while retaining the linesearch-free nature of adaptive schemes. Furthermore, we prove full sequence convergence without prior knowledge of local H\"older constants nor of the order of H\"older continuity. In numerical experiments we present comparisons to baseline methods on diverse tasks from machine learning covering both the locally and the globally H\"older setting.
&lt;/p&gt;</description></item><item><title>YAMLE&#26159;&#19968;&#20010;&#24320;&#28304;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#65292;&#26088;&#22312;&#20943;&#23569;&#37325;&#22797;&#24037;&#20316;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#23427;&#21253;&#25324;&#21629;&#20196;&#34892;&#30028;&#38754;&#21644;&#19982;PyTorch&#24211;&#30340;&#38598;&#25104;&#65292;&#33268;&#21147;&#20110;&#25104;&#20026;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.06268</link><description>&lt;p&gt;
YAMLE&#65306;&#21448;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
YAMLE: Yet Another Machine Learning Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06268
&lt;/p&gt;
&lt;p&gt;
YAMLE&#26159;&#19968;&#20010;&#24320;&#28304;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#65292;&#26088;&#22312;&#20943;&#23569;&#37325;&#22797;&#24037;&#20316;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#23427;&#21253;&#25324;&#21629;&#20196;&#34892;&#30028;&#38754;&#21644;&#19982;PyTorch&#24211;&#30340;&#38598;&#25104;&#65292;&#33268;&#21147;&#20110;&#25104;&#20026;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
YAMLE&#65306;&#21448;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#23427;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#26041;&#27861;&#30340;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#21644;&#23454;&#39564;&#12290;&#20854;&#20027;&#35201;&#21160;&#26426;&#26159;&#22312;&#23454;&#29616;&#26032;&#26041;&#27861;&#26102;&#20943;&#23569;&#37325;&#22797;&#24037;&#20316;&#65292;&#24182;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;YAMLE&#21253;&#25324;&#19968;&#20010;&#21629;&#20196;&#34892;&#30028;&#38754;&#20197;&#21450;&#19982;&#27969;&#34892;&#19988;&#32500;&#25252;&#33391;&#22909;&#30340;&#22522;&#20110;PyTorch&#30340;&#24211;&#30340;&#38598;&#25104;&#65292;&#20197;&#31616;&#21270;&#35757;&#32451;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#26085;&#24535;&#35760;&#24405;&#12290;YAMLE&#30340;&#38596;&#24515;&#22766;&#24535;&#26159;&#21457;&#23637;&#25104;&#19968;&#20010;&#20849;&#20139;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21487;&#20197;&#24555;&#36895;&#26500;&#24314;&#21644;&#27604;&#36739;&#29616;&#26377;&#30340;&#23454;&#29616;&#12290;&#22312;https://github.com/martinferianc/yamle&#25214;&#21040;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
YAMLE: Yet Another Machine Learning Environment is an open-source framework that facilitates rapid prototyping and experimentation with machine learning (ML) models and methods. The key motivation is to reduce repetitive work when implementing new approaches and improve reproducibility in ML research. YAMLE includes a command-line interface and integrations with popular and well-maintained PyTorch-based libraries to streamline training, hyperparameter optimisation, and logging. The ambition for YAMLE is to grow into a shared ecosystem where researchers and practitioners can quickly build on and compare existing implementations. Find it at: https://github.com/martinferianc/yamle.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20215;&#20540;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22914;&#26524;&#29992;&#25143;&#30340;&#25928;&#29992;&#20989;&#25968;&#23558;&#24191;&#27867;&#21464;&#21270;&#30340;&#21521;&#37327;&#20540;&#26144;&#23556;&#20026;&#30456;&#20284;&#30340;&#25928;&#29992;&#27700;&#24179;&#65292;&#20250;&#23548;&#33268;&#20215;&#20540;&#20989;&#25968;&#24178;&#25200;&#24182;&#25910;&#25947;&#21040;&#27425;&#20248;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.06266</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#24178;&#25200;&#21644;&#36138;&#23146;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Value function interference and greedy action selection in value-based multi-objective reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06266
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22914;&#26524;&#29992;&#25143;&#30340;&#25928;&#29992;&#20989;&#25968;&#23558;&#24191;&#27867;&#21464;&#21270;&#30340;&#21521;&#37327;&#20540;&#26144;&#23556;&#20026;&#30456;&#20284;&#30340;&#25928;&#29992;&#27700;&#24179;&#65292;&#20250;&#23548;&#33268;&#20215;&#20540;&#20989;&#25968;&#24178;&#25200;&#24182;&#25910;&#25947;&#21040;&#27425;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#31639;&#27861;&#23558;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30446;&#26631;&#30340;&#26356;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#30446;&#26631;&#30001;&#21521;&#37327;&#20540;&#22870;&#21169;&#34920;&#31034;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#37327;RL&#26041;&#27861;&#65288;&#22914;Q&#23398;&#20064;&#65289;&#21487;&#20197;&#36890;&#36807;&#65288;1&#65289;&#23398;&#20064;&#21521;&#37327;&#20540;&#30340;&#20215;&#20540;&#20989;&#25968;&#21644;&#65288;2&#65289;&#20351;&#29992;&#21453;&#26144;&#29992;&#25143;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#25928;&#29992;&#30340;&#26631;&#37327;&#21270;&#25110;&#25490;&#24207;&#31639;&#23376;&#26469;&#22788;&#29702;&#22810;&#20010;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#37324;&#25152;&#31034;&#65292;&#22914;&#26524;&#29992;&#25143;&#30340;&#25928;&#29992;&#20989;&#25968;&#23558;&#24191;&#27867;&#21464;&#21270;&#30340;&#21521;&#37327;&#20540;&#26144;&#23556;&#20026;&#30456;&#20284;&#30340;&#25928;&#29992;&#27700;&#24179;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20195;&#29702;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#24178;&#25200;&#65292;&#20174;&#32780;&#25910;&#25947;&#21040;&#27425;&#20248;&#31574;&#30053;&#12290;&#36825;&#22312;&#20248;&#21270;&#39044;&#26399;&#26631;&#37327;&#21270;&#22238;&#25253;&#20934;&#21017;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#26368;&#20026;&#26222;&#36941;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#35777;&#26126;&#24178;&#25200;&#20063;&#21487;&#33021;&#22312;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective reinforcement learning (MORL) algorithms extend conventional reinforcement learning (RL) to the more general case of problems with multiple, conflicting objectives, represented by vector-valued rewards. Widely-used scalar RL methods such as Q-learning can be modified to handle multiple objectives by (1) learning vector-valued value functions, and (2) performing action selection using a scalarisation or ordering operator which reflects the user's utility with respect to the different objectives. However, as we demonstrate here, if the user's utility function maps widely varying vector-values to similar levels of utility, this can lead to interference in the value-function learned by the agent, leading to convergence to sub-optimal policies. This will be most prevalent in stochastic environments when optimising for the Expected Scalarised Return criterion, but we present a simple example showing that interference can also arise in deterministic environments. We demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#24378;&#22411;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;AHGNN&#65289;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20851;&#38190;&#25317;&#22622;&#32447;&#36335;&#24182;&#21019;&#24314;N-1&#20248;&#21270;&#28526;&#27969;&#30340;&#38477;&#32500;&#65288;N-1 ROPF&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AHGNN&#21551;&#29992;&#30340;N-1 ROPF&#22312;&#20445;&#25345;&#35299;&#30340;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.06226</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;N-1&#20248;&#21270;&#28526;&#27969;&#30340;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
N-1 Reduced Optimal Power Flow Using Augmented Hierarchical Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22686;&#24378;&#22411;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;AHGNN&#65289;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20851;&#38190;&#25317;&#22622;&#32447;&#36335;&#24182;&#21019;&#24314;N-1&#20248;&#21270;&#28526;&#27969;&#30340;&#38477;&#32500;&#65288;N-1 ROPF&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AHGNN&#21551;&#29992;&#30340;N-1 ROPF&#22312;&#20445;&#25345;&#35299;&#30340;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#28526;&#27969;&#65288;OPF&#65289;&#29992;&#20110;&#22312;&#30005;&#21147;&#31995;&#32479;&#23454;&#26102;&#36816;&#34892;&#20013;&#36827;&#34892;&#21457;&#30005;&#37325;&#35843;&#12290;N-1 OPF&#21487;&#20197;&#30830;&#20445;&#22312;&#21508;&#31181;&#20107;&#25925;&#24773;&#20917;&#19979;&#23433;&#20840;&#36816;&#34892;&#30005;&#32593;&#12290;&#23545;&#20110;&#21464;&#37327;&#21644;&#32422;&#26463;&#20247;&#22810;&#30340;&#22823;&#22411;&#22797;&#26434;&#30005;&#21147;&#32593;&#32476;&#65292;&#23454;&#26102;N-1 OPF&#30340;&#26368;&#20248;&#35299;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20316;&#20026;&#39044;&#27979;&#25317;&#22622;&#25110;&#36127;&#36733;&#36807;&#37325;&#32447;&#36335;&#30340;&#39069;&#22806;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#31216;&#20026;&#22686;&#24378;&#22411;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;AHGNN&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#20851;&#38190;&#30340;&#25317;&#22622;&#32447;&#36335;&#24182;&#21019;&#24314;N-1&#20248;&#21270;&#28526;&#27969;&#30340;&#38477;&#32500;&#65288;N-1 ROPF&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;AHGNN&#21551;&#29992;&#30340;N-1 ROPF&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#35299;&#30340;&#36136;&#37327;&#12290;&#36824;&#23454;&#29616;&#20102;&#20960;&#31181;&#22522;&#20110;GNN&#30340;ML&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;AHGNN&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AHGNN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal power flow (OPF) is used to perform generation redispatch in power system real-time operations. N-1 OPF can ensure safe grid operations under diverse contingency scenarios. For large and intricate power networks with numerous variables and constraints, achieving an optimal solution for real-time N-1 OPF necessitates substantial computational resources. To mitigate this challenge, machine learning (ML) is introduced as an additional tool for predicting congested or heavily loaded lines dynamically. In this paper, an advanced ML model known as the augmented hierarchical graph neural network (AHGNN) was proposed to predict critical congested lines and create N-1 reduced OPF (N-1 ROPF). The proposed AHGNN-enabled N-1 ROPF can result in a remarkable reduction in computing time while retaining the solution quality. Several variations of GNN-based ML models are also implemented as benchmark to demonstrate effectiveness of the proposed AHGNN approach. Case studies prove the proposed AH
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#24191;&#27867;&#30340;&#38750;&#20984;&#22810;&#30446;&#26631;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#21019;&#26032;&#30340;&#22810;&#26799;&#24230;&#25237;&#24433;&#26041;&#27861;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22823;&#35268;&#27169;&#25361;&#25112;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06224</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22810;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#25311;&#20984;&#21521;&#37327;&#20248;&#21270;&#21450;&#20854;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive multi-gradient methods for quasiconvex vector optimization and applications to multi-task learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06224
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#24191;&#27867;&#30340;&#38750;&#20984;&#22810;&#30446;&#26631;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#21019;&#26032;&#30340;&#22810;&#26799;&#24230;&#25237;&#24433;&#26041;&#27861;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22823;&#35268;&#27169;&#25361;&#25112;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#21253;&#25324;&#32447;&#25628;&#32034;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#20010;&#24191;&#27867;&#30340;&#38750;&#20984;&#22810;&#30446;&#26631;&#35268;&#21010;&#38382;&#39064;&#22312;&#19968;&#20010;&#26080;&#30028;&#32422;&#26463;&#38598;&#19978;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#36866;&#24230;&#30340;&#20551;&#35774;&#19979;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30446;&#26631;&#20989;&#25968;&#21487;&#33021;&#19981;&#28385;&#36275;&#20984;&#24615;&#26631;&#20934;&#12290;&#19982;&#19979;&#38477;&#32447;&#25628;&#32034;&#31639;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#19968;&#20010;&#21021;&#22987;&#27493;&#38271;&#30001;&#20043;&#21069;&#30830;&#23450;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#26469;&#30830;&#23450;&#12290;&#36807;&#31243;&#30340;&#20027;&#35201;&#29305;&#28857;&#26159;&#30452;&#21040;&#36798;&#21040;&#39044;&#23450;&#26465;&#20214;&#25165;&#36827;&#34892;&#28176;&#36827;&#27493;&#38271;&#30340;&#20943;&#23567;&#12290;&#23427;&#21487;&#20197;&#29305;&#21035;&#24212;&#29992;&#20110;&#20026;&#26080;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#26799;&#24230;&#25237;&#24433;&#26041;&#27861;&#12290;&#19968;&#20123;&#35745;&#31639;&#23454;&#20363;&#30340;&#21021;&#27493;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#31574;&#30053;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#24212;&#29992;&#21040;&#19968;&#20123;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#39564;&#20013;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#22823;&#35268;&#27169;&#25361;&#25112;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an adaptive step-size method, which does not include line-search techniques, for solving a wide class of nonconvex multiobjective programming problems on an unbounded constraint set. We also prove convergence of a general approach under modest assumptions. More specifically, the convexity criterion might not be satisfied by the objective function. Unlike descent line-search algorithms, it does not require an initial step-size to be determined by a previously determined Lipschitz constant. The process's primary characteristic is its gradual step-size reduction up until a predetermined condition is met. It can be specifically applied to offer an innovative multi-gradient projection method for unbounded constrained optimization issues. Preliminary findings from a few computational examples confirm the accuracy of the strategy. We apply the proposed technique to some multi-task learning experiments to show its efficacy for large-scale challenges.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06223</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#25581;&#31034;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revealing Multimodal Contrastive Representation Learning through Latent Partial Causal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06223
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#29616;&#35937;&#30340;&#26377;&#24847;&#20041;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#36825;&#20123;&#33719;&#24471;&#30340;&#34920;&#31034;&#30340;&#28145;&#24230;&#20998;&#26512;&#21644;&#29702;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#21035;&#38024;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#35774;&#35745;&#30340;&#32479;&#19968;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#23548;&#33268;&#30340;&#32447;&#24615;&#25110;&#32622;&#25442;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#36825;&#19968;&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#24037;&#20855;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#22312;&#34987;&#36829;&#21453;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#23398;&#20064;&#30142;&#30149;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal contrastive representation learning methods have proven successful across a range of domains, partly due to their ability to generate meaningful shared representations of complex phenomena. To enhance the depth of analysis and understanding of these acquired representations, we introduce a unified causal model specifically designed for multimodal data. By examining this model, we show that multimodal contrastive representation learning excels at identifying latent coupled variables within the proposed unified model, up to linear or permutation transformations resulting from different assumptions. Our findings illuminate the potential of pre-trained multimodal models, eg, CLIP, in learning disentangled representations through a surprisingly simple yet highly effective tool: linear independent component analysis. Experiments demonstrate the robustness of our findings, even when the assumptions are violated, and validate the effectiveness of the proposed method in learning dise
&lt;/p&gt;</description></item><item><title>&#20271;&#20811;&#21033;&#21333;&#32454;&#32990;&#35745;&#31639;&#26174;&#24494;&#38236;&#65288;BSCCM&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#32422;12,000,000&#24352;&#20010;&#20307;&#30333;&#34880;&#32454;&#32990;&#30340;&#22270;&#20687;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26174;&#24494;&#38236;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#24320;&#21457;&#21644;&#27979;&#35797;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.06191</link><description>&lt;p&gt;
&#20271;&#20811;&#21033;&#21333;&#32454;&#32990;&#35745;&#31639;&#26174;&#24494;&#38236;&#65288;BSCCM&#65289;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Berkeley Single Cell Computational Microscopy (BSCCM) Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06191
&lt;/p&gt;
&lt;p&gt;
&#20271;&#20811;&#21033;&#21333;&#32454;&#32990;&#35745;&#31639;&#26174;&#24494;&#38236;&#65288;BSCCM&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#32422;12,000,000&#24352;&#20010;&#20307;&#30333;&#34880;&#32454;&#32990;&#30340;&#22270;&#20687;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26174;&#24494;&#38236;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#24320;&#21457;&#21644;&#27979;&#35797;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26174;&#24494;&#38236;&#65292;&#21363;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#32852;&#21512;&#35774;&#35745;&#65292;&#26174;&#31034;&#20986;&#38477;&#20302;&#25104;&#26412;&#12289;&#26356;&#31283;&#20581;&#22320;&#25191;&#34892;&#21644;&#25910;&#38598;&#26032;&#31867;&#22411;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#35745;&#31639;&#26174;&#24494;&#38236;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#34701;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#36890;&#24120;&#26159;&#26679;&#26412;&#30456;&#20851;&#30340;&#12290;&#22240;&#27492;&#65292;&#26631;&#20934;&#25968;&#25454;&#38598;&#26159;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20271;&#20811;&#21033;&#21333;&#32454;&#32990;&#35745;&#31639;&#26174;&#24494;&#38236;&#65288;BSCCM&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#32422;12,000,000&#24352;&#20010;&#20307;&#30333;&#34880;&#32454;&#32990;&#30340;&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#22312;LED&#38453;&#21015;&#26174;&#24494;&#38236;&#19978;&#37319;&#29992;&#22810;&#31181;&#29031;&#26126;&#27169;&#24335;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#26631;&#35760;&#19981;&#21516;&#32454;&#32990;&#31867;&#22411;&#34920;&#38754;&#34507;&#30333;&#30340;&#33639;&#20809;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#20026;&#35745;&#31639;&#26174;&#24494;&#38236;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26032;&#31639;&#27861;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#25552;&#20379;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational microscopy, in which hardware and algorithms of an imaging system are jointly designed, shows promise for making imaging systems that cost less, perform more robustly, and collect new types of information. Often, the performance of computational imaging systems, especially those that incorporate machine learning, is sample-dependent. Thus, standardized datasets are an essential tool for comparing the performance of different approaches. Here, we introduce the Berkeley Single Cell Computational Microscopy (BSCCM) dataset, which contains over ~12,000,000 images of 400,000 of individual white blood cells. The dataset contains images captured with multiple illumination patterns on an LED array microscope and fluorescent measurements of the abundance of surface proteins that mark different cell types. We hope this dataset will provide a valuable resource for the development and testing of new algorithms in computational microscopy and computer vision with practical biomedical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.06190</link><description>&lt;p&gt;
Masked LoGoNet&#65306;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#24555;&#36895;&#20934;&#30830;3D&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#26041;&#27861;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#37096;&#32626;&#26102;&#36890;&#24120;&#29992;&#20110;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#65292;&#32473;&#21307;&#30103;&#35774;&#26045;&#24102;&#26469;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;LoGoNet&#65292;&#37319;&#29992;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;LoGoNet&#22312;U&#24418;&#26550;&#26500;&#20869;&#25972;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#65288;LKA&#65289;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#22320;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22686;&#21152;&#32593;&#32476;&#23481;&#37327;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#25216;&#26415;&#30340;&#32452;&#21512;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#65292;&#32771;&#34385;&#21040;&#20854;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of le
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;S3L&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;&#12290;&#23427;&#32467;&#21512;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#37197;&#23545;&#35270;&#22270;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.06188</link><description>&lt;p&gt;
&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A self-supervised framework for learning whole slide representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06188
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;S3L&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;&#12290;&#23427;&#32467;&#21512;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#37197;&#23545;&#35270;&#22270;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#20010;&#20999;&#29255;&#25104;&#20687;&#23545;&#20110;&#29983;&#29289;&#21307;&#23398;&#26174;&#24494;&#38236;&#21644;&#35745;&#31639;&#30149;&#29702;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21315;&#20806;&#20687;&#32032;&#30340;&#22823;&#23567;&#12289;&#22810;&#26679;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#29305;&#24449;&#12289;&#31354;&#38388;&#24322;&#36136;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;/&#19981;&#23384;&#22312;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687; (WSIs) &#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#31361;&#26174;&#20102;&#20165;&#20381;&#38752;&#30417;&#30563;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25972;&#20010;&#20999;&#29255;&#34920;&#31034;&#12290;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#20026;&#19979;&#28216;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;&#30284;&#30151;&#35786;&#26029;&#25110;&#20998;&#23376;&#36951;&#20256;&#39044;&#27979;&#65289;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#23398;&#20064;&#65288;S3L&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21315;&#20806;&#20687;&#32032;&#35268;&#27169;&#30340;WSI&#33258;&#30417;&#30563;&#12290;S3L&#23558;&#26469;&#33258;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#25968;&#25454;&#36716;&#25442;&#31574;&#30053;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#33258;&#30417;&#30563;&#30340;&#37197;&#23545;&#35270;&#22270;&#12290;S3L&#21033;&#29992;&#20869;&#22312;&#30340;&#21306;&#22495;&#24322;&#36136;&#24615;&#12289;&#32452;&#32455;&#23398;&#29305;&#24449;&#30340;&#21487;&#21464;&#24615;&#21644;&#20449;&#24687;&#20887;&#20313;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy wi
&lt;/p&gt;</description></item><item><title>Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06187</link><description>&lt;p&gt;
Premier-TACO: &#36890;&#36807;&#26102;&#38388;&#39537;&#21160;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06187
&lt;/p&gt;
&lt;p&gt;
Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Premier-TACO&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#23569;&#26679;&#26412;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;Premier-TACO&#21033;&#29992;&#19968;&#37096;&#20998;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#25429;&#25417;&#20102;&#20851;&#38190;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#25512;&#21160;&#20102;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#65288;TACO&#65289;&#30446;&#26631;&#30340;&#21457;&#23637;&#65292;TACO&#22312;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#26174;&#33879;&#25552;&#39640;TACO&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#31163;&#32447;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Deepmind Control Suite&#12289;MetaWorld&#21644;LIBERO&#22312;&#20869;&#30340;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Premier-TACO&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SpinePose&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;&#65292;&#26080;&#38656;&#25163;&#21160;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.06185</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SpinePose&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;&#65292;&#26080;&#38656;&#25163;&#21160;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#33034;&#30424;&#30406;&#23545;&#40784;&#19982;&#20020;&#24202;&#30151;&#29366;&#30340;&#25913;&#21892;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#33034;&#30424;&#30406;&#25918;&#23556;&#23398;&#21442;&#25968;&#30340;&#27979;&#37327;&#36153;&#26102;&#19988;&#35266;&#23519;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20540;&#24471;&#20851;&#27880;&#12290;&#33258;&#21160;&#27979;&#37327;&#24037;&#20855;&#33021;&#22815;&#20197;&#36805;&#36895;&#32780;&#19968;&#33268;&#30340;&#26041;&#24335;&#36827;&#34892;&#27979;&#37327;&#65292;&#20294;&#29616;&#26377;&#24037;&#20855;&#20173;&#28982;&#21463;&#21040;&#26576;&#31181;&#31243;&#24230;&#30340;&#25163;&#21160;&#36755;&#20837;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;SpinePose&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective. Achieving appropriate spinopelvic alignment has been shown to be associated with improved clinical symptoms. However, measurement of spinopelvic radiographic parameters is time-intensive and interobserver reliability is a concern. Automated measurement tools have the promise of rapid and consistent measurements, but existing tools are still limited by some degree of manual user-entry requirements. This study presents a novel artificial intelligence (AI) tool called SpinePose that automatically predicts spinopelvic parameters with high accuracy without the need for manual entry.   Methods. SpinePose was trained and validated on 761 sagittal whole-spine X-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic incidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle (T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was labeled by 4 reviewers, including fellowship-trained spine surgeons and a fellowship-trained radiologist with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36793;&#30028;&#26159;&#20998;&#24418;&#30340;&#65292;&#23545;&#20110;&#36229;&#21442;&#25968;&#30340;&#24494;&#23567;&#25913;&#21464;&#38750;&#24120;&#25935;&#24863;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21487;&#34892;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.06184</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#35757;&#32451;&#24615;&#30340;&#36793;&#30028;&#26159;&#20998;&#24418;&#30340;
&lt;/p&gt;
&lt;p&gt;
The boundary of neural network trainability is fractal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36793;&#30028;&#26159;&#20998;&#24418;&#30340;&#65292;&#23545;&#20110;&#36229;&#21442;&#25968;&#30340;&#24494;&#23567;&#25913;&#21464;&#38750;&#24120;&#25935;&#24863;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21487;&#34892;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#20998;&#24418;&#65292;&#20363;&#22914;&#19982;Mandelbrot&#21644;&#20108;&#27425;Julia&#38598;&#30456;&#20851;&#30340;&#20998;&#24418;&#65292;&#36890;&#36807;&#36845;&#20195;&#20989;&#25968;&#35745;&#31639;&#65292;&#24182;&#35782;&#21035;&#23548;&#33268;&#32467;&#26524;&#24207;&#21015;&#21457;&#25955;&#25110;&#20445;&#25345;&#26377;&#30028;&#30340;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21516;&#26679;&#28041;&#21450;&#36845;&#20195;&#26356;&#26032;&#20989;&#25968;&#65288;&#20363;&#22914;&#26799;&#24230;&#19979;&#38477;&#30340;&#37325;&#22797;&#27493;&#39588;&#65289;&#65292;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#25110;&#21457;&#25955;&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#23545;&#36229;&#21442;&#25968;&#30340;&#24494;&#23567;&#25913;&#21464;&#38750;&#24120;&#25935;&#24863;&#12290;&#21463;&#21040;&#36825;&#20123;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#23548;&#33268;&#31283;&#23450;&#21644;&#21457;&#25955;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#27979;&#35797;&#37197;&#32622;&#20013;&#65292;&#36825;&#20010;&#36793;&#30028;&#22312;&#21313;&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#30340;&#23610;&#24230;&#33539;&#22260;&#20869;&#37117;&#26159;&#20998;&#24418;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.
&lt;/p&gt;</description></item><item><title>SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06173</link><description>&lt;p&gt;
SMC&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#24182;&#34892;&#24378;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
SMC Is All You Need: Parallel Strong Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06173
&lt;/p&gt;
&lt;p&gt;
SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#33324;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#20998;&#24067;&#21482;&#33021;&#25353;&#27604;&#20363;&#24120;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#20256;&#32479;&#30340;&#19968;&#33268;Bayesian&#26041;&#27861;&#65292;&#22914;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(SMC)&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#65292;&#20855;&#26377;&#26080;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#24182;&#34892;&#30340;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(pSMC)&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#20855;&#26377;&#24182;&#34892;&#24378;&#25193;&#23637;&#24615;&#65292;&#21363;&#22914;&#26524;&#20801;&#35768;&#24322;&#27493;&#36827;&#31243;&#25968;&#37327;&#22686;&#38271;&#65292;&#26102;&#38388;&#22797;&#26434;&#24615;(&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20869;&#23384;)&#20173;&#28982;&#20445;&#25345;&#26377;&#30028;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;pSMC&#20855;&#26377;MSE$=O(1/NR)$&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$N$&#34920;&#31034;&#27599;&#20010;&#22788;&#29702;&#22120;&#20013;&#30340;&#36890;&#20449;&#26679;&#26412;&#25968;&#37327;&#65292;$R$&#34920;&#31034;&#22788;&#29702;&#22120;&#25968;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#36866;&#24403;&#22823;&#30340;&#38382;&#39064;&#30456;&#20851;$N$&#65292;&#24403;$R\rightarrow \infty$&#26102;&#65292;&#35813;&#26041;&#27861;&#20197;&#22266;&#23450;&#26377;&#38480;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;Cost$=O(1)$&#25910;&#25947;&#21040;&#26080;&#31351;&#23567;&#31934;&#24230;MSE$=O(\varepsilon^2)$&#65292;&#27809;&#26377;&#25928;&#29575;&#27844;&#28431;&#65292;&#21363;&#35745;&#31639;&#22797;&#26434;&#24615;Cost$=O(\varepsilon)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the general framework of Bayesian inference, the target distribution can only be evaluated up-to a constant of proportionality. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes the number of communicating samples in each processor and $R$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $R \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Mixup&#23545;&#31070;&#32463;&#22604;&#38519;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;Mixup&#30340;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#25910;&#25947;&#21040;&#19982;&#39044;&#26399;&#19981;&#21516;&#30340;&#29420;&#29305;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.06171</link><description>&lt;p&gt;
Pushing Boundaries: Mixup&#23545;&#31070;&#32463;&#22604;&#38519;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Pushing Boundaries: Mixup's Influence on Neural Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Mixup&#23545;&#31070;&#32463;&#22604;&#38519;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;Mixup&#30340;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#25910;&#25947;&#21040;&#19982;&#39044;&#26399;&#19981;&#21516;&#30340;&#29420;&#29305;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23427;&#21033;&#29992;&#35757;&#32451;&#23454;&#20363;&#21450;&#20854;&#30456;&#24212;&#30340;&#26631;&#31614;&#30340;&#20984;&#32452;&#21512;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#25104;&#21151;&#30340;&#32454;&#24494;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#35266;&#23519;&#21040;&#30340;&#31070;&#32463;&#22604;&#38519;&#29616;&#35937;&#65292;&#21363;&#28145;&#24230;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#21644;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#19968;&#20010;&#31616;&#21333;&#20809;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;&#20026;&#25506;&#32034;mixup&#26159;&#21542;&#24341;&#21457;&#20102;&#26367;&#20195;&#20960;&#20309;&#37197;&#32622;&#21450;&#20854;&#33021;&#35299;&#37322;&#20854;&#25104;&#21151;&#30340;&#21160;&#26426;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#32463;&#36807;mixup&#22788;&#29702;&#30340;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#65292;&#26088;&#22312;&#25581;&#31034;&#20854;&#36816;&#34892;&#26377;&#25928;&#24615;&#30340;&#27934;&#35265;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#21508;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#23545;&#65292;&#25581;&#31034;&#20102;mixup&#30340;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#20027;&#35201;&#25910;&#25947;&#21040;&#19982;&#39044;&#26399;&#19981;&#21516;&#30340;&#29420;&#29305;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to augment the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame (ETF), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this study, we delve into the last-layer activations of training data for deep networks subjected to mixup, aiming to uncover insights into its operational efficacy. Our investigation, spanning various architectures and dataset pairs, reveals that mixup's last-layer activations predominantly converge to a distinctive configuration different than one might expect. In this configuration,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#36890;&#36807;Wasserstein&#36817;&#31471;&#31639;&#23376;&#21644;&#24179;&#22343;&#22330;&#21338;&#24328;&#21487;&#20197;&#25551;&#36848;&#29983;&#25104;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36890;&#36807;&#35299;&#32806;&#21512;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#21487;&#20197;&#33719;&#24471;&#20248;&#21270;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20989;&#25968;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06162</link><description>&lt;p&gt;
Wasserstein&#36817;&#31471;&#31639;&#23376;&#25551;&#36848;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#24182;&#35299;&#20915;&#35760;&#24518;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Wasserstein proximal operators describe score-based generative models and resolve memorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#36890;&#36807;Wasserstein&#36817;&#31471;&#31639;&#23376;&#21644;&#24179;&#22343;&#22330;&#21338;&#24328;&#21487;&#20197;&#25551;&#36848;&#29983;&#25104;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36890;&#36807;&#35299;&#32806;&#21512;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#21487;&#20197;&#33719;&#24471;&#20248;&#21270;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20989;&#25968;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#30340;&#22522;&#26412;&#25968;&#23398;&#32467;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#29992;Wasserstein&#36817;&#31471;&#31639;&#23376;&#65288;WPO&#65289;&#26469;&#26500;&#24314;SGMs&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#24179;&#22343;&#22330;&#21338;&#24328;&#65288;MFGs&#65289;&#65292;WPO&#30340;&#32467;&#26500;&#25581;&#31034;&#20102;&#25551;&#36848;&#25193;&#25955;&#21644;&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#25968;&#23398;&#32467;&#26500;&#12290;&#29305;&#21035;&#26159;&#65292;MFGs&#20197;&#19968;&#23545;&#32806;&#21512;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#24418;&#24335;&#32473;&#20986;&#20102;&#26368;&#20248;&#24615;&#26465;&#20214;&#65306;&#19968;&#31181;&#21069;&#21521;&#25511;&#21046;&#30340;Fokker-Planck&#65288;FP&#65289;&#26041;&#31243;&#21644;&#19968;&#31181;&#21521;&#21518;&#30340;Hamilton-Jacobi-Bellman&#65288;HJB&#65289;&#26041;&#31243;&#12290;&#36890;&#36807;Cole-Hopf&#21464;&#25442;&#24182;&#21033;&#29992;&#20132;&#21449;&#29109;&#21487;&#20197;&#19982;&#23494;&#24230;&#30340;&#32447;&#24615;&#27867;&#20989;&#30456;&#20851;&#32852;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HJB&#26041;&#31243;&#26159;&#19968;&#31181;&#26080;&#25511;&#21046;&#30340;FP&#26041;&#31243;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#25163;&#22836;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20989;&#25968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;SGMs&#22312;&#35757;&#32451;&#26679;&#26412;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the fundamental mathematical structure of score-based generative models (SGMs). We first formulate SGMs in terms of the Wasserstein proximal operator (WPO) and demonstrate that, via mean-field games (MFGs), the WPO formulation reveals mathematical structure that describes the inductive bias of diffusion and score-based models. In particular, MFGs yield optimality conditions in the form of a pair of coupled partial differential equations: a forward-controlled Fokker-Planck (FP) equation, and a backward Hamilton-Jacobi-Bellman (HJB) equation. Via a Cole-Hopf transformation and taking advantage of the fact that the cross-entropy can be related to a linear functional of the density, we show that the HJB equation is an uncontrolled FP equation. Second, with the mathematical structure at hand, we present an interpretable kernel-based model for the score function which dramatically improves the performance of SGMs in terms of training samples and training time. In addition, the WP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#26469;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06160</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improved Evidential Deep Learning via a Mixture of Dirichlet Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28151;&#21512;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#26469;&#25913;&#36827;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#23567;&#21270;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20197;&#23398;&#20064;&#39044;&#27979;&#20998;&#24067;&#19978;&#30340;&#20803;&#20998;&#24067;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#32463;&#39564;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;Bengs&#31561;&#20154;&#30340;&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#26681;&#26412;&#32570;&#38519;&#65306;&#21363;&#20351;&#22312;&#26080;&#38480;&#26679;&#26412;&#38480;&#21046;&#19979;&#65292;&#23398;&#20064;&#21040;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#19981;&#20250;&#28040;&#22833;&#12290;&#36890;&#36807;&#25552;&#20379;&#25991;&#29486;&#20013;&#19968;&#31867;&#24191;&#27867;&#20351;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36825;&#20010;&#35266;&#23519;&#30340;&#35777;&#23454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;EDL&#26041;&#27861;&#26412;&#36136;&#19978;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#24067;&#19982;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#29305;&#23450;&#24046;&#24322;&#24230;&#37327;&#26469;&#35757;&#32451;&#20803;&#20998;&#24067;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#29702;&#35770;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#20854;&#24314;&#27169;&#20026;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#28151;&#21512;&#29289;&#26469;&#23398;&#20064;&#19968;&#33268;&#30446;&#26631;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;EDL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores a modern predictive uncertainty estimation approach, called evidential deep learning (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. Despite their strong empirical performance, recent studies by Bengs et al. identify a fundamental pitfall of the existing methods: the learned epistemic uncertainty may not vanish even in the infinite-sample limit. We corroborate the observation by providing a unifying view of a class of widely used objectives from the literature. Our analysis reveals that the EDL methods essentially train a meta distribution by minimizing a certain divergence measure between the distribution and a sample-size-independent target distribution, resulting in spurious epistemic uncertainty. Grounded in theoretical principles, we propose learning a consistent target distribution by modeling it with a mixture of Dirichlet distributions and lear
&lt;/p&gt;</description></item><item><title>POTEC&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31574;&#30053;&#20998;&#35299;&#30340;&#31639;&#27861;&#65292;&#22312;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26377;&#25928;&#36827;&#34892;&#31163;&#31574;&#30053;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#32858;&#31867;&#36873;&#25321;&#31532;&#19968;&#38454;&#27573;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#22238;&#24402;&#26041;&#27861;&#36873;&#25321;&#27599;&#20010;&#32858;&#31867;&#20869;&#30340;&#20855;&#20307;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.06151</link><description>&lt;p&gt;
POTEC:&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#20998;&#35299;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#31163;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
POTEC: Off-Policy Learning for Large Action Spaces via Two-Stage Policy Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06151
&lt;/p&gt;
&lt;p&gt;
POTEC&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31574;&#30053;&#20998;&#35299;&#30340;&#31639;&#27861;&#65292;&#22312;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26377;&#25928;&#36827;&#34892;&#31163;&#31574;&#30053;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#32858;&#31867;&#36873;&#25321;&#31532;&#19968;&#38454;&#27573;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#22238;&#24402;&#26041;&#27861;&#36873;&#25321;&#27599;&#20010;&#32858;&#31867;&#20869;&#30340;&#20855;&#20307;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#22659;&#21534;&#22124;&#26426;&#21046;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;(OPL)&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22238;&#24402;&#27169;&#22411;&#25110;&#37325;&#35201;&#24615;&#21152;&#26435;&#31574;&#30053;&#26799;&#24230;&#65292;&#20294;&#30001;&#20110;&#36807;&#39640;&#30340;&#20559;&#24046;&#25110;&#26041;&#24046;&#32780;&#22833;&#36133;&#12290;&#20026;&#20102;&#20811;&#26381;OPL&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#31216;&#20026;&#20004;&#38454;&#27573;&#31574;&#30053;&#20998;&#35299;&#30340;&#31574;&#30053;&#20248;&#21270;(POTEC)&#12290;&#23427;&#21033;&#29992;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#32858;&#31867;&#65292;&#24182;&#20998;&#21035;&#36890;&#36807;&#22522;&#20110;&#31574;&#30053;&#21644;&#22238;&#24402;&#30340;&#26041;&#27861;&#23398;&#20064;&#20004;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#39640;&#25928;&#22320;&#23398;&#20064;&#31532;&#19968;&#38454;&#27573;&#31574;&#30053;&#20197;&#36873;&#25321;&#32858;&#31867;&#12290;&#20026;&#20102;&#22312;&#31532;&#19968;&#38454;&#27573;&#31574;&#30053;&#37319;&#26679;&#30340;&#32858;&#31867;&#20013;&#36873;&#25321;&#29305;&#23450;&#21160;&#20316;&#65292;POTEC&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#20351;&#29992;&#26469;&#33258;&#22238;&#24402;&#26041;&#27861;&#30340;&#31532;&#20108;&#38454;&#27573;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#23616;&#37096;&#27491;&#30830;&#24615;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#20165;&#35201;&#27714;&#22238;&#24402;&#27169;&#22411;&#20445;&#25345;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the rela
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#19981;&#36275;&#24773;&#20917;&#19979;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#23884;&#20837;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#27979;&#37327;&#28151;&#21512;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06150</link><description>&lt;p&gt;
&#29992;&#23567;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization with Small Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#19981;&#36275;&#24773;&#20917;&#19979;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#23884;&#20837;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#27979;&#37327;&#28151;&#21512;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#8220;&#26679;&#26412;&#19981;&#36275;&#8221;&#24773;&#20917;&#19979;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#19981;&#26159;&#22522;&#20110;&#30830;&#23450;&#24615;&#27169;&#22411;&#25552;&#21462;&#28508;&#22312;&#29305;&#24449;&#23884;&#20837;&#65292;&#32780;&#26159;&#25552;&#20986;&#22522;&#20110;&#27010;&#29575;&#26694;&#26550;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#25968;&#25454;&#28857;&#26144;&#23556;&#20026;&#27010;&#29575;&#23884;&#20837;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#32463;&#39564;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#25193;&#23637;&#20026;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;MMD&#65292;&#21487;&#20197;&#24230;&#37327;&#30001;&#19968;&#31995;&#21015;&#28508;&#22312;&#20998;&#24067;&#65288;&#21363;&#28304;&#39046;&#22495;&#65289;&#32452;&#25104;&#30340;&#28151;&#21512;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#28508;&#22312;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#23545;&#27604;&#35821;&#20041;&#23545;&#40784;&#65288;CSA&#65289;&#25439;&#22833;&#26469;&#25552;&#20513;&#20351;&#27491;&#27010;&#29575;&#23884;&#20837;&#23545;&#26356;&#25509;&#36817;&#65292;&#32780;&#23558;&#20854;&#20182;&#36127;&#27010;&#29575;&#23884;&#20837;&#25289;&#24320;&#12290;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#25429;&#25417;&#21040;&#30340;&#23398;&#21040;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#32467;&#21512;&#22312;&#20998;&#24067;&#19978;&#30340;&#24230;&#37327;&#65292;&#20174;&#32780;&#25552;&#21319;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose to tackle the problem of domain generalization in the context of \textit{insufficient samples}. Instead of extracting latent feature embeddings based on deterministic models, we propose to learn a domain-invariant representation based on the probabilistic framework by mapping each data point into probabilistic embeddings. Specifically, we first extend empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can measure the discrepancy between mixture distributions (i.e., source domains) consisting of a series of latent distributions rather than latent points. Moreover, instead of imposing the contrastive semantic alignment (CSA) loss based on pairs of latent points, a novel probabilistic CSA loss encourages positive probabilistic embedding pairs to be closer while pulling other negative ones apart. Benefiting from the learned representation captured by probabilistic models, our proposed method can marriage the measurement on the \textit{distri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36873;&#25321;&#26426;&#21046;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24213;&#23618;&#26597;&#35810;&#26159;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25552;&#20379;&#32431;&#31929;&#30340;&#21069;&#26399;&#21644;&#21518;&#26399;&#24046;&#20998;&#38544;&#31169;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06137</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36873;&#25321;&#26426;&#21046;&#30340;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Privacy of Selection Mechanisms with Gaussian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36873;&#25321;&#26426;&#21046;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#24213;&#23618;&#26597;&#35810;&#26159;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25552;&#20379;&#32431;&#31929;&#30340;&#21069;&#26399;&#21644;&#21518;&#26399;&#24046;&#20998;&#38544;&#31169;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25253;&#21578;&#22122;&#22768;&#26368;&#22823;&#20540;&#21644;&#38408;&#20540;&#20197;&#19978;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#24046;&#20998;&#38544;&#31169;(DP)&#36873;&#25321;&#26426;&#21046;&#12290;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#20302;&#28789;&#25935;&#24230;&#30340;&#26597;&#35810;&#28155;&#21152;&#22122;&#22768;&#65292;&#24182;&#25253;&#21578;&#28385;&#36275;&#26576;&#20010;&#26465;&#20214;&#30340;&#26597;&#35810;(&#22122;&#22768;&#30340;)&#31572;&#26696;&#30340;&#36523;&#20221;&#26469;&#33719;&#24471;&#30340;&#12290;&#24403;&#22312;&#26597;&#35810;&#19978;&#28155;&#21152;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#26102;&#65292;&#36825;&#20123;&#26426;&#21046;&#30340;&#32431;DP&#20445;&#35777;&#24456;&#23481;&#26131;&#33719;&#24471;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#23454;&#20363;&#21270;&#26102;&#65292;&#26631;&#20934;&#20998;&#26512;&#21482;&#33021;&#25552;&#20379;&#36817;&#20284;&#30340;DP&#20445;&#35777;&#65292;&#23613;&#31649;&#36825;&#20123;&#26426;&#21046;&#30340;&#36755;&#20986;&#20301;&#20110;&#31163;&#25955;&#31354;&#38388;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#30340;&#25253;&#21578;&#22122;&#22768;&#26368;&#22823;&#20540;&#21644;&#38408;&#20540;&#20197;&#19978;&#30340;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39069;&#22806;&#30340;&#20551;&#35774;&#19979;&#65292;&#21363;&#24213;&#23618;&#26597;&#35810;&#26159;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20026;&#25253;&#21578;&#22122;&#22768;&#26368;&#22823;&#20540;&#25552;&#20379;&#32431;&#31929;&#30340;&#21069;&#26399;DP&#30028;&#38480;&#65292;&#20197;&#21450;&#20026;&#38408;&#20540;&#20197;&#19978;&#25552;&#20379;&#32431;&#31929;&#30340;&#21518;&#26399;DP&#30028;&#38480;&#12290;&#24471;&#21040;&#30340;&#30028;&#38480;&#26159;&#32039;&#23494;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#20803;&#26041;&#27861;&#25968;&#20540;&#35780;&#20272;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Report Noisy Max and Above Threshold are two classical differentially private (DP) selection mechanisms. Their output is obtained by adding noise to a sequence of low-sensitivity queries and reporting the identity of the query whose (noisy) answer satisfies a certain condition. Pure DP guarantees for these mechanisms are easy to obtain when Laplace noise is added to the queries. On the other hand, when instantiated using Gaussian noise, standard analyses only yield approximate DP guarantees despite the fact that the outputs of these mechanisms lie in a discrete space. In this work, we revisit the analysis of Report Noisy Max and Above Threshold with Gaussian noise and show that, under the additional assumption that the underlying queries are bounded, it is possible to provide pure ex-ante DP bounds for Report Noisy Max and pure ex-post DP bounds for Above Threshold. The resulting bounds are tight and depend on closed-form expressions that can be numerically evaluated using standard met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24322;&#26500;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#31867;&#21035;&#30340;&#22320;&#22270;&#23454;&#20307;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#28508;&#22312;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.06135</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#26500;&#22270;&#23545;&#27604;&#23398;&#20064;&#20849;&#21516;&#23398;&#20064;&#22320;&#22270;&#23454;&#20307;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Jointly Learning Representations for Map Entities via Heterogeneous Graph Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24322;&#26500;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#31867;&#21035;&#30340;&#22320;&#22270;&#23454;&#20307;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#28508;&#22312;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#22320;&#22270;&#22312;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20026;&#21508;&#31181;&#22478;&#24066;&#31649;&#29702;&#22330;&#26223;&#21644;&#26085;&#24120;&#29983;&#27963;&#26381;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#24320;&#21457;&#26377;&#25928;&#30340;&#22320;&#22270;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#65288;MERL&#65289;&#26041;&#27861;&#23545;&#20110;&#20174;&#30005;&#23376;&#22320;&#22270;&#20013;&#25552;&#21462;&#23884;&#20837;&#20449;&#24687;&#24182;&#23558;&#22320;&#22270;&#23454;&#20307;&#36716;&#21270;&#20026;&#34920;&#31034;&#21521;&#37327;&#20197;&#20379;&#19979;&#28216;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MERL&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#19968;&#31181;&#29305;&#23450;&#31867;&#21035;&#30340;&#22320;&#22270;&#23454;&#20307;&#65292;&#22914;&#20852;&#36259;&#28857;&#65288;POIs&#65289;&#12289;&#36947;&#36335;&#27573;&#25110;&#22303;&#22320;&#20998;&#22359;&#65292;&#36825;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#30340;&#22522;&#20110;&#22320;&#22270;&#30340;&#24212;&#29992;&#26469;&#35828;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#21487;&#33021;&#20250;&#20002;&#22833;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#20043;&#38388;&#30340;&#28508;&#22312;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19981;&#21516;&#22320;&#22270;&#23454;&#20307;&#30340;&#20998;&#21035;&#29983;&#25104;&#30340;&#34920;&#31034;&#21487;&#33021;&#20250;&#24341;&#20837;&#19981;&#19968;&#33268;&#24615;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOME-GCL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#31181;&#31867;&#21035;&#22320;&#22270;&#23454;&#20307;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#24322;&#26500;&#22320;&#22270;&#23454;&#20307;&#22270;&#65288;HOME&#22270;&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#22320;&#22270;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#30340;&#28508;&#22312;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electronic map plays a crucial role in geographic information systems, serving various urban managerial scenarios and daily life services. Developing effective Map Entity Representation Learning (MERL) methods is crucial to extracting embedding information from electronic maps and converting map entities into representation vectors for downstream applications. However, existing MERL methods typically focus on one specific category of map entities, such as POIs, road segments, or land parcels, which is insufficient for real-world diverse map-based applications and might lose latent structural and semantic information interacting between entities of different types. Moreover, using representations generated by separate models for different map entities can introduce inconsistencies. Motivated by this, we propose a novel method named HOME-GCL for learning representations of multiple categories of map entities. Our approach utilizes a heterogeneous map entity graph (HOME graph) that in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25299;&#25169;&#24863;&#30693;&#20256;&#25773;&#65288;ATP&#65289;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#20013;&#33410;&#28857;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#33021;&#23545;&#19981;&#21516;&#33410;&#28857;&#30340;&#25299;&#25169;&#35282;&#33394;&#36827;&#34892;&#20010;&#24615;&#21270;&#20256;&#25773;&#65292;&#24182;&#20943;&#23569;&#20256;&#25773;&#24102;&#26469;&#30340;&#20559;&#24046;&#21644;&#39069;&#22806;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.06128</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#20256;&#25773;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rethinking Node-wise Propagation for Large-scale Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25299;&#25169;&#24863;&#30693;&#20256;&#25773;&#65288;ATP&#65289;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#20013;&#33410;&#28857;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#33021;&#23545;&#19981;&#21516;&#33410;&#28857;&#30340;&#25299;&#25169;&#35282;&#33394;&#36827;&#34892;&#20010;&#24615;&#21270;&#20256;&#25773;&#65292;&#24182;&#20943;&#23569;&#20256;&#25773;&#24102;&#26469;&#30340;&#20559;&#24046;&#21644;&#39069;&#22806;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#22522;&#20110;&#22270;&#30340;Web&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#39640;&#25928;&#36816;&#34892;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21487;&#25193;&#23637;&#30340;GNN&#20542;&#21521;&#20110;&#20197;&#30456;&#21516;&#30340;&#20256;&#25773;&#35268;&#21017;&#22788;&#29702;&#22270;&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#25299;&#25169;&#29420;&#29305;&#24615;&#65307;&#29616;&#26377;&#30340;&#33410;&#28857;&#32423;&#20256;&#25773;&#20248;&#21270;&#31574;&#30053;&#22312;&#22797;&#26434;&#30340;Web&#35268;&#27169;&#22270;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#38656;&#35201;&#23545;&#33410;&#28857;&#30340;&#23616;&#37096;&#23646;&#24615;&#36827;&#34892;&#20840;&#38754;&#25551;&#32472;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;Web&#35268;&#27169;&#22270;&#20013;&#30340;&#19981;&#21516;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#30340;&#25299;&#25169;&#35282;&#33394;&#65292;&#22240;&#27492;&#26080;&#24046;&#21035;&#22320;&#20256;&#25773;&#25110;&#24573;&#35270;&#23616;&#37096;&#19978;&#19979;&#25991;&#21487;&#33021;&#20250;&#24433;&#21709;&#33410;&#28857;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23567;&#35268;&#27169;&#24773;&#26223;&#26080;&#27861;&#21305;&#37197;Web&#35268;&#27169;&#22270;&#20013;&#30340;&#36825;&#31181;&#22797;&#26434;&#25299;&#25169;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25299;&#25169;&#24863;&#30693;&#20256;&#25773;&#65288;ATP&#65289;&#26041;&#27861;&#65292;&#20943;&#23569;&#28508;&#22312;&#30340;&#39640;&#20559;&#24046;&#20256;&#25773;&#21644;&#39069;&#22806;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalable graph neural networks (GNNs) have emerged as a promising technique, which exhibits superior predictive performance and high running efficiency across numerous large-scale graph-based web applications. However, (i) Most scalable GNNs tend to treat all nodes in graphs with the same propagation rules, neglecting their topological uniqueness; (ii) Existing node-wise propagation optimization strategies are insufficient on web-scale graphs with intricate topology, where a full portrayal of nodes' local properties is required. Intuitively, different nodes in web-scale graphs possess distinct topological roles, and therefore propagating them indiscriminately or neglect local contexts may compromise the quality of node representations. This intricate topology in web-scale graphs cannot be matched by small-scale scenarios. To address the above issues, we propose \textbf{A}daptive \textbf{T}opology-aware \textbf{P}ropagation (ATP), which reduces potential high-bias propagation and extrac
&lt;/p&gt;</description></item><item><title>CityFlowER&#26159;&#19968;&#31181;&#39640;&#25928;&#30495;&#23454;&#30340;&#20132;&#36890;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#30495;&#23454;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06127</link><description>&lt;p&gt;
CityFlowER:&#19968;&#31181;&#20855;&#26377;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#25928;&#30495;&#23454;&#20132;&#36890;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
CityFlowER: An Efficient and Realistic Traffic Simulator with Embedded Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06127
&lt;/p&gt;
&lt;p&gt;
CityFlowER&#26159;&#19968;&#31181;&#39640;&#25928;&#30495;&#23454;&#30340;&#20132;&#36890;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#30495;&#23454;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27169;&#25311;&#26159;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#12289;&#26234;&#33021;&#20132;&#36890;&#25511;&#21046;&#25919;&#31574;&#23398;&#20064;&#21644;&#20132;&#36890;&#27969;&#20998;&#26512;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20854;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#25152;&#20351;&#29992;&#27169;&#25311;&#22120;&#30340;&#30495;&#23454;&#24615;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#27169;&#25311;&#22120;&#65288;&#20363;&#22914;SUMO&#21644;CityFlow&#65289;&#24448;&#24448;&#21463;&#38480;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#21644;&#36807;&#20110;&#31616;&#21270;&#39550;&#39542;&#34892;&#20026;&#30340;&#36229;&#21442;&#25968;&#65292;&#23548;&#33268;&#27169;&#25311;&#32467;&#26524;&#19981;&#30495;&#23454;&#12290;&#20026;&#20102;&#22686;&#24378;&#30495;&#23454;&#24615;&#65292;&#19968;&#20123;&#27169;&#25311;&#22120;&#25552;&#20379;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#30340;&#24212;&#29992;&#31243;&#24207;&#25509;&#21475;&#65288;API&#65289;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22797;&#26434;&#30340;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#36710;&#36742;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CityFlowER&#65292;&#19968;&#31181;&#22312;&#29616;&#26377;CityFlow&#27169;&#25311;&#22120;&#22522;&#30784;&#19978;&#30340;&#25913;&#36827;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#25928;&#30495;&#23454;&#30340;&#22478;&#24066;&#20132;&#36890;&#27169;&#25311;&#12290;CityFlowER&#21019;&#26032;&#24615;&#22320;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#23884;&#20837;&#21040;&#27169;&#25311;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#25311;&#30340;&#25928;&#29575;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic simulation is an essential tool for transportation infrastructure planning, intelligent traffic control policy learning, and traffic flow analysis. Its effectiveness relies heavily on the realism of the simulators used. Traditional traffic simulators, such as SUMO and CityFlow, are often limited by their reliance on rule-based models with hyperparameters that oversimplify driving behaviors, resulting in unrealistic simulations. To enhance realism, some simulators have provided Application Programming Interfaces (APIs) to interact with Machine Learning (ML) models, which learn from observed data and offer more sophisticated driving behavior models. However, this approach faces challenges in scalability and time efficiency as vehicle numbers increase. Addressing these limitations, we introduce CityFlowER, an advancement over the existing CityFlow simulator, designed for efficient and realistic city-wide traffic simulation. CityFlowER innovatively pre-embeds ML models within the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEAK&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#30340;&#26816;&#39564;&#12290;PEAK&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06122</link><description>&lt;p&gt;
&#20351;&#29992;PEAK&#36827;&#34892;&#31397;&#25506;&#65306;&#22810;&#20010;&#25968;&#25454;&#27969;&#22343;&#20540;&#30340;&#39034;&#24207;&#12289;&#38750;&#21442;&#25968;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEAK&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#30340;&#26816;&#39564;&#12290;PEAK&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;PEAK&#65288;&#22522;&#20110;&#26399;&#26395;&#24179;&#22343;&#36164;&#20135;&#30340;&#31397;&#25506;&#65289;&#65292;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#30340;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#27979;&#35797;&#12290;PEAK&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#25105;&#20204;&#30340;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#38408;&#20540;&#35782;&#21035;&#20219;&#21153;&#20013;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. PEAK is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. We numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating the computational efficiency of our method against state-of-the-art testing methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31639;&#27861;&#30340;&#26032;&#39062;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#21644;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#28369;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.06121</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#33021;&#37327;&#21305;&#37197;&#20174;&#29627;&#23572;&#20857;&#26364;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Iterated Denoising Energy Matching for Sampling from Boltzmann Densities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06121
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31639;&#27861;&#30340;&#26032;&#39062;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#37327;&#20989;&#25968;&#21644;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#25968;&#25454;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#23545;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#28369;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#32479;&#35745;&#29420;&#31435;&#30340;&#26679;&#26412;&#65292;&#27604;&#22914;&#22810;&#20307;&#31995;&#32479;&#30340;&#24179;&#34913;&#26679;&#26412;&#65292;&#26159;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#21435;&#22122;&#33021;&#37327;&#21305;&#37197;&#65288;iDEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#24471;&#20998;&#21305;&#37197;&#30446;&#26631;&#65292;&#20165;&#20351;&#29992;&#33021;&#37327;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230; - &#32780;&#19981;&#26159;&#25968;&#25454;&#26679;&#26412; - &#26469;&#35757;&#32451;&#25193;&#25955;&#22522;&#30784;&#30340;&#37319;&#26679;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;iDEM&#22312;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65306;&#65288;I&#65289;&#20174;&#25193;&#25955;&#22522;&#30784;&#30340;&#37319;&#26679;&#22120;&#20013;&#37319;&#26679;&#39640;&#27169;&#22411;&#23494;&#24230;&#30340;&#21306;&#22495;&#65292;&#21644;&#65288;II&#65289;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#22312;&#25105;&#20204;&#30340;&#38543;&#26426;&#21305;&#37197;&#30446;&#26631;&#20013;&#36827;&#19968;&#27493;&#25913;&#36827;&#37319;&#26679;&#22120;&#12290;iDEM&#22312;&#39640;&#32500;&#24230;&#19978;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#20869;&#37096;&#21305;&#37197;&#30446;&#26631;&#26159;&#26080;&#38656;&#27169;&#25311;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;MCMC&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#24555;&#36895;&#27169;&#24335;&#28151;&#21512;&#34892;&#20026;&#65292;iDEM&#24179;&#28369;&#20102;&#33021;&#37327;&#32972;&#26223;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#20998;&#25674;&#37319;&#26679;&#22120;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#20219;&#21153;&#36827;&#34892;&#20102;iDEM&#30340;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. In this paper, we propose Iterated Denoising Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient -- and no data samples -- to train a diffusion-based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion-based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks rang
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#21516;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22320;&#36136;&#30899;&#23553;&#23384;&#30340;&#20195;&#29702;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#22312;&#32500;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#21152;&#24555;&#21516;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#36739;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06110</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#25968;&#25454;&#21516;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#22320;&#36136;&#30899;&#23553;&#23384;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI enhanced data assimilation and uncertainty quantification applied to Geological Carbon Storage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#21516;&#21270;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22320;&#36136;&#30899;&#23553;&#23384;&#30340;&#20195;&#29702;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#22312;&#32500;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#21152;&#24555;&#21516;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#36739;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#25968;&#25454;&#21516;&#21270;&#65288;DA&#65289;&#25216;&#26415;&#30340;&#25972;&#21512;&#65292;&#37325;&#28857;&#26159;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#29289;&#29702;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#22320;&#36136;&#30899;&#23553;&#23384;&#65288;GCS&#65289;&#39033;&#30446;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNOs&#65289;&#21644;Transformer UNet&#65288;T-UNet&#65289;&#22312;&#27785;&#31215;&#36890;&#36947;&#20648;&#23618;&#20013;CO$_2$&#27880;&#20837;&#27169;&#25311;&#20013;&#30340;&#20195;&#29702;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#28151;&#21512;ESMDA&#65288;SH-ESMDA&#65289;&#65292;&#36825;&#26159;&#20256;&#32479;&#38598;&#21512;&#24179;&#28369;&#22120;&#19982;&#22810;&#25968;&#25454;&#21516;&#21270;&#65288;ESMDA&#65289;&#30340;&#19968;&#31181;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;FNOs&#21644;T-UNet&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#20351;&#26631;&#20934;&#30340;ESMDA&#36807;&#31243;&#33267;&#23569;&#24555;50&#65285;&#25110;&#26356;&#22810;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#21516;&#21270;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#28151;&#21512;RML&#65288;SH-RML&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20381;&#36182;&#20110;&#38543;&#26426;&#26368;&#22823;&#20284;&#28982;&#65288;RML&#65289;&#30340;&#21464;&#20998;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the integration of machine learning (ML) and data assimilation (DA) techniques, focusing on implementing surrogate models for Geological Carbon Storage (GCS) projects while maintaining high fidelity physical results in posterior states. Initially, we evaluate the surrogate modeling capability of two distinct machine learning models, Fourier Neural Operators (FNOs) and Transformer UNet (T-UNet), in the context of CO$_2$ injection simulations within channelized reservoirs. We introduce the Surrogate-based hybrid ESMDA (SH-ESMDA), an adaptation of the traditional Ensemble Smoother with Multiple Data Assimilation (ESMDA). This method uses FNOs and T-UNet as surrogate models and has the potential to make the standard ESMDA process at least 50% faster or more, depending on the number of assimilation steps. Additionally, we introduce Surrogate-based Hybrid RML (SH-RML), a variational data assimilation approach that relies on the randomized maximum likelihood (RML) wher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20316;&#24330;&#26816;&#27979;&#26694;&#26550;CHEESE&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#20102;&#22836;&#37096;&#23039;&#21183;&#12289;&#20957;&#35270;&#35282;&#24230;&#12289;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#20449;&#24687;&#31561;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26631;&#31614;&#29983;&#25104;&#22120;&#21644;&#29305;&#24449;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20316;&#24330;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2402.06107</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#32771;&#35797;&#20013;&#30340;&#20316;&#24330;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning for Cheating Detection and Localization in Online Examinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20316;&#24330;&#26816;&#27979;&#26694;&#26550;CHEESE&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#20102;&#22836;&#37096;&#23039;&#21183;&#12289;&#20957;&#35270;&#35282;&#24230;&#12289;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#20449;&#24687;&#31561;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26631;&#31614;&#29983;&#25104;&#22120;&#21644;&#29305;&#24449;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20316;&#24330;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#24180;&#20896;&#29366;&#30149;&#27602;&#30149;&#27969;&#34892;&#30123;&#24773;&#30340;&#34067;&#24310;&#23548;&#33268;&#35768;&#22810;&#35838;&#31243;&#21644;&#32771;&#35797;&#21464;&#25104;&#22312;&#32447;&#24418;&#24335;&#12290;&#32771;&#35797;&#30417;&#32771;&#31995;&#32479;&#20013;&#30340;&#20316;&#24330;&#34892;&#20026;&#26816;&#27979;&#27169;&#22411;&#22312;&#20445;&#35777;&#36828;&#31243;&#32771;&#35797;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20316;&#24330;&#34892;&#20026;&#24456;&#23569;&#35265;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#22312;&#20316;&#24330;&#34892;&#20026;&#26816;&#27979;&#20219;&#21153;&#20013;&#27809;&#26377;&#20840;&#38754;&#32771;&#34385;&#22836;&#37096;&#23039;&#21183;&#12289;&#20957;&#35270;&#35282;&#24230;&#12289;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#20449;&#24687;&#31561;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#25552;&#20986;&#20102;CHEESE&#65292;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20316;&#24330;&#26816;&#27979;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23454;&#29616;&#24369;&#30417;&#30563;&#30340;&#26631;&#31614;&#29983;&#25104;&#22120;&#21644;&#23398;&#20064;&#21028;&#21035;&#24615;&#29305;&#24449;&#30340;&#29305;&#24449;&#32534;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#23558;3D&#21367;&#31215;&#25552;&#21462;&#30340;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#29305;&#24449;&#19982;OpenFace 2.0&#25429;&#33719;&#30340;&#30524;&#30555;&#20957;&#35270;&#12289;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#25340;&#25509;&#65292;&#36825;&#20123;&#29305;&#24449;&#34987;&#36865;&#20837;&#26102;&#31354;&#22270;&#27169;&#22359;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of the Coronavirus disease-2019 epidemic has caused many courses and exams to be conducted online. The cheating behavior detection model in examination invigilation systems plays a pivotal role in guaranteeing the equality of long-distance examinations. However, cheating behavior is rare, and most researchers do not comprehensively take into account features such as head posture, gaze angle, body posture, and background information in the task of cheating behavior detection. In this paper, we develop and present CHEESE, a CHEating detection framework via multiplE inStancE learning. The framework consists of a label generator that implements weak supervision and a feature encoder to learn discriminative features. In addition, the framework combines body posture and background features extracted by 3D convolution with eye gaze, head posture and facial features captured by OpenFace 2.0. These features are fed into the spatio-temporal graph module by stitching to analyze the spa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06104</link><description>&lt;p&gt;
&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#26126;&#30830;&#23398;&#20064;&#20989;&#25968;&#23548;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22238;&#24402;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#27599;&#20010;&#20010;&#20307;&#25968;&#25454;&#26679;&#26412;&#30340;&#30495;&#23454;&#20540;&#23545;&#40784;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#20851;&#31995;&#30340;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24037;&#20316;&#24341;&#20837;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#25913;&#36827;&#22238;&#24402;&#26041;&#27861;&#65292;&#20294;&#22312;&#23436;&#20840;&#25429;&#25417;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAR&#65288;&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65289;&#20316;&#20026;&#19968;&#31181;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#39046;&#22495;&#30340;&#20843;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#27969;&#20307;&#30418;&#23376;&#23454;&#39564;&#25511;&#21046;&#31995;&#32479;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21512;&#25104;&#22797;&#26434;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#25968;&#25454;&#39640;&#25928;&#20551;&#35774;&#27979;&#35797;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06102</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30495;&#23454;&#19990;&#30028;&#27969;&#20307;&#24341;&#23548;&#21018;&#20307;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06102
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#27969;&#20307;&#30418;&#23376;&#23454;&#39564;&#25511;&#21046;&#31995;&#32479;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21512;&#25104;&#22797;&#26434;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#25968;&#25454;&#39640;&#25928;&#20551;&#35774;&#27979;&#35797;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#20381;&#36182;&#20110;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;&#36825;&#26679;&#30340;&#39046;&#22495;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#21160;&#24577;&#29616;&#35937;&#65292;&#24456;&#38590;&#20197;&#39640;&#31215;&#20998;&#36895;&#29575;&#36827;&#34892;&#27169;&#25311;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29616;&#20195;&#28145;&#24230;RL&#31639;&#27861;&#22312;&#26114;&#36149;&#25110;&#23433;&#20840;&#20851;&#38190;&#30828;&#20214;&#19978;&#30340;&#30452;&#25509;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#27969;&#20307;&#30418;&#23376;&#65288;Box o Flows&#65289;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21488;&#24335;&#23454;&#39564;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#21160;&#24577;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#31995;&#32479;&#22320;&#35780;&#20272;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#27969;&#20307;&#30418;&#23376;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#22870;&#21169;&#35268;&#33539;&#21512;&#25104;&#21508;&#31181;&#22797;&#26434;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#31163;&#32447;RL&#22312;&#25968;&#25454;&#39640;&#25928;&#20551;&#35774;&#27979;&#35797;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#37325;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#20174;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;&#20197;&#21450;&#27969;&#20307;&#30418;&#23376;&#31561;&#31995;&#32479;&#30340;&#21487;&#29992;&#24615;&#23558;&#25512;&#21160;RL&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in real-world applications of reinforcement learning (RL) have relied on the ability to accurately simulate systems at scale. However, domains such as fluid dynamical systems exhibit complex dynamic phenomena that are hard to simulate at high integration rates, limiting the direct application of modern deep RL algorithms to often expensive or safety critical hardware. In this work, we introduce "Box o Flows", a novel benchtop experimental control system for systematically evaluating RL algorithms in dynamic real-world scenarios. We describe the key components of the Box o Flows, and through a series of experiments demonstrate how state-of-the-art model-free RL algorithms can synthesize a variety of complex behaviors via simple reward specifications. Furthermore, we explore the role of offline RL in data-efficient hypothesis testing by reusing past experiences. We believe that the insights gained from this preliminary study and the availability of systems like the Box o 
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#38754;&#20020;&#30528;&#32570;&#20047;&#19987;&#23478;&#30693;&#35782;&#25972;&#21512;&#12289;&#33410;&#28857;&#24230;&#25968;&#26497;&#31471;&#24615;&#19981;&#31283;&#23450;&#12289;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#32771;&#34385;&#20197;&#21450;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#23581;&#35797;&#22823;&#22810;&#26159;&#23396;&#31435;&#30340;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06098</link><description>&lt;p&gt;
&#26469;&#65292;&#35265;&#65292;&#32988;&#65306;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#21069;&#30340;&#20247;&#22810;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06098
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#38754;&#20020;&#30528;&#32570;&#20047;&#19987;&#23478;&#30693;&#35782;&#25972;&#21512;&#12289;&#33410;&#28857;&#24230;&#25968;&#26497;&#31471;&#24615;&#19981;&#31283;&#23450;&#12289;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#32771;&#34385;&#20197;&#21450;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#23581;&#35797;&#22823;&#22810;&#26159;&#23396;&#31435;&#30340;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;(KG)&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#22823;&#35268;&#27169;&#38142;&#25509;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;KG&#30340;&#24040;&#22823;&#35268;&#27169;&#65292;&#22270;&#35889;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#24110;&#21161;&#20154;&#31867;&#36827;&#34892;&#20998;&#26512;&#12289;&#35299;&#37322;&#21644;&#27169;&#24335;&#26816;&#27979;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;KG&#23398;&#20064;&#31995;&#32479;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20102;&#36171;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22270;&#35889;&#23398;&#20064;&#20013;&#23384;&#22312;&#22235;&#20010;&#20851;&#38190;&#19981;&#36275;&#65292;&#36825;&#20123;&#19981;&#36275;&#21516;&#26102;&#38480;&#21046;&#20102;KG&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#20154;&#31867;&#19982;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#30340;&#26368;&#20339;&#25509;&#21475;&#33021;&#21147;&#12290;&#36825;&#20123;&#19981;&#36275;&#21253;&#25324;&#65306;1)&#32570;&#20047;&#19987;&#23478;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;2)&#23545;KG&#20013;&#33410;&#28857;&#24230;&#25968;&#26497;&#31471;&#24615;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;3)&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32570;&#20047;&#23545;&#19981;&#30830;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#32771;&#34385;&#65292;4)&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35299;&#20915;&#27599;&#20010;&#38382;&#39064;&#30340;&#29616;&#26377;&#23581;&#35797;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25351;&#20986;&#27599;&#20010;&#23581;&#35797;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#30340;&#23581;&#35797;&#30456;&#38548;&#31163;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) have become increasingly common for representing large-scale linked data. However, their immense size has required graph learning systems to assist humans in analysis, interpretation, and pattern detection. While there have been promising results for researcher- and clinician- empowerment through a variety of KG learning systems, we identify four key deficiencies in state-of-the-art graph learning that simultaneously limit KG learning performance and diminish the ability of humans to interface optimally with these learning systems. These deficiencies are: 1) lack of expert knowledge integration, 2) instability to node degree extremity in the KG, 3) lack of consideration for uncertainty and relevance while learning, and 4) lack of explainability. Furthermore, we characterise state-of-the-art attempts to solve each of these problems and note that each attempt has largely been isolated from attempts to solve the other problems. Through a formalisation of these probl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TWIG&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25299;&#25169;&#29305;&#24449;&#23398;&#20064;&#26435;&#37325;&#26469;&#27169;&#25311;KGE&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#20855;&#26377;&#39044;&#20808;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#36328;&#22270;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06097</link><description>&lt;p&gt;
TWIG&#65306;&#36890;&#36807;&#27169;&#25311;KGE&#27169;&#22411;&#23454;&#29616;&#39044;&#20808;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#36328;&#22270;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TWIG&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25299;&#25169;&#29305;&#24449;&#23398;&#20064;&#26435;&#37325;&#26469;&#27169;&#25311;KGE&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#20855;&#26377;&#39044;&#20808;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#36328;&#22270;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TWIG&#65288;Topologically-Weighted Intelligence Generation&#65289;&#30340;&#26032;&#39062;&#30340;&#12289;&#26080;&#38656;&#23884;&#20837;&#30340;&#27169;&#25311;KGE&#36755;&#20986;&#30340;&#33539;&#24335;&#65292;&#23427;&#21482;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#12290;TWIG&#20174;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#23398;&#20064;&#26435;&#37325;&#65292;&#27809;&#26377;&#23545;&#23454;&#20307;&#25110;&#36793;&#30340;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;UMLS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21333;&#20010;TWIG&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#25152;&#26377;&#36229;&#21442;&#25968;&#37197;&#32622;&#19979;&#26368;&#20808;&#36827;&#30340;ComplEx-N3 KGE&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#23427;&#21482;&#20351;&#29992;&#20102;2590&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20294;&#20934;&#30830;&#39044;&#27979;&#20102;1215&#20010;&#19981;&#21516;&#36229;&#21442;&#25968;&#32452;&#21512;&#30340;&#32467;&#26524;&#65292;&#30456;&#24403;&#20110;29322000&#20010;&#21442;&#25968;&#30340;&#24635;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce TWIG (Topologically-Weighted Intelligence Generation), a novel, embedding-free paradigm for simulating the output of KGEs that uses a tiny fraction of the parameters. TWIG learns weights from inputs that consist of topological features of the graph data, with no coding for latent representations of entities or edges. Our experiments on the UMLS dataset show that a single TWIG neural network can predict the results of state-of-the-art ComplEx-N3 KGE model nearly exactly on across all hyperparameter configurations. To do this it uses a total of 2590 learnable parameters, but accurately predicts the results of 1215 different hyperparameter combinations with a combined cost of 29,322,000 parameters. Based on these results, we make two claims: 1) that KGEs do not learn latent semantics, but only latent representations of structural patterns; 2) that hyperparameter choice in KGEs is a deterministic function of the KGE model and graph structure. We further hypothesi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#26680;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#25913;&#36827;&#38543;&#26426;&#28216;&#36208;&#26680;&#24182;&#24341;&#20837;&#39068;&#33394;&#21305;&#37197;&#38543;&#26426;&#28216;&#36208;&#65292;&#25552;&#21319;&#20102;&#22270;&#26680;&#22312;&#29305;&#24449;&#24037;&#31243;&#20013;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#38543;&#26426;&#28216;&#36208;&#26680;&#19982;GCN&#23618;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06087</link><description>&lt;p&gt;
&#20855;&#26377;&#25913;&#36827;&#30340;&#38543;&#26426;&#28216;&#36208;&#26680;&#30340;&#25551;&#36848;&#24615;&#26680;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Descriptive Kernel Convolution Network with Improved Random Walk Kernel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#26680;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#25913;&#36827;&#38543;&#26426;&#28216;&#36208;&#26680;&#24182;&#24341;&#20837;&#39068;&#33394;&#21305;&#37197;&#38543;&#26426;&#28216;&#36208;&#65292;&#25552;&#21319;&#20102;&#22270;&#26680;&#22312;&#29305;&#24449;&#24037;&#31243;&#20013;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#38543;&#26426;&#28216;&#36208;&#26680;&#19982;GCN&#23618;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26680;&#26366;&#32463;&#26159;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20027;&#35201;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21487;&#23398;&#20064;&#24615;&#65292;&#24050;&#34987;&#29616;&#20195;GNN&#21462;&#20195;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26680;&#21367;&#31215;&#32593;&#32476;(KCNs)&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#38544;&#34255;&#22270;&#26469;&#21367;&#31215;&#36755;&#20837;&#25968;&#25454;&#65292;&#25104;&#21151;&#22320;&#20351;&#22270;&#26680;&#24471;&#20197;&#22797;&#33487;&#12290;&#38543;&#26426;&#28216;&#36208;&#26680;(RWK)&#20316;&#20026;&#35768;&#22810;KCNs&#20013;&#30340;&#40664;&#35748;&#26680;&#65292;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#23457;&#35270;&#20102;RWK&#21450;&#20854;&#22312;KCNs&#20013;&#30340;&#29616;&#26377;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#35774;&#35745;&#30340;&#20960;&#20010;&#19981;&#36275;&#20043;&#22788;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22270;&#26680;RWK+&#65292;&#36890;&#36807;&#24341;&#20837;&#39068;&#33394;&#21305;&#37197;&#38543;&#26426;&#28216;&#36208;&#24182;&#25512;&#23548;&#20854;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RWK+CN&#65292;&#19968;&#20010;&#20351;&#29992;RWK+&#20316;&#20026;&#26680;&#24515;&#26680;&#20989;&#25968;&#30340;KCN&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30446;&#26631;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#22270;&#29305;&#24449;&#65292;&#36825;&#26159;GNNs&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23637;&#24320;RWK+&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#19982;&#24120;&#35268;GCN&#23618;&#23384;&#22312;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph kernels used to be the dominant approach to feature engineering for structured data, which are superseded by modern GNNs as the former lacks learnability. Recently, a suite of Kernel Convolution Networks (KCNs) successfully revitalized graph kernels by introducing learnability, which convolves input with learnable hidden graphs using a certain graph kernel. The random walk kernel (RWK) has been used as the default kernel in many KCNs, gaining increasing attention. In this paper, we first revisit the RWK and its current usage in KCNs, revealing several shortcomings of the existing designs, and propose an improved graph kernel RWK+, by introducing color-matching random walks and deriving its efficient computation. We then propose RWK+CN, a KCN that uses RWK+ as the core kernel to learn descriptive graph features with an unsupervised objective, which can not be achieved by GNNs. Further, by unrolling RWK+, we discover its connection with a regular GCN layer, and propose a novel GNN 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SubGen&#30340;&#39640;&#25928;&#32531;&#23384;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Attention&#27169;&#22359;&#20013;&#36827;&#34892;&#22312;&#32447;&#32858;&#31867;&#21644;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#23376;&#32447;&#24615;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#35823;&#24046;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.06082</link><description>&lt;p&gt;
SubGen&#65306;&#23376;&#32447;&#24615;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20196;&#29260;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SubGen: Token Generation in Sublinear Time and Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06082
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SubGen&#30340;&#39640;&#25928;&#32531;&#23384;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Attention&#27169;&#22359;&#20013;&#36827;&#34892;&#22312;&#32447;&#32858;&#31867;&#21644;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#23376;&#32447;&#24615;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24191;&#27867;&#30340;&#20869;&#23384;&#38656;&#27714;&#20351;&#24471;&#22312;&#38271;&#19978;&#19979;&#25991;&#20196;&#29260;&#29983;&#25104;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#23384;&#22312;&#25361;&#25112;&#12290;LLM&#35299;&#30721;&#22120;&#30340;&#24040;&#22823;&#20869;&#23384;&#21344;&#29992;&#37327;&#26469;&#33258;&#20110;&#22312;&#27880;&#24847;&#27169;&#22359;&#20013;&#23384;&#20648;&#25152;&#26377;&#20808;&#21069;&#20196;&#29260;&#30340;&#24517;&#35201;&#24615;&#65292;&#36825;&#26159;&#30001;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#25152;&#24378;&#21046;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#24320;&#21457;&#19968;&#31181;&#39640;&#25928;&#30340;&#38190;&#20540;&#32531;&#23384;&#21387;&#32553;&#25216;&#26415;&#12290;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;&#38190;&#23884;&#20837;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#32858;&#31867;&#20542;&#21521;&#12290;&#22522;&#20110;&#36825;&#19968;&#20851;&#38190;&#27934;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#23376;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#32531;&#23384;&#26041;&#27861;&#65292;&#37319;&#29992;&#38190;&#20196;&#29260;&#30340;&#22312;&#32447;&#32858;&#31867;&#21644;&#20540;&#30340;&#22312;&#32447;l2&#37319;&#26679;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#21487;&#20197;&#35777;&#26126;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35299;&#30721;&#31639;&#27861;&#65292;&#31216;&#20026;SubGen&#12290;&#36825;&#20010;&#31639;&#27861;&#19981;&#20165;&#30830;&#20445;&#20102;&#23376;&#32447;&#24615;&#20869;&#23384;&#21344;&#29992;&#21644;&#23376;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#36824;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#35823;&#24046;&#30028;&#12290;&#32463;&#39564;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation. The substantial memory footprint of LLM decoders arises from the necessity to store all previous tokens in the attention module, a requirement imposed by key-value (KV) caching. In this work, our focus is on developing an efficient compression technique for the KV cache. Empirical evidence indicates a significant clustering tendency within key embeddings in the attention module. Building on this key insight, we have devised a novel caching method with sublinear complexity, employing online clustering on key tokens and online $\ell_2$ sampling on values. The result is a provably accurate and efficient attention decoding algorithm, termed SubGen. Not only does this algorithm ensure a sublinear memory footprint and sublinear time complexity, but we also establish a tight error bound for our approach. Empirical evaluations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;DNA&#24207;&#21015;&#65292;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#29289;&#31181;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#20135;&#29983;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.06079</link><description>&lt;p&gt;
DiscDiff: DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiscDiff: Latent Diffusion Model for DNA Sequence Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;DNA&#24207;&#21015;&#65292;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#29289;&#31181;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#20135;&#29983;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNA&#24207;&#21015;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;DiscDiff&#65292;&#19968;&#31181;&#20026;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#32780;&#23450;&#21046;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#65292;&#20197;&#21450;Absorb-Escape&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#36825;&#20123;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;Absorb-Escape&#36890;&#36807;&#32416;&#27491;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#36716;&#25442;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#8220;&#33293;&#20837;&#35823;&#24046;&#8221;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24207;&#21015;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#32780;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;EPD-GenDNA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;15&#20010;&#29289;&#31181;&#30340;&#12289;&#32508;&#21512;&#24615;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;160,000&#20010;&#29420;&#29305;&#24207;&#21015;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#33021;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#21487;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel framework for DNA sequence generation, comprising two key components: DiscDiff, a Latent Diffusion Model (LDM) tailored for generating discrete DNA sequences, and Absorb-Escape, a post-training algorithm designed to refine these sequences. Absorb-Escape enhances the realism of the generated sequences by correcting `round errors' inherent in the conversion process between latent and input spaces. Our approach not only sets new standards in DNA sequence generation but also demonstrates superior performance over existing diffusion models, in generating both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the first comprehensive, multi-species dataset for DNA generation, encompassing 160,000 unique sequences from 15 species. We hope this study will advance the generative modelling of DNA, with potential implications for gene therapy and protein production.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#20915;&#31574;&#25903;&#25345;&#19979;&#30340;&#25112;&#20105;&#28216;&#25103;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35268;&#27169;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#25552;&#39640;&#20840;&#22495;&#24847;&#35782;&#12289;&#25913;&#21892;&#20915;&#31574;&#36895;&#24230;&#21644;&#36136;&#37327;&#12289;&#25552;&#20379;&#21019;&#26032;&#34892;&#21160;&#24314;&#35758;&#20197;&#21450;&#26356;&#24555;&#36895;&#22320;&#24212;&#23545;&#23545;&#25163;&#34892;&#21160;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#29616;&#20195;&#25361;&#25112;&#21644;&#22256;&#22659;&#65292;&#22686;&#24378;&#20154;&#31867;&#20915;&#31574;&#30340;&#25351;&#23548;&#21644;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.06075</link><description>&lt;p&gt;
&#23454;&#29616;&#25968;&#23383;&#21270;&#20915;&#31574;&#25903;&#25345;&#19979;&#30340;&#35268;&#27169;&#21270;&#20154;&#24037;&#26234;&#33021;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06075
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20915;&#31574;&#25903;&#25345;&#19979;&#30340;&#25112;&#20105;&#28216;&#25103;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35268;&#27169;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#25552;&#39640;&#20840;&#22495;&#24847;&#35782;&#12289;&#25913;&#21892;&#20915;&#31574;&#36895;&#24230;&#21644;&#36136;&#37327;&#12289;&#25552;&#20379;&#21019;&#26032;&#34892;&#21160;&#24314;&#35758;&#20197;&#21450;&#26356;&#24555;&#36895;&#22320;&#24212;&#23545;&#23545;&#25163;&#34892;&#21160;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#29616;&#20195;&#25361;&#25112;&#21644;&#22256;&#22659;&#65292;&#22686;&#24378;&#20154;&#31867;&#20915;&#31574;&#30340;&#25351;&#23548;&#21644;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#30001;&#25216;&#26415;&#39537;&#21160;&#30340;&#21464;&#38761;&#26102;&#20195;&#65292;&#25105;&#20204;&#26356;&#38656;&#35201;&#31215;&#26497;&#25237;&#36164;&#20110;&#24320;&#21457;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#25903;&#25345;&#20915;&#31574;&#30340;&#25112;&#20105;&#28216;&#25103;&#12290;&#36890;&#36807;&#25512;&#36827;AI&#25216;&#26415;&#31995;&#32479;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#33021;&#22815;&#25552;&#39640;&#20840;&#22495;&#24847;&#35782;&#65292;&#25913;&#21892;&#20915;&#31574;&#21608;&#26399;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#65292;&#25552;&#20379;&#21019;&#26032;&#34892;&#21160;&#30340;&#24314;&#35758;&#65292;&#26356;&#36805;&#36895;&#22320;&#24212;&#23545;&#23545;&#25163;&#30340;&#34892;&#21160;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#21152;&#24555;AI&#30340;&#24320;&#21457;&#65292;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#24212;&#23545;&#29616;&#20195;&#25361;&#25112;&#21644;&#22256;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#25361;&#25112;&#30446;&#21069;&#38656;&#35201;&#20154;&#31867;&#26234;&#33021;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#35797;&#22270;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;-&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#32780;&#26159;&#20197;&#26426;&#22120;&#36895;&#24230;&#22686;&#24378;&#21644;&#26356;&#22909;&#22320;&#25351;&#23548;&#20154;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this unprecedented era of technology-driven transformation, it becomes more critical than ever that we aggressively invest in developing robust artificial intelligence (AI) for wargaming in support of decision-making. By advancing AI-enabled systems and pairing these with human judgment, we will be able to enhance all-domain awareness, improve the speed and quality of our decision cycles, offer recommendations for novel courses of action, and more rapidly counter our adversary's actions. It therefore becomes imperative that we accelerate the development of AI to help us better address the complexity of modern challenges and dilemmas that currently requires human intelligence and, if possible, attempt to surpass human intelligence--not to replace humans, but to augment and better inform human decision-making at machine speed. Although deep reinforcement learning continues to show promising results in intelligent agent behavior development for the long-horizon, complex tasks typically
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;PRUNe&#30340;&#19977;&#32500;&#20108;&#32500;&#30456;&#20301;&#24674;&#22797;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#22122;&#22768;&#30340;&#24178;&#28041;&#25104;&#20687;&#20013;&#30340;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#30456;&#20301;&#22122;&#22768;&#65292;&#24182;&#22312;&#24674;&#22797;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.06063</link><description>&lt;p&gt;
&#19977;&#32500;&#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22122;&#22768;&#24178;&#28041;&#25104;&#20687;&#20013;&#30340;&#30456;&#20301;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06063
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;PRUNe&#30340;&#19977;&#32500;&#20108;&#32500;&#30456;&#20301;&#24674;&#22797;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#22122;&#22768;&#30340;&#24178;&#28041;&#25104;&#20687;&#20013;&#30340;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#30456;&#20301;&#22122;&#22768;&#65292;&#24182;&#22312;&#24674;&#22797;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#25104;&#20687;&#20013;&#30340;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22312;&#24178;&#28041;&#25104;&#20687;&#30340;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#30456;&#20301;&#22122;&#22768;&#30340;&#22788;&#29702;&#24456;&#23569;&#12290;&#36825;&#31181;&#22122;&#22768;&#22312;&#24178;&#28041;&#20202;&#20013;&#33258;&#28982;&#20135;&#29983;&#65292;&#30001;&#20110;&#26426;&#26800;&#19981;&#31283;&#23450;&#24615;&#25110;&#22823;&#27668;&#28237;&#27969;&#65292;&#38480;&#21046;&#20102;&#27979;&#37327;&#37319;&#38598;&#26102;&#38388;&#65292;&#24182;&#22312;&#20809;&#24378;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#36965;&#24863;&#65289;&#36896;&#25104;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19977;&#32500;&#20108;&#32500;&#30456;&#20301;&#24674;&#22797;U-Net&#65288;PRUNe&#65289;&#65292;&#35813;&#32593;&#32476;&#20197;&#22122;&#22768;&#21644;&#38543;&#26426;&#30456;&#31227;&#30340;&#24178;&#28041;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#20108;&#32500;&#30456;&#20301;&#22270;&#20687;&#12290;&#19968;&#20010;&#19977;&#32500;&#19979;&#37319;&#26679;&#30340;&#21367;&#31215;&#32534;&#30721;&#22120;&#25429;&#25417;&#24103;&#20869;&#21644;&#24103;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20135;&#29983;&#19968;&#20010;&#20108;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388;&#36890;&#36807;&#19968;&#20010;&#20108;&#32500;&#35299;&#30721;&#22120;&#19978;&#37319;&#26679;&#25104;&#19968;&#20010;&#30456;&#20301;&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#26368;&#20808;&#36827;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#21457;&#29616;PRUNe&#20855;&#26377;&#26356;&#39640;&#30340;&#24674;&#22797;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, neural networks have been used to solve phase retrieval problems in imaging with superior accuracy and speed than traditional techniques, especially in the presence of noise. However, in the context of interferometric imaging, phase noise has been largely unaddressed by existing neural network architectures. Such noise arises naturally in an interferometer due to mechanical instabilities or atmospheric turbulence, limiting measurement acquisition times and posing a challenge in scenarios with limited light intensity, such as remote sensing. Here, we introduce a 3D-2D Phase Retrieval U-Net (PRUNe) that takes noisy and randomly phase-shifted interferograms as inputs, and outputs a single 2D phase image. A 3D downsampling convolutional encoder captures correlations within and between frames to produce a 2D latent space, which is upsampled by a 2D decoder into a phase image. We test our model against a state-of-the-art singular value decomposition algorithm and find PRUNe 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#37327;&#34903;&#26223;&#22270;&#20687;&#21644;&#32445;&#32422;&#24066;&#30340;&#20581;&#24247;&#25968;&#25454;&#65292;&#21457;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#22312;&#27809;&#26377;&#32771;&#34385;&#25968;&#25454;&#20581;&#22766;&#24615;&#21644;&#22522;&#20110;&#34394;&#20551;&#30456;&#20851;&#24615;&#26102;&#23384;&#22312;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.06059</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#25968;&#25454;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#20915;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact on Public Health Decision Making by Utilizing Big Data Without Domain Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#37327;&#34903;&#26223;&#22270;&#20687;&#21644;&#32445;&#32422;&#24066;&#30340;&#20581;&#24247;&#25968;&#25454;&#65292;&#21457;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#22312;&#27809;&#26377;&#32771;&#34385;&#25968;&#25454;&#20581;&#22766;&#24615;&#21644;&#22522;&#20110;&#34394;&#20551;&#30456;&#20851;&#24615;&#26102;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#30340;&#25968;&#25454;&#26469;&#28304;&#21644;&#20174;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#27491;&#22312;&#21464;&#24471;&#20016;&#23500;&#22810;&#26679;&#65292;&#24182;&#19988;&#19982;&#35768;&#22810;&#31038;&#20250;&#24212;&#29992;&#30340;&#20915;&#31574;&#30456;&#20851;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;&#34903;&#26223;&#22270;&#20687;&#65292;&#22312;100&#22810;&#20010;&#22269;&#23478;&#21487;&#29992;&#65292;&#24182;&#34987;&#32771;&#34385;&#29992;&#20110;&#35780;&#20272;&#24314;&#31569;&#29615;&#22659;&#19982;&#31038;&#21306;&#20581;&#24247;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#31181;&#20351;&#29992;&#24773;&#22659;&#19979;&#65292;&#24403;&#22522;&#20110;&#25968;&#25454;&#30340;&#20915;&#31574;&#27809;&#26377;&#32771;&#34385;&#21040;&#25968;&#25454;&#30340;&#20581;&#22766;&#24615;&#65292;&#25110;&#32773;&#39044;&#27979;&#22522;&#20110;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#26102;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#37325;&#35201;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#39118;&#38505;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26469;&#33258;&#32445;&#32422;&#24066;&#30340;200.2&#19975;&#20010;&#34903;&#26223;&#22270;&#20687;&#20197;&#21450;&#20581;&#24247;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#31038;&#20250;&#32463;&#27982;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22478;&#24066;&#20869;&#37096;&#30001;&#34903;&#26223;&#22270;&#20687;&#26631;&#31614;&#25512;&#26029;&#20986;&#30340;&#24314;&#31569;&#29615;&#22659;&#29305;&#24449;&#21487;&#33021;&#19982;&#23454;&#38469;&#24773;&#20917;&#19981;&#31526;&#21512;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20010;&#20307;&#32423;&#21035;&#30340;&#20307;&#21147;&#27963;&#21160;&#19981;&#36275;&#34892;&#20026;&#26174;&#33879;&#24433;&#21709;&#20102;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
New data sources, and artificial intelligence (AI) methods to extract information from them are becoming plentiful, and relevant to decision making in many societal applications. An important example is street view imagery, available in over 100 countries, and considered for applications such as assessing built environment aspects in relation to community health outcomes. Relevant to such uses, important examples of bias in the use of AI are evident when decision-making based on data fails to account for the robustness of the data, or predictions are based on spurious correlations. To study this risk, we utilize 2.02 million GSV images along with health, demographic, and socioeconomic data from New York City. Initially, we demonstrate that built environment characteristics inferred from GSV labels at the intra-city level may exhibit inadequate alignment with the ground truth. We also find that the average individual-level behavior of physical inactivity significantly mediates the impac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ActiveDP&#65292;&#19968;&#20010;&#23558;&#20027;&#21160;&#23398;&#20064;&#21644;&#25968;&#25454;&#32534;&#31243;&#30456;&#32467;&#21512;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#30340;&#26631;&#31614;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#26631;&#35760;&#39044;&#31639;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.06056</link><description>&lt;p&gt;
ActiveDP&#65306;&#23558;&#20027;&#21160;&#23398;&#20064;&#21644;&#25968;&#25454;&#32534;&#31243;&#26694;&#26550;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
ActiveDP: Bridging Active Learning and Data Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ActiveDP&#65292;&#19968;&#20010;&#23558;&#20027;&#21160;&#23398;&#20064;&#21644;&#25968;&#25454;&#32534;&#31243;&#30456;&#32467;&#21512;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#30340;&#26631;&#31614;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#26631;&#35760;&#39044;&#31639;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#25163;&#21160;&#26631;&#35760;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#25968;&#25454;&#32534;&#31243;&#33539;&#24335;&#21487;&#20197;&#39640;&#25928;&#22320;&#26631;&#35760;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20294;&#20250;&#20135;&#29983;&#22122;&#22768;&#26631;&#31614;&#65292;&#20174;&#32780;&#38477;&#20302;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32780;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#33719;&#21462;&#20934;&#30830;&#30340;&#26631;&#31614;&#65292;&#20294;&#21482;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ActiveDP&#65292;&#19968;&#20010;&#23558;&#20027;&#21160;&#23398;&#20064;&#21644;&#25968;&#25454;&#32534;&#31243;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#20197;&#29983;&#25104;&#39640;&#31934;&#24230;&#21644;&#35206;&#30422;&#24230;&#20855;&#22791;&#30340;&#26631;&#31614;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#33539;&#24335;&#30340;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ActiveDP&#20248;&#20110;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#26631;&#35760;&#39044;&#31639;&#19979;&#22987;&#32456;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning models require large labelled datasets to achieve good performance, but manually labelling large datasets is expensive and time-consuming. The data programming paradigm enables users to label large datasets efficiently but produces noisy labels, which deteriorates the downstream model's performance. The active learning paradigm, on the other hand, can acquire accurate labels but only for a small fraction of instances. In this paper, we propose ActiveDP, an interactive framework bridging active learning and data programming together to generate labels with both high accuracy and coverage, combining the strengths of both paradigms. Experiments show that ActiveDP outperforms previous weak supervision and active learning approaches and consistently performs well under different labelling budgets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#27169;&#24335;&#20999;&#25442;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20999;&#25442;&#21644;&#36890;&#20449;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36828;&#31243;&#25805;&#20316;&#20013;&#30340;&#38590;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#24847;&#22270;&#24182;&#33258;&#20027;&#25191;&#34892;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#65292;&#20943;&#23569;&#20102;&#23545;&#25805;&#20316;&#32773;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06047</link><description>&lt;p&gt;
&#26234;&#33021;&#27169;&#24335;&#20999;&#25442;&#26694;&#26550;&#29992;&#20110;&#36828;&#31243;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Intelligent Mode-switching Framework for Teleoperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#27169;&#24335;&#20999;&#25442;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#24335;&#20999;&#25442;&#21644;&#36890;&#20449;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36828;&#31243;&#25805;&#20316;&#20013;&#30340;&#38590;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#24847;&#22270;&#24182;&#33258;&#20027;&#25191;&#34892;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#65292;&#20943;&#23569;&#20102;&#23545;&#25805;&#20316;&#32773;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24863;&#30693;&#33021;&#21147;&#26377;&#38480;&#12289;&#36890;&#20449;&#24310;&#36831;&#39640;&#21644;&#25805;&#20316;&#32773;&#20391;&#33258;&#30001;&#24230;&#26377;&#38480;&#65292;&#36828;&#31243;&#25805;&#20316;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#33258;&#20027;&#36828;&#31243;&#25805;&#20316;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#24847;&#22270;&#24182;&#33258;&#20027;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#65292;&#20197;&#20943;&#23569;&#23545;&#25805;&#20316;&#32773;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;&#28982;&#32780;&#65292;&#27169;&#24335;&#20999;&#25442;&#30340;&#20915;&#31574;&#36890;&#24120;&#20551;&#35774;&#30001;&#25805;&#20316;&#32773;&#23436;&#25104;&#65292;&#36825;&#22686;&#21152;&#20102;&#25805;&#20316;&#32773;&#35201;&#25511;&#21046;&#30340;&#33258;&#30001;&#24230;&#65292;&#24182;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#24515;&#29702;&#36127;&#25285;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#24182;&#26410;&#30740;&#31350;&#36890;&#20449;&#30340;&#35282;&#24230;&#65292;&#23613;&#31649;&#36890;&#20449;&#19981;&#23436;&#32654;&#21644;&#36164;&#28304;&#38480;&#21046;&#26159;&#36828;&#31243;&#25805;&#20316;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#27169;&#24335;&#20999;&#25442;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#27169;&#24335;&#20999;&#25442;&#21644;&#36890;&#20449;&#31995;&#32479;&#12290;&#29992;&#25143;&#24847;&#22270;&#35782;&#21035;&#22312;&#25805;&#20316;&#32773;&#19968;&#20391;&#23436;&#25104;&#12290;&#22522;&#20110;&#29992;&#25143;&#24847;&#22270;&#35782;&#21035;&#65292;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34987;&#29992;&#20110;&#20915;&#23450;&#20309;&#26102;&#20999;&#25442;&#21040;&#33258;&#20027;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teleoperation can be very difficult due to limited perception, high communication latency, and limited degrees of freedom (DoFs) at the operator side. Autonomous teleoperation is proposed to overcome this difficulty by predicting user intentions and performing some parts of the task autonomously to decrease the demand on the operator and increase the task completion rate. However, decision-making for mode-switching is generally assumed to be done by the operator, which brings an extra DoF to be controlled by the operator and introduces extra mental demand. On the other hand, the communication perspective is not investigated in the current literature, although communication imperfections and resource limitations are the main bottlenecks for teleoperation. In this study, we propose an intelligent mode-switching framework by jointly considering mode-switching and communication systems. User intention recognition is done at the operator side. Based on user intention recognition, a deep rei
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#30340;&#30452;&#25509;&#37319;&#38598;&#20248;&#21270;&#31639;&#27861;&#65288;DAO&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24635;&#20307;&#35823;&#24046;&#20943;&#23569;&#65292;&#25928;&#26524;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06045</link><description>&lt;p&gt;
&#38024;&#23545;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#30340;&#30452;&#25509;&#37319;&#38598;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Acquisition Optimization for Low-Budget Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#30340;&#30452;&#25509;&#37319;&#38598;&#20248;&#21270;&#31639;&#27861;&#65288;DAO&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24635;&#20307;&#35823;&#24046;&#20943;&#23569;&#65292;&#25928;&#26524;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#22312;&#23558;&#25968;&#25454;&#23494;&#38598;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#38598;&#25104;&#21040;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#24403;&#26631;&#35760;&#39044;&#31639;&#36739;&#20302;&#26102;&#20854;&#25928;&#26524;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#20102;&#29616;&#26377;&#20302;&#39044;&#31639;&#35774;&#32622;&#19979;AL&#31639;&#27861;&#30340;&#24615;&#33021;&#36864;&#21270;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;AL&#31639;&#27861;&#8212;&#8212;&#30452;&#25509;&#37319;&#38598;&#20248;&#21270;&#65288;DAO&#65289;&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#26399;&#26395;&#30495;&#23454;&#25439;&#22833;&#20943;&#23569;&#26469;&#20248;&#21270;&#26679;&#26412;&#36873;&#25321;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DAO&#21033;&#29992;&#24433;&#21709;&#20989;&#25968;&#26469;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#32467;&#21512;&#39069;&#22806;&#30340;&#37319;&#38598;&#31574;&#30053;&#26469;&#20943;&#36731;&#25439;&#22833;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24635;&#20307;&#35823;&#24046;&#20943;&#23569;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#35745;&#31639;&#25110;&#20381;&#36182;&#20110;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DAO&#22312;&#20302;&#39044;&#31639;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Learning (AL) has gained prominence in integrating data-intensive machine learning (ML) models into domains with limited labeled data. However, its effectiveness diminishes significantly when the labeling budget is low. In this paper, we first empirically observe the performance degradation of existing AL algorithms in the low-budget settings, and then introduce Direct Acquisition Optimization (DAO), a novel AL algorithm that optimizes sample selections based on expected true loss reduction. Specifically, DAO utilizes influence functions to update model parameters and incorporates an additional acquisition strategy to mitigate bias in loss estimation. This approach facilitates a more accurate estimation of the overall error reduction, without extensive computations or reliance on labeled data. Experiments demonstrate DAO's effectiveness in low budget settings, outperforming state-of-the-arts approaches across seven benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#20808;&#39564;&#27491;&#26080;&#26631;&#23398;&#20064;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#27987;&#24230;&#29305;&#24615;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#22788;&#29702;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#31867;&#20808;&#39564;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.06038</link><description>&lt;p&gt;
&#20813;&#20808;&#39564;&#27491;&#26080;&#26631;&#65288;Positive Unlabeled&#65289;&#23398;&#20064;&#30340;&#23545;&#27604;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contrastive Approach to Prior Free Positive Unlabeled Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#20808;&#39564;&#27491;&#26080;&#26631;&#23398;&#20064;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#27987;&#24230;&#29305;&#24615;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#22788;&#29702;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#31867;&#20808;&#39564;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#26080;&#26631;&#65288;Positive Unlabeled&#65289;&#23398;&#20064;&#26159;&#25351;&#22312;&#32473;&#23450;&#23569;&#37327;&#26631;&#35760;&#30340;&#27491;&#26679;&#26412;&#21644;&#19968;&#32452;&#26410;&#26631;&#35760;&#26679;&#26412;&#65288;&#21487;&#33021;&#26159;&#27491;&#20363;&#25110;&#36127;&#20363;&#65289;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19968;&#20010;&#20108;&#20998;&#31867;&#22120;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#26080;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20445;&#35777;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#27987;&#24230;&#29305;&#24615;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#22788;&#29702;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26631;&#20934;&#27491;&#26080;&#26631;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36731;&#26494;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27491;&#26080;&#26631;&#23398;&#20064;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#31867;&#20808;&#39564;&#30340;&#20272;&#35745;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#32780;&#22823;&#22810;&#25968;&#27491;&#26080;&#26631;&#23398;&#20064;&#31639;&#27861;&#21017;&#22833;&#36133;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25512;&#21160;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#33324;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive Unlabeled (PU) learning refers to the task of learning a binary classifier given a few labeled positive samples, and a set of unlabeled samples (which could be positive or negative). In this paper, we propose a novel PU learning framework, that starts by learning a feature space through pretext-invariant representation learning and then applies pseudo-labeling to the unlabeled examples, leveraging the concentration property of the embeddings. Overall, our proposed approach handily outperforms state-of-the-art PU learning methods across several standard PU benchmark datasets, while not requiring a-priori knowledge or estimate of class prior. Remarkably, our method remains effective even when labeled data is scant, where most PU learning algorithms falter. We also provide simple theoretical analysis motivating our proposed algorithms and establish generalization guarantee for our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#65288;MPGD&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#39044;&#27979;AI&#12290;&#23454;&#39564;&#35777;&#26126;MPGD&#22312;&#21508;&#31181;&#29289;&#29702;&#35774;&#35745;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.06034</link><description>&lt;p&gt;
&#20351;&#29992;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#39044;&#27979;AI
&lt;/p&gt;
&lt;p&gt;
Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#65288;MPGD&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#39044;&#27979;AI&#12290;&#23454;&#39564;&#35777;&#26126;MPGD&#22312;&#21508;&#31181;&#29289;&#29702;&#35774;&#35745;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29190;&#28856;&#24335;&#30340;&#39044;&#27979;AI&#22312;&#29616;&#20195;&#33455;&#29255;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#32780;&#26377;&#25928;&#30340;&#35780;&#20272;&#21644;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26694;&#26550;&#36890;&#24120;&#21253;&#25324;&#26368;&#23567;&#21270;&#39044;&#27979;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35748;&#20026;MSE&#30340;&#24179;&#22343;&#25928;&#26524;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#21644;&#37096;&#32626;&#20004;&#26041;&#38754;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#33391;&#22909;&#30340;MSE&#34892;&#20026;&#19981;&#33021;&#20445;&#35777;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#33021;&#30001;&#20110;&#23569;&#37327;&#39044;&#27979;&#35823;&#24046;&#32780;&#21463;&#25439;&#30340;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#65288;MPGD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26465;&#30446;&#65292;&#21487;&#33021;&#25552;&#20379;&#26356;&#24555;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#20195;&#34920;&#24615;&#22522;&#20934;&#22871;&#20214;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MPGD&#22312;&#20351;&#29992;CNN&#25110;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#36827;&#34892;&#21508;&#31181;&#29289;&#29702;&#35774;&#35745;&#39044;&#27979;&#20219;&#21153;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploding predictive AI has enabled fast yet effective evaluation and decision-making in modern chip physical design flows. State-of-the-art frameworks typically include the objective of minimizing the mean square error (MSE) between the prediction and the ground truth. We argue the averaging effect of MSE induces limitations in both model training and deployment, and good MSE behavior does not guarantee the capability of these models to assist physical design flows which are likely sabotaged due to a small portion of prediction error. To address this, we propose mini-pixel batch gradient descent (MPGD), a plug-and-play optimization algorithm that takes the most informative entries into consideration, offering probably faster and better convergence. Experiments on representative benchmark suits show the significant benefits of MPGD on various physical design prediction tasks using CNN or Graph-based models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;Halpern&#36845;&#20195;&#31639;&#27861;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#36825;&#20123;&#21464;&#31181;&#23637;&#29616;&#20986;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#24182;&#19988;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#31867;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06033</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;Halpern&#36845;&#20195;&#31639;&#27861;&#21450;&#20854;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Inexact Halpern Iteration for with Application to Distributionally Robust Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;Halpern&#36845;&#20195;&#31639;&#27861;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#36825;&#20123;&#21464;&#31181;&#23637;&#29616;&#20986;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#24182;&#19988;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#31867;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Halpern&#36845;&#20195;&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24418;&#24335;&#21644;&#21560;&#24341;&#20154;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#36817;&#24180;&#26469;&#22312;&#35299;&#20915;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;&#35813;&#26041;&#26696;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#19981;&#31934;&#30830;&#26041;&#26696;&#22312;&#65288;&#26399;&#26395;&#30340;&#65289;&#27531;&#24046;&#33539;&#25968;&#19978;&#20855;&#26377;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25918;&#23485;&#20102;&#25991;&#29486;&#20013;&#37319;&#29992;&#30340;&#26368;&#26032;&#19981;&#31934;&#30830;&#24615;&#26465;&#20214;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#31454;&#20105;&#24615;&#25910;&#25947;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20004;&#31867;&#20855;&#26377;&#20984;&#20985;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#37325;&#26500;&#30340;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20854;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#30340;&#19981;&#31934;&#30830;&#35745;&#31639;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#31639;&#23376;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#36866;&#24212;&#26377;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#26144;&#23556;&#26694;&#26550;&#65292;&#24182;&#21457;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#23398;&#20064;PtO&#26144;&#23556;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#21644;&#20808;&#23398;&#20064;&#35299;&#31639;&#23376;&#20877;&#35745;&#31639;&#21487;&#35266;&#27979;&#20540;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06031</link><description>&lt;p&gt;
&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#26144;&#23556;&#30340;&#31639;&#23376;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An operator learning perspective on parameter-to-observable maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#31639;&#23376;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#36866;&#24212;&#26377;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#26144;&#23556;&#26694;&#26550;&#65292;&#24182;&#21457;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#23398;&#20064;PtO&#26144;&#23556;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#21644;&#20808;&#23398;&#20064;&#35299;&#31639;&#23376;&#20877;&#35745;&#31639;&#21487;&#35266;&#27979;&#20540;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#29289;&#29702;&#27169;&#22411;&#26367;&#20195;&#21697;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#31639;&#23376;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#26377;&#26377;&#38480;&#32500;&#30340;&#27169;&#22411;&#36755;&#20837;&#21442;&#25968;&#21270;&#25110;&#26377;&#38480;&#32500;&#30340;&#27169;&#22411;&#36755;&#20986;&#21487;&#35266;&#27979;&#25968;&#25454;&#21487;&#29992;&#65292;&#32780;&#19981;&#26159;&#20840;&#22330;&#27979;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65292;&#24341;&#20837;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#26144;&#23556;&#65288;Fourier Neural Mappings&#65292;FNMs&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#36866;&#24212;&#36825;&#26679;&#30340;&#26377;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#26412;&#25991;&#20026;&#35813;&#26041;&#27861;&#21457;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#24213;&#23618;&#30340;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#65288;PtO&#65289;&#26144;&#23556;&#26159;&#36890;&#36807;&#26080;&#31351;&#32500;&#31639;&#23376;&#26469;&#38544;&#24335;&#23450;&#20041;&#30340;&#65292;&#20363;&#22914;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#26159;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;PtO&#26144;&#23556;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36824;&#26159;&#39318;&#20808;&#23398;&#20064;&#35299;&#31639;&#23376;&#65292;&#28982;&#21518;&#35745;&#31639;&#21487;&#35266;&#27979;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computationally efficient surrogates for parametrized physical models play a crucial role in science and engineering. Operator learning provides data-driven surrogates that map between function spaces. However, instead of full-field measurements, often the available data are only finite-dimensional parametrizations of model inputs or finite observables of model outputs. Building off of Fourier Neural Operators, this paper introduces the Fourier Neural Mappings (FNMs) framework that is able to accommodate such finite-dimensional inputs and outputs. The paper develops universal approximation theorems for the method. Moreover, in many applications the underlying parameter-to-observable (PtO) map is defined implicitly through an infinite-dimensional operator, such as the solution operator of a partial differential equation. A natural question is whether it is more data-efficient to learn the PtO map end-to-end or first learn the solution operator and subsequently compute the observable fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20540;&#27861;&#30340;&#12289;&#38750;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#19982;&#20854;&#20182;&#27969;&#34892;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#22312;&#35782;&#21035;&#21453;&#20107;&#23454;&#35299;&#37322;&#26102;&#38656;&#35201;&#26356;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#22235;&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.06030</link><description>&lt;p&gt;
&#21338;&#24328;&#35770;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic Counterfactual Explanation for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20540;&#27861;&#30340;&#12289;&#38750;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#19982;&#20854;&#20182;&#27969;&#34892;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#22312;&#35782;&#21035;&#21453;&#20107;&#23454;&#35299;&#37322;&#26102;&#38656;&#35201;&#26356;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#22235;&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#29992;&#25143;&#26469;&#35828;&#20173;&#28982;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#20351;&#24471;&#29702;&#35299;&#20854;&#39044;&#27979;&#32972;&#21518;&#30340;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFE&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#35745;&#31639;GNNs&#30340;CFE&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#39069;&#22806;&#30340;&#22270;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#20540;&#30340;&#12289;&#38750;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;CFE&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#39069;&#22806;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#35745;&#31639;Shapley&#20540;&#31561;&#20854;&#20182;&#27969;&#34892;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#38656;&#35201;&#26356;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#26469;&#35782;&#21035;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#19982;Shapley&#20540;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#21487;&#20197;&#23454;&#29616;&#22235;&#20493;&#30340;&#21152;&#36895;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38408;&#20540;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been a powerful tool for node classification tasks in complex networks. However, their decision-making processes remain a black-box to users, making it challenging to understand the reasoning behind their predictions. Counterfactual explanations (CFE) have shown promise in enhancing the interpretability of machine learning models. Prior approaches to compute CFE for GNNS often are learning-based approaches that require training additional graphs. In this paper, we propose a semivalue-based, non-learning approach to generate CFE for node classification tasks, eliminating the need for any additional training. Our results reveals that computing Banzhaf values requires lower sample complexity in identifying the counterfactual explanations compared to other popular methods such as computing Shapley values. Our empirical evidence indicates computing Banzhaf values can achieve up to a fourfold speed up compared to Shapley values. We also design a thresholding
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DT-guided DRL&#65289;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#29702;&#35770;&#21407;&#21017;&#65292;&#23454;&#29616;&#20102;&#23545;DRL&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#21021;&#22987;&#24341;&#23548;&#65292;&#24182;&#20419;&#36827;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#26356;&#39640;&#25928;&#21487;&#38752;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.06023</link><description>&lt;p&gt;
&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24555;&#36895;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decision Theory-Guided Deep Reinforcement Learning for Fast Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06023
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DT-guided DRL&#65289;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#29702;&#35770;&#21407;&#21017;&#65292;&#23454;&#29616;&#20102;&#23545;DRL&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#21021;&#22987;&#24341;&#23548;&#65292;&#24182;&#20419;&#36827;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#26356;&#39640;&#25928;&#21487;&#38752;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DT-guided DRL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#22266;&#26377;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#29702;&#35770;&#21407;&#21017;&#65292;DT-guided DRL&#22686;&#24378;&#20102;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21021;&#22987;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#26356;&#39640;&#25928;&#21487;&#38752;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#32972;&#26223;&#65306;&#26438;&#36710;&#21644;&#36855;&#23467;&#23548;&#33322;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#29702;&#35770;&#30340;&#25972;&#21512;&#19981;&#20165;&#26377;&#21161;&#20110;&#23545;DRL&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#25928;&#30340;&#21021;&#22987;&#24341;&#23548;&#65292;&#36824;&#20419;&#36827;&#20102;&#22312;&#20855;&#26377;&#22823;&#35268;&#27169;&#21644;&#22797;&#26434;&#29366;&#24577;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#26356;&#26377;&#32467;&#26500;&#21644;&#30693;&#24773;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24120;&#35268;&#30340;DRL&#30456;&#27604;&#65292;DT-guided DRL&#33021;&#22815;&#25552;&#20379;&#26174;&#33879;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;DT-guided DRL&#30340;&#32047;&#31215;&#22870;&#21169;&#22686;&#21152;&#20102;184%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach, Decision Theory-guided Deep Reinforcement Learning (DT-guided DRL), to address the inherent cold start problem in DRL. By integrating decision theory principles, DT-guided DRL enhances agents' initial performance and robustness in complex environments, enabling more efficient and reliable convergence during learning. Our investigation encompasses two primary problem contexts: the cart pole and maze navigation challenges. Experimental results demonstrate that the integration of decision theory not only facilitates effective initial guidance for DRL agents but also promotes a more structured and informed exploration strategy, particularly in environments characterized by large and intricate state spaces. The results of experiment demonstrate that DT-guided DRL can provide significantly higher rewards compared to regular DRL. Specifically, during the initial phase of training, the DT-guided DRL yields up to an 184% increase in accumulated reward. Mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#38750;&#20984;&#20248;&#21270;&#36719;&#20214;Gurobi&#35299;&#20915;&#36275;&#22815;&#20998;&#25955;&#26465;&#20214;&#26816;&#39564;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#26816;&#26597;&#12290;</title><link>https://arxiv.org/abs/2402.06019</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#23616;&#38750;&#20984;&#20248;&#21270;&#36719;&#20214;&#26816;&#39564;&#36275;&#22815;&#20998;&#25955;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Checking the Sufficiently Scattered Condition using a Global Non-Convex Optimization Software
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#38750;&#20984;&#20248;&#21270;&#36719;&#20214;Gurobi&#35299;&#20915;&#36275;&#22815;&#20998;&#25955;&#26465;&#20214;&#26816;&#39564;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36275;&#22815;&#20998;&#25955;&#26465;&#20214;&#65288;SSC&#65289;&#26159;&#30740;&#31350;&#21508;&#31181;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#21487;&#36776;&#35782;&#24615;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#21253;&#25324;&#38750;&#36127;&#12289;&#26368;&#23567;&#20307;&#31215;&#12289;&#23545;&#31216;&#12289;&#21333;&#32431;&#32467;&#26500;&#21644;&#22810;&#38754;&#20307;&#30697;&#38453;&#20998;&#35299;&#12290;&#36275;&#22815;&#20998;&#25955;&#26465;&#20214;&#21487;&#20197;&#30830;&#20445;&#35745;&#31639;&#24471;&#21040;&#30340;&#30697;&#38453;&#20998;&#35299;&#26159;&#21807;&#19968;&#21487;&#36776;&#35782;&#30340;&#65292;&#38500;&#20102;&#24179;&#20961;&#30340;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#26465;&#20214;&#26159;NP&#38590;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22312;&#30697;&#38453;&#30340;&#31209;&#19981;&#22826;&#22823;&#26102;&#65292;&#23427;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#36827;&#34892;&#26816;&#26597;&#65292;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#38750;&#20984;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#27714;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20840;&#23616;&#38750;&#20984;&#20248;&#21270;&#36719;&#20214;Gurobi&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#23637;&#31034;&#20102;&#35813;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sufficiently scattered condition (SSC) is a key condition in the study of identifiability of various matrix factorization problems, including nonnegative, minimum-volume, symmetric, simplex-structured, and polytopic matrix factorizations. The SSC allows one to guarantee that the computed matrix factorization is unique/identifiable, up to trivial ambiguities. However, this condition is NP-hard to check in general. In this paper, we show that it can however be checked in a reasonable amount of time in realistic scenarios, when the factorization rank is not too large. This is achieved by formulating the problem as a non-convex quadratic optimization problem over a bounded set. We use the global non-convex optimization software Gurobi, and showcase the usefulness of this code on synthetic data sets and on real-world hyperspectral images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#38750;&#24182;&#34892;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;(NPSVCs)&#30340;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#25552;&#20986;&#20102;NPSVC++&#65292;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;NPSVC++&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#20102;NPSVC&#21450;&#20854;&#29305;&#24449;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#36861;&#27714;&#24085;&#32047;&#25176;&#26368;&#20248;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29305;&#24449;&#27425;&#20248;&#21644;&#31867;&#21035;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06010</link><description>&lt;p&gt;
NPSVC++: &#38750;&#24182;&#34892;&#20998;&#31867;&#22120;&#36935;&#21040;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NPSVC++: Nonparallel Classifiers Encounter Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#38750;&#24182;&#34892;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;(NPSVCs)&#30340;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#25552;&#20986;&#20102;NPSVC++&#65292;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;NPSVC++&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#20102;NPSVC&#21450;&#20854;&#29305;&#24449;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#36861;&#27714;&#24085;&#32047;&#25176;&#26368;&#20248;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29305;&#24449;&#27425;&#20248;&#21644;&#31867;&#21035;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#19968;&#31181;&#29305;&#23450;&#30340;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#38750;&#24182;&#34892;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;(NPSVCs)&#12290;&#19982;&#20856;&#22411;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;NPSVC&#30340;&#35757;&#32451;&#28041;&#21450;&#22810;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#65292;&#23548;&#33268;&#29305;&#24449;&#27425;&#20248;&#21644;&#31867;&#21035;&#20381;&#36182;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23578;&#26410;&#24314;&#31435;&#26377;&#25928;&#30340;&#23398;&#20064;&#26041;&#26696;&#26469;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#26469;&#25913;&#21892;NPSVC&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#31361;&#30772;&#36825;&#19968;&#29942;&#39048;&#65292;&#25105;&#20204;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#24320;&#21457;&#20102;NPSVC++&#65292;&#23454;&#29616;&#20102;NPSVC&#21450;&#20854;&#29305;&#24449;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#36890;&#36807;&#36861;&#27714;&#24085;&#32047;&#25176;&#26368;&#20248;&#65292;NPSVC++&#22312;&#29702;&#35770;&#19978;&#30830;&#20445;&#20102;&#36328;&#31867;&#21035;&#30340;&#29305;&#24449;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#20248;&#21270;&#30340;&#36890;&#29992;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#24212;&#29992;&#30340;&#23454;&#20363;&#65292;K-NPSVC++&#21644;D-NPSVC++&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;NPSVC++&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on a specific family of classifiers called nonparallel support vector classifiers (NPSVCs). Different from typical classifiers, the training of an NPSVC involves the minimization of multiple objectives, resulting in the potential concerns of feature suboptimality and class dependency. Consequently, no effective learning scheme has been established to improve NPSVCs' performance through representation learning, especially deep learning. To break this bottleneck, we develop NPSVC++ based on multi-objective optimization, enabling the end-to-end learning of NPSVC and its features. By pursuing Pareto optimality, NPSVC++ theoretically ensures feature optimality across classes, hence effectively overcoming the two issues above. A general learning procedure via duality optimization is proposed, based on which we provide two applicable instances, K-NPSVC++ and D-NPSVC++. The experiments show their superiority over the existing methods and verify the efficacy of NPSVC++.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21435;&#38500;&#29615;&#24418;&#20266;&#24433;&#26469;&#22686;&#24378;X&#23556;&#32447;&#24494;CT&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05983</link><description>&lt;p&gt;
X&#23556;&#32447;&#24494;CT&#31995;&#32479;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26041;&#27861;&#30340;&#33021;&#21147;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Capability enhancement of the X-ray micro-tomography system via ML-assisted approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21435;&#38500;&#29615;&#24418;&#20266;&#24433;&#26469;&#22686;&#24378;X&#23556;&#32447;&#24494;CT&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
X&#23556;&#32447;&#24494;CT&#22270;&#20687;&#20013;&#30340;&#29615;&#24418;&#20266;&#24433;&#26159;&#20934;&#30830;&#35270;&#35273;&#35299;&#37322;&#21644;&#23450;&#37327;&#20998;&#26512;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;X&#23556;&#32447;&#24494;CT&#25195;&#25551;&#20202;&#30340;&#20960;&#20309;&#32467;&#26500;&#31867;&#20284;&#20110;&#21307;&#29992;CT&#26426;&#22120;&#65292;&#21482;&#26159;&#26679;&#26412;&#20197;&#22266;&#23450;&#30340;&#28304;&#21644;&#25506;&#27979;&#22120;&#26059;&#36716;&#12290;&#29615;&#24418;&#20266;&#24433;&#26159;&#30001;MicroCT&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#25506;&#27979;&#22120;&#20687;&#32032;&#30340;&#32570;&#38519;&#25110;&#38750;&#32447;&#24615;&#21709;&#24212;&#24341;&#36215;&#30340;&#12290;MicroCT&#22270;&#20687;&#20013;&#30340;&#20266;&#24433;&#32463;&#24120;&#20005;&#37325;&#21040;&#26080;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#20266;&#24433;&#30340;&#21407;&#22240;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#20197;&#26368;&#22823;&#21270;&#22270;&#20687;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#28789;&#24863;&#26469;&#33258;UNet&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31995;&#21015;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21333;&#20803;&#65292;&#29992;&#20110;&#21435;&#38500;&#29615;&#24418;&#20266;&#24433;&#12290;&#35813;&#25552;&#20986;&#30340;&#26550;&#26500;&#21033;&#29992;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#65288;SSIM&#65289;&#21644;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
Ring artifacts in X-ray micro-CT images are one of the primary causes of concern in their accurate visual interpretation and quantitative analysis. The geometry of X-ray micro-CT scanners is similar to the medical CT machines, except the sample is rotated with a stationary source and detector. The ring artifacts are caused by a defect or non-linear responses in detector pixels during the MicroCT data acquisition. Artifacts in MicroCT images can often be so severe that the images are no longer useful for further analysis. Therefore, it is essential to comprehend the causes of artifacts and potential solutions to maximize image quality. This article presents a convolution neural network (CNN)-based Deep Learning (DL) model inspired by UNet with a series of encoder and decoder units with skip connections for removal of ring artifacts. The proposed architecture has been evaluated using the Structural Similarity Index Measure (SSIM) and Mean Squared Error (MSE). Additionally, the results ar
&lt;/p&gt;</description></item><item><title>Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences.</title><link>https://arxiv.org/abs/2402.05982</link><description>&lt;p&gt;
Anfinsen Goes Neural: &#19968;&#31181;&#29992;&#20110;&#26465;&#20214;&#25239;&#20307;&#35774;&#35745;&#30340;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05982
&lt;/p&gt;
&lt;p&gt;
Anfinsen Goes Neural (AGN) is a graphical model for conditional antibody design that combines a pre-trained protein language model with a graph neural network. It outperforms existing methods and addresses the limitation of generating unrealistic sequences.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#22312;&#25512;&#21160;&#27835;&#30103;&#23398;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23545;&#19968;&#33324;&#34507;&#30333;&#36136;&#30693;&#35782;&#30340;&#21033;&#29992;&#26377;&#38480;&#65292;&#24182;&#20551;&#35774;&#22270;&#27169;&#22411;&#36829;&#21453;&#34507;&#30333;&#36136;&#30340;&#32463;&#39564;&#21457;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Anfinsen Goes Neural (AGN)&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;(pLM)&#24182;&#32534;&#30721;&#20102;&#19968;&#31181;&#20851;&#20110;&#34507;&#30333;&#36136;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#21363;Anfinsen's dogma&#30340;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36981;&#24490;&#24207;&#21015;&#29983;&#25104;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#32467;&#26500;&#39044;&#27979;&#30340;&#20004;&#27493;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#23454;&#39564;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#36807;&#22810;&#37325;&#22797;&#26631;&#35760;&#30340;&#19981;&#29616;&#23454;&#24207;&#21015;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#32452;&#21512;&#30340;&#27491;&#21017;&#21270;&#39033;&#21040;&#20132;&#21449;&#29109;&#30446;&#26631;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody design plays a pivotal role in advancing therapeutics. Although deep learning has made rapid progress in this field, existing methods make limited use of general protein knowledge and assume a graphical model (GM) that violates empirical findings on proteins. To address these limitations, we present Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained protein language model (pLM) and encodes a seminal finding on proteins called Anfinsen's dogma. Our framework follows a two-step process of sequence generation with pLM and structure prediction with graph neural network (GNN). Experiments show that our approach outperforms state-of-the-art results on benchmark experiments. We also address a critical limitation of non-autoregressive models -- namely, that they tend to generate unrealistic sequences with overly repeating tokens. To resolve this, we introduce a composition-based regularization term to the cross-entropy objective that allows an efficient trade-off be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65292;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05981</link><description>&lt;p&gt;
&#25506;&#32034;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of In-Browser Deep Learning Inference on Quality of User Experience and Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#65292;&#25506;&#32034;&#20102;&#27983;&#35272;&#22120;&#20869;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65292;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#36890;&#36807;&#8220;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#8221;&#36825;&#31181;&#26041;&#27861;&#25972;&#21512;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20854;&#20013;DL&#22788;&#29702;&#30452;&#25509;&#22312;Web&#27983;&#35272;&#22120;&#20013;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21450;&#20854;&#23545;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#30340;&#24433;&#21709;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#36825;&#31181;&#30693;&#35782;&#30340;&#31354;&#30333;&#38656;&#35201;&#26032;&#24418;&#24335;&#30340;QoE&#27979;&#37327;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#25351;&#26631;&#65292;&#22914;&#39029;&#38754;&#21152;&#36733;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#30340;&#39318;&#27425;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#65306;&#21709;&#24212;&#24615;&#65292;&#27969;&#30021;&#24230;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#30740;&#31350;&#21253;&#25324;9&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#22312;50&#20010;&#24120;&#29992;&#30340;PC Web&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27983;&#35272;&#22120;&#20869;&#25512;&#29702;&#23384;&#22312;&#20005;&#37325;&#30340;&#24310;&#36831;&#38382;&#39064;&#65306;&#22312;CPU&#19978;&#24179;&#22343;&#27604;&#21407;&#29983;&#25512;&#29702;&#26041;&#27861;&#24930;16.9&#20493;&#65292;&#22312;GPU&#19978;&#24930;4.9&#20493;&#12290;&#36825;&#31181;&#24310;&#36831;&#26377;&#20960;&#20010;&#22240;&#32032;&#23548;&#33268;&#65292;&#21253;&#25324;&#26410;&#20805;&#20998;&#20351;&#29992;&#30340;&#30828;&#20214;&#25351;&#20196;&#38598;&#65292;&#22266;&#26377;&#30340;&#24310;&#36831;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is increasingly being integrated into Web applications through a method known as "in-browser inference", where the DL processes occur directly within Web browsers. However, the actual performance of this method and its effect on user experience quality (QoE) is not well-understood. This gap in knowledge necessitates new forms of QoE measurement, going beyond traditional metrics such as page load time. To address this, we conducted the first extensive performance evaluation of in-browser inference. We introduced new metrics for this purpose: responsiveness, smoothness, and inference accuracy.   Our thorough study included 9 widely-used DL models and tested them across 50 popular PC Web browsers. The findings show a significant latency issue with in-browser inference: it's on average 16.9 times slower on CPU and 4.9 times slower on GPU than native inference methods. Several factors contribute to this latency, including underused hardware instruction sets, inherent dela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.05980</link><description>&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#65311;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Large Code Models Understand Programming Concepts? A Black-box Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#25104;&#21151;&#20063;&#20351;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#24037;&#20316;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#21644;&#32534;&#36753;&#31561;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20026;&#20160;&#20040;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#24213;&#23618;&#31243;&#24207;&#30340;&#36923;&#36753;&#32467;&#26500;&#29702;&#35299;&#31243;&#24230;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#32534;&#31243;&#27010;&#24565;&#35859;&#35789;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#65288;CACP&#65289;&#20316;&#20026;&#19968;&#31181;&#21453;&#20107;&#23454;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#12290;&#21482;&#36890;&#36807;&#40657;&#30418;&#35775;&#38382;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;CACP&#35780;&#20272;&#20102;&#21313;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#20010;&#19981;&#21516;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models' success on text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#36718;&#24275;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#38115;&#21066;&#36807;&#31243;&#20013;&#25554;&#20837;&#29289;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#23558;&#20004;&#20010;&#25551;&#36848;&#31526;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05978</link><description>&lt;p&gt;
&#32467;&#21512;&#24418;&#29366;&#21644;&#36718;&#24275;&#29305;&#24449;&#26469;&#25552;&#39640;&#38115;&#21066;&#36807;&#31243;&#20013;&#30340;&#20992;&#20855;&#30952;&#25439;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining shape and contour features to improve tool wear monitoring in milling processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#36718;&#24275;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#38115;&#21066;&#36807;&#31243;&#20013;&#25554;&#20837;&#29289;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#23558;&#20004;&#20010;&#25551;&#36848;&#31526;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#36718;&#24275;&#25551;&#36848;&#31526;&#32452;&#21512;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#26681;&#25454;&#30952;&#25439;&#31243;&#24230;&#23545;&#38115;&#21066;&#36807;&#31243;&#20013;&#30340;&#25554;&#20837;&#29289;&#36827;&#34892;&#20998;&#31867;&#65292;&#37319;&#29992;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25551;&#36848;&#30952;&#25439;&#21306;&#22495;&#30340;&#24418;&#29366;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25551;&#36848;&#31526;ShapeFeat&#65292;&#24182;&#20351;&#29992;BORCHIZ&#26041;&#27861;&#23545;&#20854;&#36718;&#24275;&#36827;&#34892;&#34920;&#24449;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35843;&#26597;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20992;&#20855;&#30952;&#25439;&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#23558;BORCHIZ&#21644;ShapeFeat&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20108;&#20803;&#20998;&#31867;&#23558;&#30952;&#25439;&#20998;&#20026;&#39640;&#25110;&#20302;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;91.44%&#65292;&#19977;&#20010;&#30446;&#26631;&#31867;&#21035;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#30952;&#25439;&#65289;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;82.90%&#12290;&#36825;&#20123;&#32467;&#26524;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#20004;&#20010;&#25551;&#36848;&#31526;&#30340;&#32467;&#26524;&#65292;&#20854;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;88.70%&#21644;80.67%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new system based on combinations of a shape descriptor and a contour descriptor has been proposed for classifying inserts in milling processes according to their wear level following a computer vision based approach. To describe the wear region shape we have proposed a new descriptor called ShapeFeat and its contour has been characterized using the method BORCHIZ that, to the best of our knowledge, achieves the best performance for tool wear monitoring following a computer vision-based approach. Results show that the combination of BORCHIZ with ShapeFeat using a late fusion method improves the classification performance significantly, obtaining an accuracy of 91.44% in the binary classification (i.e. the classification of the wear as high or low) and 82.90% using three target classes (i.e. classification of the wear as high, medium or low). These results outperform the ones obtained by both descriptors used on their own, which achieve accuracies of 88.70 and 80.67% for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22312;&#32447;&#12289;&#20302;&#25104;&#26412;&#21644;&#24555;&#36895;&#26041;&#27861;&#65292;&#29992;&#20110;&#20999;&#21066;&#24037;&#20855;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#36890;&#36807;&#23558;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#30340;&#32441;&#29702;&#25551;&#36848;&#31526;&#26469;&#21028;&#26029;&#27599;&#20010;&#21306;&#22495;&#30340;&#30952;&#25439;&#31243;&#24230;&#65292;&#20174;&#32780;&#30830;&#23450;&#20999;&#21066;&#36793;&#32536;&#21644;&#20992;&#20855;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#12290;</title><link>https://arxiv.org/abs/2402.05977</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#32441;&#29702;&#30340;&#22312;&#32447;&#12289;&#33258;&#21160;&#21644;&#20302;&#25104;&#26412;&#31995;&#32479;&#36827;&#34892;&#20992;&#20855;&#30952;&#25439;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tool wear monitoring using an online, automatic and low cost system based on local texture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22312;&#32447;&#12289;&#20302;&#25104;&#26412;&#21644;&#24555;&#36895;&#26041;&#27861;&#65292;&#29992;&#20110;&#20999;&#21066;&#24037;&#20855;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#36890;&#36807;&#23558;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#30340;&#32441;&#29702;&#25551;&#36848;&#31526;&#26469;&#21028;&#26029;&#27599;&#20010;&#21306;&#22495;&#30340;&#30952;&#25439;&#31243;&#24230;&#65292;&#20174;&#32780;&#30830;&#23450;&#20999;&#21066;&#36793;&#32536;&#21644;&#20992;&#20855;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30340;&#22312;&#32447;&#12289;&#20302;&#25104;&#26412;&#21644;&#24555;&#36895;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#36793;&#32536;&#36718;&#24275;&#38115;&#21066;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#20999;&#21066;&#24037;&#20855;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#30952;&#25439;&#31243;&#24230;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;254&#24352;&#36793;&#32536;&#36718;&#24275;&#20999;&#21066;&#22836;&#22270;&#20687;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#26681;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#19988;&#20855;&#26377;&#36275;&#22815;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#26377;&#20992;&#29255;&#37117;&#34987;&#20998;&#21106;&#65292;&#24182;&#19988;&#20854;&#20999;&#21066;&#36793;&#32536;&#34987;&#35009;&#21098;&#65292;&#33719;&#24471;&#20102;577&#24352;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#65306;301&#24352;&#21487;&#29992;&#21644;276&#24352;&#21487;&#20002;&#24323;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#65288;1&#65289;&#23558;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#20998;&#20026;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#31216;&#20026;&#30952;&#25439;&#26001;&#22359;&#65288;WP&#65289;&#65292;&#65288;2&#65289;&#20351;&#29992;&#22522;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#65288;LBP&#65289;&#30340;&#32441;&#29702;&#25551;&#36848;&#31526;&#26469;&#34920;&#24449;&#27599;&#20010;&#21306;&#22495;&#26159;&#30952;&#25439;&#36824;&#26159;&#21487;&#29992;&#65292;&#24182;&#65288;3&#65289;&#26681;&#25454;&#36825;&#20123;WP&#30340;&#29366;&#24577;&#26469;&#30830;&#23450;&#20999;&#21066;&#36793;&#32536;&#65288;&#22240;&#27492;&#20063;&#26159;&#20992;&#20855;&#65289;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#26001;&#22359;&#20998;&#21106;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose a new online, low cost and fast approach based on computer vision and machine learning to determine whether cutting tools used in edge profile milling processes are serviceable or disposable based on their wear level. We created a new dataset of 254 images of edge profile cutting heads which is, to the best of our knowledge, the first publicly available dataset with enough quality for this purpose. All the inserts were segmented and their cutting edges were cropped, obtaining 577 images of cutting edges: 301 functional and 276 disposable. The proposed method is based on (1) dividing the cutting edge image in different regions, called Wear Patches (WP), (2) characterising each one as worn or serviceable using texture descriptors based on different variants of Local Binary Patterns (LBP) and (3) determine, based on the state of these WP, if the cutting edge (and, therefore, the tool) is serviceable or disposable. We proposed and assessed five different patch divis
&lt;/p&gt;</description></item><item><title>RankSum&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#32500;&#24230;&#21477;&#23376;&#29305;&#24449;&#23545;&#21477;&#23376;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#34701;&#21512;&#30830;&#23450;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#25490;&#21517;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30417;&#30563;&#20449;&#21495;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.05976</link><description>&lt;p&gt;
RankSum&#65306;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#34701;&#21512;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RankSum An unsupervised extractive text summarization based on rank fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05976
&lt;/p&gt;
&lt;p&gt;
RankSum&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#32500;&#24230;&#21477;&#23376;&#29305;&#24449;&#23545;&#21477;&#23376;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#34701;&#21512;&#30830;&#23450;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#25490;&#21517;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30417;&#30563;&#20449;&#21495;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ranksum&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25490;&#21517;&#34701;&#21512;&#30340;&#26080;&#30417;&#30563;&#21333;&#25991;&#26723;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20026;&#27599;&#20010;&#21477;&#23376;&#25552;&#21462;&#30340;&#22235;&#20010;&#22810;&#32500;&#24230;&#21477;&#23376;&#29305;&#24449;&#36827;&#34892;&#21477;&#23376;&#26174;&#33879;&#24615;&#25490;&#21517;&#65306;&#20027;&#39064;&#20449;&#24687;&#12289;&#35821;&#20041;&#20869;&#23481;&#12289;&#37325;&#35201;&#20851;&#38190;&#35789;&#21644;&#20301;&#32622;&#12290;Ranksum&#26681;&#25454;&#27599;&#20010;&#29305;&#24449;&#29983;&#25104;&#30340;&#21477;&#23376;&#26174;&#33879;&#24615;&#25490;&#21517;&#36827;&#34892;&#21152;&#26435;&#34701;&#21512;&#65292;&#20197;&#30830;&#23450;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#25490;&#21517;&#12290;&#34701;&#21512;&#26435;&#37325;&#26159;&#23436;&#20840;&#26080;&#30417;&#30563;&#29983;&#25104;&#30340;&#65292;&#38656;&#35201;&#26631;&#35760;&#30340;&#25991;&#26723;&#38598;&#21512;&#26469;&#23398;&#20064;&#34701;&#21512;&#26435;&#37325;&#12290;&#25105;&#20204;&#21457;&#29616;&#34701;&#21512;&#26435;&#37325;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23558;Ranksum&#35270;&#20026;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#23450;&#20027;&#39064;&#25490;&#21517;&#65292;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#26469;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#12290;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#26469;&#29983;&#25104;&#25490;&#21517;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36830;&#20307;&#32593;&#32476;&#20135;&#29983;&#25277;&#35937;&#21270;&#30340;&#21477;&#23376;&#34920;&#31034;&#65292;&#28982;&#21518;&#24418;&#25104;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Ranksum, an approach for extractive text summarization of single documents based on the rank fusion of four multi-dimensional sentence features extracted for each sentence: topic information, semantic content, significant keywords, and position. The Ranksum obtains the sentence saliency rankings corresponding to each feature in an unsupervised way followed by the weighted fusion of the four scores to rank the sentences according to their significance. The scores are generated in completely unsupervised way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an unsupervised approach. To determine topic rank, we employ probabilistic topic models whereas semantic information is captured using sentence embeddings. To derive rankings using sentence embeddings, we utilize Siamese networks to produce abstractive sentence representation and then we form
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#33041;&#32959;&#30244;&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05975</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#31867;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Brain Tumor Classification and Segmentation Using a Multiscale Convolutional Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#33041;&#32959;&#30244;&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#21253;&#25324;&#22810;&#23610;&#24230;&#26041;&#27861;&#22312;&#20869;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30340;&#19968;&#20010;&#21306;&#21035;&#26159;&#36755;&#20837;&#22270;&#20687;&#22312;&#19981;&#21516;&#22788;&#29702;&#36335;&#24452;&#19978;&#20197;&#19977;&#20010;&#31354;&#38388;&#23610;&#24230;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#26426;&#21046;&#26159;&#21463;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#20869;&#22312;&#25805;&#20316;&#30340;&#21551;&#31034;&#12290;&#25552;&#20986;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;&#21253;&#21547;&#19977;&#31181;&#31867;&#22411;&#32959;&#30244;&#65288;&#33041;&#33180;&#30244;&#12289;&#33014;&#36136;&#30244;&#21644;&#22402;&#20307;&#30244;&#65289;&#30340;MRI&#22270;&#20687;&#65292;&#21253;&#25324;&#30690;&#29366;&#38754;&#12289;&#20896;&#29366;&#38754;&#21644;&#36724;&#38754;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#22788;&#29702;&#36755;&#20837;&#22270;&#20687;&#20107;&#20808;&#31227;&#38500;&#22836;&#39592;&#25110;&#26894;&#39592;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;233&#21517;&#24739;&#32773;3064&#24352;&#20999;&#29255;&#30340;MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#20043;&#21069;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#27604;&#36739;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#22320;&#33719;&#24471;&#20102;0.973&#30340;&#32959;&#30244;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#39640;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a fully automatic brain tumor segmentation and classification model using a Deep Convolutional Neural Network that includes a multiscale approach. One of the differences of our proposal with respect to previous works is that input images are processed in three spatial scales along different processing pathways. This mechanism is inspired in the inherent operation of the Human Visual System. The proposed neural model can analyze MRI images containing three types of tumors: meningioma, glioma, and pituitary tumor, over sagittal, coronal, and axial views and does not need preprocessing of input images to remove skull or vertebral column parts in advance. The performance of our method on a publicly available MRI image dataset of 3064 slices from 233 patients is compared with previously classical machine learning and deep learning published methods. In the comparison, our method remarkably obtained a tumor classification accuracy of 0.973, higher than the other app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#35843;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#23558;&#26080;&#20154;&#26426;&#32593;&#32476;&#21010;&#20998;&#20026;&#32858;&#31751;&#65292;&#24182;&#30001;&#32858;&#31751;&#22836;&#26080;&#20154;&#26426;&#36827;&#34892;&#21327;&#35843;&#65292;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05973</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#25903;&#25345;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#22312;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) Framework in UAV Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#35843;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#23558;&#26080;&#20154;&#26426;&#32593;&#32476;&#21010;&#20998;&#20026;&#32858;&#31751;&#65292;&#24182;&#30001;&#32858;&#31751;&#22836;&#26080;&#20154;&#26426;&#36827;&#34892;&#21327;&#35843;&#65292;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#38752;&#24615;&#26159;&#26080;&#20154;&#26426;&#32593;&#32476;&#20316;&#20026;&#20998;&#24067;&#24335;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#20132;&#25442;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26102;&#12290;&#26368;&#36817;&#65292;&#22312;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25913;&#21892;&#20102;&#21327;&#20316;&#12289;&#38544;&#31169;&#12289;&#38887;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25104;&#20026;&#26080;&#20154;&#26426;&#24212;&#29992;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20026;&#26080;&#20154;&#26426;&#32593;&#32476;&#23454;&#29616;FL&#24341;&#20837;&#20102;&#36890;&#20449;&#24320;&#38144;&#12289;&#21516;&#27493;&#38382;&#39064;&#12289;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#21644;&#36164;&#28304;&#32422;&#26463;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#23558;&#26080;&#20154;&#26426;&#32593;&#32476;&#21010;&#20998;&#20026;&#20998;&#31163;&#30340;&#32858;&#31751;&#65292;&#24182;&#30001;&#32858;&#31751;&#22836;&#26080;&#20154;&#26426;&#65288;CHs&#65289;&#36827;&#34892;&#21327;&#35843;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#36830;&#36890;&#22270;&#12290;&#32858;&#31751;&#21270;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#35843;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy, scalability, and reliability are significant challenges in unmanned aerial vehicle (UAV) networks as distributed systems, especially when employing machine learning (ML) technologies with substantial data exchange. Recently, the application of federated learning (FL) to UAV networks has improved collaboration, privacy, resilience, and adaptability, making it a promising framework for UAV applications. However, implementing FL for UAV networks introduces drawbacks such as communication overhead, synchronization issues, scalability limitations, and resource constraints. To address these challenges, this paper presents the Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) framework for UAV networks. This improves the decentralization, coordination, scalability, and efficiency of FL in large-scale UAV networks. The framework partitions UAV networks into separate clusters, coordinated by cluster head UAVs (CHs), to establish a connected graph. Clustering enables
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#20849;&#25391;&#35889;&#20013;&#23450;&#20301;&#20363;&#22806;&#28857;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;GPR&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#20123;&#21021;&#22987;&#30340;&#29305;&#24449;&#20540;&#23545;&#36827;&#34892;&#26681;&#25628;&#32034;&#65292;&#26469;&#23545;&#20363;&#22806;&#28857;&#30340;&#20301;&#32622;&#36827;&#34892;&#21021;&#27493;&#20272;&#35745;&#12290;&#28982;&#21518;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#30830;&#20999;&#30340;&#29305;&#24449;&#20540;&#23545;&#26469;&#25913;&#36827;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#20302;&#32500;&#30697;&#38453;&#27169;&#22411;&#21644;&#30495;&#23454;&#29289;&#29702;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05972</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#22797;&#20849;&#25391;&#35889;&#20013;&#20363;&#22806;&#28857;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gaussian-process-regression-based method for the localization of exceptional points in complex resonance spectra
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#20849;&#25391;&#35889;&#20013;&#23450;&#20301;&#20363;&#22806;&#28857;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;GPR&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#20123;&#21021;&#22987;&#30340;&#29305;&#24449;&#20540;&#23545;&#36827;&#34892;&#26681;&#25628;&#32034;&#65292;&#26469;&#23545;&#20363;&#22806;&#28857;&#30340;&#20301;&#32622;&#36827;&#34892;&#21021;&#27493;&#20272;&#35745;&#12290;&#28982;&#21518;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#30830;&#20999;&#30340;&#29305;&#24449;&#20540;&#23545;&#26469;&#25913;&#36827;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#20302;&#32500;&#30697;&#38453;&#27169;&#22411;&#21644;&#30495;&#23454;&#29289;&#29702;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#20110;&#33267;&#23569;&#20004;&#20010;&#21487;&#25511;&#21442;&#25968;&#30340;&#24320;&#25918;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#20849;&#25391;&#21487;&#20197;&#26174;&#31034;&#20986;&#20363;&#22806;&#28857;&#65288;EP&#65289;&#29616;&#35937;&#65292;&#20854;&#20013;&#20004;&#20010;&#25110;&#22810;&#20010;&#20849;&#25391;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#21516;&#26102;&#23384;&#22312;&#12290;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#20934;&#30830;&#23450;&#20301;&#36825;&#20123;&#28857;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#37327;&#23376;&#35889;&#21644;&#20849;&#25391;&#30340;&#25968;&#20540;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#31995;&#32479;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#30340;&#39640;&#25928;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23547;&#25214;&#20363;&#22806;&#28857;&#30340;&#26041;&#27861;&#12290;GPR&#27169;&#22411;&#22522;&#20110;&#19968;&#20010;&#21021;&#22987;&#30340;&#23646;&#20110;EP&#30340;&#29305;&#24449;&#20540;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#36739;&#20415;&#23452;&#30340;&#26681;&#25628;&#32034;&#36827;&#34892;EP&#20301;&#32622;&#30340;&#21021;&#27493;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#36873;&#25321;&#30340;&#30830;&#20999;&#29305;&#24449;&#20540;&#23545;&#28155;&#21152;&#20026;&#35757;&#32451;&#28857;&#26469;&#36845;&#20195;&#25913;&#36827;&#20272;&#35745;&#12290;&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20302;&#32500;&#30697;&#38453;&#27169;&#22411;&#24320;&#21457;&#21644;&#27979;&#35797;&#20102;GPR&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#29289;&#29702;&#31995;&#32479;&#65292;&#21363;&#23450;&#20301;EP&#12290;
&lt;/p&gt;
&lt;p&gt;
Resonances in open quantum systems depending on at least two controllable parameters can show the phenomenon of exceptional points (EPs), where not only the eigenvalues but also the eigenvectors of two or more resonances coalesce. Their exact localization in the parameter space is challenging, in particular in systems, where the computation of the quantum spectra and resonances is numerically very expensive. We introduce an efficient machine learning algorithm to find exceptional points based on Gaussian process regression (GPR). The GPR-model is trained with an initial set of eigenvalue pairs belonging to an EP and used for a first estimation of the EP position via a numerically cheap root search. The estimate is then improved iteratively by adding selected exact eigenvalue pairs as training points to the GPR-model. The GPR-based method is developed and tested on a simple low-dimensional matrix model and then applied to a challenging real physical system, viz., the localization of EPs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#21270;&#23398;&#21453;&#24212;&#25910;&#29575;&#39044;&#27979;&#12290;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#65292;&#20934;&#30830;&#30340;&#39640;&#25910;&#29575;&#39044;&#27979;&#23545;&#20110;&#21270;&#23398;&#23478;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#23548;&#33268;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.05971</link><description>&lt;p&gt;
&#25105;&#20204;&#21462;&#24471;&#20102;&#22810;&#23569;&#36827;&#23637;&#65311;&#20174;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#21270;&#23398;&#21453;&#24212;&#25910;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Are we making much progress? Revisiting chemical reaction yield prediction from an imbalanced regression perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#21270;&#23398;&#21453;&#24212;&#25910;&#29575;&#39044;&#27979;&#12290;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#65292;&#20934;&#30830;&#30340;&#39640;&#25910;&#29575;&#39044;&#27979;&#23545;&#20110;&#21270;&#23398;&#23478;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#23548;&#33268;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21453;&#24212;&#30340;&#25910;&#29575;&#26159;&#25351;&#30446;&#26631;&#20135;&#29289;&#24418;&#25104;&#30340;&#30334;&#20998;&#27604;&#19982;&#21270;&#23398;&#21453;&#24212;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#21453;&#24212;&#29289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20934;&#30830;&#30340;&#25910;&#29575;&#39044;&#27979;&#21487;&#20197;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#25351;&#23548;&#21270;&#23398;&#23478;&#36873;&#25321;&#39640;&#25910;&#29575;&#21453;&#24212;&#65292;&#22312;&#25237;&#20837;&#26102;&#38388;&#21644;&#36164;&#28304;&#36827;&#34892;&#28287;&#23454;&#39564;&#20043;&#21069;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#25972;&#20307;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#25552;&#39640;&#39640;&#25910;&#29575;&#21453;&#24212;&#30340;&#39044;&#27979;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#21270;&#23398;&#23478;&#26469;&#35828;&#26356;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#35748;&#20026;&#39640;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#26159;&#30001;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#25152;&#33268;&#65292;&#36825;&#20123;&#25968;&#25454;&#20559;&#21521;&#20110;&#20302;&#25910;&#29575;&#21453;&#24212;&#65292;&#36890;&#24120;&#26159;&#30001;&#20110;&#26410;&#21453;&#24212;&#30340;&#36215;&#22987;&#29289;&#36136;&#21644;&#21453;&#24212;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#29616;&#26377;&#30340;&#25910;&#29575;&#39044;&#27979;&#26041;&#27861;&#32487;&#32493;&#23558;&#19981;&#21516;&#25910;&#29575;&#33539;&#22260;&#35270;&#20026;&#24179;&#34913;&#30340;&#35757;&#32451;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The yield of a chemical reaction quantifies the percentage of the target product formed in relation to the reactants consumed during the chemical reaction. Accurate yield prediction can guide chemists toward selecting high-yield reactions during synthesis planning, offering valuable insights before dedicating time and resources to wet lab experiments. While recent advancements in yield prediction have led to overall performance improvement across the entire yield range, an open challenge remains in enhancing predictions for high-yield reactions, which are of greater concern to chemists. In this paper, we argue that the performance gap in high-yield predictions results from the imbalanced distribution of real-world data skewed towards low-yield reactions, often due to unreacted starting materials and inherent ambiguities in the reaction processes. Despite this data imbalance, existing yield prediction methods continue to treat different yield ranges equally, assuming a balanced training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31163;&#25955;&#23398;&#20064;&#21644;&#19987;&#23478;&#32423;&#21035;&#24314;&#27169;&#31354;&#26102;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#30340;&#19987;&#23478;&#27169;&#22359;&#21644;&#31934;&#32454;&#35774;&#35745;&#30340;&#29289;&#29702;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#20272;&#35745;&#31354;&#26102;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05970</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31163;&#25955;&#23398;&#20064;&#21644;&#19987;&#23478;&#32423;&#21035;&#24314;&#27169;&#31354;&#26102;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31163;&#25955;&#23398;&#20064;&#21644;&#19987;&#23478;&#32423;&#21035;&#24314;&#27169;&#31354;&#26102;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#30340;&#19987;&#23478;&#27169;&#22359;&#21644;&#31934;&#32454;&#35774;&#35745;&#30340;&#29289;&#29702;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#20272;&#35745;&#31354;&#26102;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#19968;&#31995;&#21015;&#35266;&#27979;&#65288;&#22914;&#35270;&#39057;&#24103;&#65289;&#30340;&#31354;&#26102;&#21160;&#24577;&#31995;&#32479;&#20013;&#29366;&#24577;&#21464;&#21270;&#30340;&#24314;&#27169;&#21644;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20256;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#31995;&#32479;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21021;&#22987;&#35774;&#32622;&#21644;&#26500;&#24314;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#27491;&#30830;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27169;&#22411;&#30340;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;&#22855;&#24322;&#22330;&#26223;&#21644;&#32570;&#20047;&#23616;&#37096;&#27934;&#23519;&#21147;&#30340;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#26356;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#19987;&#23478;&#27169;&#22359;&#8212;&#8212;&#20809;&#27969;&#20272;&#35745;&#32452;&#20214;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#25429;&#25417;&#19968;&#33324;&#29289;&#29702;&#36807;&#31243;&#30340;&#28436;&#21270;&#35268;&#24459;&#12290;&#20026;&#20102;&#22686;&#24378;&#23616;&#37096;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#29289;&#29702;&#27969;&#27700;&#32447;&#65292;&#22240;&#20026;&#23616;&#37096;&#29305;&#24449;&#21487;&#33021;&#21463;&#21040;&#21508;&#31181;&#20869;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#19982;&#23439;&#35266;&#23646;&#24615;&#30456;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the issue of modeling and estimating changes in the state of the spatio-temporal dynamical systems based on a sequence of observations like video frames. Traditional numerical simulation systems depend largely on the initial settings and correctness of the constructed partial differential equations (PDEs). Despite recent efforts yielding significant success in discovering data-driven PDEs with neural networks, the limitations posed by singular scenarios and the absence of local insights prevent them from performing effectively in a broader real-world context. To this end, this paper propose the universal expert module -- that is, optical flow estimation component, to capture the evolution laws of general physical processes in a data-driven fashion. To enhance local insight, we painstakingly design a finer-grained physical pipeline, since local characteristics may be influenced by various internal contextual information, which may contradict the macroscopic pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;Transformer&#26102;&#65292;&#21024;&#38500;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#21518;&#65292;&#36755;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#27531;&#24046;&#36830;&#25509;&#23545;Transformer&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.05969</link><description>&lt;p&gt;
&#25171;&#30772;&#35757;&#32451;Transformer&#26102;&#30340;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Breaking Symmetry When Training Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05969
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;Transformer&#26102;&#65292;&#21024;&#38500;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#21518;&#65292;&#36755;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#27531;&#24046;&#36830;&#25509;&#23545;Transformer&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#26550;&#26500;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;1, 2, ..., n-1&#30340;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20004;&#31181;&#26426;&#21046;&#37117;&#20250;&#34987;&#20351;&#29992;&#65292;&#20197;&#25171;&#30772;&#23545;&#36755;&#20837;&#31526;&#21495;&#30340;&#23545;&#31216;&#24615;&#12290;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;Transformer&#12290;&#36825;&#24517;&#39035;&#36890;&#36807;&#22240;&#26524;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#24517;&#39035;&#26159;&#20351;Transformer&#33021;&#22815;&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#21407;&#22240;&#12290;Transformer&#30340;&#22402;&#30452;&#8220;&#20999;&#29255;&#8221;&#37117;&#34987;&#40723;&#21169;&#34920;&#31034;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#30456;&#21516;&#20301;&#32622;k&#12290;&#25105;&#20204;&#20551;&#35774;&#27531;&#24046;&#36830;&#25509;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#36215;&#21040;&#20102;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we show in this paper, the prediction for output token $n+1$ of Transformer architectures without one of the mechanisms of positional encodings and causal attention is invariant to permutations of input tokens $1, 2, ..., n-1$. Usually, both mechanisms are employed and the symmetry with respect to the input tokens is broken. Recently, it has been shown that one can train Transformers without positional encodings. This must be enabled by the causal attention mechanism. In this paper, we elaborate on the argument that the causal connection mechanism must be responsible for the fact that Transformers are able to model input sequences where the order is important. Vertical "slices" of Transformers are all encouraged to represent the same location $k$ in the input sequence. We hypothesize that residual connections contribute to this phenomenon, and demonstrate evidence for this.
&lt;/p&gt;</description></item><item><title>&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#21487;&#33021;&#25512;&#21160;&#32852;&#37030;&#23398;&#20064;&#26397;&#20027;&#27969;&#37319;&#29992;&#26041;&#21521;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#31561;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05968</link><description>&lt;p&gt;
&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#20248;&#20808;&#20107;&#39033;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Priorities Under the European Union Artificial Intelligence Act
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05968
&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#21487;&#33021;&#25512;&#21160;&#32852;&#37030;&#23398;&#20064;&#26397;&#20027;&#27969;&#37319;&#29992;&#26041;&#21521;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#31561;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30417;&#31649;&#26102;&#20195;&#24050;&#32463;&#26469;&#20020;&#65292;&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#65288;AI Act&#65289;&#24341;&#39046;&#30528;&#28526;&#27969;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#36825;&#23558;&#22914;&#20309;&#24433;&#21709;&#20197;&#25968;&#25454;&#38544;&#31169;&#20026;&#20248;&#20808;&#24182;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#20854;&#19982;&#38598;&#20013;&#24335;&#23398;&#20064;&#30340;&#20986;&#21457;&#28857;&#26681;&#26412;&#19981;&#21516;&#12290;&#25105;&#20204;&#30456;&#20449;AI&#27861;&#26696;&#21644;&#26410;&#26469;&#30340;&#30417;&#31649;&#21487;&#33021;&#26159;&#25512;&#21160;FL&#36208;&#21521;&#20027;&#27969;&#37319;&#29992;&#30340;&#32570;&#22833;&#20652;&#21270;&#21058;&#12290;&#28982;&#32780;&#65292;&#36825;&#21482;&#33021;&#21457;&#29983;&#22312;FL&#31038;&#21306;&#37325;&#26032;&#20248;&#20808;&#32771;&#34385;&#20854;&#30740;&#31350;&#37325;&#28857;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#25105;&#20204;&#30340;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#36328;&#23398;&#31185;&#20998;&#26512;&#65288;&#27861;&#24459;&#21644;&#26426;&#22120;&#23398;&#20064;&#65289;&#65292;&#20998;&#26512;&#20102;AI&#27861;&#26696;&#23545;FL&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#25105;&#20204;&#20027;&#35201;&#35266;&#28857;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25968;&#25454;&#27835;&#29702;&#38382;&#39064;&#21644;&#23545;&#38544;&#31169;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#29983;&#21629;&#21608;&#26399;&#30417;&#35270;&#20013;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#12290;&#32508;&#21512;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;FL&#26377;&#30528;&#24040;&#22823;&#30340;&#26426;&#20250;&#65292;
&lt;/p&gt;
&lt;p&gt;
The age of AI regulation is upon us, with the European Union Artificial Intelligence Act (AI Act) leading the way. Our key inquiry is how this will affect Federated Learning (FL), whose starting point of prioritizing data privacy while performing ML fundamentally differs from that of centralized learning. We believe the AI Act and future regulations could be the missing catalyst that pushes FL toward mainstream adoption. However, this can only occur if the FL community reprioritizes its research focus. In our position paper, we perform a first-of-its-kind interdisciplinary analysis (legal and ML) of the impact the AI Act may have on FL and make a series of observations supporting our primary position through quantitative and qualitative analysis. We explore data governance issues and the concern for privacy. We establish new challenges regarding performance and energy efficiency within lifecycle monitoring. Taken together, our analysis suggests there is a sizable opportunity for FL to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.05966</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#21644;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Model Re-Basin and the Linear Mode Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22823;&#37096;&#20998;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#35299;&#21487;&#20197;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22522;&#24213;&#65292;&#21482;&#26159;&#39034;&#24207;&#21487;&#33021;&#19981;&#21516;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#38454;&#27573;&#65292;&#23545;&#20110;&#27169;&#22411;&#24179;&#22343;&#21270;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#26032;&#22522;&#24213;&#31574;&#30053;&#22312;&#25928;&#26524;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#19981;&#22815;&#20840;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#20934;&#20570;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#39057;&#32321;&#19981;&#36275;&#20043;&#22788;&#65292;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#30452;&#25509;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21305;&#37197;&#31639;&#27861;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35266;&#28857;&#19981;&#20165;&#28548;&#28165;&#21644;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36824;&#20419;&#36827;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;&#20363;&#22914;&#65292;&#23427;&#23558;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;&#19982;&#21098;&#26525;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#65292;&#21487;&#20197;&#30452;&#25509;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21098;&#26525;&#25216;&#26415;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29699;&#38754;&#25968;&#25454;&#30340;&#28151;&#21512;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29699;&#24418;&#29305;&#24449;&#26684;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#39640;&#24230;&#38750;&#32447;&#24615;&#20449;&#21495;&#30340;&#22797;&#26434;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.05965</link><description>&lt;p&gt;
&#29699;&#38754;&#25968;&#25454;&#30340;&#28151;&#21512;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hybrid Neural Representations for Spherical Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29699;&#38754;&#25968;&#25454;&#30340;&#28151;&#21512;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29699;&#24418;&#29305;&#24449;&#26684;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#39640;&#24230;&#38750;&#32447;&#24615;&#20449;&#21495;&#30340;&#22797;&#26434;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29699;&#38754;&#25968;&#25454;&#30340;&#28151;&#21512;&#31070;&#32463;&#34920;&#31034;&#65292;&#36825;&#26159;&#31185;&#23398;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#22825;&#27668;&#21644;&#27668;&#20505;&#25968;&#25454;&#20197;&#21450;&#23431;&#23449;&#24494;&#27874;&#32972;&#26223;&#65288;CMB&#65289;&#25968;&#25454;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;&#29699;&#24418;&#20449;&#21495;&#31070;&#32463;&#34920;&#31034;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#39640;&#24230;&#38750;&#32447;&#24615;&#20449;&#21495;&#30340;&#22797;&#26434;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#31070;&#32463;&#34920;&#31034;&#29699;&#38754;&#25968;&#25454;&#65288;HNeR-S&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#29699;&#24418;&#29305;&#24449;&#26684;&#26469;&#33719;&#21462;&#20301;&#32622;&#29305;&#24449;&#65292;&#28982;&#21518;&#19982;&#22810;&#23618;&#24863;&#30693;&#22120;&#32467;&#21512;&#26469;&#39044;&#27979;&#30446;&#26631;&#20449;&#21495;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19982;&#22825;&#27668;&#25968;&#25454;&#21644;CMB&#25968;&#25454;&#23545;&#40784;&#30340;&#31561;&#36317;&#20687;&#32032;&#21270;&#32467;&#26500;&#30340;&#31561;&#30697;&#24418;&#21644;&#31561;&#38754;&#31215;&#38548;&#31561;&#32428;&#24230;&#29305;&#24449;&#26684;&#12290;&#25105;&#20204;&#24191;&#27867;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;HNeR-S&#22312;&#22238;&#24402;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#26102;&#38388;&#25554;&#20540;&#21644;&#21387;&#32553;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study hybrid neural representations for spherical data, a domain of increasing relevance in scientific research. In particular, our work focuses on weather and climate data as well as comic microwave background (CMB) data. Although previous studies have delved into coordinate-based neural representations for spherical signals, they often fail to capture the intricate details of highly nonlinear signals. To address this limitation, we introduce a novel approach named Hybrid Neural Representations for Spherical data (HNeR-S). Our main idea is to use spherical feature-grids to obtain positional features which are combined with a multilayer perception to predict the target signal. We consider feature-grids with equirectangular and hierarchical equal area isolatitude pixelization structures that align with weather data and CMB data, respectively. We extensively verify the effectiveness of our HNeR-S for regression, super-resolution, temporal interpolation, and compression 
&lt;/p&gt;</description></item><item><title>&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05964</link><description>&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformer Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05964
&lt;/p&gt;
&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#26159;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#37492;&#20110;Transformer&#30340;&#29420;&#29305;&#26550;&#26500;&#65292;&#20855;&#26377;&#20132;&#26367;&#30340;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#27169;&#22359;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#12290;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#30340;&#25928;&#29575;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#35843;&#30740;&#25552;&#20379;&#20102;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#29420;&#29305;&#26679;&#26412;&#24182;&#28155;&#21152;&#21040;&#22238;&#25918;&#32531;&#20914;&#22120;&#20013;&#20197;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#25511;&#21046;&#31574;&#30053;&#21512;&#25104;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05963</link><description>&lt;p&gt;
&#33410;&#20461;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#27169;&#22411;&#65306;&#20351;&#29992;&#29420;&#29305;&#32463;&#21382;&#30340;&#39640;&#25928;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05963
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#29420;&#29305;&#26679;&#26412;&#24182;&#28155;&#21152;&#21040;&#22238;&#25918;&#32531;&#20914;&#22120;&#20013;&#20197;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#25511;&#21046;&#31574;&#30053;&#21512;&#25104;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29992;&#20110;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#25511;&#21046;&#31574;&#30053;&#21512;&#25104;&#20013;&#65292;&#23545;&#22238;&#25918;&#32531;&#20914;&#22120;&#30340;&#39640;&#25928;&#21033;&#29992;&#22312;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#36873;&#25321;&#29420;&#29305;&#26679;&#26412;&#24182;&#23558;&#20854;&#28155;&#21152;&#21040;&#22238;&#25918;&#32531;&#20914;&#22120;&#20013;&#65292;&#26088;&#22312;&#20943;&#23567;&#32531;&#20914;&#22120;&#30340;&#22823;&#23567;&#24182;&#20445;&#25345;&#26679;&#26412;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22312;&#38543;&#26426;&#25506;&#32034;&#30340;&#21021;&#22987;&#38454;&#27573;&#36935;&#21040;&#30340;&#32463;&#21382;&#20013;&#36873;&#25321;&#19968;&#32452;&#37325;&#35201;&#30340;&#29366;&#24577;&#21464;&#37327;&#30340;&#37325;&#35201;&#23376;&#38598;&#65292;&#26681;&#25454;&#25152;&#36873;&#37325;&#35201;&#29366;&#24577;&#21464;&#37327;&#23558;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#20026;&#19968;&#32452;&#25277;&#35937;&#29366;&#24577;&#65292;&#26368;&#21518;&#36890;&#36807;&#20351;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#36873;&#25321;&#20855;&#26377;&#29420;&#29305;&#29366;&#24577;-&#22870;&#21169;&#32452;&#21512;&#30340;&#32463;&#21382;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#23558;&#25152;&#25552;&#20986;&#30340;&#29420;&#29305;&#32463;&#21382;&#26041;&#27861;&#32435;&#20837;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient utilization of the replay buffer plays a significant role in the off-policy actor-critic reinforcement learning (RL) algorithms used for model-free control policy synthesis for complex dynamical systems. We propose a method for achieving sample efficiency, which focuses on selecting unique samples and adding them to the replay buffer during the exploration with the goal of reducing the buffer size and maintaining the independent and identically distributed (IID) nature of the samples. Our method is based on selecting an important subset of the set of state variables from the experiences encountered during the initial phase of random exploration, partitioning the state space into a set of abstract states based on the selected important state variables, and finally selecting the experiences with unique state-reward combination by using a kernel density estimator. We formally prove that the off-policy actor-critic algorithm incorporating the proposed method for unique experience
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;EXGC&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#24179;&#22343;&#22330;&#21464;&#20998;&#36817;&#20284;&#21644;&#26799;&#24230;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#25552;&#39640;&#22270;&#21387;&#32553;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05962</link><description>&lt;p&gt;
EXGC: &#22312;&#22270;&#21387;&#32553;&#20013;&#24179;&#34913;&#25928;&#29575;&#19982;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
EXGC: Bridging Efficiency and Explainability in Graph Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;EXGC&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#24179;&#22343;&#22330;&#21464;&#20998;&#36817;&#20284;&#21644;&#26799;&#24230;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#25552;&#39640;&#22270;&#21387;&#32553;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28023;&#37327;&#25968;&#25454;&#38598;&#65288;&#22914;&#32593;&#32476;&#25968;&#25454;&#65289;&#19978;&#36827;&#34892;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#22270;&#21387;&#32553;&#65288;GCond&#65289;&#26469;&#23558;&#36825;&#20123;&#22823;&#22411;&#30495;&#23454;&#25968;&#25454;&#38598;&#33976;&#39311;&#20026;&#26356;&#31616;&#27905;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#22270;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#21152;&#36895;&#21162;&#21147;&#65292;&#29616;&#26377;&#30340;GCond&#26041;&#27861;&#20027;&#35201;&#22312;&#28023;&#37327;&#32593;&#32476;&#25968;&#25454;&#22270;&#19978;&#38754;&#20020;&#25928;&#29575;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#24403;&#21069;&#33539;&#20363;&#30340;&#20004;&#20010;&#20027;&#35201;&#19981;&#36275;&#20043;&#22788;&#65306;&#65288;1&#65289;&#22823;&#37327;&#21442;&#25968;&#38598;&#30340;&#24182;&#21457;&#26356;&#26032;&#65292;&#65288;2&#65289;&#26126;&#26174;&#30340;&#21442;&#25968;&#20887;&#20313;&#12290;&#20026;&#20102;&#30456;&#24212;&#22320;&#20811;&#26381;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#24179;&#22343;&#22330;&#21464;&#20998;&#36817;&#20284;&#36827;&#34892;&#25910;&#25947;&#21152;&#36895;&#65292;&#28982;&#21518;&#25552;&#20986;&#26799;&#24230;&#20449;&#24687;&#29942;&#39048;&#65288;GDIB&#65289;&#30340;&#30446;&#26631;&#26469;&#20943;&#23569;&#20887;&#20313;&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#20808;&#30340;&#35299;&#37322;&#25216;&#26415;&#65288;&#22914;GNNExplainer&#21644;GSAT&#65289;&#26469;&#23454;&#20363;&#21270;GDIB&#65292;&#25105;&#20204;&#30340;EXGC&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (GCond) has been introduced to distill these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing GCond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (e.g., GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05961</link><description>&lt;p&gt;
&#22522;&#22240;&#24341;&#23548;GFlowNets&#65306;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GFlowNet&#21464;&#20307;&#65292;&#21363;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN)&#65292;&#23427;&#23558;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#38598;&#25104;&#21040;GFlowNet&#20013;&#12290;&#36951;&#20256;&#25628;&#32034;&#26377;&#25928;&#22320;&#24341;&#23548;GFlowNet&#36827;&#20837;&#39640;&#22238;&#25253;&#21306;&#22495;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#36807;&#24230;&#25506;&#32034;&#23548;&#33268;&#30340;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#21644;&#25506;&#32034;&#26377;&#38480;&#21306;&#22495;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#25918;&#35757;&#32451;&#21644;&#26080;&#30417;&#30563;&#26368;&#22823;&#20284;&#28982;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#22522;&#22240;&#24341;&#23548;GFlowNet&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270; (PMO) &#39046;&#22495;&#30340;&#23448;&#26041;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27979;&#35797;&#20013;&#25253;&#21578;&#30340;&#26368;&#20339;&#24471;&#20998;15.185&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;23&#20010;&#20219;&#21153;&#20013;&#30340;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;GFlowNets&#21644;&#36951;&#20256;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and unsupervised maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including reinforcement learning, Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20301;&#39537;&#21160;&#30340;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;PhASER&#65292;&#36890;&#36807;&#30456;&#20301;&#22686;&#24378;&#12289;&#20998;&#31163;&#29305;&#24449;&#32534;&#30721;&#21644;&#29305;&#24449;&#24191;&#25773;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#25968;&#25454;&#30340;&#36890;&#29992;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05960</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20301;&#39537;&#21160;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#36890;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Phase-driven Domain Generalizable Learning for Nonstationary Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20301;&#39537;&#21160;&#30340;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;PhASER&#65292;&#36890;&#36807;&#30456;&#20301;&#22686;&#24378;&#12289;&#20998;&#31163;&#29305;&#24449;&#32534;&#30721;&#21644;&#29305;&#24449;&#24191;&#25773;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#25968;&#25454;&#30340;&#36890;&#29992;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#21644;&#35782;&#21035;&#36830;&#32493;&#24863;&#30693;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#20854;&#32479;&#35745;&#21644;&#35889;&#29305;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#22312;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#19981;&#21516;&#20998;&#24067;&#30340;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38750;&#24179;&#31283;&#32479;&#35745;&#19982;&#30456;&#20301;&#20449;&#24687;&#20869;&#22312;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;PhASER&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#26032;&#39062;&#30340;&#20803;&#32032;&#65306;1&#65289;&#30456;&#20301;&#22686;&#24378;&#65292;&#20351;&#38750;&#24179;&#31283;&#24615;&#22810;&#26679;&#21270;&#21516;&#26102;&#20445;&#30041;&#26377;&#21306;&#21035;&#24615;&#30340;&#35821;&#20041;&#65307;2&#65289;&#23558;&#26102;&#21464;&#24133;&#24230;&#21644;&#30456;&#20301;&#35270;&#20026;&#29420;&#31435;&#27169;&#24577;&#36827;&#34892;&#21333;&#29420;&#29305;&#24449;&#32534;&#30721;&#65307;3&#65289;&#21033;&#29992;&#26032;&#39062;&#30340;&#27531;&#24046;&#36830;&#25509;&#23558;&#30456;&#20301;&#19982;&#29305;&#24449;&#32467;&#21512;&#65292;&#20197;&#24378;&#21270;&#20998;&#24067;&#19981;&#21464;&#24615;&#23398;&#20064;&#30340;&#22266;&#26377;&#27491;&#21017;&#21270;&#20316;&#29992;&#12290;&#36890;&#36807;&#22312;5&#20010;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;
&lt;/p&gt;
&lt;p&gt;
Monitoring and recognizing patterns in continuous sensing data is crucial for many practical applications. These real-world time-series data are often nonstationary, characterized by varying statistical and spectral properties over time. This poses a significant challenge in developing learning models that can effectively generalize across different distributions. In this work, based on our observation that nonstationary statistics are intrinsically linked to the phase information, we propose a time-series learning framework, PhASER. It consists of three novel elements: 1) phase augmentation that diversifies non-stationarity while preserving discriminatory semantics, 2) separate feature encoding by viewing time-varying magnitude and phase as independent modalities, and 3) feature broadcasting by incorporating phase with a novel residual connection for inherent regularization to enhance distribution invariant learning. Upon extensive evaluation on 5 datasets from human activity recognit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#23616;&#37096;&#20256;&#25773;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#22788;&#29702;&#29615;&#22659;&#20449;&#24687;&#32780;&#19981;&#20381;&#36182;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#32467;&#21512;&#25968;&#25454;&#34920;&#31034;&#21644;&#23398;&#20064;&#65292;&#20197;&#23562;&#37325;&#26102;&#31354;&#23616;&#37096;&#24615;&#65292;&#24182;&#19988;&#24403;&#20256;&#25773;&#36895;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#31561;&#25928;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05959</link><description>&lt;p&gt;
&#33258;&#28982;&#21551;&#21457;&#30340;&#23616;&#37096;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Nature-Inspired Local Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#23616;&#37096;&#20256;&#25773;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#22788;&#29702;&#29615;&#22659;&#20449;&#24687;&#32780;&#19981;&#20381;&#36182;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#32467;&#21512;&#25968;&#25454;&#34920;&#31034;&#21644;&#23398;&#20064;&#65292;&#20197;&#23562;&#37325;&#26102;&#31354;&#23616;&#37096;&#24615;&#65292;&#24182;&#19988;&#24403;&#20256;&#25773;&#36895;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#31561;&#25928;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#21253;&#25324;&#26368;&#36817;&#22312;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37117;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#30028;&#20013;&#30340;&#26234;&#33021;&#36807;&#31243;&#24182;&#19981;&#38656;&#35201;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#21482;&#38656;&#36890;&#36807;&#23545;&#29615;&#22659;&#20449;&#24687;&#30340;&#22312;&#32447;&#22788;&#29702;&#21363;&#21487;&#20135;&#29983;&#12290;&#29305;&#21035;&#26159;&#65292;&#33258;&#28982;&#23398;&#20064;&#36807;&#31243;&#20381;&#36182;&#20110;&#25968;&#25454;&#34920;&#31034;&#21644;&#23398;&#20064;&#30456;&#20114;&#20132;&#32455;&#20197;&#23562;&#37325;&#26102;&#31354;&#23616;&#37096;&#24615;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#31181;&#29305;&#24615;&#26469;&#33258;&#20110;&#23545;&#23398;&#20064;&#30340;&#39044;&#31639;&#27861;&#35270;&#35282;&#65292;&#35813;&#35270;&#35282;&#21463;&#21040;&#20102;&#29702;&#35770;&#29289;&#29702;&#23398;&#30456;&#20851;&#30740;&#31350;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20256;&#25773;&#36895;&#24230;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#8220;&#23398;&#20064;&#27861;&#21017;&#8221;&#30340;&#31639;&#27861;&#35299;&#37322;&#65288;&#37319;&#29992;&#21704;&#23494;&#39039;&#26041;&#31243;&#32467;&#26500;&#65289;&#23558;&#24402;&#32467;&#20026;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#36825;&#20026;&#22522;&#20110;&#20840;&#38754;&#22312;&#32447;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#34987;&#25552;&#20986;&#30340;&#26102;&#31354;&#23616;&#37096;&#31639;&#27861;&#21462;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections. On the opposite, intelligent processes in nature arises without the need for such collections, but simply by online processing of the environmental information. In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality. This paper shows that such a feature arises from a pre-algorithmic view of learning that is inspired by related studies in Theoretical Physics. We show that the algorithmic interpretation of the derived "laws of learning", which takes the structure of Hamiltonian equations, reduces to Backpropagation when the speed of propagation goes to infinity. This opens the doors to machine learning studies based on full on-line information processing that are based the replacement of Backpropagation with the proposed spatiotemporal local algori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31359;&#25140;&#35774;&#22791;&#21644;&#21333;&#25668;&#20687;&#22836;&#35270;&#39057;&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#19979;&#30340;&#19978;&#32930;&#27963;&#21160;&#35782;&#21035;&#65292;&#20026;&#22312;&#23454;&#39564;&#23460;&#22806;&#36319;&#36394;&#24739;&#32773;&#27963;&#21160;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#22788;&#29702;&#20020;&#24202;&#30456;&#20851;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#29702;&#24819;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.05958</link><description>&lt;p&gt;
&#31359;&#25140;&#35774;&#22791;&#21644;&#21333;&#25668;&#20687;&#22836;&#35270;&#39057;&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#19979;&#30340;&#19978;&#32930;&#27963;&#21160;&#35782;&#21035;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study on wearables and single-camera video for upper-limb out-of-thelab activity recognition with different deep learning architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#31359;&#25140;&#35774;&#22791;&#21644;&#21333;&#25668;&#20687;&#22836;&#35270;&#39057;&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#19979;&#30340;&#19978;&#32930;&#27963;&#21160;&#35782;&#21035;&#65292;&#20026;&#22312;&#23454;&#39564;&#23460;&#22806;&#36319;&#36394;&#24739;&#32773;&#27963;&#21160;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#22788;&#29702;&#20020;&#24202;&#30456;&#20851;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#29702;&#24819;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#21644;&#30740;&#31350;&#29615;&#22659;&#20013;&#65292;&#24191;&#27867;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#35299;&#20915;&#26041;&#26696;&#21644;&#26368;&#26032;&#30340;&#39640;&#31471;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#26469;&#35780;&#20272;&#20154;&#31867;&#30340;&#20307;&#21147;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22686;&#21152;&#22312;&#23454;&#39564;&#23460;&#22806;&#36319;&#36394;&#24739;&#32773;&#27963;&#21160;&#30340;&#21487;&#34892;&#24615;&#65292;&#38656;&#35201;&#20351;&#29992;&#36739;&#23569;&#30340;&#35774;&#22791;&#36827;&#34892;&#36816;&#21160;&#33719;&#21462;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;IMU&#30340;&#31359;&#25140;&#35774;&#22791;&#21644;&#21333;&#25668;&#20687;&#22836;&#31995;&#32479;&#26159;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36824;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#22788;&#29702;&#20020;&#24202;&#30456;&#20851;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#22240;&#27492;&#30830;&#23450;&#36825;&#20123;&#31995;&#32479;&#30340;&#29702;&#24819;&#36755;&#20837;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of a wide range of computer vision solutions, and more recently high-end Inertial Measurement Units (IMU) have become increasingly popular for assessing human physical activity in clinical and research settings. Nevertheless, to increase the feasibility of patient tracking in out-of-the-lab settings, it is necessary to use a reduced number of devices for movement acquisition. Promising solutions in this context are IMU-based wearables and single camera systems. Additionally, the development of machine learning systems able to recognize and digest clinically relevant data in-the-wild is needed, and therefore determining the ideal input to those is crucial.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35299;&#31354;&#38388;&#20013;&#24212;&#29992;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;PDE&#25968;&#25454;&#29983;&#25104;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;DiffOAS&#12290;&#23427;&#33021;&#22815;&#22312;&#29983;&#25104;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.05957</link><description>&lt;p&gt;
&#22312;&#35299;&#31354;&#38388;&#20013;&#21152;&#36895;PDE&#25968;&#25454;&#29983;&#25104;&#30340;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerating PDE Data Generation via Differential Operator Action in Solution Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05957
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35299;&#31354;&#38388;&#20013;&#24212;&#29992;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;PDE&#25968;&#25454;&#29983;&#25104;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;DiffOAS&#12290;&#23427;&#33021;&#22815;&#22312;&#29983;&#25104;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#19978;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65288;&#22914;&#31070;&#32463;&#31639;&#23376;&#65289;&#22312;&#20943;&#23569;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#26102;&#38388;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#22823;&#37327;&#39640;&#31934;&#24230;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#38656;&#35201;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PDE&#25968;&#25454;&#38598;&#29983;&#25104;&#31639;&#27861;&#65292;&#21363;&#35299;&#31354;&#38388;&#20013;&#30340;&#24494;&#20998;&#31639;&#23376;&#20316;&#29992;&#65288;DiffOAS&#65289;&#65292;&#23427;&#21516;&#26102;&#21152;&#24555;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#31934;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DiffOAS&#33719;&#21462;&#20102;&#20960;&#20010;&#22522;&#26412;&#30340;PDE&#35299;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#20197;&#33719;&#24471;&#35299;&#12290;&#23427;&#23545;&#36825;&#20123;&#35299;&#24212;&#29992;&#24494;&#20998;&#31639;&#23376;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#31639;&#23376;&#20316;&#29992;&#8221;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#31934;&#30830;&#30340;PDE&#25968;&#25454;&#28857;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;DiffOAS&#26041;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#27604;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in data-driven approaches, such as Neural Operator (NO), have demonstrated their effectiveness in reducing the solving time of Partial Differential Equations (PDEs). However, one major challenge faced by these approaches is the requirement for a large amount of high-precision training data, which needs significant computational costs during the generation process. To address this challenge, we propose a novel PDE dataset generation algorithm, namely Differential Operator Action in Solution space (DiffOAS), which speeds up the data generation process and enhances the precision of the generated data simultaneously. Specifically, DiffOAS obtains a few basic PDE solutions and then combines them to get solutions. It applies differential operators on these solutions, a process we call 'operator action', to efficiently generate precise PDE data points. Theoretical analysis shows that the time complexity of DiffOAS method is one order lower than the existing generation meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05956</link><description>&lt;p&gt;
Pathformer: &#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20174;&#26377;&#38480;&#25110;&#22266;&#23450;&#23610;&#24230;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#36328;&#22810;&#20010;&#23610;&#24230;&#30340;&#19981;&#21516;&#29305;&#24449;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#65288;Pathformer&#65289;&#30340;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21516;&#26102;&#25972;&#21512;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#22810;&#23610;&#24230;&#21010;&#20998;&#36816;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#22359;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#22522;&#20110;&#27599;&#20010;&#23610;&#24230;&#30340;&#21010;&#20998;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#22359;&#36827;&#34892;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#24615;&#21644;&#23616;&#37096;&#32454;&#33410;&#20316;&#20026;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20016;&#23500;&#22810;&#23610;&#24230;Transformer&#65292;&#35813;&#36335;&#24452;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#26102;&#38388;&#21160;&#24577;&#35843;&#25972;&#22810;&#23610;&#24230;&#24314;&#27169;&#36807;&#31243;&#65292;&#25552;&#39640;Pathformer&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;11&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#20998;&#31163;&#21487;&#34892;&#24615;&#32422;&#26463;&#30340;&#21487;&#25511;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;&#30340;&#36229;&#32423;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36817;&#20284;&#21644;&#23450;&#20301;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#26469;&#35299;&#20915;&#20998;&#35010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#38480;&#21046;&#20102;&#20915;&#31574;&#32773;&#30446;&#26631;&#30340;&#32422;&#26463;&#21306;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.05955</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20855;&#26377;&#20998;&#31163;&#21487;&#34892;&#24615;&#32422;&#26463;&#30340;&#21487;&#25511;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;&#30340;&#36229;&#32423;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Hyper-Transformer model for Controllable Pareto Front Learning with Split Feasibility Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#20998;&#31163;&#21487;&#34892;&#24615;&#32422;&#26463;&#30340;&#21487;&#25511;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;&#30340;&#36229;&#32423;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36817;&#20284;&#21644;&#23450;&#20301;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#26469;&#35299;&#20915;&#20998;&#35010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#38480;&#21046;&#20102;&#20915;&#31574;&#32773;&#30446;&#26631;&#30340;&#32422;&#26463;&#21306;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;&#65288;CPFL&#65289;&#36890;&#36807;&#36817;&#20284;&#24085;&#32047;&#25176;&#35299;&#38598;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#21442;&#32771;&#21521;&#37327;&#19979;&#23450;&#20301;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#21463;&#21040;&#32422;&#26463;&#21306;&#22495;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#25105;&#20204;&#21482;&#22312;&#32422;&#26463;&#21306;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#20915;&#31574;&#31354;&#38388;&#12290;&#20855;&#26377;&#20998;&#31163;&#21487;&#34892;&#24615;&#32422;&#26463;&#65288;SFC&#65289;&#30340;&#21487;&#25511;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;&#26159;&#19968;&#31181;&#23547;&#25214;&#28385;&#36275;&#26576;&#20123;&#32422;&#26463;&#26465;&#20214;&#30340;&#20998;&#35010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20339;&#24085;&#32047;&#25176;&#35299;&#30340;&#26041;&#27861;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;CPFL&#20351;&#29992;&#20102;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;Hyper-MLP&#65289;&#27169;&#22359;&#32452;&#25104;&#30340;&#36229;&#32593;&#32476;&#27169;&#22411;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#20013;&#21464;&#21387;&#22120;&#32467;&#26500;&#30340;&#26174;&#33879;&#36827;&#27493;&#65292;&#21464;&#21387;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21487;&#20197;&#36229;&#36234;&#20854;&#20182;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#20855;&#26377;SFC&#30340;CPFL&#24320;&#21457;&#20102;&#19968;&#31181;&#36229;&#32423;&#21464;&#21387;&#22120;&#65288;Hyper-Trans&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#29702;&#35770;&#26469;&#23637;&#31034;Hyper-Trans&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable Pareto front learning (CPFL) approximates the Pareto solution set and then locates a Pareto optimal solution with respect to a given reference vector. However, decision-maker objectives were limited to a constraint region in practice, so instead of training on the entire decision space, we only trained on the constraint region. Controllable Pareto front learning with Split Feasibility Constraints (SFC) is a way to find the best Pareto solutions to a split multi-objective optimization problem that meets certain constraints. In the previous study, CPFL used a Hypernetwork model comprising multi-layer perceptron (Hyper-MLP) blocks. With the substantial advancement of transformer architecture in deep learning, transformers can outperform other architectures in various tasks. Therefore, we have developed a hyper-transformer (Hyper-Trans) model for CPFL with SFC. We use the theory of universal approximation for the sequence-to-sequence function to show that the Hyper-Trans model
&lt;/p&gt;</description></item><item><title>EasyFS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#24377;&#24615;&#25193;&#23637;&#21644;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#24182;&#21457;&#29616;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26032;&#30340;&#20887;&#20313;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20887;&#20313;&#29305;&#24449;&#30340;&#39640;&#25928;&#36807;&#28388;&#12290;</title><link>https://arxiv.org/abs/2402.05954</link><description>&lt;p&gt;
EasyFS:&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#30340;&#24377;&#24615;&#21464;&#25442;&#23454;&#29616;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyFS: an Efficient Model-free Feature Selection Framework via Elastic Transformation of Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05954
&lt;/p&gt;
&lt;p&gt;
EasyFS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#24377;&#24615;&#25193;&#23637;&#21644;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#24182;&#21457;&#29616;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26032;&#30340;&#20887;&#20313;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20887;&#20313;&#29305;&#24449;&#30340;&#39640;&#25928;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#23558;&#27599;&#20010;&#29305;&#24449;&#29420;&#31435;&#22788;&#29702;&#65292;&#24573;&#35270;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#36825;&#23548;&#33268;&#20854;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#65292;&#19982;&#27169;&#22411;&#24863;&#30693;&#26041;&#27861;&#30456;&#27604;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#24377;&#24615;&#25193;&#23637;&#21644;&#21387;&#32553;&#30340;&#39640;&#25928;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#8212;&#8212;EasyFS&#65292;&#20197;&#23454;&#29616;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24863;&#30693;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#29616;&#26377;&#26080;&#27169;&#22411;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EasyFS&#37319;&#29992;&#38543;&#26426;&#38750;&#32447;&#24615;&#25237;&#24433;&#32593;&#32476;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#21407;&#22987;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#20197;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#24182;&#21457;&#29616;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#29575;&#21464;&#21270;&#30340;&#26032;&#22411;&#20887;&#20313;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#36807;&#28388;&#20887;&#20313;&#29305;&#24449;&#12290;&#22312;21&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Traditional model-free feature selection methods treat each feature independently while disregarding the interrelationships among features, which leads to relatively poor performance compared with the model-aware methods. To address this challenge, we propose an efficient model-free feature selection framework via elastic expansion and compression of the features, namely EasyFS, to achieve better performance than state-of-the-art model-aware methods while sharing the characters of efficiency and flexibility with the existing model-free methods. In particular, EasyFS expands the feature space by using the random non-linear projection network to achieve the non-linear combinations of the original features, so as to model the interrelationships among the features and discover most correlated features. Meanwhile, a novel redundancy measurement based on the change of coding rate is proposed for efficient filtering of redundant features. Comprehensive experiments on 21 different datasets sho
&lt;/p&gt;</description></item><item><title>idMotif&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#35782;&#21035;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27169;&#20307;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#21457;&#29616;&#28508;&#22312;&#30340;&#27169;&#20307;&#20505;&#36873;&#24207;&#21015;&#12290;&#23427;&#25552;&#20379;&#22810;&#20010;&#20132;&#20114;&#24335;&#35270;&#22270;&#65292;&#29992;&#20110;&#20998;&#26512;&#34507;&#30333;&#36136;&#32858;&#31867;&#21644;&#24207;&#21015;&#12290;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;idMotif&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#27169;&#20307;&#20998;&#26512;&#19982;&#35782;&#21035;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05953</link><description>&lt;p&gt;
idMotif&#65306;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#20114;&#21160;&#27169;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
idMotif: An Interactive Motif Identification in Protein Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05953
&lt;/p&gt;
&lt;p&gt;
idMotif&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#35782;&#21035;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27169;&#20307;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#21457;&#29616;&#28508;&#22312;&#30340;&#27169;&#20307;&#20505;&#36873;&#24207;&#21015;&#12290;&#23427;&#25552;&#20379;&#22810;&#20010;&#20132;&#20114;&#24335;&#35270;&#22270;&#65292;&#29992;&#20110;&#20998;&#26512;&#34507;&#30333;&#36136;&#32858;&#31867;&#21644;&#24207;&#21015;&#12290;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;idMotif&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#27169;&#20307;&#20998;&#26512;&#19982;&#35782;&#21035;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;idMotif&#65292;&#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#35782;&#21035;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#27169;&#20307;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#12290;&#27169;&#20307;&#26159;&#30001;&#27688;&#22522;&#37240;&#32452;&#25104;&#30340;&#30701;&#24207;&#21015;&#65292;&#23545;&#20110;&#29702;&#35299;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#35782;&#21035;&#36825;&#20123;&#27169;&#20307;&#23545;&#20110;&#39044;&#27979;&#30142;&#30149;&#25110;&#24863;&#26579;&#33267;&#20851;&#37325;&#35201;&#12290;idMotif&#37319;&#29992;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#30340;&#23616;&#37096;&#35299;&#37322;&#65292;&#21487;&#20197;&#21457;&#29616;&#34507;&#30333;&#36136;&#32452;&#20013;&#28508;&#22312;&#30340;&#27169;&#20307;&#20505;&#36873;&#24207;&#21015;&#12290;&#23427;&#25552;&#20379;&#22810;&#20010;&#20132;&#20114;&#24335;&#35270;&#22270;&#65292;&#29992;&#20110;&#20998;&#26512;&#34507;&#30333;&#36136;&#32858;&#31867;&#25110;&#32452;&#21450;&#20854;&#24207;&#21015;&#12290;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#65292;&#35828;&#26126;&#20102;idMotif&#22312;&#20419;&#36827;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#27169;&#20307;&#20998;&#26512;&#21644;&#35782;&#21035;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces idMotif, a visual analytics framework designed to aid domain experts in the identification of motifs within protein sequences. Motifs, short sequences of amino acids, are critical for understanding the distinct functions of proteins. Identifying these motifs is pivotal for predicting diseases or infections. idMotif employs a deep learning-based method for the categorization of protein sequences, enabling the discovery of potential motif candidates within protein groups through local explanations of deep learning model decisions. It offers multiple interactive views for the analysis of protein clusters or groups and their sequences. A case study, complemented by expert feedback, illustrates idMotif's utility in facilitating the analysis and identification of protein sequences and motifs.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30456;&#32467;&#21512;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20026;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.05952</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#36827;&#22270;&#34920;&#31034;&#23398;&#20064;&#65306;&#25216;&#26415;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30456;&#32467;&#21512;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20026;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#20998;&#26512;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#21512;&#20316;&#21033;&#29992;LLM&#30340;&#20808;&#36827;&#35821;&#35328;&#33021;&#21147;&#26469;&#25913;&#36827;&#22270;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;GRL&#30340;&#33539;&#22260;&#21644;&#28508;&#21147;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#23558;LLM&#38598;&#25104;&#21040;&#22270;&#39046;&#22495;&#20013;&#65292;&#20294;&#32570;&#20047;&#19968;&#20221;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#26469;&#20998;&#35299;&#36825;&#20123;&#27169;&#22411;&#20026;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20174;&#26032;&#30340;&#25216;&#26415;&#35282;&#24230;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#26368;&#36817;&#30340;&#25991;&#29486;&#20998;&#35299;&#20026;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#30693;&#35782;&#25552;&#21462;&#22120;&#21644;&#32452;&#32455;&#32773;&#65292;&#20197;&#21450;&#20004;&#20010;&#25805;&#20316;&#25216;&#26415;&#65292;&#21253;&#25324;&#38598;&#25104;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#25581;&#31034;&#20986;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#30340;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures. This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL. Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking. Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective. We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies. Additio
&lt;/p&gt;</description></item><item><title>\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05951</link><description>&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05951
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20445;&#23432;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65288;$Q$-&#20272;&#35745;&#36807;&#39640;&#20272;&#35745;&#20102;&#30495;&#23454;&#30340;$Q$&#20540;&#65289;&#12290;&#20854;&#26680;&#24515;&#20844;&#24335;&#20381;&#36182;&#20110;$Q$-&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#37319;&#29992;&#26368;&#23567;&#25209;&#27425;&#26368;&#22823;&#26368;&#23567;$Q$-&#32593;&#32476;&#36317;&#31163;&#20316;&#20026;$Q$-&#30446;&#26631;&#21152;&#20837;&#65292;&#24182;&#20316;&#20026;&#20248;&#20808;&#32423;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;TD3&#21644;TD7&#20043;&#19978;&#23454;&#26045;&#20102;\textit{MinMaxMin}&#65292;&#24182;&#23545;&#20854;&#22312;&#27969;&#34892;&#30340;MuJoCo&#21644;Bullet&#29615;&#22659;&#20013;&#23545;&#25239;&#29616;&#26377;&#30340;&#36830;&#32493;&#31354;&#38388;&#31639;&#27861;-DDPG&#65292;TD3&#21644;TD7&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#20219;&#21153;&#20013;&#65292;\textit{MinMaxMin}&#30456;&#23545;&#20110;DDPG&#65292;TD3&#21644;TD7&#22343;&#34920;&#29616;&#20986;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
&lt;/p&gt;</description></item><item><title>SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05950</link><description>&lt;p&gt;
SQT - std Q-target
&lt;/p&gt;
&lt;p&gt;
\textit{SQT} -- \textit{std} $Q$-target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05950
&lt;/p&gt;
&lt;p&gt;
SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Std Q-target&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#30340;Q&#20844;&#24335;&#65306;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#65292;&#36825;&#20010;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#26159;&#23545;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#32422;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TD3/TD7&#20195;&#30721;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;SQT&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;actor-critic&#31639;&#27861;DDPG&#12289;TD3&#21644;TD7&#22312;&#19971;&#20010;&#24120;&#35265;&#30340;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;SQT&#30340;Q-target&#20844;&#24335;&#30456;&#23545;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#22312;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#20445;&#23432;&#35299;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;SQT&#30456;&#23545;&#20110;DDPG&#12289;TD3&#21644;TD7&#37117;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#32447;&#23458;&#25143;&#25968;&#25454;&#65292;&#25552;&#21462;&#20135;&#21697;&#24320;&#21457;&#30340;&#20840;&#38754;&#35774;&#35745;&#21551;&#31034;&#65292;&#24182;&#35780;&#20272;&#27599;&#20010;&#29305;&#24615;&#23545;&#24635;&#20307;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05949</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#23458;&#25143;&#30340;&#22312;&#32447;&#25968;&#25454;&#20197;&#30830;&#23450;&#20135;&#21697;&#29305;&#24615;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
An explainable machine learning-based approach for analyzing customers' online data to identify the importance of product attributes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#32447;&#23458;&#25143;&#25968;&#25454;&#65292;&#25552;&#21462;&#20135;&#21697;&#24320;&#21457;&#30340;&#20840;&#38754;&#35774;&#35745;&#21551;&#31034;&#65292;&#24182;&#35780;&#20272;&#27599;&#20010;&#29305;&#24615;&#23545;&#24635;&#20307;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23458;&#25143;&#25968;&#25454;&#20026;&#20135;&#21697;&#35774;&#35745;&#21644;&#24066;&#22330;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20449;&#24687;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25581;&#31034;&#23458;&#25143;&#30340;&#21916;&#22909;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#35774;&#35745;&#30340;&#20998;&#26512;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#21487;&#33021;&#23384;&#22312;&#38544;&#34255;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38480;&#20110;&#21457;&#29616;&#23458;&#25143;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#35770;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22312;&#32447;&#35780;&#32423;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#12289;&#25490;&#24207;&#21644;&#32452;&#21512;&#26368;&#22823;&#21270;&#23458;&#25143;&#28385;&#24847;&#24230;&#30340;&#20135;&#21697;&#29305;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#20840;&#38754;&#30340;&#35774;&#35745;&#21551;&#31034;&#65292;&#29992;&#20110;&#20135;&#21697;&#24320;&#21457;&#30340;&#25351;&#23548;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;SHAP&#65288;SHapley Additive exPlanations&#65289;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;&#26681;&#25454;&#20854;&#23545;&#39044;&#27979;&#30340;&#36129;&#29486;&#20026;&#27599;&#20010;&#29305;&#24449;&#20998;&#37197;&#19968;&#20010;&#20540;&#65292;&#20026;&#35780;&#20272;&#27599;&#20010;&#29305;&#24449;&#23545;&#24635;&#20307;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#24615;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;Kaggle&#30340;&#31508;&#35760;&#26412;&#30005;&#33041;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online customer data provides valuable information for product design and marketing research, as it can reveal the preferences of customers. However, analyzing these data using artificial intelligence (AI) for data-driven design is a challenging task due to potential concealed patterns. Moreover, in these research areas, most studies are only limited to finding customers' needs. In this study, we propose a game theory machine learning (ML) method that extracts comprehensive design implications for product development. The method first uses a genetic algorithm to select, rank, and combine product features that can maximize customer satisfaction based on online ratings. Then, we use SHAP (SHapley Additive exPlanations), a game theory method that assigns a value to each feature based on its contribution to the prediction, to provide a guideline for assessing the importance of each feature for the total satisfaction. We apply our method to a real-world dataset of laptops from Kaggle, and d
&lt;/p&gt;</description></item><item><title>DE$^3$-BERT&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#21644;&#36317;&#31163;&#24230;&#37327;&#30340;&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;BERT&#31561;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05948</link><description>&lt;p&gt;
DE$^3$-BERT: &#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#65292;&#29992;&#20110;BERT
&lt;/p&gt;
&lt;p&gt;
DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05948
&lt;/p&gt;
&lt;p&gt;
DE$^3$-BERT&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#21644;&#36317;&#31163;&#24230;&#37327;&#30340;&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;BERT&#31561;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#25191;&#34892;&#30340;&#23618;&#25968;&#65292;&#25552;&#39640;&#20102;&#20687;BERT&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#20165;&#32771;&#34385;&#20102;&#26469;&#33258;&#21333;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#23616;&#37096;&#20449;&#24687;&#26469;&#30830;&#23450;&#26089;&#26399;&#20572;&#27490;&#30340;&#25351;&#26631;&#65292;&#32780;&#26410;&#21033;&#29992;&#26679;&#26412;&#32676;&#20307;&#25552;&#20379;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36825;&#23548;&#33268;&#23545;&#39044;&#27979;&#27491;&#30830;&#24615;&#30340;&#20272;&#35745;&#19981;&#22815;&#20934;&#30830;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#30340;&#26089;&#26399;&#20572;&#27490;&#20915;&#31574;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26377;&#25928;&#32467;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#26089;&#26399;&#20572;&#27490;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21407;&#22411;&#32593;&#32476;&#23398;&#20064;&#31867;&#21035;&#21407;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#26679;&#26412;&#21644;&#31867;&#21035;&#21407;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#26089;&#26399;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DE$^3$-BERT&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early exiting has demonstrated its effectiveness in accelerating the inference of pre-trained language models like BERT by dynamically adjusting the number of layers executed. However, most existing early exiting methods only consider local information from an individual test sample to determine their exiting indicators, failing to leverage the global information offered by sample population. This leads to suboptimal estimation of prediction correctness, resulting in erroneous exiting decisions. To bridge the gap, we explore the necessity of effectively combining both local and global information to ensure reliable early exiting during inference. Purposefully, we leverage prototypical networks to learn class prototypes and devise a distance metric between samples and class prototypes. This enables us to utilize global information for estimating the correctness of early predictions. On this basis, we propose a novel Distance-Enhanced Early Exiting framework for BERT (DE$^3$-BERT). DE$^3
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21487;&#20998;&#31163;&#30340;&#22810;&#27010;&#24565;&#25273;&#38500;&#22120;&#65288;SepME&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#27010;&#24565;&#26080;&#20851;&#34920;&#31034;&#21644;&#26435;&#37325;&#35299;&#32806;&#26469;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22810;&#27010;&#24565;&#25273;&#38500;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#29983;&#25104;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2402.05947</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#22810;&#27010;&#24565;&#25273;&#38500;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Separable Multi-Concept Erasure from Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05947
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21487;&#20998;&#31163;&#30340;&#22810;&#27010;&#24565;&#25273;&#38500;&#22120;&#65288;SepME&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#27010;&#24565;&#26080;&#20851;&#34920;&#31034;&#21644;&#26435;&#37325;&#35299;&#32806;&#26469;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22810;&#27010;&#24565;&#25273;&#38500;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#29983;&#25104;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#36825;&#24341;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#20854;&#31038;&#20250;&#24433;&#21709;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#23545;&#29256;&#26435;&#33402;&#26415;&#39118;&#26684;&#30340;&#27169;&#20223;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#26426;&#22120;&#36951;&#24536;&#25216;&#26415;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#28040;&#38500;&#19981;&#23433;&#20840;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#25439;&#23475;&#29983;&#25104;&#24615;&#33021;&#65292;&#24573;&#35270;&#22810;&#27010;&#24565;&#28040;&#38500;&#20043;&#38388;&#30340;&#32806;&#21512;&#65292;&#20197;&#21450;&#27010;&#24565;&#24674;&#22797;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#22810;&#27010;&#24565;&#25273;&#38500;&#22120;&#65288;SepME&#65289;&#65292;&#20027;&#35201;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#27010;&#24565;&#26080;&#20851;&#34920;&#31034;&#30340;&#29983;&#25104;&#21644;&#26435;&#37325;&#35299;&#32806;&#12290;&#21069;&#32773;&#26088;&#22312;&#36991;&#20813;&#36951;&#24536;&#19982;&#36951;&#24536;&#27010;&#24565;&#26080;&#20851;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#21518;&#32773;&#20998;&#31163;&#21487;&#20248;&#21270;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#20351;&#27599;&#20010;&#26435;&#37325;&#22686;&#37327;&#23545;&#24212;&#20110;&#29305;&#23450;&#27010;&#24565;&#30340;&#28040;&#38500;&#65292;&#32780;&#19981;&#24433;&#21709;&#23545;&#20854;&#20182;&#27010;&#24565;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale diffusion models, known for their impressive image generation capabilities, have raised concerns among researchers regarding social impacts, such as the imitation of copyrighted artistic styles. In response, existing approaches turn to machine unlearning techniques to eliminate unsafe concepts from pre-trained models. However, these methods compromise the generative performance and neglect the coupling among multi-concept erasures, as well as the concept restoration problem. To address these issues, we propose a Separable Multi-concept Eraser (SepME), which mainly includes two parts: the generation of concept-irrelevant representations and the weight decoupling. The former aims to avoid unlearning substantial information that is irrelevant to forgotten concepts. The latter separates optimizable model weights, making each weight increment correspond to a specific concept erasure without affecting generative performance on other concepts. Specifically, the weight increment fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05946</link><description>&lt;p&gt;
&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#65306;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24322;&#24120;&#20107;&#20214;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#29702;&#35299;&#24322;&#24120;&#20107;&#20214;&#32972;&#21518;&#30340;&#22240;&#26524;&#21407;&#22240;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#31361;&#28982;&#21464;&#21270;&#12290;&#25581;&#31034;&#22240;&#26524;&#21407;&#22240;&#26377;&#21161;&#20110;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#25581;&#31034;&#35299;&#37322;&#35266;&#23519;&#20107;&#20214;&#30340;&#8220;&#22914;&#26524;-&#37027;&#20040;&#8221;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#38388;&#28857;&#36807;&#31243;&#26469;&#24314;&#27169;&#25152;&#20851;&#27880;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19968;&#32452;&#28508;&#22312;&#35268;&#21017;&#26469;&#35299;&#37322;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#22312;E&#27493;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#20107;&#20214;&#34987;&#27599;&#20010;&#21457;&#29616;&#30340;&#35268;&#21017;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;M&#27493;&#20013;&#65292;&#25105;&#20204;&#26356;&#26032;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#22686;&#24378;&#21487;&#33021;&#24615;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20197;&#24494;&#20998;&#30340;&#26041;&#24335;&#20248;&#21270;&#35268;&#21017;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#35268;&#21017;&#21644;&#35782;&#21035;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23637;&#31034;&#20102;&#23427;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using syntheti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#31614;&#30417;&#30563;&#21644;&#26500;&#24314;&#20998;&#23618;&#27010;&#24565;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CBMs&#33539;&#20363;&#65288;SupCBM&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#27010;&#24565;&#21644;&#24178;&#39044;&#30697;&#38453;&#23454;&#29616;&#26631;&#31614;&#39044;&#27979;&#65292;&#24182;&#19988;&#21482;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.05945</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#12289;&#20998;&#23618;&#27010;&#24565;&#23398;&#20064;&#28040;&#38500;&#30828;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Eliminating Information Leakage in Hard Concept Bottleneck Models with Supervised, Hierarchical Concept Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#31614;&#30417;&#30563;&#21644;&#26500;&#24314;&#20998;&#23618;&#27010;&#24565;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CBMs&#33539;&#20363;&#65288;SupCBM&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#27010;&#24565;&#21644;&#24178;&#39044;&#30697;&#38453;&#23454;&#29616;&#26631;&#31614;&#39044;&#27979;&#65292;&#24182;&#19988;&#21482;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#29305;&#24449;&#21644;&#26631;&#31614;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#21487;&#24178;&#39044;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;CBMs&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#21363;&#22312;&#27010;&#24565;&#34920;&#31034;&#20026;&#27010;&#29575;&#25110;&#20108;&#36827;&#21046;&#29366;&#24577;&#26102;&#65292;&#36229;&#20986;&#27010;&#24565;&#30340;&#24847;&#22270;&#20449;&#24687;&#27844;&#28431;&#21040;&#21518;&#32493;&#30340;&#26631;&#31614;&#39044;&#27979;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#26080;&#27861;&#21306;&#20998;&#30340;&#27010;&#24565;&#26469;&#38169;&#35823;&#20998;&#31867;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21066;&#24369;&#20102;CBMs&#30340;&#35299;&#37322;&#21644;&#24178;&#39044;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#27010;&#24565;&#39044;&#27979;&#20013;&#24341;&#20837;&#26631;&#31614;&#30417;&#30563;&#21644;&#26500;&#24314;&#20998;&#23618;&#27010;&#24565;&#38598;&#26469;&#32531;&#35299;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CBMs&#33539;&#20363;&#65292;&#21363;SupCBM&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#30340;&#27010;&#24565;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#24178;&#39044;&#30697;&#38453;&#23454;&#29616;&#26631;&#31614;&#39044;&#27979;&#12290;SupCBM&#23558;&#37325;&#28857;&#25918;&#22312;&#19982;&#39044;&#27979;&#26631;&#31614;&#26368;&#30456;&#20851;&#30340;&#27010;&#24565;&#19978;&#65292;&#24182;&#19988;&#20165;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) aim to deliver interpretable and interventionable predictions by bridging features and labels with human-understandable concepts. While recent CBMs show promising potential, they suffer from information leakage, where unintended information beyond the concepts (either when concepts are represented with probabilities or binary states) are leaked to the subsequent label prediction. Consequently, distinct classes are falsely classified via indistinguishable concepts, undermining the interpretation and intervention of CBMs.   This paper alleviates the information leakage issue by introducing label supervision in concept predication and constructing a hierarchical concept set. Accordingly, we propose a new paradigm of CBMs, namely SupCBM, which achieves label predication via predicted concepts and a deliberately-designed intervention matrix. SupCBM focuses on concepts that are mostly relevant to the predicted label and only distinguishes classes when differe
&lt;/p&gt;</description></item><item><title>Todyformer&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#32534;&#30721;&#21644;&#20840;&#23616;&#32534;&#30721;&#33021;&#21147;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#26631;&#35760;&#21270;&#31574;&#30053;&#21644;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#26469;&#35299;&#20915;&#21160;&#24577;&#22270;&#24418;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05944</link><description>&lt;p&gt;
Todyformer&#65306;&#38754;&#21521;&#32467;&#26500;&#24863;&#30693;&#26631;&#35760;&#21270;&#30340;&#25972;&#20307;&#21160;&#24577;&#22270;&#24418;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05944
&lt;/p&gt;
&lt;p&gt;
Todyformer&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#32534;&#30721;&#21644;&#20840;&#23616;&#32534;&#30721;&#33021;&#21147;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#26631;&#35760;&#21270;&#31574;&#30053;&#21644;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#26469;&#35299;&#20915;&#21160;&#24577;&#22270;&#24418;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20851;&#32852;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22240;&#20854;&#33021;&#22815;&#27169;&#25311;&#28436;&#21270;&#32467;&#26500;&#21644;&#26102;&#38388;&#27169;&#24335;&#24182;&#23637;&#31034;&#20986;&#33391;&#22909;&#24615;&#33021;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#36825;&#20123;&#26550;&#26500;&#21463;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#22914;&#36807;&#24230;&#21387;&#32553;&#21644;&#36807;&#24230;&#24179;&#28369;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21464;&#21387;&#22120;&#24050;&#32463;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#19982;&#38271;&#31243;&#20381;&#36182;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;Todyformer&#65292;&#19987;&#20026;&#21160;&#24577;&#22270;&#24418;&#35774;&#35745;&#12290;&#23427;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30340;&#23616;&#37096;&#32534;&#30721;&#33021;&#21147;&#19982;&#21464;&#21387;&#22120;&#30340;&#20840;&#23616;&#32534;&#30721;&#33021;&#21147;&#32479;&#19968;&#36215;&#26469;&#65306;i&#65289;&#37319;&#29992;&#26032;&#39062;&#30340;&#38754;&#21521;&#21160;&#24577;&#22270;&#24418;&#30340;&#22359;&#29366;&#21270;&#33539;&#24335;&#26469;&#25913;&#21892;&#36807;&#24230;&#21387;&#32553;&#65292;ii&#65289;&#21033;&#29992;MPNNs&#30340;&#32467;&#26500;&#24863;&#30693;&#21442;&#25968;&#21270;&#26631;&#35760;&#21270;&#31574;&#30053;&#65292;iii&#65289;&#24341;&#20837;&#24102;&#26377;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#30340;&#21464;&#21387;&#22120;&#26469;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;iv&#65289;&#20132;&#26367;&#30340;&#32534;&#30721;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Neural Networks have garnered substantial attention for their capacity to model evolving structural and temporal patterns while exhibiting impressive performance. However, it is known that these architectures are encumbered by issues that constrain their performance, such as over-squashing and over-smoothing. Meanwhile, Transformers have demonstrated exceptional computational capacity to effectively address challenges related to long-range dependencies. Consequently, we introduce Todyformer-a novel Transformer-based neural network tailored for dynamic graphs. It unifies the local encoding capacity of Message-Passing Neural Networks (MPNNs) with the global encoding of Transformers through i) a novel patchifying paradigm for dynamic graphs to improve over-squashing, ii) a structure-aware parametric tokenization strategy leveraging MPNNs, iii) a Transformer with temporal positional-encoding to capture long-range dependencies, and iv) an encoding architecture that alternates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512; IndRNNLSTM &#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512; IndRNN &#21644; LSTM &#30340;&#29305;&#28857;&#65292;&#23398;&#20064;&#30456;&#20851;&#21644;&#38750;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#35270;&#35282;&#12290;&#22312; NSL-KDD &#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#36739;&#20302;&#30340; MAE &#21644; RMSE &#20540;&#12290;</title><link>https://arxiv.org/abs/2402.05943</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#30340;&#28151;&#21512; IndRNNLSTM &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A hybrid IndRNNLSTM approach for real-time anomaly detection in software-defined networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512; IndRNNLSTM &#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512; IndRNN &#21644; LSTM &#30340;&#29305;&#28857;&#65292;&#23398;&#20064;&#30456;&#20851;&#21644;&#38750;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#35270;&#35282;&#12290;&#22312; NSL-KDD &#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#36739;&#20302;&#30340; MAE &#21644; RMSE &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#20351;&#29992;&#25968;&#25454;&#27969;&#39044;&#27979;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#24402;&#31867;&#20026;&#26102;&#24207;&#21644;&#22238;&#24402;&#38382;&#39064;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#29305;&#24449;&#12290;&#32780;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#33021;&#22815;&#33258;&#21160;&#36873;&#25321;&#29305;&#24449;&#20855;&#26377;&#37325;&#35201;&#30340;&#29305;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110; RNN &#30340;&#26041;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;LSTM &#21644; GRU &#26041;&#27861;&#33021;&#22815;&#24456;&#22909;&#22320;&#23398;&#20064;&#30456;&#20851;&#23454;&#20307;&#65307;&#32780; IndRNN &#26041;&#27861;&#21017;&#33021;&#22815;&#23398;&#20064;&#26102;&#24207;&#20013;&#30340;&#38750;&#30456;&#20851;&#23454;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#23581;&#35797;&#20351;&#29992; IndRNN &#21644; LSTM &#30340;&#32452;&#21512;&#26469;&#23398;&#20064;&#30456;&#20851;&#21644;&#38750;&#30456;&#20851;&#29305;&#24449;&#12290;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36824;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#29305;&#24449;&#35270;&#35282;&#65307;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#20351;&#29992;&#20102;&#22235;&#31181;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#65306;Filter&#12289;Wrapper&#12289;Embedded &#21644; Autoencoder&#12290;&#25552;&#20986;&#30340; IndRNNLSTM &#31639;&#27861;&#19982; Embedded &#30340;&#32452;&#21512;&#33021;&#22815;&#22312; NSL-KDD &#25968;&#25454;&#38598;&#19978;&#23454;&#29616; MAE=1.22 &#21644; RMSE=9.92&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in SDN using data flow prediction is a difficult task. This problem is included in the category of time series and regression problems. Machine learning approaches are challenging in this field due to the manual selection of features. On the other hand, deep learning approaches have important features due to the automatic selection of features. Meanwhile, RNN-based approaches have been used the most. The LSTM and GRU approaches learn dependent entities well; on the other hand, the IndRNN approach learns non-dependent entities in time series. The proposed approach tried to use a combination of IndRNN and LSTM approaches to learn dependent and non-dependent features. Feature selection approaches also provide a suitable view of features for the models; for this purpose, four feature selection models, Filter, Wrapper, Embedded, and Autoencoder were used. The proposed IndRNNLSTM algorithm, in combination with Embedded, was able to achieve MAE=1.22 and RMSE=9.92 on NSL-KDD 
&lt;/p&gt;</description></item><item><title>&#21512;&#20316;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#30456;&#20114;&#21512;&#20316;&#26469;&#20256;&#36882;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24357;&#34917;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#38480;&#24615;&#12290;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.05942</link><description>&lt;p&gt;
&#21512;&#20316;&#30693;&#35782;&#33976;&#39311;&#65306;&#19968;&#31181;&#23398;&#20064;&#32773;&#26080;&#20851;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cooperative Knowledge Distillation: A Learner Agnostic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05942
&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#30456;&#20114;&#21512;&#20316;&#26469;&#20256;&#36882;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24357;&#34917;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#38480;&#24615;&#12290;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#33267;&#23569;&#19968;&#31181;&#20851;&#38190;&#38480;&#21046;&#65292;&#38480;&#21046;&#20854;&#20351;&#29992;&#33539;&#22260;&#21644;&#26041;&#21521;&#65306;&#26080;&#35770;&#35813;&#30693;&#35782;&#26159;&#21542;&#26377;&#29992;&#65292;&#25152;&#26377;&#30693;&#35782;&#37117;&#20174;&#25945;&#24072;&#20256;&#36882;&#32473;&#23398;&#29983;&#65307;&#23398;&#29983;&#26159;&#36825;&#31181;&#20132;&#27969;&#20013;&#21807;&#19968;&#23398;&#20064;&#30340;&#19968;&#26041;&#65307;&#20856;&#22411;&#30340;&#33976;&#39311;&#21482;&#20174;&#19968;&#20010;&#25945;&#24072;&#21521;&#19968;&#20010;&#23398;&#29983;&#20256;&#36882;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#21363;&#21512;&#20316;&#33976;&#39311;&#65292;&#20854;&#20013;&#35768;&#22810;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#20805;&#24403;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#35282;&#33394;&#12290;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#26041;&#24335;&#22914;&#19979;&#65306;&#19968;&#20010;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#35782;&#21035;&#20854;&#24615;&#33021;&#20013;&#30340;&#29305;&#23450;&#32570;&#38519;&#65292;&#24182;&#25628;&#32034;&#21478;&#19968;&#20010;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#24212;&#20107;&#23454;&#24773;&#20917;&#30340;&#34394;&#25311;&#23454;&#20363;&#26469;&#32534;&#30721;&#25152;&#23398;&#30693;&#35782;&#12290;&#30001;&#20110;&#19981;&#21516;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#22240;&#27492;&#21512;&#20316;&#33976;&#39311;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#29983;&#25104;&#26381;&#35013;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.05941</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#19982;&#36890;&#36807;LLMs&#36827;&#34892;&#35270;&#35273;&#22686;&#24378;&#30340;&#39118;&#26684;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#29983;&#25104;&#26381;&#35013;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#35013;&#29983;&#25104;&#38382;&#39064;&#28041;&#21450;&#26681;&#25454;&#29992;&#25143;&#30340;&#20852;&#36259;&#25512;&#33616;&#19968;&#20010;&#23436;&#25972;&#30340;&#26381;&#35013;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#38170;&#23450;&#21830;&#21697;&#25110;&#25351;&#23450;&#26597;&#35810;&#39118;&#26684;&#26469;&#25512;&#33616;&#29289;&#21697;&#65292;&#20294;&#19981;&#32771;&#34385;&#29992;&#25143;&#23545;&#30005;&#24433;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#20013;&#33879;&#21517;&#20154;&#29289;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#65288;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#65289;&#29983;&#25104;&#23436;&#25972;&#30340;&#26381;&#35013;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#65288;&#20363;&#22914;&#20154;&#29289;&#20449;&#24687;&#65289;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#20934;&#30830;&#29702;&#35299;&#29992;&#25143;&#30340;&#21916;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#65288;&#20107;&#23454;&#25110;&#21453;&#20107;&#23454;&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;LLMs&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25972;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The outfit generation problem involves recommending a complete outfit to a user based on their interests. Existing approaches focus on recommending items based on anchor items or specific query styles but do not consider customer interests in famous characters from movie, social media, etc. In this paper, we define a new Character-based Outfit Generation (COG) problem, designed to accurately interpret character information and generate complete outfit sets according to customer specifications such as age and gender. To tackle this problem, we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences. Additionally, we incorporate text-to-image models to enhance the visual understanding and generation (factual or counterfactual) of cohesive outfits. Our framework integrates LLMs with text-to-image models 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20117;&#19979;&#29028;&#30719;&#30340;&#20260;&#23475;&#35760;&#24405;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#21457;&#29616;&#20851;&#38190;&#30340;&#22240;&#26524;&#20851;&#31995;&#21253;&#25324;&#39118;&#28304;&#21644;&#24037;&#20316;&#29366;&#24577;&#31561;&#19981;&#21516;&#22240;&#32032;&#20043;&#38388;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05940</link><description>&lt;p&gt;
&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#39118;&#38505;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Causal Relationship Network of Risk Factors Impacting Workday Loss in Underground Coal Mines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20117;&#19979;&#29028;&#30719;&#30340;&#20260;&#23475;&#35760;&#24405;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#21457;&#29616;&#20851;&#38190;&#30340;&#22240;&#26524;&#20851;&#31995;&#21253;&#25324;&#39118;&#28304;&#21644;&#24037;&#20316;&#29366;&#24577;&#31561;&#19981;&#21516;&#22240;&#32032;&#20043;&#38388;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#24314;&#31435;&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#20998;&#26512;&#21033;&#29992;&#20102;&#20174;&#22269;&#23478;&#32844;&#19994;&#23433;&#20840;&#19982;&#20581;&#24247;&#30740;&#31350;&#25152;&#65288;NIOSH&#65289;&#33719;&#24471;&#30340;&#25968;&#25454;&#12290;&#20174;NIOSH&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#20102;&#26469;&#33258;1990&#24180;&#33267;2020&#24180;&#30340;&#20849;&#35745;101,010&#20221;&#20260;&#23475;&#35760;&#24405;&#65292;&#28085;&#30422;&#20102;3,982&#20010;&#29420;&#31435;&#30340;&#20117;&#19979;&#29028;&#30719;&#12290;&#21033;&#29992;&#19968;&#31181;&#21517;&#20026;&#32676;&#32452;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#65288;GGES&#65289;&#30340;&#26032;&#39062;&#22240;&#26524;AI&#26041;&#27861;&#36827;&#34892;&#20102;&#22240;&#26524;&#20851;&#31995;&#30340;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;&#36890;&#36807;&#24178;&#39044;&#35745;&#31639;&#35843;&#25972;&#65288;IDA&#65289;&#24471;&#20998;&#23545;&#27599;&#20010;&#21464;&#37327;&#23545;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20351;&#29992;10&#25240;&#20132;&#21449;&#39564;&#35777;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#21033;&#29992;&#25509;&#37051;&#28857;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#12289;&#25509;&#37051;&#28857;&#21484;&#22238;&#29575;&#65288;AR&#65289;&#12289;&#31661;&#22836;&#22836;&#37096;&#31934;&#30830;&#24230;&#65288;AHP&#65289;&#21644;&#31661;&#22836;&#22836;&#37096;&#21484;&#22238;&#29575;&#65288;AHR&#65289;&#31561;&#24615;&#33021;&#25351;&#26631;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;2006&#24180;&#20043;&#21518;&#65292;&#20851;&#38190;&#30340;&#22240;&#26524;&#20851;&#31995;&#21253;&#25324;&#39118;&#28304;&#21644;&#24037;&#20316;&#29366;&#24577;&#31561;&#19981;&#21516;&#22240;&#32032;&#20043;&#38388;&#30340;&#20316;&#29992;&#26377;&#25152;     changed
&lt;/p&gt;
&lt;p&gt;
This study aims to establish the causal relationship network between various factors leading to workday loss in underground coal mines using a novel causal artificial intelligence (AI) method. The analysis utilizes data obtained from the National Institute for Occupational Safety and Health (NIOSH). A total of 101,010 injury records from 3,982 unique underground coal mines spanning the years from 1990 to 2020 were extracted from the NIOSH database. Causal relationships were analyzed and visualized using a novel causal AI method called Grouped Greedy Equivalence Search (GGES). The impact of each variable on workday loss was assessed through intervention do-calculus adjustment (IDA) scores. Model training and validation were performed using the 10-fold cross-validation technique. Performance metrics, including adjacency precision (AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall (AHR), were utilized to evaluate the models. Findings revealed that after 2006, key
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#27010;&#29575;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05939</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#27010;&#29575;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#20154;&#31867;&#29983;&#20135;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#21508;&#31181;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#36755;&#20986;&#19981;&#19968;&#33268;&#12290;&#23613;&#31649;&#20247;&#25152;&#21608;&#30693;&#65292;&#27010;&#29575;&#26041;&#27861;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#20272;&#35745;&#21487;&#20197;&#20943;&#36731;&#27492;&#31867;&#24433;&#21709;&#65292;&#20294;&#19982;&#20854;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#35821;&#35328;&#39046;&#22495;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#31181;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#30340;&#23454;&#38469;&#27169;&#24335;&#65292;&#24378;&#24230;&#21508;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;CodeLlama&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;CodeLlama&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#26657;&#20934;&#36136;&#37327;&#21644;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#19981;&#21516;&#26631;&#20934;&#19979;&#30340;&#24615;&#33021;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65288;BCI&#65289;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#21033;&#29992;&#22810;&#27493;&#39044;&#27979;&#26469;&#25552;&#20379;&#26657;&#20934;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21306;&#38388;&#12290;BCI&#22312;&#20219;&#24847;&#20998;&#24067;&#36716;&#25442;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#19979;&#23454;&#29616;&#20102;&#38271;&#26399;&#35206;&#30422;&#65292;&#19988;&#22312;&#27874;&#21160;&#29575;&#39044;&#27979;&#38382;&#39064;&#19978;&#29983;&#25104;&#26356;&#30701;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.05203</link><description>&lt;p&gt;
&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65306;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#39044;&#27979;&#21306;&#38388;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05203
&lt;/p&gt;
&lt;p&gt;
&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65288;BCI&#65289;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#21033;&#29992;&#22810;&#27493;&#39044;&#27979;&#26469;&#25552;&#20379;&#26657;&#20934;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21306;&#38388;&#12290;BCI&#22312;&#20219;&#24847;&#20998;&#24067;&#36716;&#25442;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#19979;&#23454;&#29616;&#20102;&#38271;&#26399;&#35206;&#30422;&#65292;&#19988;&#22312;&#27874;&#21160;&#29575;&#39044;&#27979;&#38382;&#39064;&#19978;&#29983;&#25104;&#26356;&#30701;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#23572;&#26364;&#31526;&#21512;&#25512;&#26029;&#65288;BCI&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22260;&#32469;&#20219;&#20309;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#26657;&#20934;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;BCI&#33021;&#22815;&#21033;&#29992;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35299;&#20915;&#19968;&#32500;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65288;SCP&#65289;&#26469;&#26174;&#24335;&#20248;&#21270;&#24179;&#22343;&#21306;&#38388;&#38271;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26469;&#25214;&#21040;SCP&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20219;&#24847;&#20998;&#24067;&#36716;&#25442;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#19979;&#65292;BCI&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#35206;&#30422;&#65292;&#21363;&#20351;&#22810;&#27493;&#39044;&#27979;&#36739;&#24046;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#21457;&#29616;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;BCI&#36991;&#20813;&#20102;&#26080;&#20449;&#24687;&#21306;&#38388;&#65288;&#38271;&#24230;&#26080;&#38480;&#65289;&#30340;&#29983;&#25104;&#65292;&#24182;&#22312;&#27874;&#21160;&#29575;&#39044;&#27979;&#38382;&#39064;&#19978;&#29983;&#25104;&#20102;&#26126;&#26174;&#26356;&#30701;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals. Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step. In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP. We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods.
&lt;/p&gt;</description></item><item><title>Moco&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.04915</link><description>&lt;p&gt;
Moco: &#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Moco: A Learnable Meta Optimizer for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04915
&lt;/p&gt;
&lt;p&gt;
Moco&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#36890;&#24120;&#26159;NP&#38590;&#30340;&#12290;&#36807;&#21435;&#65292;&#36825;&#20123;&#38382;&#39064;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#30340;&#65292;&#20294;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#26080;&#27861;&#36827;&#19968;&#27493;&#25913;&#36827;&#24050;&#32463;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Moco&#23398;&#20064;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#20174;&#24403;&#21069;&#25628;&#32034;&#29366;&#24577;&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#12290;&#36825;&#31181;&#20803;&#35757;&#32451;&#36807;&#31243;&#20197;&#25628;&#32034;&#36807;&#31243;&#20013;&#25214;&#21040;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#20026;&#30446;&#26631;&#65292;&#32473;&#23450;&#25628;&#32034;&#39044;&#31639;&#31561;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;Moco&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#12290;Moco&#26159;&#19968;&#20010;&#23436;&#20840;&#21487;&#23398;&#20064;&#30340;&#20803;&#20248;&#21270;&#22120;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#38382;&#39064;&#30340;&#23616;&#37096;&#25628;&#32034;&#25110;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#26368;&#22823;&#26368;&#23567;&#36153;&#29992;&#27969;&#38382;&#39064;&#20013;&#27979;&#35797;&#20102;Moco&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state. This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget. This allows Moco to adapt to varying circumstances such as different computational budgets. Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition. We test Moco on the Traveling Salesman Problem (TSP) and Maximum In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29468;&#24819;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25945;&#23398;&#32500;&#24230;&#30340;&#32467;&#26524;&#12290;&#35813;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#20915;&#20102;&#36229;&#31435;&#26041;&#20307;&#36793;&#30028;&#31561;&#21608;&#38382;&#39064;&#30340;&#23450;&#29702;&#30340;&#25512;&#24191;&#12290;</title><link>https://arxiv.org/abs/2402.04907</link><description>&lt;p&gt;
&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#32452;&#21512;&#38382;&#39064;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On a Combinatorial Problem Arising in Machine Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#35777;&#26126;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29468;&#24819;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25945;&#23398;&#32500;&#24230;&#30340;&#32467;&#26524;&#12290;&#35813;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#20915;&#20102;&#36229;&#31435;&#26041;&#20307;&#36793;&#30028;&#31561;&#21608;&#38382;&#39064;&#30340;&#23450;&#29702;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#27169;&#22411;&#65292;&#20854;&#20013;&#25945;&#24072;&#26144;&#23556;&#26159;&#30001;&#27010;&#24565;&#21644;&#31034;&#20363;&#30340;&#22823;&#23567;&#20989;&#25968;&#26500;&#24314;&#30340;&#12290;&#26426;&#22120;&#25945;&#23398;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#20219;&#20309;&#27010;&#24565;&#25152;&#38656;&#30340;&#26368;&#23567;&#31034;&#20363;&#25968;&#37327;&#65292;&#21363;&#25152;&#35859;&#30340;&#25945;&#23398;&#32500;&#24230;&#12290;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;[7]&#29468;&#27979;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#20316;&#20026;&#27010;&#24565;&#31867;&#22823;&#23567;&#30340;&#20989;&#25968;&#26102;&#65292;&#26368;&#22351;&#24773;&#20917;&#21457;&#29983;&#22312;&#19968;&#33268;&#24615;&#30697;&#38453;&#21253;&#21547;&#20174;&#38646;&#21450;&#20197;&#19978;&#30340;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#25968;&#23383;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20182;&#20204;&#30340;&#29468;&#24819;&#12290;&#35813;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#20915;&#36229;&#31435;&#26041;&#20307;&#30340;&#36793;&#30028;&#31561;&#21608;&#38382;&#39064;&#30340;&#23450;&#29702;[12]&#30340;&#25512;&#24191;&#65292;&#25105;&#20204;&#30340;&#35777;&#26126;&#22522;&#20110;[10]&#30340;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a model of machine teaching where the teacher mapping is constructed from a size function on both concepts and examples. The main question in machine teaching is the minimum number of examples needed for any concept, the so-called teaching dimension. A recent paper [7] conjectured that the worst case for this model, as a function of the size of the concept class, occurs when the consistency matrix contains the binary representations of numbers from zero and up. In this paper we prove their conjecture. The result can be seen as a generalization of a theorem resolving the edge isoperimetry problem for hypercubes [12], and our proof is based on a lemma of [10].
&lt;/p&gt;</description></item><item><title>NeRCC&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#25239;&#25302;&#23614;&#33410;&#28857;&#30340;&#36817;&#20284;&#32534;&#30721;&#35745;&#31639;&#26694;&#26550;&#65292;&#21253;&#25324;&#22238;&#24402;&#32534;&#30721;&#12289;&#35745;&#31639;&#21644;&#22238;&#24402;&#35299;&#30721;&#19977;&#20010;&#23618;&#27425;&#65292;&#36890;&#36807;&#20248;&#21270;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#35299;&#20915;&#23884;&#22871;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04377</link><description>&lt;p&gt;
$\texttt{NeRCC}$: &#20869;&#23884;&#22238;&#24402;&#32534;&#30721;&#35745;&#31639;&#29992;&#20110;&#20855;&#26377;&#24377;&#24615;&#30340;&#20998;&#24067;&#24335;&#39044;&#27979;&#26381;&#21153;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
$\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04377
&lt;/p&gt;
&lt;p&gt;
NeRCC&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#25239;&#25302;&#23614;&#33410;&#28857;&#30340;&#36817;&#20284;&#32534;&#30721;&#35745;&#31639;&#26694;&#26550;&#65292;&#21253;&#25324;&#22238;&#24402;&#32534;&#30721;&#12289;&#35745;&#31639;&#21644;&#22238;&#24402;&#35299;&#30721;&#19977;&#20010;&#23618;&#27425;&#65292;&#36890;&#36807;&#20248;&#21270;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#35299;&#20915;&#23884;&#22871;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25302;&#23614;&#33410;&#28857;(stragglers)&#26159;&#39044;&#27979;&#26381;&#21153;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#65292;&#20219;&#21153;&#26159;&#22312;&#39044;&#20808;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25191;&#34892;&#36755;&#20837;&#25968;&#25454;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeRCC&#30340;&#36890;&#29992;&#30340;&#25239;&#25302;&#23614;&#33410;&#28857;&#30340;&#36817;&#20284;&#32534;&#30721;&#35745;&#31639;&#26694;&#26550;&#12290;NeRCC&#21253;&#25324;&#19977;&#20010;&#23618;&#27425;&#65306;(1)&#22238;&#24402;&#32534;&#30721;&#21644;&#25277;&#26679;&#65292;&#29983;&#25104;&#32534;&#30721;&#25968;&#25454;&#28857;&#65292;&#20316;&#20026;&#21407;&#22987;&#25968;&#25454;&#28857;&#30340;&#32452;&#21512;&#65307;(2)&#35745;&#31639;&#65292;&#20854;&#20013;&#19968;&#20010;&#24037;&#20316;&#38598;&#32676;&#22312;&#32534;&#30721;&#25968;&#25454;&#28857;&#19978;&#36816;&#34892;&#25512;&#29702;&#65307;(3)&#22238;&#24402;&#35299;&#30721;&#21644;&#25277;&#26679;&#65292;&#20174;&#32534;&#30721;&#25968;&#25454;&#28857;&#30340;&#21487;&#29992;&#39044;&#27979;&#20013;&#36817;&#20284;&#24674;&#22797;&#20986;&#21407;&#22987;&#25968;&#25454;&#28857;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#26694;&#26550;&#30340;&#24635;&#20307;&#30446;&#26631;&#25581;&#31034;&#20102;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#20013;&#20004;&#20010;&#22238;&#24402;&#27169;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#23884;&#22871;&#22238;&#24402;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24635;&#32467;&#23427;&#20204;&#23545;&#20004;&#20010;&#32852;&#21512;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#39033;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model. In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing. NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points. We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers. We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized. Our extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#22122;&#22768;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#36861;&#36394;&#25972;&#25968;&#26694;&#26550;&#26469;&#29983;&#25104;&#21487;&#23450;&#21046;&#30340;&#22122;&#22768;&#25191;&#34892;&#36319;&#36394;&#12290;&#36890;&#36807;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#31639;&#27861;&#21487;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#22122;&#22768;&#30340;&#31867;&#22411;&#21644;&#24378;&#24230;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04004</link><description>&lt;p&gt;
&#20102;&#35299;&#31639;&#27861;&#24335;&#24605;&#32500;&#38142;&#20013;&#22122;&#22768;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#22122;&#22768;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#36861;&#36394;&#25972;&#25968;&#26694;&#26550;&#26469;&#29983;&#25104;&#21487;&#23450;&#21046;&#30340;&#22122;&#22768;&#25191;&#34892;&#36319;&#36394;&#12290;&#36890;&#36807;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#31639;&#27861;&#21487;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#22122;&#22768;&#30340;&#31867;&#22411;&#21644;&#24378;&#24230;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#20250;&#20351;&#29992;&#25968;&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#36136;&#37327;&#21508;&#24322;&#12290;&#22312;&#35757;&#32451;&#30340;&#20004;&#20010;&#38454;&#27573;&#20013;&#65292;&#36890;&#24120;&#20250;&#26681;&#25454;&#21551;&#21457;&#24335;&#26041;&#27861;&#36807;&#28388;&#25481;&#8220;&#20302;&#36136;&#37327;&#8221;&#25110;&#8220;&#26377;&#22122;&#22768;&#8221;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#28982;&#32780;&#24456;&#23569;&#26377;&#20154;&#37327;&#21270;&#22320;&#20102;&#35299;&#22122;&#22768;&#30340;&#31867;&#22411;&#25110;&#24378;&#24230;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#20013;&#30340;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22312;&#31639;&#27861;&#21487;&#35299;&#20219;&#21153;&#30340;&#39640;&#24230;&#25511;&#21046;&#29615;&#22659;&#19979;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36861;&#36394;&#25972;&#25968;&#65288;TInt&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#20219;&#24847;&#25972;&#25968;&#21015;&#34920;&#19978;&#30340;&#31639;&#26415;&#20989;&#25968;&#29983;&#25104;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#22122;&#22768;&#25191;&#34892;&#36319;&#36394;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#65306;&#23616;&#37096;&#24418;&#24335;&#30340;&#38745;&#24577;&#22122;&#22768;&#65292;&#22312;&#35745;&#31639;CoT&#36319;&#36394;&#21518;&#24212;&#29992;&#65307;&#20197;&#21450;&#20840;&#23616;&#24418;&#24335;&#30340;&#21160;&#24577;&#22122;&#22768;&#65292;&#22312;&#35745;&#31639;&#20013;&#20256;&#25773;&#36319;&#36394;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27979;&#35797;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
During both pretraining and fine-tuning, Large Language Models (\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.03646</link><description>&lt;p&gt;
Lens: &#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lens: A Foundation Model for Network Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03646
&lt;/p&gt;
&lt;p&gt;
"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#37327;&#26159;&#25351;&#36890;&#36807;&#20114;&#32852;&#32593;&#25110;&#36830;&#25509;&#35745;&#31639;&#26426;&#30340;&#20219;&#20309;&#31995;&#32479;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#20449;&#24687;&#37327;&#12290;&#20998;&#26512;&#21644;&#29702;&#35299;&#32593;&#32476;&#27969;&#37327;&#23545;&#20110;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#21253;&#30340;&#29305;&#27530;&#29305;&#24615;&#65292;&#22914;&#24322;&#26500;&#26631;&#22836;&#21644;&#32570;&#20047;&#35821;&#20041;&#30340;&#21152;&#23494;&#36127;&#36733;&#65292;&#32593;&#32476;&#27969;&#37327;&#30340;&#20998;&#26512;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25429;&#25417;&#27969;&#37327;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#27969;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#22312;&#27969;&#37327;&#29702;&#35299;&#65288;&#20998;&#31867;&#65289;&#25110;&#27969;&#37327;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Lens&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#21033;&#29992;T5&#26550;&#26500;&#20174;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#39044;&#35757;&#32451;&#34920;&#31034;&#12290;&#20511;&#21161;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#27969;&#37327;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the glob
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03256</link><description>&lt;p&gt;
&#23398;&#20064;Predict-then-Optimize&#26694;&#26550;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Best-in-Class Policies for the Predict-then-Optimize Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03256
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#31216;&#20026;Perturbation Gradient&#65288;PG&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#12290;&#36825;&#20123;&#25439;&#22833;&#30452;&#25509;&#36817;&#20284;&#20102;&#19979;&#28216;&#20915;&#31574;&#25439;&#22833;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#26367;&#20195;&#25439;&#22833;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#30340;&#36817;&#20284;&#35823;&#24046;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#28040;&#22833;&#12290;&#36825;&#24847;&#21619;&#30528;&#20248;&#21270;&#25105;&#20204;&#30340;&#26367;&#20195;&#25439;&#22833;&#21487;&#20197;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#24471;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#35823;&#35774;&#32622;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#24403;&#22522;&#30784;&#27169;&#22411;&#35823;&#35774;&#32622;&#19988;&#22122;&#22768;&#19981;&#26159;&#20013;&#24515;&#23545;&#31216;&#26102;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#26696;&#12290;&#37492;&#20110;&#22312;&#23454;&#36341;&#20013;&#35823;&#35774;&#32622;&#24456;&#24120;&#35265;--&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#21487;&#33021;&#26356;&#21916;&#27426;&#19968;&#20010;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;--PG&#25439;&#22833;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#29702;&#35770;&#19978;&#26377;&#20381;&#25454;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#20915;&#31574;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02304</link><description>&lt;p&gt;
&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20174;&#22320;&#38663;&#24314;&#27169;&#21040;&#21307;&#23398;&#25104;&#20687;&#65292;&#23545;&#20110;&#39640;&#39057;&#27874;&#20256;&#25773;&#30340;&#39640;&#20445;&#30495;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#27874;&#20256;&#25773;&#27169;&#22411;&#20013;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#36275;&#22815;&#20934;&#30830;&#30340;&#32454;&#27714;&#35299;&#22120;&#36755;&#20986;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#31895;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#31283;&#23450;&#19988;&#24555;&#36895;&#30340;&#27714;&#35299;&#22120;&#36824;&#20801;&#35768;&#20351;&#29992;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;Parareal&#26469;&#25552;&#21462;&#21644;&#32416;&#27491;&#39640;&#39057;&#27874;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;Nguyen&#21644;Tsai&#65288;2023&#65289;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#31995;&#32479;&#65292;&#23558;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#26694;&#26550;&#20013;&#12290;&#22312;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#21644;Parareal&#26041;&#26696;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#21327;&#35843;&#30340;&#32467;&#26500;&#22312;&#19981;&#29306;&#29298;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.01703</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#24220;&#23448;&#21592;&#19982;&#24066;&#27665;&#20043;&#38388;&#30340;&#20114;&#21160;&#24433;&#21709;&#20844;&#20849;&#31119;&#31049;&#21644;&#27665;&#20027;&#31038;&#20250;&#30340;&#27491;&#24403;&#24615;&#12290;&#35686;&#23519;&#26159;&#22269;&#23478;&#26368;&#26174;&#32780;&#26131;&#35265;&#12289;&#26368;&#25509;&#35302;&#24066;&#27665;&#30340;&#20195;&#29702;&#20154;&#65292;&#22312;&#20132;&#36890;&#31449;&#20572;&#26399;&#38388;&#65292;&#20182;&#20204;&#27599;&#24180;&#19982;&#20844;&#20247;&#20114;&#21160;&#36229;&#36807;2000&#19975;&#27425;&#12290;&#22914;&#20170;&#65292;&#36825;&#20123;&#20114;&#21160;&#32463;&#24120;&#34987;&#25140;&#22312;&#36523;&#19978;&#30340;&#25668;&#20687;&#26426;&#35760;&#24405;&#19979;&#26469;&#65292;&#36825;&#34987;&#35270;&#20026;&#25552;&#39640;&#35686;&#23519;&#38382;&#36131;&#21046;&#21644;&#25913;&#21892;&#35686;&#27665;&#20114;&#21160;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#20998;&#26512;&#36825;&#20123;&#22797;&#26434;&#32780;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#65292;&#36825;&#20123;&#35760;&#24405;&#30340;&#21450;&#26102;&#20998;&#26512;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35282;&#24230;&#12289;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#26469;&#33258;&#36825;&#20123;&#36523;&#19978;&#25668;&#20687;&#26426;&#35760;&#24405;&#30340;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#30830;&#23450;&#19982;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#26368;&#30456;&#20851;&#30340;&#27807;&#36890;&#26041;&#38754;&#65292;&#21253;&#25324;&#20849;&#21516;&#24863;&#30693;&#20114;&#21160;&#30340;&#26631;&#24535;&#26631;&#35760;&#20197;&#21450;&#20855;&#26377;&#36825;&#20123;&#26631;&#35760;&#30340;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2401.17350</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#20379;&#24212;&#21830;&#20998;&#37197;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17350
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BL&#27169;&#22411;&#21644;Perspective&#30697;&#38453;&#65292;&#20197;&#20248;&#21270;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#37325;&#28857;&#20851;&#27880;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#20379;&#24212;&#21830;&#20851;&#31995;&#32593;&#32476;&#65292;&#22686;&#24378;&#20102;&#23545;&#22797;&#26434;&#20379;&#24212;&#21830;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;Masked Ranking&#26426;&#21046;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20379;&#24212;&#21830;&#25490;&#24207;&#25928;&#29575;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;DBLM&#22312;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#21644;&#31934;&#30830;&#32622;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#20998;&#36776;&#29575;&#24773;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the BL model and the Perspective Matrix to optimize supplier selection and order allocation, focusing on both temporal and spatial dynamics. Our development of a Supplier Relationship Network, using a Spatio-Temporal Graph Neural Network, enhances the understanding of complex supplier interdependencies. Additionally, we address credibility issues in zero-order scenarios with a Masked Ranking Mechanism, improving supplier ranking efficiency. Our model demonstrates superior results on two datasets compared to the traditional models. Our evaluations using real-world datasets highlight DBLM's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.15122</link><description>&lt;p&gt;
&#25193;&#23637;&#23601;&#26159;&#19968;&#20999;&#65306;&#20351;&#29992;JAX&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#22797;&#26434;&#39046;&#22495;&#22914;&#35270;&#39057;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#36229;&#36234;&#26368;&#20248;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#36816;&#34892;&#24517;&#35201;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#38750;&#24120;&#22256;&#38590;&#12290;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24182;&#22312;&#22810;&#20010;GPU&#19978;&#36827;&#34892;&#20998;&#24067;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#30495;&#23454;&#19990;&#30028;&#36710;&#36742;&#19978;&#25910;&#38598;&#32463;&#39564;&#20174;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#19988;&#30495;&#23454;&#30340;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#22823;&#37327;&#26469;&#33258;&#30495;&#23454;&#39550;&#39542;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#33021;&#21147;&#38598;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#29616;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#25105;&#20204;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#27604;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#21512;&#24182;&#65288;MAM&#65289;&#20351;&#29992;&#38646;-shot&#33539;&#24335;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#30340;&#27169;&#22411;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#30452;&#25509;&#30693;&#35782;&#20256;&#36755;&#21040;&#38899;&#39057;&#39046;&#22495;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#35789;&#38169;&#35823;&#29575;&#21644;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;&#30340;&#20998;&#31867;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2312.14378</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#21512;&#24182;&#29992;&#20110;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#21644;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multimodal Attention Merging for Improved Speech Recognition and Audio Event Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14378
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#21512;&#24182;&#65288;MAM&#65289;&#20351;&#29992;&#38646;-shot&#33539;&#24335;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#30340;&#27169;&#22411;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#30452;&#25509;&#30693;&#35782;&#20256;&#36755;&#21040;&#38899;&#39057;&#39046;&#22495;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#35789;&#38169;&#35823;&#29575;&#21644;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;&#30340;&#20998;&#31867;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#24494;&#35843;&#24050;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#30340;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#32463;&#24120;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#24494;&#35843;&#35745;&#31639;&#36164;&#28304;&#21644;&#26631;&#35760;&#19979;&#28216;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#21512;&#24182;&#65288;MAM&#65289;&#65292;&#35797;&#22270;&#36890;&#36807;&#38646;-shot&#33539;&#24335;&#23558;&#39640;&#36164;&#28304;&#27169;&#24577;&#65288;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#20013;&#27169;&#22411;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#30452;&#25509;&#30693;&#35782;&#20256;&#36755;&#21040;&#36164;&#28304;&#21463;&#38480;&#39046;&#22495;&#65288;&#35821;&#38899;&#21644;&#38899;&#39057;&#65289;&#20013;&#12290;MAM&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;&#26368;&#22810;6.70&#65285;&#65292;&#23558;&#38899;&#39057;&#20107;&#20214;&#20998;&#31867;&#65288;AEC&#65289;&#27169;&#22411;&#30340;&#30456;&#23545;&#20998;&#31867;&#38169;&#35823;&#38477;&#20302;&#20102;10.63&#65285;&#12290;&#22312;&#19968;&#20123;&#25968;&#25454;/&#35745;&#31639;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#23398;&#20064;&#30340;MAM&#65292;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#21512;&#24182;&#27880;&#24847;&#21147;&#30697;&#38453;&#65292;&#20351;ASR&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;&#36827;&#19968;&#27493;&#30340;2.90&#65285;&#65292;&#32780;AEC&#30340;&#30456;&#23545;&#38477;&#20302;&#20102;18.42&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large foundation models using self-supervised objectives on unlabeled data, followed by fine-tuning on downstream tasks, has emerged as a standard procedure. Unfortunately, the efficacy of this approach is often constrained by both limited fine-tuning compute and scarcity in labeled downstream data. We introduce Multimodal Attention Merging (MAM), an attempt that facilitates direct knowledge transfer from attention matrices of models rooted in high resource modalities, text and images, to those in resource-constrained domains, speech and audio, employing a zero-shot paradigm. MAM reduces the relative Word Error Rate (WER) of an Automatic Speech Recognition (ASR) model by up to 6.70%, and relative classification error of an Audio Event Classification (AEC) model by 10.63%. In cases where some data/compute is available, we present Learnable-MAM, a data-driven approach to merging attention matrices, resulting in a further 2.90% relative reduction in WER for ASR and 18.42% relativ
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#24067;&#28418;&#31227;&#22522;&#20934;&#27979;&#35797;TableShift&#65292;&#21253;&#21547;15&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#20998;&#24067;&#28418;&#31227;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#36807;Python&#20195;&#30721;&#21487;&#20197;&#36731;&#26494;&#35775;&#38382;&#12290;</title><link>https://arxiv.org/abs/2312.07577</link><description>&lt;p&gt;
&#20351;&#29992;TableShift&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#24067;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Distribution Shift in Tabular Data with TableShift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#24067;&#28418;&#31227;&#22522;&#20934;&#27979;&#35797;TableShift&#65292;&#21253;&#21547;15&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#20998;&#24067;&#28418;&#31227;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#36807;Python&#20195;&#30721;&#21487;&#20197;&#36731;&#26494;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#22411;&#20174;&#30740;&#31350;&#23545;&#35937;&#36716;&#21521;&#23454;&#38469;&#37096;&#32626;&#65292;&#23545;&#20110;&#20998;&#24067;&#28418;&#31227;&#30340;&#40065;&#26834;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34920;&#26684;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#20294;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#24067;&#28418;&#31227;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#27979;&#35797;&#20173;&#28982;&#32570;&#20047;&#65292;&#32780;&#19988;&#19982;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#24046;&#24322;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#34920;&#26684;&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#28418;&#31227;&#26102;&#30340;&#40065;&#26834;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TableShift&#65292;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#24067;&#28418;&#31227;&#22522;&#20934;&#27979;&#35797;&#12290;TableShift&#24635;&#20849;&#21253;&#21547;15&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#30456;&#24212;&#30340;&#20998;&#24067;&#28418;&#31227;&#65292;&#24182;&#21253;&#21547;&#20102;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26469;&#28304;&#12289;&#39044;&#27979;&#30446;&#26631;&#21644;&#20998;&#24067;&#28418;&#31227;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;&#37329;&#34701;&#12289;&#25945;&#32946;&#12289;&#20844;&#20849;&#25919;&#31574;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#27665;&#21442;&#19982;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20165;&#20960;&#34892;Python&#20195;&#30721;&#30340;TableShift API&#36827;&#34892;&#35775;&#38382;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;f&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to distribution shift has become a growing concern for text and image models as they transition from research subjects to deployment in the real world. However, high-quality benchmarks for distribution shift in tabular machine learning tasks are still lacking despite the widespread real-world use of tabular data and differences in the models used for tabular data in comparison to text and images. As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TableShift, a distribution shift benchmark for tabular data. TableShift contains 15 binary classification tasks in total, each with an associated shift, and includes a diverse set of data sources, prediction targets, and distribution shifts. The benchmark covers domains including finance, education, public policy, healthcare, and civic participation, and is accessible using only a few lines of Python code via the TableShift API. We conduct a large-scale 
&lt;/p&gt;</description></item><item><title>LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.17943</link><description>&lt;p&gt;
LayerCollapse: &#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LayerCollapse: Adaptive compression of neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17943
&lt;/p&gt;
&lt;p&gt;
LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#19981;&#26029;&#22686;&#38271;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36229;&#21442;&#25968;&#21270;&#30340;Transformer&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#19994;&#32489;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#25216;&#26415;&#12290;&#36825;&#20123;&#27169;&#22411;&#21547;&#26377;&#25968;&#20159;&#20010;&#21442;&#25968;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayerCollapse&#65292;&#19968;&#31181;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#24418;&#24335;&#65292;&#29992;&#20110;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20801;&#35768;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#21518;&#21387;&#32553;&#65292;&#24182;&#23545;&#24615;&#33021;&#20135;&#29983;&#26377;&#38480;&#30340;&#24433;&#21709;&#12290;LayerCollapse&#36890;&#36807;&#23545;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#30340;&#28608;&#27963;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#35843;&#33410;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#23558;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#38477;&#20302;&#21040;&#30456;&#24212;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;LayerCollapse&#30340;&#21387;&#32553;&#33021;&#21147;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#30697;&#38453;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#23545;&#20110;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16609</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#30340;&#29305;&#24449;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Eigenmatrix for unstructured sparse recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#30697;&#38453;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#23545;&#20110;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#33324;&#24418;&#24335;&#30340;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#21253;&#25324;&#26377;&#29702;&#36924;&#36817;&#12289;&#35889;&#20989;&#25968;&#20272;&#35745;&#12289;&#20613;&#37324;&#21494;&#21453;&#28436;&#12289;&#25289;&#26222;&#25289;&#26031;&#21453;&#28436;&#21644;&#31232;&#30095;&#21453;&#21367;&#31215;&#31561;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#30697;&#38453;&#65292;&#19968;&#31181;&#20855;&#26377;&#25152;&#38656;&#36817;&#20284;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#20026;&#36825;&#20123;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the unstructured sparse recovery problems in a general form. Examples include rational approximation, spectral function estimation, Fourier inversion, Laplace inversion, and sparse deconvolution. The main challenges are the noise in the sample values and the unstructured nature of the sample locations. This paper proposes the eigenmatrix, a data-driven construction with desired approximate eigenvalues and eigenvectors. The eigenmatrix offers a new way for these sparse recovery problems. Numerical results are provided to demonstrate the efficiency of the proposed method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65288;POMP&#65289;&#65292;&#22312;&#38598;&#25104;&#31243;&#24207;&#21512;&#25104;&#21644;&#29366;&#24577;&#26426;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#20219;&#21153;&#24182;&#34920;&#31034;&#22797;&#26434;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2311.15960</link><description>&lt;p&gt;
&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65306;&#36890;&#36807;&#38598;&#25104;&#31243;&#24207;&#21512;&#25104;&#21644;&#29366;&#24577;&#26426;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Program Machine Policy: Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15960
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65288;POMP&#65289;&#65292;&#22312;&#38598;&#25104;&#31243;&#24207;&#21512;&#25104;&#21644;&#29366;&#24577;&#26426;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#20219;&#21153;&#24182;&#34920;&#31034;&#22797;&#26434;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32534;&#31243;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#23558;&#20854;&#35270;&#20026;&#21512;&#25104;&#21487;&#35299;&#37322;&#30340;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23616;&#38480;&#20110;&#30701;&#26399;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#29366;&#24577;&#26426;&#34920;&#31034;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21487;&#20197;&#24402;&#32435;&#25512;&#24191;&#21040;&#38271;&#26399;&#20219;&#21153;&#65307;&#28982;&#32780;&#65292;&#23427;&#22312;&#33719;&#21462;&#22810;&#26679;&#21644;&#22797;&#26434;&#34892;&#20026;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65288;POMP&#65289;&#65292;&#20197;&#26725;&#25509;&#32534;&#31243;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#29366;&#24577;&#26426;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#20801;&#35768;&#34920;&#31034;&#22797;&#26434;&#34892;&#20026;&#24182;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#32034;&#19968;&#32452;&#26377;&#25928;&#12289;&#22810;&#26679;&#19988;&#20860;&#23481;&#30340;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31243;&#24207;&#29992;&#20316;&#29366;&#24577;&#26426;&#30340;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#36716;&#31227;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (deep RL) excels in various domains but lacks generalizability and interpretability. On the other hand, programmatic RL methods (Trivedi et al., 2021; Liu et al., 2023) reformulate RL tasks as synthesizing interpretable programs that can be executed in the environments. Despite encouraging results, these methods are limited to short-horizon tasks. On the other hand, representing RL policies using state machines (Inala et al., 2020) can inductively generalize to long-horizon tasks; however, it struggles to scale up to acquire diverse and complex behaviors. This work proposes the Program Machine Policy (POMP), which bridges the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, and compatible programs. Then, we use these programs as modes of a state machine and learn a transition func
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;Pareto Set Learning (PSL)&#26041;&#27861;&#65292;&#36890;&#36807;&#28909;&#21551;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#25511;&#30340;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;&#26469;&#35299;&#20915;&#29616;&#26377;PSL&#26041;&#27861;&#19981;&#31283;&#23450;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;MOO&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.15297</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#39640;&#26114;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;&#28909;&#21551;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Controllable Expensive Multi-objective Learning with Warm-starting Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15297
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;Pareto Set Learning (PSL)&#26041;&#27861;&#65292;&#36890;&#36807;&#28909;&#21551;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#25511;&#30340;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;&#26469;&#35299;&#20915;&#29616;&#26377;PSL&#26041;&#27861;&#19981;&#31283;&#23450;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;MOO&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pareto Set Learning (PSL)&#26159;&#19968;&#31181;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#38382;&#39064;&#20013;&#36817;&#20284;&#25972;&#20010;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26080;&#23548;&#25968;PSL&#26041;&#27861;&#36890;&#24120;&#19981;&#31283;&#23450;&#19988;&#20302;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26114;&#36149;&#30340;&#40657;&#31665;MOO&#38382;&#39064;&#65292;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25511;PSL&#26041;&#27861;&#65292;&#31216;&#20026;Co-PSL&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;PSL&#26041;&#27861;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#20302;&#25928;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;Co-PSL&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;(1)&#28909;&#21551;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20197;&#33719;&#24471;&#36136;&#37327;&#39640;&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;(2)&#21487;&#25511;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;&#65292;&#20197;&#20934;&#30830;&#33719;&#21462;&#20174;&#20559;&#22909;&#21040;&#30456;&#24212;&#24085;&#32047;&#25176;&#35299;&#30340;&#21442;&#25968;&#26144;&#23556;&#12290;&#21069;&#32773;&#26377;&#21161;&#20110;&#31283;&#23450;PSL&#36807;&#31243;&#24182;&#20943;&#23569;&#26114;&#36149;&#20989;&#25968;&#35780;&#20272;&#30340;&#25968;&#37327;&#12290;&#21518;&#32773;&#25903;&#25345;&#20914;&#31361;&#30446;&#26631;&#20043;&#38388;&#30340;&#23454;&#26102;&#26435;&#34913;&#25511;&#21046;&#12290;&#36890;&#36807;&#21512;&#25104;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;MOO&#38382;&#39064;&#30340;&#24615;&#33021;&#23637;&#31034;&#20102;Co-PSL&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto Set Learning (PSL) is a promising approach for approximating the entire Pareto front in multi-objective optimization (MOO) problems. However, existing derivative-free PSL methods are often unstable and inefficient, especially for expensive black-box MOO problems where objective function evaluations are costly. In this work, we propose to address the instability and inefficiency of existing PSL methods with a novel controllable PSL method, called Co-PSL. Particularly, Co-PSL consists of two stages: (1) warm-starting Bayesian optimization to obtain quality Gaussian Processes priors and (2) controllable Pareto set learning to accurately acquire a parametric mapping from preferences to the corresponding Pareto solutions. The former is to help stabilize the PSL process and reduce the number of expensive function evaluations. The latter is to support real-time trade-off control between conflicting objectives. Performances across synthesis and real-world MOO problems showcase the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.12944</link><description>&lt;p&gt;
DroneOptiNet: &#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20116;&#20195;&#21450;&#20854;&#21518;&#30340;&#34562;&#31389;&#32593;&#32476;&#23545;&#21151;&#29575;&#38656;&#27714;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#33021;&#28304;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#65288;BS&#65289;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#30340;&#29992;&#25143;&#36127;&#36733;&#36716;&#31227;&#26041;&#27861;&#65292;&#20197;&#36328;&#36234;&#30001;&#32511;&#33394;&#23567;&#22411;&#34562;&#31389;BS&#32452;&#25104;&#30340;&#24494;&#32593;&#32593;&#32476;&#12290;&#26681;&#25454;&#29992;&#25143;&#23494;&#24230;&#21644;&#31354;&#20013;&#22522;&#31449;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#31354;&#20013;&#22522;&#31449;&#20174;&#39640;&#33021;&#32791;&#23567;&#21306;&#36801;&#31227;&#21040;&#20302;&#33021;&#32791;&#23567;&#21306;&#65292;&#26469;&#28385;&#36275;&#33021;&#37327;&#19981;&#36275;&#30340;&#23567;&#21306;&#30340;&#33021;&#37327;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#26080;&#20154;&#26426;&#26694;&#26550;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#19982;&#29420;&#29305;&#30340;&#25104;&#26412;&#20989;&#25968;&#32467;&#21512;&#65292;&#20351;&#29992;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#31649;&#29702;&#26080;&#20154;&#26426;&#21644;&#22522;&#31449;&#30340;&#33021;&#37327;&#21644;&#36127;&#36733;&#37325;&#20998;&#37197;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20943;&#23569;&#20102;&#22522;&#31449;&#20572;&#30005;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#21534;&#21520;&#37327;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20854;&#25552;&#21319;&#26080;&#32447;&#32593;&#32476;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The power requirements posed by the fifth-generation and beyond cellular networks are an important constraint in network deployment and require energy-efficient solutions. In this work, we propose a novel user load transfer approach using airborne base stations (BS) mounted on drones for reliable and secure power redistribution across the micro-grid network comprising green small cell BSs. Depending on the user density and the availability of an aerial BS, the energy requirement of a cell with an energy deficit is accommodated by migrating the aerial BS from a high-energy to a low-energy cell. The proposed hybrid drone-based framework integrates long short-term memory with unique cost functions using an evolutionary neural network for drones and BSs and efficiently manages energy and load redistribution. The proposed algorithm reduces power outages at BSs and maintains consistent throughput stability, thereby demonstrating its capability to boost the reliability and robustness of wirel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#24555;&#36895;&#20056;&#27861;&#26041;&#27861;&#65292;&#22312;&#29305;&#23450;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;&#35813;&#26041;&#27861;&#25226;&#25968;&#23383;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2311.09922</link><description>&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#25972;&#25968;&#21015;&#34920;&#20316;&#20026;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#30340;&#38598;&#21512;&#26469;&#23454;&#29616;&#24555;&#36895;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast multiplication by two's complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#24555;&#36895;&#20056;&#27861;&#26041;&#27861;&#65292;&#22312;&#29305;&#23450;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;&#35813;&#26041;&#27861;&#25226;&#25968;&#23383;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25972;&#25968;&#21015;&#34920;&#34920;&#31034;&#30340;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#20056;&#27861;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;python&#20195;&#30721;&#23454;&#29616;&#20102;&#19968;&#32452;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#26576;&#19968;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#25968;&#35770;&#21464;&#25442;(NTT)&#21644;&#21345;&#25289;&#33576;&#24052;(Karatsuba)&#20056;&#27861;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#29992;python&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#65292;&#19982;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25972;&#25968;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#25972;&#25968;&#25110;&#23454;&#25968;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#34920;&#31034;&#20108;&#36827;&#21046;&#20013;&#30340;&#26377;&#38480;&#32423;&#25968;&#12290;&#35813;&#25968;&#23383;&#30340;&#25972;&#25968;&#32034;&#24341;&#26377;&#38480;&#32423;&#25968;&#21487;&#20197;&#23384;&#20648;&#21644;&#20998;&#24067;&#22312;&#22810;&#20010;CPU / GPU&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#27861;&#21644;&#20056;&#27861;&#36816;&#31639;&#21487;&#20197;&#24212;&#29992;&#20110;&#20316;&#20026;&#32034;&#24341;&#25972;&#25968;&#34920;&#31034;&#30340;&#20004;&#20010;&#34917;&#30721;&#21152;&#27861;&#65292;&#24182;&#21487;&#20197;&#23436;&#20840;&#20998;&#24067;&#22312;&#32473;&#23450;&#30340;CPU / GPU&#26550;&#26500;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23436;&#20840;&#30340;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distribute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21355;&#26143;&#22270;&#20687;&#65292;&#20197;&#35299;&#37322;&#21644;&#35780;&#20272;&#24418;&#25104;&#33258;&#28982;&#24230;&#30340;&#27169;&#24335;&#65292;&#24182;&#24102;&#26377;&#30456;&#24212;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2311.08936</link><description>&lt;p&gt;
&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#65306;&#35299;&#37322;&#21644;&#35780;&#20272;&#24418;&#25104;&#33258;&#28982;&#24230;&#30340;&#27169;&#24335;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21355;&#26143;&#22270;&#20687;&#65292;&#20197;&#35299;&#37322;&#21644;&#35780;&#20272;&#24418;&#25104;&#33258;&#28982;&#24230;&#30340;&#27169;&#24335;&#65292;&#24182;&#24102;&#26377;&#30456;&#24212;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20154;&#31867;&#27963;&#21160;&#65288;&#22914;&#22478;&#24066;&#21270;&#12289;&#20892;&#19994;&#21644;&#20854;&#20182;&#20154;&#20026;&#24178;&#39044;&#65289;&#26368;&#23567;&#24433;&#21709;&#30340;&#33258;&#28982;&#20445;&#25252;&#21306;&#26159;&#25351;&#37027;&#20123;&#22320;&#21306;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#32472;&#21046;&#36825;&#20123;&#21306;&#22495;&#30340;&#33258;&#28982;&#24230;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#21355;&#26143;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25581;&#31034;&#36825;&#20123;&#20445;&#25252;&#29615;&#22659;&#20013;&#26377;&#21161;&#20110;&#33258;&#28982;&#24230;&#27010;&#24565;&#30340;&#27169;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35299;&#20915;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#23545;&#20110;&#20840;&#38754;&#20102;&#35299;&#36825;&#20010;&#27010;&#24565;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20182;&#20204;&#35201;&#20040;&#26080;&#27861;&#25552;&#20379;&#26082;&#26377;&#25928;&#21448;&#23458;&#35266;&#30340;&#35299;&#37322;&#65292;&#35201;&#20040;&#38590;&#20197;&#25552;&#20379;&#20934;&#30830;&#34913;&#37327;&#29305;&#23450;&#27169;&#24335;&#23545;&#33258;&#28982;&#24230;&#36129;&#29486;&#30340;&#23450;&#37327;&#25351;&#26631;&#20197;&#21450;&#30456;&#20851;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;(FWC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#65292;&#29983;&#25104;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.05436</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#20844;&#24179;&#30340;&#26680;&#24515;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fair Coresets via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;(FWC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#65292;&#29983;&#25104;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31934;&#28860;&#21644;&#26680;&#24515;&#38598;&#24050;&#25104;&#20026;&#29983;&#25104;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#30340;&#36739;&#23567;&#20195;&#34920;&#24615;&#26679;&#26412;&#38598;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#31038;&#20250;&#23618;&#38754;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20351;&#24471;&#27169;&#22411;&#26500;&#24314;&#32773;&#24517;&#39035;&#35299;&#20915;&#23384;&#22312;&#20110;&#25968;&#25454;&#20013;&#30340;&#23376;&#32676;&#20307;&#30340;&#22266;&#26377;&#20559;&#35265;&#38382;&#39064;&#12290;&#24403;&#21069;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#30456;&#23545;&#20110;&#21407;&#22987;&#26679;&#26412;&#30340;&#23616;&#37096;&#23646;&#24615;&#26469;&#21019;&#24314;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#20294;&#20854;&#23545;&#19979;&#28216;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;&#65288;FWC&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#24515;&#38598;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#26082;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21448;&#20855;&#26377;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#32423;&#26435;&#37325;&#12290;FWC&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21516;&#26102;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FWC&#30340;&#26080;&#32422;&#26463;&#29256;&#26412;&#31561;&#20215;&#20110;&#36890;&#24120;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FWC&#30340;&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. Current approaches create fair synthetic representative samples by optimizing local properties relative to the original samples, but their effect on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC minimizes the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equiv
&lt;/p&gt;</description></item><item><title>LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2310.14894</link><description>&lt;p&gt;
&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;-- &#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#20855;&#26377;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14894
&lt;/p&gt;
&lt;p&gt;
LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#36817;&#24180;&#26469;&#26368;&#34987;&#24191;&#27867;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20043;&#19968;&#12290;&#23427;&#20063;&#26159;&#26368;&#20998;&#25955;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#19987;&#27880;&#20110;&#35299;&#37322;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#20351;&#24471;&#19968;&#27425;&#24615;&#20197;&#32039;&#20945;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#33719;&#24471;&#23436;&#25972;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#12290;&#23427;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#20801;&#35768;&#26012;&#20132;&#21644;&#38598;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;XAI&#26041;&#27861;&#65292;&#22914;SHAP&#25110;LIME&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21453;&#65292;&#23427;&#19981;&#20351;&#29992;&#25968;&#25454;&#29983;&#25104;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36873;&#25321;&#20197;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#20986;&#29616;&#30340;&#30495;&#23454;&#25968;&#25454;&#30340;&#23616;&#37096;&#27010;&#24565;&#65292;&#36825;&#20123;&#23616;&#37096;&#27010;&#24565;&#23545;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#24418;&#25104;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#20854;&#20013;&#20851;&#27880;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#35299;&#37322;&#24615;&#21644;&#20943;&#23567;&#19981;&#30830;&#23450;&#24615;&#30340;&#30446;&#26631;&#12290;&#25552;&#20986;&#30340;CODE&#31639;&#27861;&#36890;&#36807;&#22312;&#31526;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#25152;&#26377;&#21487;&#33021;&#25805;&#20316;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#31243;&#24230;&#30340;&#35299;&#37322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#20943;&#23567;&#12290;</title><link>https://arxiv.org/abs/2310.14751</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient and Interpretable Bandit Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14751
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#20854;&#20013;&#20851;&#27880;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#35299;&#37322;&#24615;&#21644;&#20943;&#23567;&#19981;&#30830;&#23450;&#24615;&#30340;&#30446;&#26631;&#12290;&#25552;&#20986;&#30340;CODE&#31639;&#27861;&#36890;&#36807;&#22312;&#31526;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#25152;&#26377;&#21487;&#33021;&#25805;&#20316;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#31243;&#24230;&#30340;&#35299;&#37322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#36172;&#21338;&#31639;&#27861;&#12290;&#22914;&#26524;&#19968;&#20010;&#36172;&#21338;&#31639;&#27861;&#25506;&#32034;&#30340;&#30446;&#26631;&#26159;&#20943;&#23567;&#26410;&#30693;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#37027;&#20040;&#23427;&#23601;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#34913;&#37327;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#35823;&#24046;&#24230;&#37327;&#65292;&#23545;&#27604;&#20102;&#25152;&#26377;&#21487;&#33021;&#25805;&#20316;&#30340;&#24179;&#22343;&#22870;&#21169;&#20272;&#35745;&#30340;&#20943;&#23569;&#29575;&#19982;&#23427;&#20204;&#30340;&#23454;&#38469;&#22343;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#26368;&#20248;&#35774;&#35745;&#30340;&#36172;&#21338;&#31639;&#27861;CODE&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23567;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;CODE&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#31526;&#21512;&#32479;&#35745;&#32422;&#26463;&#26465;&#20214;&#30340;&#25152;&#26377;&#21487;&#33021;&#25805;&#20316;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#39640;&#25928;&#22320;&#23454;&#29616;&#20102;CODE&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36817;&#20284;&#26368;&#20248;&#35774;&#35745;&#30340;&#26368;&#20248;&#24615;&#20934;&#21017;&#25512;&#23548;&#20986;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;CODE&#36824;&#21487;&#20197;&#30475;&#20316;&#26159;&#21435;&#38500;&#20256;&#32479;&#30340;&#30456;&#20301;&#28040;&#38500;&#20013;&#30340;&#38454;&#27573;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the importance of explainability in modern machine learning, we design bandit algorithms that are efficient and interpretable. A bandit algorithm is interpretable if it explores with the objective of reducing uncertainty in the unknown model parameter. To quantify the interpretability, we introduce a novel metric of model error, which compares the rate reduction of the mean reward estimates to their actual means among all the plausible actions. We propose CODE, a bandit algorithm based on a Constrained Optimal DEsign, that is interpretable and maximally reduces the uncertainty. The key idea in CODE is to explore among all plausible actions, determined by a statistical constraint, to achieve interpretability. We implement CODE efficiently in both multi-armed and linear bandits and derive near-optimal regret bounds by leveraging the optimality criteria of the approximate optimal design. CODE can be also viewed as removing phases in conventional phased elimination, which make
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#39564;&#35777;&#38598;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;AUROC&#12290;</title><link>https://arxiv.org/abs/2310.10461</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#19979;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Model Selection of Zero-shot Anomaly Detectors in the Absence of Labeled Validation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#39564;&#35777;&#38598;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#38656;&#35201;&#22312;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24322;&#24120;&#26679;&#26412;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#24120;&#24120;&#21463;&#21040;&#26631;&#31614;&#25968;&#25454;&#30340;&#32570;&#20047;&#30340;&#38480;&#21046; - &#22312;&#27809;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35780;&#20272;&#20854;&#26816;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#26469;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#21512;&#25104;&#39564;&#35777;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24322;&#24120;&#29983;&#25104;&#26041;&#27861;&#20551;&#35774;&#21482;&#26377;&#23569;&#37327;&#30340;&#27491;&#24120;&#22270;&#20687;&#25903;&#25345;&#38598;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#29983;&#25104;&#21518;&#65292;&#25105;&#20204;&#30340;&#21512;&#25104;&#39564;&#35777;&#38598;&#34987;&#29992;&#20110;&#21019;&#24314;&#27169;&#22411;&#36873;&#25321;&#30340;&#39564;&#35777;&#26694;&#26550;&#20013;&#30340;&#26816;&#27979;&#20219;&#21153;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;SWSA&#24120;&#24120;&#36873;&#25321;&#19982;&#30495;&#23454;&#39564;&#35777;&#38598;&#36873;&#25321;&#30456;&#21305;&#37197;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#27604;&#22522;&#32447;&#26041;&#27861;&#30340;AUROC&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection requires detecting abnormal samples in large unlabeled datasets. While progress in deep learning and the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the lack of labeled data -- without it, their detection performance cannot be evaluated reliably. In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors with a generated synthetic validation set. Our proposed anomaly generation method assumes access to only a small support set of normal images and requires no training or fine-tuning. Once generated, our synthetic validation set is used to create detection tasks that compose a validation framework for model selection. In an empirical study, we find that SWSA often selects models that match selections made with a ground-truth validation set, resulting in higher AUROCs than baseline methods. We also find
&lt;/p&gt;</description></item><item><title>Sorted LLaMA&#36890;&#36807;&#25193;&#23637;SortedNet&#21040;&#29983;&#25104;NLP&#20219;&#21153;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21160;&#24577;&#25512;&#29702;&#20013;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#21482;&#38656;&#23558;&#26631;&#20934;&#24494;&#35843;&#26367;&#25442;&#20026;&#25490;&#24207;&#24494;&#35843;&#21363;&#21487;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#37322;&#25918;transformers&#20013;&#38388;&#23618;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23384;&#20648;&#38656;&#27714;&#21644;&#36807;&#28193;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2309.08968</link><description>&lt;p&gt;
Sorted LLaMA: &#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38388;&#23618;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#21160;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08968
&lt;/p&gt;
&lt;p&gt;
Sorted LLaMA&#36890;&#36807;&#25193;&#23637;SortedNet&#21040;&#29983;&#25104;NLP&#20219;&#21153;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21160;&#24577;&#25512;&#29702;&#20013;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#21482;&#38656;&#23558;&#26631;&#20934;&#24494;&#35843;&#26367;&#25442;&#20026;&#25490;&#24207;&#24494;&#35843;&#21363;&#21487;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#37322;&#25918;transformers&#20013;&#38388;&#23618;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23384;&#20648;&#38656;&#27714;&#21644;&#36807;&#28193;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;SortedNet&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#20013;&#30340;&#27169;&#22359;&#21270;&#21644;&#22522;&#20110;&#35745;&#31639;/&#20934;&#30830;&#24615;&#23545;&#23376;&#27169;&#22411;&#36827;&#34892;&#23884;&#22871;&#25490;&#24207;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25512;&#29702;&#12290;&#25105;&#20204;&#23558;SortedNet&#25193;&#23637;&#21040;&#29983;&#25104;NLP&#20219;&#21153;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#21160;&#24577;&#65292;&#20165;&#36890;&#36807;&#23558;&#26631;&#20934;&#24494;&#35843;&#65288;SFT&#65289;&#26367;&#25442;&#20026;&#25490;&#24207;&#24494;&#35843;&#65288;SoFT&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#28040;&#38500;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#37322;&#25918;transformers&#20013;&#38388;&#23618;&#22312;&#29983;&#25104;&#30446;&#26631;&#36755;&#20986;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23376;&#27169;&#22411;&#20173;&#28982;&#26159;&#21407;&#22987;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#26368;&#23567;&#21270;&#20102;&#23384;&#20648;&#38656;&#27714;&#21644;&#22312;&#19981;&#21516;&#35745;&#31639;/&#24310;&#36831;&#39044;&#31639;&#20043;&#38388;&#30340;&#36807;&#28193;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the power of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The ef
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22270;&#35268;&#33539;&#21270;&#26368;&#22823;&#21270;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#20174;&#27169;&#22411;&#31283;&#23450;&#24615;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20123;GNNs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#31283;&#23450;GNN&#23558;&#30456;&#20284;&#30340;&#22270;&#26144;&#23556;&#21040;&#32039;&#23494;&#30456;&#36830;&#30340;&#21521;&#37327;&#34920;&#31034;&#20013;&#65292;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22270;&#35268;&#33539;&#21270;&#22686;&#24378;&#30340;GNNs&#22312;&#34920;&#36798;&#33021;&#21147;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;&#26222;&#36941;&#22270;&#35268;&#33539;&#21270;&#30340;&#27010;&#24565;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#24191;&#27867;&#36866;&#29992;&#30340;&#20805;&#20998;&#26465;&#20214;&#26469;&#35299;&#20915;&#26222;&#36941;&#22270;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.00738</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22270;&#35268;&#33539;&#21270;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22270;&#35268;&#33539;&#21270;&#26368;&#22823;&#21270;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#20174;&#27169;&#22411;&#31283;&#23450;&#24615;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20123;GNNs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#31283;&#23450;GNN&#23558;&#30456;&#20284;&#30340;&#22270;&#26144;&#23556;&#21040;&#32039;&#23494;&#30456;&#36830;&#30340;&#21521;&#37327;&#34920;&#31034;&#20013;&#65292;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22270;&#35268;&#33539;&#21270;&#22686;&#24378;&#30340;GNNs&#22312;&#34920;&#36798;&#33021;&#21147;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;&#26222;&#36941;&#22270;&#35268;&#33539;&#21270;&#30340;&#27010;&#24565;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#24191;&#27867;&#36866;&#29992;&#30340;&#20805;&#20998;&#26465;&#20214;&#26469;&#35299;&#20915;&#26222;&#36941;&#22270;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#25581;&#31034;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;GNNs&#30340;&#21407;&#21017;&#12290;&#22270;&#35268;&#33539;&#21270;&#20316;&#20026;&#19968;&#31181;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#30340;&#20856;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#24320;&#21457;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;GNNs&#26102;&#24456;&#23569;&#34987;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22270;&#35268;&#33539;&#21270;&#26368;&#22823;&#21270;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#28982;&#21518;&#20174;&#27169;&#22411;&#31283;&#23450;&#24615;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#36825;&#20123;GNNs&#30340;&#33021;&#21147;&#12290;&#31283;&#23450;&#30340;GNN&#20250;&#23558;&#30456;&#20284;&#30340;&#22270;&#26144;&#23556;&#21040;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#32039;&#23494;&#30456;&#36830;&#30340;&#22270;&#34920;&#31034;&#20013;&#65292;&#32780;GNN&#30340;&#31283;&#23450;&#24615;&#23545;&#20110;&#23558;&#24615;&#33021;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#22270;&#24456;&#20851;&#38190;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22270;&#35268;&#33539;&#21270;&#22686;&#24378;&#30340;GNNs&#22312;&#34920;&#36798;&#33021;&#21147;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26222;&#36941;&#22270;&#35268;&#33539;&#21270;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#35299;&#20915;&#25240;&#34935;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#34920;&#24449;&#20102;&#19968;&#31181;&#24191;&#27867;&#36866;&#29992;&#30340;&#20805;&#20998;&#26465;&#20214;&#26469;&#35299;&#20915;&#26222;&#36941;&#22270;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressivity of Graph Neural Networks (GNNs) has been studied broadly in recent years to reveal the design principles for more powerful GNNs. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive GNNs. This paper proposes to maximize the expressivity of GNNs by graph canonization, then the power of such GNNs is studies from the perspective of model stability. A stable GNN will map similar graphs to close graph representations in the vectorial space, and the stability of GNNs is critical to generalize their performance to unseen graphs. We theoretically reveal the trade-off of expressivity and stability in graph-canonization-enhanced GNNs. Then we introduce a notion of universal graph canonization as the general solution to address the trade-off and characterize a widely applicable sufficient condition to solve the universal graph canonization. A comprehensive set of experiments demonstrates the effectiv
&lt;/p&gt;</description></item><item><title>CAMMARL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#39044;&#27979;&#30340;&#27010;&#24565;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.11128</link><description>&lt;p&gt;
CAMMARL: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#34892;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11128
&lt;/p&gt;
&lt;p&gt;
CAMMARL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#39044;&#27979;&#30340;&#27010;&#24565;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19982;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#20013;&#37319;&#21462;&#34892;&#21160;&#20043;&#21069;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#20174;&#25512;&#29702;&#20854;&#20182;&#26234;&#33021;&#20307;&#21644;&#21033;&#29992;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#25110;&#20449;&#24515;&#30340;&#27010;&#24565;&#20013;&#33719;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;CAMMARL&#65292;&#35813;&#31639;&#27861;&#28041;&#21450;&#20197;&#32622;&#20449;&#38598;&#30340;&#24418;&#24335;&#23545;&#19981;&#21516;&#24773;&#20917;&#19979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#36827;&#34892;&#24314;&#27169;&#65292;&#21363;&#21253;&#21547;&#20854;&#30495;&#23454;&#34892;&#21160;&#30340;&#39640;&#27010;&#29575;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#32467;&#26524;&#26469;&#25351;&#23548;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#20272;&#35745;&#36825;&#20123;&#38598;&#21512;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#33268;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#33719;&#24471;&#26368;&#26377;&#21487;&#33021;&#30340;&#32467;&#26524;&#30340;&#20272;&#35745;&#65292;&#36824;&#21487;&#20197;&#37327;&#21270;&#21487;&#25805;&#20316;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#19968;&#20010;&#21487;&#35777;&#26126;&#20197;&#39640;&#27010;&#29575;&#65288;&#20363;&#22914;95&#65285;&#65289;&#35206;&#30422;&#30495;&#23454;&#39044;&#27979;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#23436;&#20840;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#19978;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CAMMARL&#25552;&#39640;&#20102;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Before taking actions in an environment with more than one intelligent agent, an autonomous agent may benefit from reasoning about the other agents and utilizing a notion of a guarantee or confidence about the behavior of the system. In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAMMARL, which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability. We then use these estimates to inform an agent's decision-making. For estimating such sets, we use the concept of conformal predictions, by means of which, we not only obtain an estimate of the most probable outcome but get to quantify the operable uncertainty as well. For instance, we can predict a set that provably covers the true predictions with high probabilities (e.g., 95%). Through several experiments in two fully cooperative multi-agent tasks, we show that CAMMARL elevates the capabi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19968;&#32423;&#23450;&#20215;&#25293;&#21334;&#20013;&#35774;&#23450;&#24213;&#20215;&#30340;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#38597;&#34382;&#24191;&#21578;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#32447;&#21457;&#24067;&#32773;&#33021;&#22815;&#22686;&#21152;&#24191;&#21578;&#25910;&#20837;&#12290;</title><link>https://arxiv.org/abs/2302.06018</link><description>&lt;p&gt;
&#20248;&#21270;&#19968;&#32423;&#23450;&#20215;&#25293;&#21334;&#20013;&#30340;&#24213;&#20215;&#65306;&#38597;&#34382;&#24191;&#21578;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Floors in First Price Auctions: an Empirical Study of Yahoo Advertising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19968;&#32423;&#23450;&#20215;&#25293;&#21334;&#20013;&#35774;&#23450;&#24213;&#20215;&#30340;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20110;&#38597;&#34382;&#24191;&#21578;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#32447;&#21457;&#24067;&#32773;&#33021;&#22815;&#22686;&#21152;&#24191;&#21578;&#25910;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24213;&#20215;&#65288;&#20063;&#31216;&#20026;&#20445;&#30041;&#20215;&#26684;&#65289;&#24110;&#21161;&#21457;&#24067;&#32773;&#22686;&#21152;&#20854;&#24191;&#21578;&#31354;&#38388;&#30340;&#39044;&#26399;&#25910;&#20837;&#65292;&#36890;&#24120;&#36890;&#36807;&#25293;&#21334;&#26041;&#24335;&#20986;&#21806;&#12290;&#24213;&#20215;&#34987;&#23450;&#20041;&#20026;&#21334;&#26041;&#65288;&#21487;&#20197;&#26159;&#21457;&#24067;&#32773;&#25110;&#24191;&#21578;&#20132;&#26131;&#25152;&#65289;&#24895;&#24847;&#25509;&#21463;&#30340;&#26368;&#20302;&#20986;&#20215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19968;&#32423;&#23450;&#20215;&#25293;&#21334;&#20013;&#35774;&#23450;&#24213;&#20215;&#30340;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#38597;&#34382;&#32593;&#31449;&#19978;&#23454;&#26045;&#30340;&#24433;&#21709;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#22312;&#32447;&#24191;&#21578;&#34892;&#19994;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#19968;&#20123;&#31454;&#26631;&#32773;&#23545;&#24191;&#21578;&#20132;&#26131;&#25152;&#22914;&#20309;&#22788;&#29702;&#31454;&#26631;&#32773;&#30340;&#25968;&#25454;&#25552;&#20986;&#38480;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#35774;&#32622;&#20445;&#30041;&#20215;&#26684;&#30340;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#22312;&#31454;&#26631;&#35831;&#27714;&#20013;&#21152;&#20837;&#24213;&#20215;&#26469;&#24341;&#23548;&#31454;&#26631;&#32773;&#25913;&#21464;&#20854;&#31454;&#26631;&#34892;&#20026;&#65292;&#24110;&#21161;&#22312;&#32447;&#21457;&#24067;&#32773;&#22686;&#21152;&#20854;&#24191;&#21578;&#25910;&#20837;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#24050;&#22312;&#38597;&#34382;&#19978;&#23454;&#26045;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#39044;&#35745;&#38597;&#34382;&#30340;&#23637;&#31034;&#24191;&#21578;&#23384;&#36135;&#22686;&#37327;&#24180;&#22343;&#25910;&#20837;&#20026;+1.3%&#12290;
&lt;/p&gt;
&lt;p&gt;
Floors (also known as reserve prices) help publishers to increase the expected revenue of their ad space, which is usually sold via auctions. Floors are defined as the minimum bid that a seller (it can be a publisher or an ad exchange) is willing to accept for the inventory opportunity. In this paper, we present a model to set floors in first price auctions, and discuss the impact of its implementation on Yahoo sites. The model captures important characteristics of the online advertising industry. For instance, some bidders impose restrictions on how ad exchanges can handle data from bidders, conditioning the model choice to set reserve prices. Our solution induces bidders to change their bidding behavior as a response to the floors enclosed in the bid request, helping online publishers to increase their ad revenue.   The outlined methodology has been implemented at Yahoo with remarkable results. The annualized incremental revenue is estimated at +1.3% on Yahoo display inventory, and +
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#38075;&#23380;&#24037;&#20855;&#30952;&#25439;&#26816;&#27979;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#36866;&#24230;&#22686;&#24378;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#20803;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2302.05262</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#21644;&#25439;&#22833;&#20989;&#25968;&#22312;&#38075;&#23380;&#24037;&#20855;&#30952;&#25439;&#26816;&#27979;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#38075;&#23380;&#24037;&#20855;&#30952;&#25439;&#26816;&#27979;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#36866;&#24230;&#22686;&#24378;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#20803;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#24037;&#20855;&#30952;&#25439;&#30417;&#27979;&#23545;&#20110;&#36136;&#37327;&#25511;&#21046;&#21644;&#25104;&#26412;&#38477;&#20302;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#27969;&#31243;&#65292;&#24212;&#29992;&#20110;&#20999;&#21106;&#25554;&#20837;&#29289;&#30340;&#26174;&#24494;&#22270;&#20687;&#65292;&#29992;&#20110;&#30952;&#25439;&#26816;&#27979;&#12290;&#30952;&#25439;&#21306;&#22495;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#20004;&#31181;&#30952;&#25439;&#31867;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#30952;&#25439;&#31867;&#21035;&#65292;&#21487;&#20197;&#25226;&#38382;&#39064;&#23450;&#20041;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#38500;&#20102;&#27604;&#36739;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#38382;&#39064;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#20132;&#21449;&#29109;&#12289;&#28966;&#28857;&#20132;&#21449;&#29109;&#21644;&#22522;&#20110;IoU&#30340;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#19981;&#21516;&#23610;&#23544;&#30340;&#22270;&#20687;&#22359;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#24378;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24230;&#22686;&#24378;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#20803;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool wear monitoring is crucial for quality control and cost reduction in manufacturing processes, of which drilling applications are one example. In this paper, we present a U-Net based semantic image segmentation pipeline, deployed on microscopy images of cutting inserts, for the purpose of wear detection. The wear area is differentiated in two different types, resulting in a multiclass classification problem. Joining the two wear types in one general wear class, on the other hand, allows the problem to be formulated as a binary classification task. Apart from the comparison of the binary and multiclass problem, also different loss functions, i. e., Cross Entropy, Focal Cross Entropy, and a loss based on the Intersection over Union (IoU), are investigated. Furthermore, models are trained on image tiles of different sizes, and augmentation techniques of varying intensities are deployed. We find, that the best performing models are binary models, trained on data with moderate augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#28508;&#22312;&#23384;&#22312;&#37325;&#23614;&#39118;&#38505;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20351;&#29992;CVaR&#25110;DRO&#39118;&#38505;&#31561;&#26367;&#20195;&#26631;&#20934;&#24471;&#21040;&#30340;&#26368;&#20339;&#20505;&#36873;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2301.11584</link><description>&lt;p&gt;
&#20855;&#26377;&#21516;&#26102;&#35843;&#25972;&#23610;&#24230;&#30340;&#20581;&#22766;&#26041;&#24046;&#27491;&#21017;&#21270;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust variance-regularized risk minimization with concomitant scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.11584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#28508;&#22312;&#23384;&#22312;&#37325;&#23614;&#39118;&#38505;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20351;&#29992;CVaR&#25110;DRO&#39118;&#38505;&#31561;&#26367;&#20195;&#26631;&#20934;&#24471;&#21040;&#30340;&#26368;&#20339;&#20505;&#36873;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28508;&#22312;&#23384;&#22312;&#37325;&#23614;&#39118;&#38505;&#30340;&#25439;&#22833;&#20989;&#25968;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#23567;&#21270;&#25439;&#22833;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#20043;&#21644;&#30340;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#31934;&#30830;&#20272;&#35745;&#26041;&#24046;&#12290;&#36890;&#36807;&#20462;&#25913;&#19968;&#31181;&#26080;&#26041;&#24046;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#20197;&#36866;&#24212;&#25105;&#20204;&#30340;&#38382;&#39064;&#35774;&#23450;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21487;&#20197;&#19982;&#26631;&#20934;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#27714;&#35299;&#22120;&#36731;&#26494;&#32467;&#21512;&#65292;&#29992;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20351;&#29992;CVaR&#25110;DRO&#39118;&#38505;&#31561;&#26367;&#20195;&#26631;&#20934;&#24471;&#21040;&#30340;&#26368;&#20339;&#20505;&#36873;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under losses which are potentially heavy-tailed, we consider the task of minimizing sums of the loss mean and standard deviation, without trying to accurately estimate the variance. By modifying a technique for variance-free robust mean estimation to fit our problem setting, we derive a simple learning procedure which can be easily combined with standard gradient-based solvers to be used in traditional machine learning workflows. Empirically, we verify that our proposed approach, despite its simplicity, performs as well or better than even the best-performing candidates derived from alternative criteria such as CVaR or DRO risks on a variety of datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#24418;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#20998;&#31867;&#22120;&#36793;&#30028;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#31216;&#20026;&#37051;&#23621;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#36793;&#30028;&#26159;&#24040;&#22823;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2212.04382</link><description>&lt;p&gt;
&#20998;&#31867;&#22120;&#36793;&#30028;&#30340;&#32467;&#26500;&#65306;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Structure of Classifier Boundaries: Case Study for a Naive Bayes Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#24418;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#20998;&#31867;&#22120;&#36793;&#30028;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#31216;&#20026;&#37051;&#23621;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#36793;&#30028;&#26159;&#24040;&#22823;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#22522;&#20110;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#36824;&#26159;&#20108;&#32773;&#32452;&#21512;&#65292;&#20998;&#31867;&#22120;&#23558;&#65288;&#21487;&#33021;&#22797;&#26434;&#30340;&#65289;&#36755;&#20837;&#25968;&#25454;&#24402;&#20837;&#30456;&#23545;&#36739;&#23569;&#30340;&#36755;&#20986;&#31867;&#21035;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#36755;&#20837;&#31354;&#38388;&#20026;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36793;&#30028;&#30340;&#32467;&#26500;&#8212;&#8212;&#37027;&#20123;&#34987;&#20998;&#31867;&#20026;&#19981;&#21516;&#31867;&#21035;&#30340;&#37051;&#36817;&#28857;&#8212;&#8212;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#31185;&#23398;&#32972;&#26223;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#30001;&#19979;&#19968;&#20195;&#27979;&#24207;&#20202;&#29983;&#25104;&#30340;DNA&#35835;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36793;&#30028;&#26082;&#26159;&#24040;&#22823;&#30340;&#65292;&#21448;&#20855;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#31216;&#20026;&#37051;&#23621;&#30456;&#20284;&#24230;&#65292;&#23427;&#23558;&#19968;&#20010;&#28857;&#30340;&#32467;&#26524;&#19982;&#20854;&#37051;&#23621;&#30340;&#32467;&#26524;&#20998;&#24067;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#20010;&#24230;&#37327;&#19981;&#20165;&#36861;&#36394;&#20102;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#20004;&#20010;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#36824;&#21487;&#20197;&#22312;&#27809;&#26377;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#20998;&#31867;&#22120;&#19978;&#23454;&#29616;&#65292;&#20294;&#38656;&#35201;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether based on models, training data or a combination, classifiers place (possibly complex) input data into one of a relatively small number of output categories. In this paper, we study the structure of the boundary--those points for which a neighbor is classified differently--in the context of an input space that is a graph, so that there is a concept of neighboring inputs, The scientific setting is a model-based naive Bayes classifier for DNA reads produced by Next Generation Sequencers. We show that the boundary is both large and complicated in structure. We create a new measure of uncertainty, called Neighbor Similarity, that compares the result for a point to the distribution of results for its neighbors. This measure not only tracks two inherent uncertainty measures for the Bayes classifier, but also can be implemented, at a computational cost, for classifiers without inherent measures of uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#36843;&#20351;&#29366;&#24577;&#34920;&#31034;&#19982;&#30456;&#37051;&#29366;&#24577;&#30340;&#34920;&#31034;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#29615;&#22659;&#30340;&#23616;&#37096;&#21464;&#21270;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2209.09441</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Locally Constrained Representations in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#36843;&#20351;&#29366;&#24577;&#34920;&#31034;&#19982;&#30456;&#37051;&#29366;&#24577;&#30340;&#34920;&#31034;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#29615;&#22659;&#30340;&#23616;&#37096;&#21464;&#21270;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20174;&#29615;&#22659;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#32431;&#31929;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#21487;&#33021;&#24046;&#24322;&#24040;&#22823;&#65292;&#36825;&#21462;&#20915;&#20110;&#20540;&#20989;&#25968;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24182;&#19981;&#19968;&#23450;&#38656;&#35201;&#19982;&#24403;&#21069;&#20219;&#21153;&#38750;&#24120;&#30456;&#20851;&#12290;&#20165;&#20381;&#36182;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#31034;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#27493;&#38271;&#20013;&#24046;&#24322;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#26377;&#19968;&#20010;&#21464;&#21270;&#30340;&#30446;&#26631;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#23558;&#21462;&#20915;&#20110;&#24403;&#21069;&#20540;/&#31574;&#30053;&#30340;&#22909;&#22351;&#12290;&#22240;&#27492;&#65292;&#23558;&#34920;&#31034;&#19982;&#20027;&#35201;&#20219;&#21153;&#35299;&#32806;&#21487;&#20197;&#20351;&#20854;&#19981;&#20165;&#20851;&#27880;&#20110;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#65292;&#36824;&#20851;&#27880;&#29615;&#22659;&#21160;&#24577;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;&#65292;&#20854;&#20013;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#36843;&#20351;&#29366;&#24577;&#34920;&#31034;&#33021;&#22815;&#30001;&#30456;&#37051;&#29366;&#24577;&#30340;&#34920;&#31034;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#40723;&#21169;&#34920;&#31034;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#29615;&#22659;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Reinforcement Learning (RL) heavily relies on the ability to learn robust representations from the observations of the environment. In most cases, the representations learned purely by the reinforcement learning loss can differ vastly across states depending on how the value functions change. However, the representations learned need not be very specific to the task at hand. Relying only on the RL objective may yield representations that vary greatly across successive time steps. In addition, since the RL loss has a changing target, the representations learned would depend on how good the current values/policies are. Thus, disentangling the representations from the main task would allow them to focus not only on the task-specific features but also the environment dynamics. To this end, we propose locally constrained representations, where an auxiliary loss forces the state representations to be predictable by the representations of the neighboring states. This encourages
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2208.11665</link><description>&lt;p&gt;
&#32479;&#35745;&#23545;&#27969;&#24418;&#20551;&#35774;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Statistical exploration of the Manifold Hypothesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.11665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;&#29702;&#35770;&#65292;&#23427;&#35748;&#20026;&#21517;&#20041;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#23454;&#38469;&#19978;&#38598;&#20013;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#20013;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#20013;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24050;&#32463;&#23548;&#33268;&#20102;&#22810;&#31181;&#32479;&#35745;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#36825;&#31181;&#36890;&#29992;&#19988;&#38750;&#24120;&#31616;&#21333;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#29983;&#25104;&#20016;&#23500;&#32780;&#26377;&#26102;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#12289;&#30456;&#20851;&#24615;&#21644;&#24179;&#31283;&#24615;&#31561;&#22522;&#26412;&#27010;&#24565;&#12290;&#36825;&#20026;&#20026;&#20160;&#20040;&#27969;&#24418;&#20551;&#35774;&#22312;&#36825;&#20040;&#22810;&#24773;&#20917;&#19979;&#20284;&#20046;&#25104;&#31435;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#22312;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;</title><link>https://arxiv.org/abs/2208.04284</link><description>&lt;p&gt;
&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#19968;&#33324;&#21270;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Rademacher Complexity-based Generalization Bounds for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#36827;&#34892;&#20998;&#31867;&#23569;&#37327;&#31867;&#21035;&#22270;&#20687;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#30340;&#21457;&#23637;&#23545;&#20110;&#39640;&#32500;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#26159;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Rademacher&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;CNNs&#30340;&#32593;&#32476;&#38271;&#24230;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35832;&#22914;ReLU&#65292;Leaky ReLU&#65292;Parametric Rectifier Linear Unit&#65292;Sigmoid&#21644;Tanh&#31561;&#29305;&#23450;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between function spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not depend on the network length for CNNs with some special types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30028;&#22495;&#20013;&#26500;&#24314;&#33258;&#36866;&#24212;&#21644;&#26080;&#21442;&#30340;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26694;&#26550;&#65292;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#24320;&#21457;&#20102;&#20855;&#26377;&#26368;&#20248;&#21160;&#24577;&#36951;&#25022;&#30028;&#38480;&#30340;&#26080;&#32422;&#26463;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;Follow-the-Regularized-Leader&#30340;&#31574;&#30053;&#26080;&#27861;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#65292;&#27492;&#22806;&#36824;&#24212;&#29992;&#38236;&#20687;&#19979;&#38477;&#26694;&#26550;&#26500;&#24314;&#20102;&#26032;&#30340;&#26080;&#21442;&#38544;&#24335;&#26356;&#26032;&#20197;&#21450;&#31616;&#21270;&#21644;&#25913;&#36827;&#30340;&#26080;&#32422;&#26463;&#26080;&#26631;&#24230;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2203.00444</link><description>&lt;p&gt;
&#26080;&#21442;&#38236;&#20687;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Parameter-free Mirror Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.00444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30028;&#22495;&#20013;&#26500;&#24314;&#33258;&#36866;&#24212;&#21644;&#26080;&#21442;&#30340;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26694;&#26550;&#65292;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#24320;&#21457;&#20102;&#20855;&#26377;&#26368;&#20248;&#21160;&#24577;&#36951;&#25022;&#30028;&#38480;&#30340;&#26080;&#32422;&#26463;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;Follow-the-Regularized-Leader&#30340;&#31574;&#30053;&#26080;&#27861;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#65292;&#27492;&#22806;&#36824;&#24212;&#29992;&#38236;&#20687;&#19979;&#38477;&#26694;&#26550;&#26500;&#24314;&#20102;&#26032;&#30340;&#26080;&#21442;&#38544;&#24335;&#26356;&#26032;&#20197;&#21450;&#31616;&#21270;&#21644;&#25913;&#36827;&#30340;&#26080;&#32422;&#26463;&#26080;&#26631;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22312;&#26080;&#30028;&#22495;&#20013;&#26500;&#24314;&#33258;&#36866;&#24212;&#21644;&#26080;&#21442;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#26080;&#32422;&#26463;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#38480;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#22522;&#20110;Follow-the-Regularized-Leader&#30340;&#33258;&#28982;&#31574;&#30053;&#26080;&#27861;&#36798;&#21040;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#38236;&#20687;&#19979;&#38477;&#26694;&#26550;&#24212;&#29992;&#20110;&#26500;&#24314;&#26080;&#21442;&#38544;&#24335;&#26356;&#26032;&#65292;&#20197;&#21450;&#19968;&#20010;&#31616;&#21270;&#21644;&#25913;&#36827;&#30340;&#26080;&#32422;&#26463;&#26080;&#26631;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a modified online mirror descent framework that is suitable for building adaptive and parameter-free algorithms in unbounded domains. We leverage this technique to develop the first unconstrained online linear optimization algorithm achieving an optimal dynamic regret bound, and we further demonstrate that natural strategies based on Follow-the-Regularized-Leader are unable to achieve similar results. We also apply our mirror descent framework to build new parameter-free implicit updates, as well as a simplified and improved unconstrained scale-free algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26681;&#25454;&#21754;&#20083;&#21160;&#29289;&#30382;&#36136;&#20013;&#30340;&#27169;&#25311;&#32416;&#38169;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#32416;&#38169;&#30721;&#30340;&#36890;&#29992;&#23481;&#38169;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#35745;&#31639;&#65307;&#21457;&#29616;&#20102;&#20174;&#25925;&#38556;&#21040;&#23481;&#38169;&#31070;&#32463;&#35745;&#31639;&#30340;&#30456;&#21464;&#65292;&#20026;&#29702;&#35299;&#22024;&#26434;&#27169;&#25311;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2202.12887</link><description>&lt;p&gt;
&#20174;&#29983;&#29289;&#32416;&#38169;&#30721;&#21040;&#23481;&#38169;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fault-Tolerant Neural Networks from Biological Error Correction Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.12887
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26681;&#25454;&#21754;&#20083;&#21160;&#29289;&#30382;&#36136;&#20013;&#30340;&#27169;&#25311;&#32416;&#38169;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#32416;&#38169;&#30721;&#30340;&#36890;&#29992;&#23481;&#38169;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#35745;&#31639;&#65307;&#21457;&#29616;&#20102;&#20174;&#25925;&#38556;&#21040;&#23481;&#38169;&#31070;&#32463;&#35745;&#31639;&#30340;&#30456;&#21464;&#65292;&#20026;&#29702;&#35299;&#22024;&#26434;&#27169;&#25311;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#26159;&#21542;&#21487;&#33021;&#23454;&#29616;&#23481;&#38169;&#35745;&#31639;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#20165;&#20351;&#29992;&#19981;&#21487;&#38752;&#30340;&#31070;&#32463;&#20803;&#23454;&#29616;&#20219;&#24847;&#21487;&#38752;&#30340;&#35745;&#31639;&#65311;&#22312;&#21754;&#20083;&#21160;&#29289;&#30382;&#36136;&#30340;&#32593;&#26684;&#32454;&#32990;&#20013;&#65292;&#35266;&#23519;&#21040;&#20102;&#27169;&#25311;&#32416;&#38169;&#30721;&#20445;&#25252;&#29366;&#24577;&#20813;&#21463;&#31070;&#32463;&#23556;&#39057;&#22122;&#22768;&#30340;&#29616;&#35937;&#65292;&#20294;&#20854;&#22312;&#20449;&#24687;&#22788;&#29702;&#20013;&#30340;&#20316;&#29992;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#29983;&#29289;&#32416;&#38169;&#30721;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#23481;&#38169;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#25925;&#38556;&#24615;&#37117;&#20302;&#20110;&#19968;&#20010;&#20005;&#26684;&#30340;&#38408;&#20540;&#65292;&#21017;&#33021;&#22815;&#23454;&#29616;&#21487;&#38752;&#30340;&#35745;&#31639;&#65307;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22024;&#26434;&#30340;&#29983;&#29289;&#31070;&#32463;&#20803;&#20302;&#20110;&#36825;&#20010;&#38408;&#20540;&#12290;&#20174;&#25925;&#38556;&#21040;&#23481;&#38169;&#31070;&#32463;&#35745;&#31639;&#30340;&#30456;&#21464;&#30340;&#21457;&#29616;&#65292;&#25581;&#31034;&#20102;&#30382;&#36136;&#20013;&#21487;&#38752;&#35745;&#31639;&#30340;&#26426;&#21046;&#65292;&#20026;&#29702;&#35299;&#19982;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26377;&#20851;&#30340;&#22024;&#26434;&#27169;&#25311;&#31995;&#32479;&#25171;&#24320;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been an open question in deep learning if fault-tolerant computation is possible: can arbitrarily reliable computation be achieved using only unreliable neurons? In the grid cells of the mammalian cortex, analog error correction codes have been observed to protect states against neural spiking noise, but their role in information processing is unclear. Here, we use these biological error correction codes to develop a universal fault-tolerant neural network that achieves reliable computation if the faultiness of each neuron lies below a sharp threshold; remarkably, we find that noisy biological neurons fall below this threshold. The discovery of a phase transition from faulty to fault-tolerant neural computation suggests a mechanism for reliable computation in the cortex and opens a path towards understanding noisy analog systems relevant to artificial intelligence and neuromorphic computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#20010;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#24694;&#24847;URL&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#32622;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#26469;&#25913;&#21892;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2202.10027</link><description>&lt;p&gt;
&#26356;&#20026;&#27867;&#21270;&#30340;&#24694;&#24847;URL&#26816;&#27979;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Toward More Generalized Malicious URL Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.10027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#20010;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#24694;&#24847;URL&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#32622;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#26469;&#25913;&#21892;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#20010;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#24694;&#24847;URL&#26816;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35782;&#21035;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#36827;&#19968;&#27493;&#35748;&#20026;&#36825;&#26679;&#30340;&#20559;&#24046;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#25968;&#25454;&#20013;&#33258;&#28982;&#23384;&#22312;&#65292;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#21435;&#20559;&#32622;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#36731;&#22240;&#29305;&#24449;&#20559;&#24046;&#32780;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22522;&#20110;CNN&#21644;RNN&#30340;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reveals a data bias issue that can severely affect the performance while conducting a machine learning model for malicious URL detection. We describe how such bias can be identified using interpretable machine learning techniques, and further argue that such biases naturally exist in the real world security data for training a classification model. We then propose a debiased training strategy that can be applied to most deep-learning based models to alleviate the negative effects from the biased features. The solution is based on the technique of self-supervised adversarial training to train deep neural networks learning invariant embedding from biased data. We conduct a wide range of experiments to demonstrate that the proposed strategy can lead to significantly better generalization capability for both CNN-based and RNN-based detection models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#38750;&#32447;&#24615;&#25511;&#21046;&#29702;&#35770;&#35299;&#37322;&#20102;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20805;&#20998;&#26465;&#20214;&#65292;&#22312;&#28608;&#27963;&#20989;&#25968;&#28385;&#36275;&#20108;&#27425;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#36275;&#22815;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#32039;&#38598;&#21512;&#19978;&#36924;&#36817;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2007.06007</link><description>&lt;p&gt;
&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#38750;&#32447;&#24615;&#25511;&#21046;&#29702;&#35770;&#23454;&#29616;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Power of Deep Residual Neural Networks via Nonlinear Control Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.06007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#38750;&#32447;&#24615;&#25511;&#21046;&#29702;&#35770;&#35299;&#37322;&#20102;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20805;&#20998;&#26465;&#20214;&#65292;&#22312;&#28608;&#27963;&#20989;&#25968;&#28385;&#36275;&#20108;&#27425;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#36275;&#22815;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#32039;&#38598;&#21512;&#19978;&#36924;&#36817;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20960;&#20309;&#38750;&#32447;&#24615;&#25511;&#21046;&#26469;&#35299;&#37322;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#12290;&#21463;&#21040;&#26368;&#36817;&#24314;&#31435;&#27531;&#24046;&#32593;&#32476;&#21644;&#25511;&#21046;&#31995;&#32479;&#20043;&#38388;&#32852;&#31995;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#35201;&#27714;&#28608;&#27963;&#20989;&#25968;&#25110;&#20854;&#23548;&#25968;&#20043;&#19968;&#28385;&#36275;&#19968;&#20010;&#20108;&#27425;&#24494;&#20998;&#26041;&#31243;&#65292;&#20197;&#20351;&#27531;&#24046;&#32593;&#32476;&#20855;&#26377;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#35768;&#22810;&#28608;&#27963;&#20989;&#25968;&#28385;&#36275;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#23646;&#24615;&#36275;&#20197;&#35753;&#19968;&#20010;&#36275;&#22815;&#28145;&#30340;&#20855;&#26377;$n+1$&#31070;&#32463;&#20803;&#27599;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#32039;&#38598;&#21512;&#19978;&#30456;&#23545;&#20110;&#26368;&#22823;&#33539;&#25968;&#36924;&#36817;&#20219;&#24847;&#36830;&#32493;&#30340;&#20174;$\mathbb{R}^n$&#21040;$\mathbb{R}^n$&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24120;&#31616;&#21333;&#30340;&#26550;&#26500;&#65292;&#21482;&#38656;&#35201;&#26435;&#37325;&#21462;&#20004;&#20010;&#20540;&#12290;&#31532;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#23558;&#36890;&#29992;&#36924;&#36817;&#19982;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#38750;&#32447;&#24615;&#25511;&#21046;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explain the universal approximation capabilities of deep residual neural networks through geometric nonlinear control. Inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. Many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with $n+1$ neurons per layer to approximate arbitrarily well, on a compact set and with respect to the supremum norm, any continuous function from $\mathbb{R}^n$ to $\mathbb{R}^n$. We further show this result to hold for very simple architectures for which the weights only need to assume two values. The first key technical contribution consists of relating the universal approxi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#21033;&#29992;&#27491;&#25968;&#25454;-&#26080;&#26631;&#31614;&#23398;&#20064;&#21644;&#26377;&#26465;&#20214;&#29983;&#25104;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#21450;&#39069;&#22806;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23545;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#22122;&#22768;&#19981;&#21464;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25552;&#39640;PU&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;PU&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#26631;&#31614;&#21644;&#39069;&#22806;&#25968;&#25454;&#26469;&#24110;&#21161;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2006.07841</link><description>&lt;p&gt;
&#21516;&#26102;&#36827;&#34892;&#27491;&#25968;&#25454;-&#26080;&#26631;&#31614;&#23398;&#20064;&#21644;&#26377;&#26465;&#20214;&#29983;&#25104;&#65292;&#21033;&#29992;&#39069;&#22806;&#25968;&#25454;&#26469;&#20998;&#31867;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Classify and Generate Reciprocally: Simultaneous Positive-Unlabelled Learning and Conditional Generation with Extra Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.07841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#21033;&#29992;&#27491;&#25968;&#25454;-&#26080;&#26631;&#31614;&#23398;&#20064;&#21644;&#26377;&#26465;&#20214;&#29983;&#25104;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#21450;&#39069;&#22806;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23545;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#22122;&#22768;&#19981;&#21464;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25552;&#39640;PU&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;PU&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#26631;&#31614;&#21644;&#39069;&#22806;&#25968;&#25454;&#26469;&#24110;&#21161;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#26631;&#35760;&#31867;&#21035;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#29942;&#39048;&#12290;&#34429;&#28982;&#23384;&#22312;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#24182;&#25552;&#20379;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21033;&#29992;&#23427;&#20204;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#27491;&#25968;&#25454;-&#26080;&#26631;&#31614;&#65288;Positive-Unlabeled&#65292;PU&#65289;&#20998;&#31867;&#21644;&#26377;&#26465;&#20214;&#29983;&#25104;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#24471;&#22312;&#38754;&#23545;&#39069;&#22806;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#20998;&#24067;&#22806;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65289;&#26102;&#65292;&#21516;&#26102;&#36827;&#34892;PU&#20998;&#31867;&#21644;&#26377;&#26465;&#20214;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;1&#65289;&#36890;&#36807;&#19968;&#20010;&#23545;&#22122;&#22768;&#26631;&#31614;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#22122;&#22768;&#19981;&#21464;&#26377;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;Classifier-Noise-Invariant Conditional GAN&#65292;CNI-CGAN&#65289;&#26469;&#25552;&#39640;PU&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;2&#65289;&#21033;&#29992;PU&#20998;&#31867;&#22120;&#39044;&#27979;&#30340;&#26631;&#31614;&#21644;&#39069;&#22806;&#25968;&#25454;&#26469;&#24110;&#21161;&#29983;&#25104;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CNI-CGAN&#30340;&#26368;&#20248;&#26465;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#26469;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of class-labeled data is a ubiquitous bottleneck in many machine learning problems. While abundant unlabeled data typically exist and provide a potential solution, it is highly challenging to exploit them. In this paper, we address this problem by leveraging Positive-Unlabeled~(PU) classification and the conditional generation with extra unlabeled data \emph{simultaneously}. In particular, we present a novel training framework to jointly target both PU classification and conditional generation when exposed to extra data, especially out-of-distribution unlabeled data, by exploring the interplay between them: 1) enhancing the performance of PU classifiers with the assistance of a novel Classifier-Noise-Invariant Conditional GAN~(CNI-CGAN) that is robust to noisy labels, 2) leveraging extra data with predicted labels from a PU classifier to help the generation. Theoretically, we prove the optimal condition of CNI-CGAN, and experimentally, we conducted extensive evaluations on
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;Syntax&#65292;&#19968;&#20010;&#20855;&#26377;&#21512;&#25104;&#23545;&#29031;&#32452;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#20122;&#32676;&#20307;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#27491;&#38754;&#27835;&#30103;&#25928;&#26524;&#30340;&#20122;&#32676;&#20307;&#65292;&#23545;&#20110;&#22810;&#26679;&#21270;&#24739;&#32773;&#21453;&#24212;&#30340;&#20020;&#24202;&#35797;&#39564;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.17205</link><description>&lt;p&gt;
&#20855;&#26377;&#21512;&#25104;&#23545;&#29031;&#32452;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Experiment Design with Synthetic Controls. (arXiv:2401.17205v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17205
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;Syntax&#65292;&#19968;&#20010;&#20855;&#26377;&#21512;&#25104;&#23545;&#29031;&#32452;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#20122;&#32676;&#20307;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#27491;&#38754;&#27835;&#30103;&#25928;&#26524;&#30340;&#20122;&#32676;&#20307;&#65292;&#23545;&#20110;&#22810;&#26679;&#21270;&#24739;&#32773;&#21453;&#24212;&#30340;&#20020;&#24202;&#35797;&#39564;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#36890;&#24120;&#29992;&#20110;&#20102;&#35299;&#26032;&#27835;&#30103;&#23545;&#32473;&#23450;&#24739;&#32773;&#32676;&#20307;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#32676;&#20307;&#20013;&#30340;&#24739;&#32773;&#24456;&#23569;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#23545;&#24453;&#30456;&#21516;&#30340;&#27835;&#30103;&#20570;&#20986;&#21453;&#24212;&#12290;&#24739;&#32773;&#21453;&#24212;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#36827;&#34892;&#22810;&#20010;&#20122;&#32676;&#20307;&#30340;&#25928;&#26524;&#30740;&#31350; - &#23588;&#20854;&#26159;&#24403;&#27835;&#30103;&#23545;&#25972;&#20307;&#32676;&#20307;&#27809;&#26377;&#25110;&#20960;&#20046;&#27809;&#26377;&#30410;&#22788;&#65292;&#32780;&#23545;&#29305;&#23450;&#20122;&#32676;&#20307;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#30410;&#22788;&#26102;&#12290;&#22522;&#20110;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Syntax&#65292;&#19968;&#31181;&#25506;&#32034;&#24615;&#35797;&#39564;&#35774;&#35745;&#65292;&#22312;&#20247;&#22810;&#20122;&#32676;&#20307;&#20013;&#35782;&#21035;&#20855;&#26377;&#27491;&#38754;&#27835;&#30103;&#25928;&#26524;&#30340;&#20122;&#32676;&#20307;&#12290;Syntax&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;(i) &#33258;&#36866;&#24212;&#25307;&#21215;&#21644;&#20998;&#37197;&#24739;&#32773;&#65292;(ii) &#36890;&#36807;&#21512;&#25104;&#23545;&#29031;&#32452;&#24418;&#25104;&#27599;&#20010;&#20122;&#32676;&#20307;&#30340;&#25511;&#21046;&#26679;&#26412;&#65292;&#20174;&#32780;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;Syntax&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20309;&#26102;&#21487;&#33021;&#20248;&#20110;&#20256;&#32479;&#35797;&#39564;&#35774;&#35745;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are typically run in order to understand the effects of a new treatment on a given population of patients. However, patients in large populations rarely respond the same way to the same treatment. This heterogeneity in patient responses necessitates trials that investigate effects on multiple subpopulations - especially when a treatment has marginal or no benefit for the overall population but might have significant benefit for a particular subpopulation. Motivated by this need, we propose Syntax, an exploratory trial design that identifies subpopulations with positive treatment effect among many subpopulations. Syntax is sample efficient as it (i) recruits and allocates patients adaptively and (ii) estimates treatment effects by forming synthetic controls for each subpopulation that combines control samples from other subpopulations. We validate the performance of Syntax and provide insights into when it might have an advantage over conventional trial designs through e
&lt;/p&gt;</description></item><item><title>SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15024</link><description>&lt;p&gt;
SliceGPT: &#36890;&#36807;&#21024;&#38500;&#34892;&#21644;&#21015;&#26469;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15024
&lt;/p&gt;
&lt;p&gt;
SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#65292;&#20294;&#20351;&#29992;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#31232;&#30095;&#21270;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#36164;&#28304;&#38480;&#21046;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#20107;&#21518;&#30340;&#31232;&#30095;&#21270;&#22788;&#29702;&#12290;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#30828;&#20214;&#19978;&#36895;&#24230;&#21463;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;SliceGPT&#65292;&#35813;&#26041;&#26696;&#29992;&#36739;&#23567;&#30340;&#65288;&#31264;&#23494;&#30340;&#65289;&#30697;&#38453;&#26367;&#25442;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#65292;&#20174;&#32780;&#20943;&#23567;&#32593;&#32476;&#30340;&#23884;&#20837;&#32500;&#24230;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SliceGPT&#22312;&#20445;&#25345;&#30456;&#24212;&#31264;&#23494;&#27169;&#22411;&#30340;99%&#12289;99%&#21644;90%&#30340;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#31227;&#38500;LLAMA2-70B&#12289;OPT 66B&#21644;Phi-2&#27169;&#22411;&#20013;&#22810;&#36798;25%&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#21253;&#25324;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#30340;&#20999;&#29255;&#27169;&#22411;&#22312;&#36739;&#23569;&#30340;GPU&#19978;&#36816;&#34892;&#24182;&#19988;&#26356;&#24555;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20195;&#30721;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14423</link><description>&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#65306;&#20171;&#32461;&#19982;&#39640;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompt Design and Engineering: Introduction and Advanced Methods. (arXiv:2401.14423v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#23398;&#31185;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts as well as review basic and more advanced approaches to prompt design and engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#31639;&#27861;DALex&#65292;&#23427;&#36890;&#36807;&#21152;&#26435;&#35757;&#32451;&#26696;&#20363;&#35823;&#24046;&#30340;&#21644;&#26469;&#36873;&#25321;&#26368;&#20339;&#20010;&#20307;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#35789;&#27861;&#36873;&#25321;&#26356;&#24555;&#36895;&#12290;</title><link>http://arxiv.org/abs/2401.12424</link><description>&lt;p&gt;
DALex: &#36890;&#36807;&#22810;&#26679;&#32858;&#21512;&#23454;&#29616;&#31867;&#20284;&#35789;&#27861;&#36873;&#25321;&#30340;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DALex: Lexicase-like Selection via Diverse Aggregation. (arXiv:2401.12424v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#31639;&#27861;DALex&#65292;&#23427;&#36890;&#36807;&#21152;&#26435;&#35757;&#32451;&#26696;&#20363;&#35823;&#24046;&#30340;&#21644;&#26469;&#36873;&#25321;&#26368;&#20339;&#20010;&#20307;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#35789;&#27861;&#36873;&#25321;&#26356;&#24555;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;&#35789;&#27861;&#36873;&#25321;&#34987;&#35777;&#26126;&#30456;&#27604;&#20854;&#20182;&#36873;&#25321;&#31639;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#35789;&#27861;&#36873;&#25321;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#65292;&#26681;&#25454;&#38543;&#26426;&#39034;&#24207;&#30340;&#35757;&#32451;&#26696;&#20363;&#36827;&#34892;&#36880;&#19968;&#32771;&#34385;&#65292;&#24182;&#22522;&#20110;&#27492;&#36807;&#31243;&#23545;&#31181;&#32676;&#25110;&#20854;&#20182;&#38598;&#21512;&#36827;&#34892;&#31579;&#36873;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36880;&#27493;&#31579;&#36873;&#30340;&#36807;&#31243;&#21487;&#33021;&#20250;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22823;&#37327;&#35757;&#32451;&#26696;&#20363;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DALex&#65288;&#21363;&#22810;&#26679;&#32858;&#21512;&#35789;&#27861;&#36873;&#25321;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#36873;&#25321;&#20010;&#20307;&#26041;&#38754;&#19982;&#35789;&#27861;&#36873;&#25321;&#20960;&#20046;&#31561;&#25928;&#65292;&#20294;&#36895;&#24230;&#26356;&#24555;&#12290;DALex&#26041;&#27861;&#26681;&#25454;&#21152;&#26435;&#35757;&#32451;&#26696;&#20363;&#35823;&#24046;&#30340;&#21644;&#36873;&#25321;&#26368;&#20339;&#20010;&#20307;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#38543;&#26426;&#37319;&#26679;&#30340;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#36873;&#25321;&#25152;&#38656;&#30340;&#26680;&#24515;&#35745;&#31639;&#24418;&#24335;&#21270;&#20026;&#30697;&#38453;&#20056;&#27861;&#65292;&#32780;&#19981;&#26159;&#36882;&#24402;&#24490;&#29615;&#27604;&#36739;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#20248;&#21270;&#21644;&#24182;&#34892;&#21270;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexicase selection has been shown to provide advantages over other selection algorithms in several areas of evolutionary computation and machine learning. In its standard form, lexicase selection filters a population or other collection based on randomly ordered training cases that are considered one at a time. This iterated filtering process can be time-consuming, particularly in settings with large numbers of training cases. In this paper, we propose a new method that is nearly equivalent to lexicase selection in terms of the individuals that it selects, but which does so significantly more quickly. The new method, called DALex (for Diversely Aggregated Lexicase), selects the best individual with respect to a weighted sum of training case errors, where the weights are randomly sampled. This allows us to formulate the core computation required for selection as matrix multiplication instead of recursive loops of comparisons, which in turn allows us to take advantage of optimized and pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.11176</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#30446;&#26631;&#23450;&#20301;: &#20351;&#29992;Cram&#233;r-Rao&#30028;&#38480;&#23545;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\'er-Rao Bound. (arXiv:2401.11176v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#38647;&#36798;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#36827;&#34892;&#31934;&#30830;&#30340;&#30446;&#26631;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36798;&#21040;Cram&#233;r-Rao&#30028;&#38480;&#65288;CRB&#65289;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#35823;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20248;&#20110;&#36825;&#20123;&#20256;&#32479;&#25216;&#26415;&#65292;&#22312;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#20195;&#34920;&#24615;&#30340;&#27169;&#25311;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22987;&#32456;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#20559;&#35265;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#20943;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern radar systems, precise target localization using azimuth and velocity estimation is paramount. Traditional unbiased estimation methods have leveraged gradient descent algorithms to reach the theoretical limits of the Cram\'er Rao Bound (CRB) for the error of the parameter estimates. In this study, we present a data-driven neural network approach that outperforms these traditional techniques, demonstrating improved accuracies in target azimuth and velocity estimation. Using a representative simulated scenario, we show that our proposed neural network model consistently achieves improved parameter estimates due to its inherently biased nature, yielding a diminished mean squared error (MSE). Our findings underscore the potential of employing deep learning methods in radar systems, paving the way for more accurate localization in cluttered and dynamic environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;</title><link>http://arxiv.org/abs/2401.10816</link><description>&lt;p&gt;
Co-Pilot for Health: &#20010;&#24615;&#21270;&#31639;&#27861;AI&#24341;&#23548;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#33258;&#21160;&#22609;&#36896;&#22823;&#22411;&#20154;&#32676;&#30340;&#20581;&#24247;&#34892;&#20026;&#65292;&#36328;&#21487;&#31359;&#25140;&#35774;&#22791;&#31867;&#22411;&#21644;&#30142;&#30149;&#29366;&#20917;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#20840;&#29699;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#20581;&#36523;&#35774;&#22791;&#30340;&#31934;&#32454;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#29992;&#20110;&#25968;&#23383;&#31639;&#27861;&#24341;&#23548;&#12290;&#22312;&#27492;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#24179;&#21488;&#22312;&#26032;&#21152;&#22369;&#38024;&#23545;$n=84,764$&#20010;&#20010;&#20307;&#30340;12&#21608;&#26399;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#30340;&#26377;&#25928;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#32479;&#35745;&#39564;&#35777;&#20102;&#30446;&#26631;&#32452;&#20013;&#25509;&#21463;&#27492;&#31867;AI&#20248;&#21270;&#26085;&#24120;&#24341;&#23548;&#30340;&#21442;&#19982;&#32773;&#30456;&#36739;&#20110;&#25511;&#21046;&#32452;&#20013;&#26410;&#25509;&#21463;&#20219;&#20309;&#24341;&#23548;&#30340;&#21305;&#37197;&#21442;&#19982;&#32773;&#65292;&#20854;&#27599;&#22825;&#30340;&#27493;&#25968;&#22686;&#21152;&#20102;6.17%&#65288;$p = 3.09\times10^{-4}$&#65289;&#65292;&#27599;&#21608;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#65288;MVPA&#65289;&#20998;&#38047;&#22686;&#21152;&#20102;7.61%&#65288;$p = 1.16\times10^{-2}$&#65289;&#12290;&#27492;&#22806;&#65292;&#27492;&#31867;&#24341;&#23548;&#38750;&#24120;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#38544;&#31169;&#20445;&#25252;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.05126</link><description>&lt;p&gt;
&#39640;&#25928;&#39046;&#22495;&#36866;&#24212;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;&#35270;&#35273;Transformer&#30340;&#31934;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer. (arXiv:2401.05126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#38544;&#31169;&#20445;&#25252;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#29992;&#35270;&#35273;&#20445;&#25252;&#30340;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#24182;&#36827;&#34892;&#27979;&#35797;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36991;&#20813;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#19981;&#33021;&#36991;&#20813;&#22270;&#20687;&#21152;&#23494;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#30340;ViT&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method for privacy-preserving deep neural networks (DNNs) with the Vision Transformer (ViT). The method allows us not only to train models and test with visually protected images but to also avoid the performance degradation caused from the use of encrypted images, whereas conventional methods cannot avoid the influence of image encryption. A domain adaptation method is used to efficiently fine-tune ViT with encrypted images. In experiments, the method is demonstrated to outperform conventional methods in an image classification task on the CIFAR-10 and ImageNet datasets in terms of classification accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.04130</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65306;&#36890;&#36807;&#25554;&#20837;&#21644;&#25773;&#25918;&#21464;&#25442;&#22120;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;(PET)&#26041;&#27861;&#65292;&#22914;LoRA&#12289;Adapter&#21644;Visual Prompt Tuning(VPT)&#65292;&#36890;&#36807;&#35843;&#25972;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#23567;&#27169;&#22359;&#65292;&#22312;&#20351;&#36866;&#24212;&#26032;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#39046;&#22495;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20063;&#19981;&#29616;&#23454;&#20026;&#27599;&#20010;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#23450;&#21046;&#30340;&#35843;&#25972;&#27169;&#22359;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PLUTO&#65306;&#19968;&#31181;&#25554;&#25300;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#19987;&#20026;&#19981;&#21516;&#30340;&#28304;&#39046;&#22495;&#36827;&#34892;&#20102;&#19987;&#38376;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;(TTA)&#26041;&#27861;&#65292;&#26469;(1)&#20174;&#24211;&#20013;&#36873;&#25321;&#20986;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;(2)&#22312;&#19981;&#35843;&#25972;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36825;&#31181;&#25554;&#25300;&#24335;&#30340;&#29305;&#24615;&#20351;&#24471;&#23427;&#21487;===
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#40479;&#40483;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26080;&#38656;&#26631;&#27880;&#30340;&#26041;&#24335;&#65292;&#20174;&#38899;&#39057;&#24405;&#38899;&#20013;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#40479;&#40483;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#39640;&#40479;&#27963;&#36291;&#31383;&#21475;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2312.15824</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#40479;&#40483;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Few-Shot Bird Sound Classification. (arXiv:2312.15824v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#40479;&#40483;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26080;&#38656;&#26631;&#27880;&#30340;&#26041;&#24335;&#65292;&#20174;&#38899;&#39057;&#24405;&#38899;&#20013;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#40479;&#40483;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#39640;&#40479;&#27963;&#36291;&#31383;&#21475;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#22312;&#29983;&#29289;&#22768;&#23398;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#25910;&#38598;&#22823;&#37327;&#30340;&#22768;&#38899;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#38656;&#26631;&#27880;&#23601;&#33021;&#22815;&#20174;&#38899;&#39057;&#24405;&#38899;&#20013;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#40479;&#40483;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#22330;&#26223;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#40479;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#39640;&#40479;&#27963;&#36291;&#30340;&#31383;&#21475;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) in audio holds significant potential across various domains, particularly in situations where abundant, unlabeled data is readily available at no cost. This is particularly pertinent in bioacoustics, where biologists routinely collect extensive sound datasets from the natural environment. In this study, we demonstrate that SSL is capable of acquiring meaningful representations of bird sounds from audio recordings without the need for annotations. Our experiments showcase that these learned representations exhibit the capacity to generalize to new bird species in few-shot learning (FSL) scenarios. Additionally, we show that selecting windows with high bird activation for self-supervised learning, using a pretrained audio neural network, significantly enhances the quality of the learned representations.
&lt;/p&gt;</description></item><item><title>FairWASP&#26159;&#19968;&#31181;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#20154;&#21475;&#24179;&#31561;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00109</link><description>&lt;p&gt;
FairWASP&#65306;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
FairWASP: Fast and Optimal Fair Wasserstein Pre-processing. (arXiv:2311.00109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00109
&lt;/p&gt;
&lt;p&gt;
FairWASP&#26159;&#19968;&#31181;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#20154;&#21475;&#24179;&#31561;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#26088;&#22312;&#20943;&#23569;&#19981;&#21516;&#23376;&#32676;&#20307;&#20043;&#38388;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#24179;&#31561;&#24615;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#20250;&#34987;&#19981;&#21516;&#29992;&#25143;&#22312;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#35757;&#32451;&#25968;&#25454;&#26412;&#36523;&#36827;&#34892;&#24178;&#39044;&#21487;&#33021;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;FairWASP&#65292;&#26088;&#22312;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#32780;&#19981;&#20250;&#20462;&#25913;&#21407;&#22987;&#25968;&#25454;&#12290;FairWASP&#36820;&#22238;&#26679;&#26412;&#32423;&#26435;&#37325;&#65292;&#20351;&#37325;&#26032;&#21152;&#26435;&#30340;&#25968;&#25454;&#38598;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21516;&#26102;&#28385;&#36275;&#65288;&#32463;&#39564;&#29256;&#26412;&#30340;&#65289;&#20154;&#21475;&#24179;&#31561;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25972;&#25968;&#26435;&#37325;&#30340;&#26368;&#20248;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#31561;&#21516;&#22320;&#29702;&#35299;&#20026;&#22797;&#21046;&#25110;&#21024;&#38500;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;FairWASP&#21487;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25509;&#21463;&#26679;&#26412;&#26435;&#37325;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33529;&#26524;&#21697;&#23581;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#20174;&#32452;&#21512;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;Effective width&#21442;&#25968;&#65292;&#32039;&#23494;&#37327;&#21270;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#65292;&#24182;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#30340;&#19977;&#20998;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19064</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33529;&#26524;&#21697;&#23581;&#30340;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Learnability of Apple Tasting. (arXiv:2310.19064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33529;&#26524;&#21697;&#23581;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#20174;&#32452;&#21512;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;Effective width&#21442;&#25968;&#65292;&#32039;&#23494;&#37327;&#21270;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#65292;&#24182;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#30340;&#19977;&#20998;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#23398;&#20064;&#32773;&#21482;&#26377;&#22312;&#39044;&#27979;&#20026;"1"&#26102;&#35266;&#23519;&#21040;&#30495;&#23454;&#26631;&#31614;&#12290;&#26412;&#25991;&#37325;&#26032;&#30740;&#31350;&#20102;&#36825;&#31181;&#32463;&#20856;&#30340;&#37096;&#20998;&#21453;&#39304;&#35774;&#32622;&#65292;&#24182;&#20174;&#32452;&#21512;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#21487;&#30693;&#35774;&#32622;&#19979;&#65292;Littlestone&#32500;&#24230;&#20173;&#28982;&#26159;&#33529;&#26524;&#21697;&#23581;&#30340;&#32039;&#23494;&#23450;&#37327;&#21051;&#30011;&#65292;&#35299;&#20915;&#20102;\cite{helmbold2000apple}&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#21442;&#25968;&#65292;&#31216;&#20026;&#26377;&#25928;&#23485;&#24230;&#65292;&#32039;&#23494;&#37327;&#21270;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#12290;&#20316;&#20026;&#25512;&#35770;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#25928;&#23485;&#24230;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#30340;&#19977;&#20998;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#65292;&#20219;&#20309;&#23398;&#20064;&#32773;&#22312;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#19979;&#30340;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#21482;&#33021;&#26159;$\Theta(1), \Theta(\sqrt{T})$, &#25110; $\Theta(T)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In online binary classification under \textit{apple tasting} feedback, the learner only observes the true label if it predicts "1". First studied by \cite{helmbold2000apple}, we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective. We show that the Littlestone dimension continues to prove a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by \cite{helmbold2000apple}. In addition, we give a new combinatorial parameter, called the Effective width, that tightly quantifies the minimax expected mistakes in the realizable setting. As a corollary, we use the Effective width to establish a \textit{trichotomy} of the minimax expected number of mistakes in the realizable setting. In particular, we show that in the realizable setting, the expected number of mistakes for any learner under apple tasting feedback can only be $\Theta(1), \Theta(\sqrt{T})$, or $\Theta(T)$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>Redco&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs&#65292;&#24182;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2310.16355</link><description>&lt;p&gt;
Redco:&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#21487;&#22312;&#20219;&#20309;GPU/TPUs&#19978;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs
&lt;/p&gt;
&lt;p&gt;
Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs. (arXiv:2310.16355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16355
&lt;/p&gt;
&lt;p&gt;
Redco&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;LLMs&#65292;&#24182;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#20869;&#23384;&#38656;&#27714;&#32473;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#21306;&#20197;&#20998;&#24067;&#22312;&#22810;&#20010;GPU&#25110;TPU&#19978;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#29616;&#26377;&#27169;&#22411;&#24182;&#34892;&#24037;&#20855;&#65288;&#22914;Megatron-LM&#12289;DeepSpeed&#21644;Alpa&#65289;&#36827;&#34892;&#30456;&#24403;&#30340;&#32534;&#30721;&#21644;&#22797;&#26434;&#30340;&#37197;&#32622;&#24037;&#20316;&#12290;&#36825;&#20123;&#24037;&#20855;&#38656;&#35201;&#29992;&#25143;&#20855;&#22791;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65288;MLSys&#65289;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#32473;LLM&#24320;&#21457;&#24102;&#26469;&#20102;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;MLSys&#32972;&#26223;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Redco&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;LLMs&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#20197;&#21450;&#31616;&#21270;ML&#27969;&#31243;&#30340;&#24320;&#21457;&#12290;Redco&#30340;&#35774;&#35745;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#33258;&#21160;&#21270;&#27169;&#22411;&#24182;&#34892;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#35268;&#21017;&#65292;&#29992;&#20110;&#20026;&#20219;&#20309;GPU / TPU&#29983;&#25104;&#24352;&#37327;&#24182;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07665</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#28335;&#23545;&#22240;&#26524;&#19968;&#33268;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#22238;&#31572;&#22312;&#25913;&#21464;&#24773;&#20917;&#19979;&#20250;&#35266;&#23519;&#21040;&#20160;&#20040;&#26469;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#26465;&#20214;&#26159;&#26681;&#25454;&#23454;&#38469;&#35266;&#23519;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20171;&#20837;&#24335;&#35299;&#37322;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22238;&#28335;&#21407;&#21017;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25345;&#25152;&#26377;&#22240;&#26524;&#23450;&#24459;&#23436;&#25972;&#24615;&#30340;&#26367;&#20195;&#21746;&#23398;&#65292;&#20294;&#20854;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#30001;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#32452;&#25104;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#20998;&#37197;&#26045;&#21152;&#20102;&#26465;&#20214;&#65292;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#19968;&#20010;&#21487;&#34892;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;&#39046;&#22495;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#36981;&#23432;&#22240;&#26524;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
&lt;/p&gt;</description></item><item><title>LLark&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#23454;&#29616;&#38899;&#20048;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#21305;&#37197;&#25110;&#36229;&#20986;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#39640;&#24230;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.07160</link><description>&lt;p&gt;
LLark: &#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLark: A Multimodal Foundation Model for Music. (arXiv:2310.07160v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07160
&lt;/p&gt;
&lt;p&gt;
LLark&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#23454;&#29616;&#38899;&#20048;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#21305;&#37197;&#25110;&#36229;&#20986;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#39640;&#24230;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#20855;&#26377;&#29420;&#29305;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#23545;&#20110;&#19987;&#19994;&#20154;&#22763;&#21644;&#29616;&#26377;&#30340;AI&#31995;&#32479;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#30456;&#23545;&#20110;&#20854;&#20182;&#24418;&#24335;&#30340;&#38899;&#39057;&#21576;&#29616;&#20986;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLark&#65292;&#19968;&#31181;&#38024;&#23545;&#38899;&#20048;&#29702;&#35299;&#30340;&#25351;&#20196;&#35843;&#35856;&#22810;&#27169;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#22686;&#24378;&#22810;&#26679;&#21270;&#30340;&#24320;&#28304;&#38899;&#20048;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#25351;&#20196;&#35843;&#35856;&#26684;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#29992;&#20110;LLark&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#22312;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65288;&#38899;&#20048;&#29702;&#35299;&#12289;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#65289;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#19978;&#19982;&#29616;&#26377;&#22522;&#20934;&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#36229;&#20986;&#65292;&#24182;&#19988;&#22312;&#23383;&#24149;&#29983;&#25104;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#20154;&#31867;&#19982;&#27169;&#22411;&#30340;&#21709;&#24212;&#26174;&#31034;&#20986;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;LLark&#23436;&#20840;&#26159;&#26681;&#25454;&#24320;&#28304;&#38899;&#20048;&#25968;&#25454;&#21644;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>http://arxiv.org/abs/2309.17179</link><description>&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17179
&lt;/p&gt;
&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36890;&#24120;&#37319;&#29992;&#37319;&#26679;&#25110;&#26463;&#25628;&#32034;&#65292;&#32467;&#21512; Chain-of-Thought (CoT) &#31561;&#25552;&#31034;&#26469;&#25552;&#39640;&#25512;&#29702;&#21644;&#35299;&#30721;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914; Tree-of-Thought (ToT) &#21644; Reasoning via Planning (RAP) &#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24341;&#23548;&#22810;&#27493;&#25512;&#29702;&#65292;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#28608;&#27963;LLM&#20316;&#20026;&#19968;&#20010;&#20215;&#20540;&#20989;&#25968;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AlphaZero&#31867;&#20284;&#30340;&#29992;&#20110;LLM&#30340;&#26641;&#25628;&#32034;&#26694;&#26550; (&#31216;&#20026;TS-LLM)&#65292;&#31995;&#32479;&#22320;&#35828;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21033;&#29992;&#26641;&#25628;&#32034;&#26469;&#25351;&#23548;LLM&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;TS-LLM&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#19982;&#20247;&#19981;&#21516;&#65306;(1)&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26222;&#36866;&#22320;&#24212;&#29992;&#20110;&#38500;&#20102;&#25512;&#29702;&#20043;&#22806;&#30340;&#19981;&#21516;&#20219;&#21153; (&#20363;&#22914;RLHF&#23545;&#40784;)&#65292;&#20197;&#21450;&#20219;&#20309;&#22823;&#23567;&#30340;LLM&#65292;&#32780;&#19981;&#38656;&#35201;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;Transformer&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#20102;&#19982;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30456;&#20851;&#30340;&#24207;&#21015;&#37096;&#20998;&#65292;&#20026;&#34507;&#30333;&#36136;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.03631</link><description>&lt;p&gt;
&#23545;&#20110;&#34507;&#30333;&#21151;&#33021;&#39044;&#27979;&#20013;Transformer&#27169;&#22411;&#20869;&#37096;&#36816;&#20316;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Insights Into the Inner Workings of Transformer Models for Protein Function Prediction. (arXiv:2309.03631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;Transformer&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#20102;&#19982;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30456;&#20851;&#30340;&#24207;&#21015;&#37096;&#20998;&#65292;&#20026;&#34507;&#30333;&#36136;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#25105;&#20204;&#25506;&#32034;&#20102;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22914;&#20309;&#24110;&#21161;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#36890;&#36807;&#25193;&#23637;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#8212;&#8212;&#38598;&#25104;&#26799;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#26816;&#26597;&#35843;&#25972;&#20026;&#22522;&#22240;&#26412;&#20307;&#26415;&#35821;&#21644;&#37238;&#22996;&#21592;&#20250;&#32534;&#21495;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;&#20869;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#32467;&#26524;&#65306;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#21464;&#21387;&#22120;&#22312;&#24207;&#21015;&#20013;&#29305;&#21035;&#20851;&#27880;&#30340;&#27688;&#22522;&#37240;&#65292;&#24182;&#23637;&#31034;&#36825;&#20123;&#30456;&#20851;&#30340;&#24207;&#21015;&#37096;&#20998;&#21453;&#26144;&#20102;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30340;&#39044;&#26399;&#65292;&#26080;&#35770;&#26159;&#22312;&#23884;&#20837;&#23618;&#36824;&#26159;&#27169;&#22411;&#20869;&#37096;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21464;&#21387;&#22120;&#22836;&#19982;&#22320;&#38754;&#30495;&#23454;&#24207;&#21015;&#27880;&#37322;&#65288;&#20363;&#22914;&#65292;&#36328;&#33180;&#21306;&#22495;&#65292;&#27963;&#24615;&#20301;&#28857;&#65289;&#20043;&#38388;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#23545;&#24212;&#30340;&#24402;&#22240;&#22270;&#30340;&#21464;&#21387;&#22120;&#22836;&#65292;&#36825;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/markuswenzel/xai-proteins &#19978;&#33719;&#21462;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CktGNN&#30340;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#35782;&#21035;&#30005;&#36335;&#30340;&#22270;&#24418;&#29305;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#21270;&#30005;&#36335;&#25299;&#25169;&#29983;&#25104;&#21644;&#22120;&#20214;&#23610;&#23544;&#35843;&#25972;&#12290;&#23427;&#20351;&#29992;&#20004;&#32423;GNN&#26694;&#26550;&#23545;&#30005;&#36335;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#35774;&#35745;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.16406</link><description>&lt;p&gt;
CktGNN&#65306;&#29992;&#20110;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#30340;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CktGNN: Circuit Graph Neural Network for Electronic Design Automation. (arXiv:2308.16406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CktGNN&#30340;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#35782;&#21035;&#30005;&#36335;&#30340;&#22270;&#24418;&#29305;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#21270;&#30005;&#36335;&#25299;&#25169;&#29983;&#25104;&#21644;&#22120;&#20214;&#23610;&#23544;&#35843;&#25972;&#12290;&#23427;&#20351;&#29992;&#20004;&#32423;GNN&#26694;&#26550;&#23545;&#30005;&#36335;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#35774;&#35745;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24040;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#22797;&#26434;&#30340;&#35774;&#35745;&#26435;&#34913;&#65292;&#27169;&#25311;&#30005;&#36335;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#19968;&#30452;&#26159;&#38598;&#25104;&#30005;&#36335;&#39046;&#22495;&#30340;&#19968;&#20010;&#38271;&#26399;&#25361;&#25112;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#22823;&#22810;&#25968;&#20851;&#27880;&#20110;&#22312;&#32473;&#23450;&#30005;&#36335;&#25299;&#25169;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#35843;&#25972;&#26230;&#20307;&#31649;&#23610;&#23544;&#12290;&#36890;&#36807;&#35782;&#21035;&#30005;&#36335;&#30340;&#22270;&#24418;&#29305;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#36335;&#22270;&#31070;&#32463;&#32593;&#32476;(CktGNN)&#65292;&#23427;&#22522;&#20110;&#32534;&#30721;&#22120;&#20381;&#36182;&#30340;&#20248;&#21270;&#23376;&#31243;&#24207;&#65292;&#21516;&#26102;&#33258;&#21160;&#21270;&#30005;&#36335;&#25299;&#25169;&#29983;&#25104;&#21644;&#22120;&#20214;&#23610;&#23544;&#35843;&#25972;&#12290;&#29305;&#21035;&#26159;&#65292;CktGNN&#20351;&#29992;&#20004;&#32423;GNN&#26694;&#26550;&#65288;&#23884;&#22871;GNN&#65289;&#23545;&#30005;&#36335;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#20854;&#20013;&#30005;&#36335;&#34920;&#31034;&#20026;&#24050;&#30693;&#23376;&#22270;&#22522;&#30784;&#19978;&#30340;&#23376;&#22270;&#32452;&#21512;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#20943;&#23569;&#23376;&#22270;&#25968;&#37327;&#26469;&#26174;&#33879;&#25552;&#39640;&#35774;&#35745;&#25928;&#29575;&#20197;&#25191;&#34892;&#28040;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have mostly been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves design efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.</title><link>http://arxiv.org/abs/2307.07514</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#19981;&#26159;&#28216;&#25103;&#12290;(arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07514
&lt;/p&gt;
&lt;p&gt;
Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#24110;&#21161;&#20154;&#31867;&#20915;&#31574;&#32773;&#29702;&#35299;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;XAI&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#26469;&#29702;&#35770;&#19978;&#35777;&#26126;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#35770;&#35777;&#65292;&#35828;&#26126;Shapley&#20540;&#21487;&#33021;&#20250;&#32473;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#25552;&#20379;&#35823;&#23548;&#65292;&#20351;&#20854;&#20026;&#39044;&#27979;&#20013;&#26080;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#26356;&#39640;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#23545;&#19982;&#39044;&#27979;&#26377;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#36739;&#20302;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#24847;&#20041;&#22312;&#20110;&#23427;&#20204;&#26377;&#25928;&#22320;&#25361;&#25112;&#20102;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#22810;&#31181;&#25552;&#35758;&#29992;&#27861;&#65292;&#36825;&#20123;&#29992;&#27861;&#27491;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding complex machine learning (ML) models. One of the hallmarks of XAI are measures of relative feature importance, which are theoretically justified through the use of Shapley values. This paper builds on recent work and offers a simple argument for why Shapley values can provide misleading measures of relative feature importance, by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction. The significance of these results is that they effectively challenge the many proposed uses of measures of relative feature importance in a fast-growing range of high-stakes application domains.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#26799;&#24230;&#26469;&#33258;&#21160;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#65292;&#20197;&#22312;&#35757;&#32451;&#25439;&#22833;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.04526</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Self Expanding Neural Networks. (arXiv:2307.04526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04526
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#26799;&#24230;&#26469;&#33258;&#21160;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#65292;&#20197;&#22312;&#35757;&#32451;&#25439;&#22833;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#32467;&#26524;&#20005;&#37325;&#20381;&#36182;&#20110;&#25152;&#36873;&#25321;&#30340;&#26550;&#26500;&#65307;&#21363;&#20351;&#21482;&#26159;&#23545;&#32593;&#32476;&#22823;&#23567;&#20570;&#24494;&#23567;&#20462;&#25913;&#65292;&#36890;&#24120;&#20063;&#38656;&#35201;&#37325;&#26032;&#24320;&#22987;&#35757;&#32451;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#23567;&#30340;&#26550;&#26500;&#24320;&#22987;&#35757;&#32451;&#65292;&#21482;&#22312;&#38382;&#39064;&#38656;&#35201;&#26102;&#25193;&#23637;&#20854;&#23481;&#37327;&#65292;&#24182;&#36991;&#20813;&#24178;&#25200;&#20808;&#21069;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#24403;&#36825;&#26679;&#20570;&#21487;&#33021;&#22823;&#24133;&#38477;&#20302;&#20551;&#35774;&#25910;&#25947;&#35757;&#32451;&#25439;&#22833;&#26102;&#65292;&#30452;&#35266;&#22320;&#25193;&#23637;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31070;&#32463;&#20803;&#28155;&#21152;&#30340;&#8220;&#36895;&#29575;&#8221;&#19978;&#30028;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#35745;&#31639;&#24265;&#20215;&#30340;&#25193;&#23637;&#35780;&#20998;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#33258;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#37027;&#20123;&#21512;&#36866;&#30340;&#26550;&#26500;&#22823;&#23567;&#22312;&#20808;&#39564;&#19978;&#30456;&#24403;&#19981;&#30830;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The results of training a neural network are heavily dependent on the architecture chosen; and even a modification of only the size of the network, however small, typically involves restarting the training process. In contrast to this, we begin training with a small architecture, only increase its capacity as necessary for the problem, and avoid interfering with previous optimization while doing so. We thereby introduce a natural gradient based approach which intuitively expands both the width and depth of a neural network when this is likely to substantially reduce the hypothetical converged training loss. We prove an upper bound on the "rate" at which neurons are added, and a computationally cheap lower bound on the expansion score. We illustrate the benefits of such Self-Expanding Neural Networks in both classification and regression problems, including those where the appropriate architecture size is substantially uncertain a priori.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#31216;&#20026;&#39034;&#24207;&#26497;&#23567;&#26497;&#22823;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23545;&#26377;&#30028;&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#36827;&#34892;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#23545;&#21521;&#37327;&#20540;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#30340;&#32039;&#23494;&#23450;&#37327;&#21051;&#30011;&#12290;</title><link>http://arxiv.org/abs/2307.03816</link><description>&lt;p&gt;
&#22312;&#26377;&#30028;&#25439;&#22833;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#30340;&#32452;&#21512;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Combinatorial Characterization of Online Learning Games with Bounded Losses. (arXiv:2307.03816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03816
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#31216;&#20026;&#39034;&#24207;&#26497;&#23567;&#26497;&#22823;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23545;&#26377;&#30028;&#25439;&#22833;&#30340;&#22312;&#32447;&#23398;&#20064;&#28216;&#25103;&#36827;&#34892;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#23545;&#21521;&#37327;&#20540;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#30340;&#32039;&#23494;&#23450;&#37327;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20551;&#35774;&#31867;&#21035;&#23545;&#20219;&#24847;&#20294;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#31216;&#20026;&#39034;&#24207;&#26497;&#23567;&#26497;&#22823;&#32500;&#24230;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#32039;&#23494;&#23450;&#37327;&#22320;&#21051;&#30011;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23545;&#20004;&#20010;&#24120;&#35265;&#23398;&#20064;&#22330;&#26223;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#30340;&#39318;&#20010;&#23450;&#37327;&#21051;&#30011;&#65306;&#21521;&#37327;&#20540;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the online learnability of hypothesis classes with respect to arbitrary, but bounded, loss functions. We give a new scale-sensitive combinatorial dimension, named the sequential Minimax dimension, and show that it gives a tight quantitative characterization of online learnability. As applications, we give the first quantitative characterization of online learnability for two natural learning settings: vector-valued regression and multilabel classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.16978</link><description>&lt;p&gt;
&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments. (arXiv:2306.16978v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26159;&#23547;&#25214;&#35206;&#30422;&#32473;&#23450;&#23553;&#38381;&#21306;&#22495;&#25972;&#20010;&#33258;&#30001;&#31354;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#26426;&#22120;&#20154;&#21106;&#33609;&#21644;&#21560;&#23576;&#21040;&#22320;&#38647;&#28165;&#38500;&#21644;&#25628;&#25937;&#20219;&#21153;&#12290;&#34429;&#28982;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#20026;&#24050;&#30693;&#29615;&#22659;&#25214;&#21040;&#21487;&#35777;&#26126;&#23436;&#22791;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#36335;&#24452;&#65292;&#20294;&#22312;&#22312;&#32447;&#22330;&#26223;&#19979;&#65292;&#29615;&#22659;&#20107;&#20808;&#26410;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38750;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#20215;&#20540;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#26500;&#24314;&#35266;&#23519;&#31354;&#38388;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#35268;&#21010;&#38271;&#26399;&#36335;&#24452;&#65292;&#24182;&#21516;&#26102;&#23545;&#30701;&#26399;&#38556;&#30861;&#29289;&#36827;&#34892;&#34892;&#21160;&#12290;&#20026;&#20102;&#32771;&#34385;&#22823;&#35268;&#27169;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#23610;&#24230;&#22320;&#22270;&#36755;&#20837;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#20943;&#23569;&#36335;&#24452;&#20559;&#31163;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage path planning is the problem of finding the shortest path that covers the entire free space of a given confined area, with applications ranging from robotic lawn mowing and vacuum cleaning, to demining and search-and-rescue tasks. While offline methods can find provably complete, and in some cases optimal, paths for known environments, their value is limited in online scenarios where the environment is not known beforehand, especially in the presence of non-static obstacles. We propose an end-to-end reinforcement learning-based approach in continuous state and action space, for the online coverage path planning problem that can handle unknown environments. We construct the observation space from both global maps and local sensory inputs, allowing the agent to plan a long-term path, and simultaneously act on short-term obstacle detections. To account for large-scale environments, we propose to use a multi-scale map input representation. Furthermore, we propose a novel total var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#65288;ARS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#28436;&#21270;&#20989;&#25968;&#21644;&#32570;&#22833;&#21464;&#37327;&#65292;&#29992;&#20110;&#39044;&#27979;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#21464;&#37327;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.16593</link><description>&lt;p&gt;
&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#30340;&#21457;&#23637;&#39044;&#27979;&#22312;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#30340;&#24110;&#21161;&#19979;
&lt;/p&gt;
&lt;p&gt;
Forecasting of the development of a partially-observed dynamical time series with the aid of time-invariance and linearity. (arXiv:2306.16593v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#65288;ARS&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#21516;&#26102;&#20272;&#35745;&#28436;&#21270;&#20989;&#25968;&#21644;&#32570;&#22833;&#21464;&#37327;&#65292;&#29992;&#20110;&#39044;&#27979;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#32570;&#22833;&#21464;&#37327;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20135;&#29983;&#19968;&#31181;&#20381;&#36182;&#22810;&#20803;&#24207;&#21015;&#65292;&#31216;&#20026;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#65292;&#36890;&#36807;&#28436;&#21270;&#20989;&#25968;&#21457;&#23637;&#32780;&#26469;&#12290;&#30001;&#20110;&#24403;&#21069;&#26102;&#38388;&#28857;&#30340;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#21464;&#37327;&#36890;&#24120;&#20381;&#36182;&#20110;&#21069;&#19968;&#20010;&#26102;&#38388;&#28857;&#30340;&#25152;&#26377;&#21464;&#37327;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#20272;&#35745;&#28436;&#21270;&#20989;&#25968;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#19968;&#20123;&#21464;&#37327;&#26159;&#32570;&#22833;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#65288;ARS&#65289;&#27169;&#22411;&#12290;ARS&#27169;&#22411;&#28041;&#21450;&#28436;&#21270;&#20989;&#25968;&#21644;&#20316;&#20026;&#26494;&#24347;&#26102;&#38388;&#24207;&#21015;&#30340;&#28508;&#22312;&#32570;&#22833;&#21464;&#37327;&#30340;&#21516;&#26102;&#20272;&#35745;&#65292;&#20511;&#21161;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#19981;&#21464;&#24615;&#21644;&#32447;&#24615;&#24615;&#12290;&#26412;&#30740;&#31350;&#23454;&#35777;&#20102;&#25552;&#20986;&#30340;ARS&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A dynamical system produces a dependent multivariate sequence called dynamical time series, developed with an evolution function. As variables in the dynamical time series at the current time-point usually depend on the whole variables in the previous time-point, existing studies forecast the variables at the future time-point by estimating the evolution function. However, some variables in the dynamical time-series are missing in some practical situations. In this study, we propose an autoregressive with slack time series (ARS) model. ARS model involves the simultaneous estimation of the evolution function and the underlying missing variables as a slack time series, with the aid of the time-invariance and linearity of the dynamical system. This study empirically demonstrates the effectiveness of the proposed ARS model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21521;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;KL&#25955;&#24230;&#39033;&#30340;&#26041;&#27861;&#65292;&#26469;&#20445;&#35777;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#24212;&#20998;&#24067;&#37325;&#21512;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10943</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#30340;&#27010;&#29575;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Probabilistic matching of real and generated data statistics in generative adversarial networks. (arXiv:2306.10943v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21521;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;KL&#25955;&#24230;&#39033;&#30340;&#26041;&#27861;&#65292;&#26469;&#20445;&#35777;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#24212;&#20998;&#24067;&#37325;&#21512;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#34429;&#28982;&#29983;&#25104;&#26679;&#26412;&#24448;&#24448;&#38590;&#20197;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#23427;&#20204;&#36981;&#24490;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#30830;&#20445;&#26576;&#20123;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#24212;&#20998;&#24067;&#37325;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#20102;Kullback-Leibler&#39033;&#65306;KL&#25955;&#24230;&#26159;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20174;&#23567;&#25209;&#37327;&#20540;&#33719;&#24471;&#30340;&#30456;&#24212;&#29983;&#25104;&#20998;&#24067;&#21644;&#30001;&#26465;&#20214;&#33021;&#37327;&#27169;&#22411;&#34920;&#31034;&#30340;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks constitute a powerful approach to generative modeling. While generated samples often are indistinguishable from real data, there is no guarantee that they will follow the true data distribution. In this work, we propose a method to ensure that the distributions of certain generated data statistics coincide with the respective distributions of the real data. In order to achieve this, we add a Kullback-Leibler term to the generator loss function: the KL divergence is taken between the true distributions as represented by a conditional energy-based model, and the corresponding generated distributions obtained from minibatch values at each iteration. We evaluate the method on a synthetic dataset and two real-world datasets and demonstrate improved performance of our method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEILO&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#19987;&#23478;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#24182;&#23454;&#29616;&#20102;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.09805</link><description>&lt;p&gt;
&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient On-Policy Imitation Learning from Observations. (arXiv:2306.09805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEILO&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#19987;&#23478;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#24182;&#23454;&#29616;&#20102;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#65292;&#27169;&#20223;&#23398;&#20064; (ILD) &#26088;&#22312;&#36890;&#36807;&#28040;&#38500;&#24378;&#21270;&#23398;&#20064;&#30340;&#35768;&#22810;&#32570;&#28857;&#26469;&#24110;&#21161;&#23398;&#20064;&#36755;&#20986;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#32570;&#20047;&#19987;&#23478;&#34892;&#21160;&#25351;&#23548;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;ILD&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#32771;&#34385;&#35266;&#27979;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064; (ILO)&#65292;&#20854;&#20013;&#27809;&#26377;&#25552;&#20379;&#19987;&#23478;&#21160;&#20316;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#31574;&#30053;&#23398;&#20064;&#65292;&#36825;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#25104;&#26412;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; SEILO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#31639;&#27861;&#65292;&#29992;&#20110; ILO&#65292;&#23558;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#19982;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#23545;&#25239;&#31243;&#24207;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#20013;&#33719;&#24471;&#21453;&#39304;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31574;&#30053; ILO &#21644; ILD &#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23454;&#29616;&#19987;&#23478;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning from demonstrations (ILD) aims to alleviate numerous shortcomings of reinforcement learning through the use of demonstrations. However, in most real-world applications, expert action guidance is absent, making the use of ILD impossible. Instead, we consider imitation learning from observations (ILO), where no expert actions are provided, making it a significantly more challenging problem to address. Existing methods often employ on-policy learning, which is known to be sample-costly. This paper presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that combines standard adversarial imitation learning with inverse dynamics modeling. This approach enables the agent to receive feedback from both the adversarial procedure and a behavior cloning loss. We empirically demonstrate that our proposed algorithm requires fewer interactions with the environment to achieve expert performance compared to other state-of-the-art on-policy ILO and ILD methods.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35821;&#20041;&#36830;&#36143;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#20173;&#28982;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#20294;&#26159;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#29255;&#27573;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;&#29255;&#27573;&#24182;&#19981;&#26159;&#30830;&#23450;&#20154;&#24037;&#20174;&#19994;&#32773;&#38382;&#39064;&#24615;&#29255;&#27573;&#30340;&#38134;&#24377;&#12290;</title><link>http://arxiv.org/abs/2306.08167</link><description>&lt;p&gt;
&#25105;&#30340;&#27169;&#22411;&#24615;&#33021;&#20026;&#20160;&#20040;&#20250;&#19979;&#38477;&#65311;&#23545;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#30340;&#20154;&#24037;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms. (arXiv:2306.08167v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08167
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35821;&#20041;&#36830;&#36143;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#20173;&#28982;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#20294;&#26159;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#29255;&#27573;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;&#29255;&#27573;&#24182;&#19981;&#26159;&#30830;&#23450;&#20154;&#24037;&#20174;&#19994;&#32773;&#38382;&#39064;&#24615;&#29255;&#27573;&#30340;&#38134;&#24377;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#20041;&#36830;&#36143;&#30340;&#25968;&#25454;&#23376;&#38598;&#65288;&#21363;&#8220;&#29255;&#27573;&#8221;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#39640;&#24179;&#22343;&#20934;&#30830;&#29575;&#30340;&#27169;&#22411;&#20173;&#28982;&#20250;&#20986;&#29616;&#36825;&#31181;&#38382;&#39064;&#12290;&#36825;&#31181;&#34892;&#20026;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#20559;&#35265;&#22312;&#37096;&#32626;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#30830;&#23450;&#36825;&#20123;&#24615;&#33021;&#19979;&#38477;&#30340;&#29255;&#27573;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#19994;&#32773;&#32570;&#20047;&#35775;&#38382;&#32676;&#32452;&#27880;&#37322;&#20197;&#23450;&#20041;&#20854;&#25968;&#25454;&#30340;&#36830;&#36143;&#23376;&#38598;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#39537;&#21160;&#65292;ML&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#26032;&#30340;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#30340;&#36830;&#36143;&#21644;&#39640;&#35823;&#24046;&#23376;&#38598;&#20998;&#32452;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#26159;&#21542;&#24110;&#21161;&#20154;&#31867;&#27491;&#30830;&#24418;&#25104;&#20182;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#30340;&#20551;&#35774;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21463;&#25511;&#29992;&#25143;&#30740;&#31350;&#65288;N = 15&#65289;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#36755;&#20986;&#30340;40&#20010;&#29255;&#27573;&#65292;&#24182;&#35201;&#27714;&#20182;&#20204;&#24418;&#25104;&#26377;&#20851;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#21709;&#24212;&#21464;&#37327;&#26159;&#21442;&#19982;&#32773;&#27491;&#30830;&#25353;&#38169;&#35823;&#29575;&#23545;&#29255;&#27573;&#36827;&#34892;&#25490;&#21517;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;&#65288;1&#65289;&#20004;&#31181;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#37117;&#19981;&#20250;&#35753;&#21442;&#19982;&#32773;&#22312;&#20551;&#35774;&#19978;&#34920;&#29616;&#20986;&#31995;&#32479;&#24615;&#30340;&#20248;&#21183;&#65307;&#65288;2&#65289;&#21363;&#20351;&#22312;&#21516;&#19968;&#31181;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#20013;&#65292;&#21442;&#19982;&#32773;&#22312;&#27491;&#30830;&#23545;&#29255;&#27573;&#36827;&#34892;&#25490;&#24207;&#30340;&#33021;&#21147;&#19978;&#20063;&#23384;&#22312;&#26174;&#30528;&#21464;&#24322;&#65307;&#65288;3&#65289;&#23545;&#35937;&#31867;&#21035;&#30340;&#38169;&#35823;&#29575;&#27604;&#23545;&#35937;&#22823;&#23567;&#25110;&#20301;&#32622;&#31561;&#38544;&#21547;&#35821;&#20041;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#29255;&#27573;&#38590;&#24230;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#29983;&#25104;&#30340;&#29255;&#27573;&#24182;&#38750;&#30830;&#23450;&#20154;&#24037;&#20174;&#19994;&#32773;&#38382;&#39064;&#24615;&#29255;&#27573;&#30340;&#38134;&#24377;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#24517;&#39035;&#23567;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models that achieve high average accuracy can still underperform on semantically coherent subsets (i.e. "slices") of data. This behavior can have significant societal consequences for the safety or bias of the model in deployment, but identifying these underperforming slices can be difficult in practice, especially in domains where practitioners lack access to group annotations to define coherent subsets of their data. Motivated by these challenges, ML researchers have developed new slice discovery algorithms that aim to group together coherent and high-error subsets of data. However, there has been little evaluation focused on whether these tools help humans form correct hypotheses about where (for which groups) their model underperforms. We conduct a controlled user study (N = 15) where we show 40 slices output by two state-of-the-art slice discovery algorithms to users, and ask them to form hypotheses about where an object detection model underperforms. Our res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;HyperCATE&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#26435;&#37325;&#20849;&#20139;&#30340;&#26041;&#24335;&#23454;&#29616;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#29616;&#26377;CATE&#23398;&#20064;&#22120;&#20013;&#30340;&#26377;&#20559;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15984</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#21160;&#24577;&#27835;&#30103;&#20449;&#24687;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Dynamic Inter-treatment Information Sharing for Heterogeneous Treatment Effects Estimation. (arXiv:2305.15984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;HyperCATE&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#26435;&#37325;&#20849;&#20139;&#30340;&#26041;&#24335;&#23454;&#29616;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#29616;&#26377;CATE&#23398;&#20064;&#22120;&#20013;&#30340;&#26377;&#20559;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#23398;&#20064;&#32773;&#32570;&#20047;&#31471;&#21040;&#31471;&#27835;&#30103;&#20449;&#24687;&#20849;&#20139;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#24517;&#39035;&#23558;&#25968;&#25454;&#20998;&#21106;&#21040;&#28508;&#22312;&#32467;&#26524;&#20989;&#25968;&#20013;&#35757;&#32451;CATE&#23398;&#20064;&#22120;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#30340;&#26377;&#20559;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;CATE&#23398;&#20064;&#22120;&#65292;&#20419;&#36827;&#27835;&#30103;&#32452;&#20043;&#38388;&#30340;&#21160;&#24577;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#8220;&#36229;&#32593;&#32476;&#8221;&#30340;&#8220;&#36719;&#26435;&#37325;&#20849;&#20139;&#8221;&#65292;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#12289;&#26356;&#24555;&#35757;&#32451;&#21644;&#25913;&#36827;&#32467;&#26524;&#31561;&#20248;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;CATE&#23398;&#20064;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;HyperCATE&#8221;&#30340;&#26032;&#22411;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;CATE&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#24120;&#29992;CATE&#23398;&#20064;&#22120;&#30340;HyperCATE&#29256;&#26412;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing heterogeneous treatment effects learners, also known as conditional average treatment effects (CATE) learners, lack a general mechanism for end-to-end inter-treatment information sharing, and data have to be split among potential outcome functions to train CATE learners which can lead to biased estimates with limited observational datasets. To address this issue, we propose a novel deep learning-based framework to train CATE learners that facilitates dynamic end-to-end information sharing among treatment groups. The framework is based on \textit{soft weight sharing} of \textit{hypernetworks}, which offers advantages such as parameter efficiency, faster training, and improved results. The proposed framework complements existing CATE learners and introduces a new class of uncertainty-aware CATE learners that we refer to as \textit{HyperCATE}. We develop HyperCATE versions of commonly used CATE learners and evaluate them on IHDP, ACIC-2016, and Twins benchmarks. Our experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26032;&#39062;&#30340;Voronoi Loss&#20989;&#25968;&#26469;&#35299;&#20915;&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#38376;&#25511;&#32593;&#32476;&#19979;&#25552;&#20379;&#29702;&#35770;&#25910;&#25947;&#36895;&#29575;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.07572</link><description>&lt;p&gt;
&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts. (arXiv:2305.07572v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26032;&#39062;&#30340;Voronoi Loss&#20989;&#25968;&#26469;&#35299;&#20915;&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#38376;&#25511;&#32593;&#32476;&#19979;&#25552;&#20379;&#29702;&#35770;&#25910;&#25947;&#36895;&#29575;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#22240;&#20854;&#22312;&#38598;&#25104;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#32780;&#34987;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36817;&#24180;&#26469;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#22522;&#26412;&#26500;&#20214;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#26031;&#38376;&#25511;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#34892;&#20026;&#30340;&#29702;&#35299;&#36824;&#19981;&#20805;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;Voronoi Loss&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#36895;&#29575;&#30340;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#22312;&#20004;&#31181;&#20998;&#31163;&#30340;&#38376;&#25511;&#32593;&#32476;&#19979;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally introduced as a neural network for ensemble learning, mixture of experts (MoE) has recently become a fundamental building block of highly successful modern deep neural networks for heterogeneous data analysis in several applications, including those in machine learning, statistics, bioinformatics, economics, and medicine. Despite its popularity in practice, a satisfactory level of understanding of the convergence behavior of Gaussian-gated MoE parameter estimation is far from complete. The underlying reason for this challenge is the inclusion of covariates in the Gaussian gating and expert networks, which leads to their intrinsically complex interactions via partial differential equations with respect to their parameters. We address these issues by designing novel Voronoi loss functions to accurately capture heterogeneity in the maximum likelihood estimator (MLE) for resolving parameter estimation in these models. Our results reveal distinct behaviors of the MLE under two se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.12522</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A New Inexact Proximal Linear Algorithm with Adaptive Stopping Criteria for Robust Phase Retrieval. (arXiv:2304.12522v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#35270;&#20026;&#19968;&#20010;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#20854;&#20013;&#23376;&#38382;&#39064;&#34987;&#19981;&#31934;&#30830;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20026;&#23376;&#38382;&#39064;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#20363;&#22914;&#21407;&#22987;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#21644;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the robust phase retrieval problem, which can be cast as a nonsmooth and nonconvex optimization problem. We propose a new inexact proximal linear algorithm with the subproblem being solved inexactly. Our contributions are two adaptive stopping criteria for the subproblem. The convergence behavior of the proposed methods is analyzed. Through experiments on both synthetic and real datasets, we demonstrate that our methods are much more efficient than existing methods, such as the original proximal linear algorithm and the subgradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#65292;&#20351;&#24471;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.11004</link><description>&lt;p&gt;
&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation Under Ideal Joint Classifier Assumption. (arXiv:2304.11004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#65292;&#20351;&#24471;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20026;&#26356;&#39640;&#25928;&#23567;&#22411;&#32593;&#32476;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;Softmax&#22238;&#24402;&#34920;&#24449;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#26469;&#25351;&#23548;&#26356;&#23567;&#30340;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;Softmax&#22238;&#24402;&#34920;&#24449;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#25552;&#20379;&#30693;&#35782;&#36716;&#31227;&#30340;&#22522;&#30784;&#26426;&#21046;&#23578;&#19981;&#22815;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#30693;&#35782;&#33976;&#39311;&#65288;IJCKD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#39046;&#22495;&#36866;&#24212;&#29702;&#35770;&#25512;&#23548;&#20986;&#30340;&#25968;&#23398;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#23398;&#29983;&#32593;&#32476;&#35823;&#24046;&#30028;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#20854;&#20316;&#20026;&#25945;&#24072;&#30340;&#20989;&#25968;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#22270;&#20687;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a powerful technique to compress large neural networks into smaller, more efficient networks. Softmax regression representation learning is a popular approach that uses a pre-trained teacher network to guide the learning of a smaller student network. While several studies explored the effectiveness of softmax regression representation learning, the underlying mechanism that provides knowledge transfer is not well understood. This paper presents Ideal Joint Classifier Knowledge Distillation (IJCKD), a unified framework that provides a clear and comprehensive understanding of the existing knowledge distillation methods and a theoretical foundation for future research. Using mathematical techniques derived from a theory of domain adaptation, we provide a detailed analysis of the student network's error bound as a function of the teacher. Our framework enables efficient knowledge transfer between teacher and student networks and can be applied to various applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#25551;&#36848;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20559;&#24207;&#29256;&#26412;&#30340;&#21333;&#32431;&#28145;&#24230;&#65292;&#29992;&#20110;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#19981;&#21516;&#65292;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2304.09872</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms. (arXiv:2304.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#25551;&#36848;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20559;&#24207;&#29256;&#26412;&#30340;&#21333;&#32431;&#28145;&#24230;&#65292;&#29992;&#20110;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#19981;&#21516;&#65292;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#23545;&#20559;&#24207;&#38598;&#21512;&#36827;&#34892;&#25551;&#36848;&#24615;&#20998;&#26512;&#12290;&#23613;&#31649;&#28145;&#24230;&#20989;&#25968;&#22312;&#32447;&#24615;&#21644;&#24230;&#37327;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20559;&#24207;&#31561;&#38750;&#26631;&#20934;&#25968;&#25454;&#31867;&#22411;&#30340;&#28145;&#24230;&#20989;&#25968;&#30340;&#35752;&#35770;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33879;&#21517;&#30340;&#21333;&#32431;&#28145;&#24230;&#30340;&#20559;&#24207;&#29256;&#26412;-&#26080;&#24182;&#36890;&#29992;&#28145;&#24230;&#65288;ufg depth&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340; ufg depth &#26469;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#19981;&#21516;&#20998;&#31867;&#22120;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#24076;&#26395;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#22240;&#27492;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#30340;&#28608;&#28872;&#36777;&#35770;&#22686;&#21152;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies of depth functions in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we analyze the distribution of different classifier performances over a sample of standard benchmark data sets. Our results promisingly demonstrate that our approach differs substantially from existing benchmarking approaches and, therefore, adds a new perspective to the vivid debate on the comparison of classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#20013;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02621</link><description>&lt;p&gt;
&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation. (arXiv:2304.02621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#20013;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#32423;&#21035;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#20219;&#21153;&#22240;&#20026;&#20854;&#21487;&#20943;&#23569;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;WSSS&#30340;&#20856;&#22411;&#26041;&#27861;&#26159;&#20351;&#29992;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#22312;&#21367;&#31215;&#29305;&#24449;&#26144;&#23556;&#19978;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#31867;&#21035;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#20272;&#35745;&#23545;&#35937;&#20301;&#32622;&#65292;CAMs&#35782;&#21035;&#22270;&#20687;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#20351;&#29992;CAMs&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#20197;&#24418;&#24335;&#21270;&#30340;&#20998;&#21106;&#25513;&#30721;&#30340;&#26041;&#24335;&#22312;&#32570;&#20047;&#20687;&#32032;&#32423;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23545;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#12290;&#22312;SEAM&#22522;&#32447;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#25552;&#39640;CAM&#23398;&#20064;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#23427;&#26159;GAP&#30340;&#26367;&#20195;&#26041;&#27861;&#65307;&#65288;2&#65289;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21363;&#23545;&#35937;&#36718;&#24275;&#20960;&#20046;&#20165;&#19982;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#36793;&#32536;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#27010;&#29575;&#35299;&#37322;CAM&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20266;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#23558;&#23616;&#37096;&#31354;&#38388;&#19968;&#33268;&#24615;&#32422;&#26463;&#34701;&#20837;CAM&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20132;&#26367;&#26356;&#26032;CAM&#21644;&#20998;&#21106;&#27169;&#22411;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of image-level weakly-supervised semantic segmentation (WSSS) has gained popularity in recent years, as it reduces the vast data annotation cost for training segmentation models. The typical approach for WSSS involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. In case of the SEAM baseline, a previous work proposed to improve CAM learning in two ways: (1) Importance sampling, which is a substitute for GAP, and (2) the feature similarity loss, which utilizes a heuristic that object contours almost exclusively align with color edges in images. In this work, we propose a different probabilistic interpretation of CAMs for thes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;GMNN&#30340;&#26032;&#27979;&#35797;&#26041;&#27861;&#65292;&#23545;&#19977;&#31867;&#19981;&#21516;&#20449;&#24687;&#28304;&#23545;GMNN&#22312;WikiVitals&#25968;&#25454;&#38598;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#26631;&#31614;&#30456;&#20851;&#24615;&#26159;&#24110;&#21161;GMNN&#33719;&#24471;&#20248;&#21183;&#30340;&#20851;&#38190;&#20449;&#24687;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.01235</link><description>&lt;p&gt;
&#22270;&#39532;&#23572;&#21487;&#22827;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fair Evaluation of Graph Markov Neural Networks. (arXiv:2304.01235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#36866;&#29992;&#20110;GMNN&#30340;&#26032;&#27979;&#35797;&#26041;&#27861;&#65292;&#23545;&#19977;&#31867;&#19981;&#21516;&#20449;&#24687;&#28304;&#23545;GMNN&#22312;WikiVitals&#25968;&#25454;&#38598;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#26631;&#31614;&#30456;&#20851;&#24615;&#26159;&#24110;&#21161;GMNN&#33719;&#24471;&#20248;&#21183;&#30340;&#20851;&#38190;&#20449;&#24687;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#37319;&#29992;&#22270;&#39532;&#23572;&#21487;&#22827;&#31070;&#32463;&#32593;&#32476;&#65288;GMNN&#65289;&#25913;&#36827;&#24120;&#35268;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#23558;&#26631;&#31614;&#20381;&#36182;&#24615;&#32435;&#20837;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;GMNN&#20174;&#29702;&#35770;&#19978;&#20197;&#20005;&#35880;&#30340;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#19977;&#31867;&#20449;&#24687;&#26469;&#39044;&#27979;&#26631;&#31614;&#12290;&#19982;&#24120;&#35268;&#30340;GNN&#19968;&#26679;&#65292;&#20182;&#20204;&#20351;&#29992;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#65292;&#20294;&#20182;&#20204;&#36824;&#21033;&#29992;&#30456;&#37051;&#33410;&#28857;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;WikiVitals&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;48k&#20010;&#20114;&#30456;&#24341;&#29992;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#65292;&#34987;&#20998;&#31867;&#20026;32&#20010;&#31867;&#21035;&#65292;&#30001;2.3M&#36793;&#36830;&#25509;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23545;GMNN&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#36129;&#29486;&#30340;&#19977;&#31181;&#19981;&#21516;&#20449;&#24687;&#28304;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65306;&#25991;&#31456;&#20869;&#23481;&#12289;&#25991;&#31456;&#20114;&#30456;&#20043;&#38388;&#30340;&#36830;&#25509;&#20197;&#21450;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#36866;&#29992;&#20110;GNN&#30340;&#26679;&#26412;&#22806;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#20110;GMNN&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GMNN&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;GNN&#65292;&#24182;&#19988;&#26631;&#31614;&#30456;&#20851;&#24615;&#26159;&#24110;&#21161;GMNN&#23454;&#29616;&#36825;&#20123;&#22686;&#30410;&#30340;&#20851;&#38190;&#20449;&#24687;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Markov Neural Networks (GMNN) have recently been proposed to improve regular graph neural networks (GNN) by including label dependencies into the semi-supervised node classification task. GMNNs do this in a theoretically principled way and use three kinds of information to predict labels. Just like ordinary GNNs, they use the node features and the graph structure but they moreover leverage information from the labels of neighboring nodes to improve the accuracy of their predictions. In this paper, we introduce a new dataset named WikiVitals which contains a graph of 48k mutually referred Wikipedia articles classified into 32 categories and connected by 2.3M edges. Our aim is to rigorously evaluate the contributions of three distinct sources of information to the prediction accuracy of GMNN for this dataset: the content of the articles, their connections with each other and the correlations among their labels. For this purpose we adapt a method which was recently proposed for perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#27599;&#20010;&#20132;&#36890;&#28857;&#30340;&#26102;&#38388;&#65292;&#35299;&#20915;&#20844;&#20132;&#36816;&#36755;&#20013;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19981;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15495</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#25353;&#38656;&#20844;&#20849;&#20132;&#36890;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit. (arXiv:2303.15495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#27599;&#20010;&#20132;&#36890;&#28857;&#30340;&#26102;&#38388;&#65292;&#35299;&#20915;&#20844;&#20132;&#36816;&#36755;&#20013;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19981;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#30340;&#20027;&#35201;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#65292;&#20844;&#20132;&#36816;&#36755;&#23384;&#22312;&#30528;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20110;&#20056;&#23458;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#30340;&#20272;&#35745;&#26356;&#21152;&#20934;&#30830;&#21644;&#21487;&#38752;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#24310;&#35823;&#21644;&#20943;&#23569;&#20056;&#23458;&#20154;&#25968;&#65292;&#23588;&#20854;&#26159;&#22312;&#20381;&#38752;&#20844;&#20849;&#20132;&#36890;&#30340;&#22478;&#24066;&#20013;&#26356;&#21152;&#20005;&#37325;&#12290;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19982;&#26102;&#38388;&#34920;&#19981;&#21305;&#37197;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#22266;&#23450;&#26102;&#21051;&#34920;&#30340;&#24310;&#36831;&#12290;&#26681;&#25454;&#26412;&#25991;&#22312;&#32445;&#32422;&#24066;&#20844;&#20132;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#30740;&#31350;&#65292;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#21644;&#23454;&#38469;&#35745;&#21010;&#26102;&#38388;&#20043;&#38388;&#23384;&#22312;&#24179;&#22343;&#32422;&#20843;&#20998;&#38047;&#25110;491&#31186;&#30340;&#24310;&#36831;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27599;&#20010;&#20132;&#36890;&#28857;&#65288;&#31449;&#65289;&#20844;&#20132;&#36710;&#30340;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#22823;&#37117;&#24066;&#21306;&#22495;&#20013;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20026;&#20272;&#31639;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the major public transportation systems in cities, bus transit has its problems, including more accuracy and reliability when estimating the bus arrival time for riders. This can lead to delays and decreased ridership, especially in cities where public transportation is heavily relied upon. A common issue is that the arrival times of buses do not match the schedules, resulting in latency for fixed schedules. According to the study in this paper on New York City bus data, there is an average delay of around eight minutes or 491 seconds mismatch between the bus arrivals and the actual scheduled time. This research paper presents a novel AI-based data-driven approach for estimating the arrival times of buses at each transit point (station). Our approach is based on a fully connected neural network and can predict the arrival time collectively across all bus lines in large metropolitan areas. Our neural-net data-driven approach provides a new way to estimate the arrival time of the b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20998;&#31867;&#22120;&#39044;&#27979;&#31163;&#25955;&#26102;&#38388;&#20116;&#31181;&#26412;&#22320;&#20998;&#23700;&#65292;&#22312;&#32463;&#27982;&#12289;&#29983;&#24577;&#12289;&#29983;&#29702;&#23398;&#31561;&#26041;&#38754;&#30340;&#35797;&#39564;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#20248;&#31168;&#34920;&#29616;&#65292;&#26159;&#25552;&#21069;&#35686;&#21578;&#20851;&#38190;&#36716;&#21464;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09669</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#31163;&#25955;&#26102;&#38388;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Predicting discrete-time bifurcations with deep learning. (arXiv:2303.09669v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20998;&#31867;&#22120;&#39044;&#27979;&#31163;&#25955;&#26102;&#38388;&#20116;&#31181;&#26412;&#22320;&#20998;&#23700;&#65292;&#22312;&#32463;&#27982;&#12289;&#29983;&#24577;&#12289;&#29983;&#29702;&#23398;&#31561;&#26041;&#38754;&#30340;&#35797;&#39564;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#20248;&#31168;&#34920;&#29616;&#65292;&#26159;&#25552;&#21069;&#35686;&#21578;&#20851;&#38190;&#36716;&#21464;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#21644;&#20154;&#36896;&#31995;&#32479;&#23481;&#26131;&#21457;&#29983;&#20851;&#38190;&#36716;&#21464;-&#21160;&#21147;&#23398;&#30340;&#31361;&#28982;&#21644;&#28508;&#22312;&#30340;&#30772;&#22351;&#24615;&#21464;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20998;&#23700;&#65288;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#65289;&#30340;&#36890;&#29992;&#29305;&#24449;&#65292;&#20026;&#20851;&#38190;&#36716;&#21464;&#25552;&#20379;&#25552;&#21069;&#35686;&#21578;&#20449;&#21495;&#65288;EWS&#65289;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20998;&#31867;&#22120;&#21482;&#34987;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#36830;&#32493;&#26102;&#38388;&#20998;&#27495;&#65292;&#32780;&#24573;&#30053;&#20102;&#31163;&#25955;&#26102;&#38388;&#20998;&#27495;&#29420;&#29305;&#30340;&#20016;&#23500;&#21160;&#24577;&#29305;&#24449;&#12290;&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#35757;&#32451;&#25552;&#20379; EWS &#30340;&#20116;&#31181;&#31163;&#25955;&#26102;&#38388;&#12289;&#20849;&#32500;&#24230;1&#30340;&#26412;&#22320;&#20998;&#23700;&#12290;&#25105;&#20204;&#22312;&#29983;&#29702;&#23398;&#12289;&#32463;&#27982;&#23398;&#21644;&#29983;&#24577;&#23398;&#20013;&#20351;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#30340;&#27169;&#25311;&#25968;&#25454;&#20197;&#21450;&#32463;&#21382;&#20102;&#20493;&#22686;&#20998;&#23700;&#30340;&#40481;&#24515;&#32858;&#38598;&#30340;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#24191;&#27867;&#30340;&#22122;&#22768;&#24378;&#24230;&#21644;&#25509;&#36817;&#20998;&#23700;&#30340;&#36895;&#29575;&#33539;&#22260;&#20869;&#65292;&#20998;&#31867;&#22120;&#20248;&#20110;&#24120;&#29992;&#30340; EWS&#12290;&#23427;&#20063;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#20998;&#23700;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural and man-made systems are prone to critical transitions -- abrupt and potentially devastating changes in dynamics. Deep learning classifiers can provide an early warning signal (EWS) for critical transitions by learning generic features of bifurcations (dynamical instabilities) from large simulated training data sets. So far, classifiers have only been trained to predict continuous-time bifurcations, ignoring rich dynamics unique to discrete-time bifurcations. Here, we train a deep learning classifier to provide an EWS for the five local discrete-time bifurcations of codimension-1. We test the classifier on simulation data from discrete-time models used in physiology, economics and ecology, as well as experimental data of spontaneously beating chick-heart aggregates that undergo a period-doubling bifurcation. The classifier outperforms commonly used EWS under a wide range of noise intensities and rates of approach to the bifurcation. It also predicts the correct bifurcation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#36739;&#23567;&#30340;&#20302;&#28145;&#24230;&#26041;&#27861;&#26469;&#20934;&#22791;&#20219;&#24847;&#37327;&#23376;&#24577;&#65292;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#37327;&#23376;&#36164;&#28304;&#20351;&#29992;&#19979;&#23454;&#29616;&#26356;&#24555;&#30340;&#20934;&#22791;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.02131</link><description>&lt;p&gt;
&#20351;&#29992;&#21344;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#36739;&#23567;&#30340;&#20302;&#28145;&#24230;&#37327;&#23376;&#24577;&#20934;&#22791;&#26041;&#27861;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Spacetime-Efficient Low-Depth Quantum State Preparation with Applications. (arXiv:2303.02131v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#36739;&#23567;&#30340;&#20302;&#28145;&#24230;&#26041;&#27861;&#26469;&#20934;&#22791;&#20219;&#24847;&#37327;&#23376;&#24577;&#65292;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#37327;&#23376;&#36164;&#28304;&#20351;&#29992;&#19979;&#23454;&#29616;&#26356;&#24555;&#30340;&#20934;&#22791;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#30830;&#23450;&#24615;&#26041;&#27861;&#26469;&#20934;&#22791;&#20219;&#24847;&#30340;&#37327;&#23376;&#24577;&#12290;&#24403;&#23558;&#25105;&#20204;&#30340;&#21327;&#35758;&#32534;&#35793;&#20026;CNOT&#38376;&#21644;&#20219;&#24847;&#30340;&#21333;&#27604;&#29305;&#38376;&#26102;&#65292;&#23427;&#21487;&#20197;&#22312;&#28145;&#24230;$O(\log(N))$&#21644;&#31354;&#38388;&#26102;&#38388;&#20998;&#37197;$O(N)$&#30340;&#24773;&#20917;&#19979;&#20934;&#22791;&#19968;&#20010;$N$&#32500;&#30340;&#37327;&#23376;&#24577;&#65292;&#36825;&#20004;&#20010;&#21442;&#25968;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;&#24403;&#32534;&#35793;&#20026;$\{\mathrm{H,S,T,CNOT}\}$&#38376;&#38598;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#25152;&#38656;&#30340;&#37327;&#23376;&#36164;&#28304;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#35201;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#28145;&#24230;$O(\log(N/\epsilon))$&#21644;&#31354;&#38388;&#26102;&#38388;&#20998;&#37197;$O(N\log(\log(N)/\epsilon))$&#19979;&#65292;&#21487;&#20197;&#20934;&#22791;&#19968;&#20010;&#35823;&#24046;&#20026;$\epsilon$&#30340;&#20219;&#24847;&#30340;&#37327;&#23376;&#24577;&#65292;&#36825;&#27604;&#20043;&#21069;&#26041;&#27861;&#30340;$O(\log(N)\log(N/\epsilon))$&#21644;$O(N\log(N/\epsilon))$&#26377;&#25152;&#25913;&#36827;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#25105;&#20204;&#21327;&#35758;&#30340;&#20943;&#23567;&#31354;&#38388;&#26102;&#38388;&#20998;&#37197;&#21487;&#20197;&#26377;&#25928;&#22320;&#24555;&#36895;&#20934;&#22791;&#22810;&#20010;&#19981;&#30456;&#20132;&#29366;&#24577;&#65292;&#21482;&#38656;&#35201;&#24120;&#25968;&#22240;&#23376;&#30340;&#36741;&#21161;&#37327;&#23376;&#27604;&#29305;&#24320;&#38144;--&#36890;&#36807;&#39640;&#25928;&#22320;&#37325;&#29992;$O(N)$&#20010;&#36741;&#21161;&#27604;&#29305;&#26469;&#20934;&#22791;&#19968;&#20010;&#20056;&#31215;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel deterministic method for preparing arbitrary quantum states. When our protocol is compiled into CNOT and arbitrary single-qubit gates, it prepares an $N$-dimensional state in depth $O(\log(N))$ and spacetime allocation (a metric that accounts for the fact that oftentimes some ancilla qubits need not be active for the entire circuit) $O(N)$, which are both optimal. When compiled into the $\{\mathrm{H,S,T,CNOT}\}$ gate set, we show that it requires asymptotically fewer quantum resources than previous methods. Specifically, it prepares an arbitrary state up to error $\epsilon$ in depth $O(\log(N/\epsilon))$ and spacetime allocation $O(N\log(\log(N)/\epsilon))$, improving over $O(\log(N)\log(N/\epsilon))$ and $O(N\log(N/\epsilon))$, respectively. We illustrate how the reduced spacetime allocation of our protocol enables rapid preparation of many disjoint states with only constant-factor ancilla overhead -- $O(N)$ ancilla qubits are reused efficiently to prepare a product
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2302.10890</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#23545;&#31216;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Low-dimensional Representation via Physical Symmetry. (arXiv:2302.10890v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#23398;&#20064;&#22312;&#21019;&#36896;&#24615;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#38899;&#20048;&#39046;&#22495;&#65292;&#24403;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#38899;&#39640;&#12289;&#38899;&#33394;&#12289;&#21644;&#24358;&#12289;&#32441;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#38899;&#20048;&#39046;&#22495;&#30693;&#35782;&#12290;&#29616;&#22312;&#36824;&#19981;&#28165;&#26970;&#20160;&#20040;&#26679;&#30340;&#19968;&#33324;&#24615;&#35745;&#31639;&#21407;&#21017;&#20250;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#24863;&#30693;&#20445;&#25345;&#19968;&#33268;&#30340;&#20302;&#32500;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29616;&#20195;&#29289;&#29702;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23558;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#35201;&#27714;&#20808;&#39564;&#27169;&#22411;&#23545;&#28508;&#22312;&#29366;&#24577;&#30340;&#21160;&#24577;&#36827;&#34892;&#25551;&#36848;&#65292;&#24182;&#20197;&#26576;&#31181;&#32676;&#21464;&#25442;&#23545;&#20854;&#36827;&#34892;&#31561;&#21464;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#23545;&#31216;&#24615;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#26410;&#26631;&#35760;&#30340;&#21333;&#22768;&#36947;&#38899;&#20048;&#38899;&#39057;&#20013;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#38899;&#39640;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#23398;&#20064;&#19968;&#20010;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable representation learning has been playing a key role in creative intelligent systems. In the music domain, current learning algorithms can successfully learn various features such as pitch, timbre, chord, texture, etc. However, most methods rely heavily on music domain knowledge. It remains an open question what general computational principles give rise to interpretable representations, especially low-dim factors that agree with human perception. In this study, we take inspiration from modern physics and use physical symmetry as a self-consistency constraint for the latent space. Specifically, it requires the prior model that characterises the dynamics of the latent states to be equivariant with respect to certain group transformations. We show that physical symmetry leads the model to learn a linear pitch factor from unlabelled monophonic music audio in a self-supervised fashion. In addition, the same methodology can be applied to computer vision, learning a 3D Cartesian
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#24555;&#36895;&#12289;&#31283;&#20581;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FedAvg&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2210.08106</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Primal-Dual Algorithm for Hybrid Federated Learning. (arXiv:2210.08106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#24555;&#36895;&#12289;&#31283;&#20581;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FedAvg&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#20165;&#25345;&#26377;&#37096;&#20998;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#23458;&#25143;&#31471;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#24555;&#36895;&#12289;&#31283;&#20581;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#21508;&#31181;&#23454;&#38469;&#24773;&#20917;&#19979;&#25910;&#25947;&#20110;&#19982;&#22312;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;FedAvg&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#38544;&#31169;&#32771;&#34385;&#21644;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very few methods for hybrid federated learning, where clients only hold subsets of both features and samples, exist. Yet, this scenario is very important in practical settings. We provide a fast, robust algorithm for hybrid federated learning that hinges on Fenchel Duality. We prove the convergence of the algorithm to the same solution as if the model was trained centrally in a variety of practical regimes. Furthermore, we provide experimental results that demonstrate the performance improvements of the algorithm over a commonly used method in federated learning, FedAvg. We also provide privacy considerations and necessary steps to protect client data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32534;&#30721;&#29702;&#35770;&#19982;&#20132;&#21449;&#39564;&#35777;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#23398;&#20064;&#31639;&#27861;&#22312;&#22266;&#23450;&#25968;&#25454;&#19978;&#33021;&#35299;&#20915;&#19981;&#21516;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#30340;&#25968;&#37327;&#19982;&#35823;&#24046;&#26816;&#27979;&#30721;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#19979;&#30340;&#26368;&#22823;&#20998;&#31867;&#38382;&#39064;&#25968;&#37327;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#21462;&#20915;&#20110;&#24120;&#26435;&#30721;&#30340;&#30721;&#23383;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;&#24120;&#26435;&#30721;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#20854;&#20182;&#20132;&#21449;&#39564;&#35777;&#38169;&#35823;&#21644;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;&#12290;</title><link>http://arxiv.org/abs/2103.11856</link><description>&lt;p&gt;
&#32534;&#30721;&#29702;&#35770;&#19982;&#20132;&#21449;&#39564;&#35777;&#30340;&#32852;&#31995;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Link between Coding Theory and Cross-Validation with Applications. (arXiv:2103.11856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.11856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32534;&#30721;&#29702;&#35770;&#19982;&#20132;&#21449;&#39564;&#35777;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#23398;&#20064;&#31639;&#27861;&#22312;&#22266;&#23450;&#25968;&#25454;&#19978;&#33021;&#35299;&#20915;&#19981;&#21516;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#30340;&#25968;&#37327;&#19982;&#35823;&#24046;&#26816;&#27979;&#30721;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#19979;&#30340;&#26368;&#22823;&#20998;&#31867;&#38382;&#39064;&#25968;&#37327;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#21462;&#20915;&#20110;&#24120;&#26435;&#30721;&#30340;&#30721;&#23383;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;&#24120;&#26435;&#30721;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#20854;&#20182;&#20132;&#21449;&#39564;&#35777;&#38169;&#35823;&#21644;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20010;&#23398;&#20064;&#31639;&#27861;&#22312;&#32473;&#23450;&#25968;&#25454;&#19978;&#33021;&#35299;&#20915;&#22810;&#23569;&#20010;&#19981;&#21516;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#35201;&#27714;&#20132;&#21449;&#39564;&#35777;&#38169;&#35823;&#20026;&#38646;&#25110;&#26368;&#22810;&#32473;&#23450;&#25968;&#37327;&#65311;&#22312;&#21069;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#26681;&#25454;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#65292;&#36825;&#20010;&#25968;&#37327;&#26159;&#26377;&#38480;&#30340;&#65292;&#25105;&#20204;&#34920;&#26126;&#31934;&#30830;&#31572;&#26696;&#30001;&#35823;&#24046;&#26816;&#27979;&#30721;&#29702;&#35770;&#32473;&#20986;&#12290;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#20851;&#27880;AUC&#24615;&#33021;&#24230;&#37327;&#21644;&#30041;&#19968;&#23545;&#20132;&#21449;&#39564;&#35777;(LPOCV)&#65292;&#20854;&#20013;&#27599;&#20010;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#31867;&#26631;&#31614;&#30340;&#25968;&#25454;&#23545;&#37117;&#20250;&#34987;&#26242;&#26102;&#20445;&#30041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#22266;&#23450;&#31867;&#27604;&#20363;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#38646;LPOCV&#38169;&#35823;&#30340;&#26368;&#22823;&#25968;&#37327;&#31561;&#20110;&#24120;&#26435;&#30721;(CWC)&#20013;&#30340;&#26368;&#22823;&#30721;&#23383;&#25968;&#37327;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;(light CWC)&#26469;&#25512;&#24191;CWC&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#38750;&#38646;LPOCV&#38169;&#35823;&#21644;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#30721;&#23383;&#25968;&#37327;&#30340;&#26368;&#22823;&#19978;&#30028;&#21644;&#26368;&#22823;&#19979;&#30028;&#20063;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
How many different binary classification problems a single learning algorithm can solve on a fixed data with exactly zero or at most a given number of cross-validation errors? While the number in the former case is known to be limited by the no-free-lunch theorem, we show that the exact answers are given by the theory of error detecting codes. As a case study, we focus on the AUC performance measure and leave-pair-out cross-validation (LPOCV), in which every possible pair of data with different class labels is held out at a time. We shown that the maximal number of classification problems with fixed class proportion, for which a learning algorithm can achieve zero LPOCV error, equals the maximal number of code words in a constant weight code (CWC), with certain technical properties. We then generalize CWCs by introducing light CWCs and prove an analogous result for nonzero LPOCV errors and light CWCs. Moreover, we prove both upper and lower bounds on the maximal numbers of code words i
&lt;/p&gt;</description></item></channel></rss>