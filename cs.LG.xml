<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#22534;&#21472;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.10211</link><description>&lt;p&gt;
&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10211
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#22534;&#21472;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10211v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20174;&#21407;&#22987;&#24863;&#30693;&#25968;&#25454;&#30340;&#24207;&#21015;&#25512;&#29702;&#26159;&#20174;&#21307;&#30103;&#35774;&#22791;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#20351;&#29992;&#38271;&#24207;&#21015;&#30340;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#20363;&#22914;&#30913;&#21147;&#35745;&#65292;&#21387;&#38459;&#22120;&#65289;&#26469;&#39044;&#27979;&#29702;&#24819;&#30340;&#29289;&#29702;&#37327;&#24207;&#21015;&#65288;&#20363;&#22914;&#21147;&#37327;&#65292;&#24815;&#24615;&#27979;&#37327;&#65289;&#12290;&#34429;&#28982;&#32463;&#20856;&#26041;&#27861;&#23545;&#20110;&#23616;&#37096;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#20351;&#29992;&#23454;&#38469;&#20256;&#24863;&#22120;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21463;&#21040;&#22806;&#30028;&#21464;&#37327;&#65288;&#20363;&#22914;&#25391;&#21160;&#65289;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#25968;&#25454;&#30456;&#20851;&#28418;&#31227;&#12290;&#23545;&#20110;&#35768;&#22810;&#38382;&#39064;&#26469;&#35828;&#65292;&#39044;&#27979;&#20219;&#21153;&#21463;&#21040;&#31232;&#32570;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#38656;&#35201;&#26114;&#36149;&#30340;&#35774;&#22791;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27010;&#24565;&#19978;&#31616;&#21333;&#12289;&#20840;&#26032;&#30340;&#36830;&#32493;&#39034;&#24207;&#39044;&#27979;&#25216;&#26415;&#12290;HiSS&#23558;&#32467;&#26500;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26242;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10211v1 Announce Type: new  Abstract: Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a tempor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#30340;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#36880;&#27493;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.10210</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#30340;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#36880;&#27493;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#21069;&#27839;&#65292;&#23588;&#20854;&#26159;&#19982;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#30456;&#27604;&#12290;&#23613;&#31649;&#29616;&#22312;&#30340;&#20808;&#36827;&#25193;&#25955;&#27169;&#22411;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#21644;SDXL&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#35266;&#23519;&#21040;&#19968;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#21518;&#24517;&#28982;&#20250;&#36798;&#21040;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#24212;&#29992;&#20110;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#38656;&#35201;&#33267;&#23569;&#20004;&#20010;&#22270;&#20687;&#65288;&#8220;&#33719;&#32988;&#32773;&#8221;&#21644;&#8220;&#22833;&#36133;&#32773;&#8221;&#22270;&#20687;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#33258;&#25105;&#23545;&#25239;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65288;SPIN-Diffusion&#65289;&#65292;&#20854;&#20013;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20808;&#21069;&#29256;&#26412;&#36827;&#34892;&#31454;&#20105;&#65292;&#20419;&#36827;&#20102;&#19968;&#20010;&#36845;&#20195;&#30340;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#21644;RL&#31574;&#30053;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10210v1 Announce Type: cross  Abstract: Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, signific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10208</link><description>&lt;p&gt;
&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Recovering the Pre-Fine-Tuning Weights of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#65292;&#20027;&#27969;&#27169;&#24335;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;i) &#22312;&#22823;&#35268;&#27169;&#20294;&#19981;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;ii) &#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#20570;&#27861;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#65292;&#22240;&#20026;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#19981;&#23433;&#20840;&#30340;&#39044;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35889;&#21453;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#20302;&#31209;&#65288;LoRA&#65289;&#24494;&#35843;&#27169;&#22411;&#24674;&#22797;&#39044;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#19982;&#20808;&#21069;&#35797;&#22270;&#24674;&#22797;&#39044;&#24494;&#35843;&#33021;&#21147;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#24674;&#22797;&#31934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#20363;&#22914;&#20010;&#24615;&#21270;&#30340;&#31283;&#23450;&#25193;&#25955;&#21644;&#23545;&#40784;&#30340;Mistral&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10208v1 Announce Type: cross  Abstract: The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10206</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#22270;&#23376;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#20854;&#25972;&#20307;&#32467;&#26500;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#20943;&#23567;&#22270;&#30340;&#26041;&#27861;&#35201;&#20040;&#21024;&#38500;&#36793;&#32536;&#65288;&#31232;&#30095;&#21270;&#65289;&#65292;&#35201;&#20040;&#21512;&#24182;&#33410;&#28857;&#65288;&#31895;&#21270;&#65289;&#65292;&#32780;&#27809;&#26377;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#33410;&#28857;&#25110;&#36793;&#19978;&#23450;&#20041;&#30340;&#20234;&#36763;&#27169;&#22411;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#23376;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#22914;&#20309;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#12290;&#25152;&#20351;&#29992;&#30340;&#20219;&#21153;&#25439;&#22833;&#20989;&#25968;&#29978;&#33267;&#19981;&#38656;&#35201;&#21487;&#24494;&#20998;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10206v1 Announce Type: cross  Abstract: Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#30340;&#33021;&#37327;&#20989;&#25968;&#21487;&#20197;&#34987;&#35270;&#20026;&#27010;&#29575;&#24314;&#27169;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#65292;&#21487;&#28789;&#27963;&#36866;&#24212;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.10202</link><description>&lt;p&gt;
&#26500;&#24314;&#32852;&#24819;&#35760;&#24518;&#19982;&#27010;&#29575;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Bridging Associative Memory and Probabilistic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10202
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#30340;&#33021;&#37327;&#20989;&#25968;&#21487;&#20197;&#34987;&#35270;&#20026;&#27010;&#29575;&#24314;&#27169;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#65292;&#21487;&#28789;&#27963;&#36866;&#24212;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#32852;&#24819;&#35760;&#24518;&#21644;&#27010;&#29575;&#24314;&#27169;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#20004;&#20010;&#22522;&#26412;&#30340;&#20027;&#39064;&#12290;&#31532;&#19968;&#20010;&#30740;&#31350;&#35774;&#35745;&#29992;&#20110;&#21435;&#22122;&#12289;&#23436;&#25104;&#21644;&#26816;&#32034;&#25968;&#25454;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#31532;&#20108;&#20010;&#30740;&#31350;&#23398;&#20064;&#21644;&#20174;&#27010;&#29575;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#30340;&#33021;&#37327;&#20989;&#25968;&#21487;&#20197;&#34987;&#35270;&#20026;&#27010;&#29575;&#24314;&#27169;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#24231;&#26725;&#26753;&#65292;&#20351;&#24471;&#24819;&#27861;&#33021;&#22312;&#20004;&#20010;&#26041;&#21521;&#19978;&#26377;&#30410;&#30340;&#27969;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22235;&#20010;&#20363;&#23376;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#20197;&#33021;&#37327;&#20026;&#22522;&#30784;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#36866;&#24212;&#26032;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#37327;&#20989;&#25968;&#8221;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#65306;&#19968;&#31181;&#26159;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#35201;&#21160;&#24577;&#21019;&#24314;&#26032;&#30340;&#35760;&#24518;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#26126;&#30830;&#35745;&#31639;&#27604;&#20363;&#35760;&#24518;&#20998;&#37197;&#65292;&#20351;&#29992;e&#20316;&#20026;&#27010;&#29575;&#20989;&#25968;&#20998;&#37197;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10202v1 Announce Type: new  Abstract: Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;SAMformer&#65292;&#36890;&#36807;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#36991;&#20813;&#20102;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#22312;&#24120;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#12290;</title><link>https://arxiv.org/abs/2402.10198</link><description>&lt;p&gt;
&#20351;&#29992;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#35299;&#38145;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;SAMformer&#65292;&#36890;&#36807;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#36991;&#20813;&#20102;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#22312;&#24120;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#22810;&#20803;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#22914;&#26356;&#31616;&#21333;&#30340;&#32447;&#24615;&#22522;&#32447;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#29609;&#20855;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#23613;&#31649;Transformer&#20855;&#26377;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#25910;&#25947;&#21040;&#30495;&#27491;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;Transformer&#30340;&#27880;&#24847;&#21147;&#26159;&#36896;&#25104;&#20854;&#20302;&#27867;&#21270;&#33021;&#21147;&#30340;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#36991;&#20813;&#20102;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#25152;&#26377;&#24120;&#29992;&#30340;&#23454;&#38469;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#26159;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#65292;SAMformer&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#20102;14.33%&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#32422;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10198v1 Announce Type: new  Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times few
&lt;/p&gt;</description></item><item><title>BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.10193</link><description>&lt;p&gt;
BitDelta&#65306;&#20320;&#30340;&#24494;&#35843;&#21487;&#33021;&#21482;&#26377;&#19968;&#20010;&#27604;&#29305;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
BitDelta: Your Fine-Tune May Only Be Worth One Bit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10193
&lt;/p&gt;
&lt;p&gt;
BitDelta&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BitDelta&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#37327;&#21270;&#20026;&#19968;&#20010;&#27604;&#29305;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#65306;&#22312;&#22823;&#35268;&#27169;&#20114;&#32852;&#32593;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#30452;&#35273;&#19978;&#35748;&#20026;&#24494;&#35843;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#28155;&#21152;&#36739;&#23569;&#65292;&#22240;&#27492;&#26356;&#20855;&#26377;&#21487;&#21387;&#32553;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#39044;&#35757;&#32451;&#32452;&#20214;&#21644;&#39069;&#22806;&#30340;&#22686;&#37327;&#26469;&#25506;&#31350;&#36825;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#8212;&#8212;BitDelta&#65292;&#25104;&#21151;&#22320;&#23558;&#36825;&#20010;&#22686;&#37327;&#37327;&#21270;&#20026;1&#27604;&#29305;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#19968;&#26377;&#36259;&#30340;&#21457;&#29616;&#19981;&#20165;&#31361;&#26174;&#20102;&#24494;&#35843;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#20449;&#24687;&#30340;&#28508;&#22312;&#20887;&#20313;&#24615;&#65292;&#32780;&#19988;&#23545;&#20110;&#22810;&#31199;&#25143;&#27169;&#22411;&#30340;&#26381;&#21153;&#21644;&#23384;&#20648;&#20063;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39640;&#31934;&#24230;&#30340;&#22522;&#30784;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;1&#27604;&#29305;&#30340;&#22686;&#37327;&#65292;BitDelta&#22823;&#22823;&#38477;&#20302;&#20102;GPU&#20869;&#23384;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10193v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requir
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10192</link><description>&lt;p&gt;
&#20511;&#37492;&#22810;&#20307;&#29289;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#12289;&#31867;&#20284;&#20110;&#31070;&#35861;&#33324;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#65292;&#23558;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22312;&#20855;&#26377;&#27010;&#24565;&#38468;&#21152;&#30340;&#39030;&#28857;&#30340;&#22270;&#19978;&#30340;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#34429;&#28982;&#36825;&#31181;&#25551;&#36848;&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#37327;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#19981;&#33021;&#33258;&#28982;&#22320;&#29992;&#26469;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#30340;&#25512;&#24191;&#65292;&#23427;&#23558;&#24605;&#32500;&#36807;&#31243;&#35270;&#20026;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10192v1 Announce Type: cross  Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAnchor&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;anchor head&#21644;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#65292;&#22686;&#24378;&#20102;&#26080;&#26631;&#31614;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.10191</link><description>&lt;p&gt;
FedAnchor:&#36890;&#36807;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#22686;&#24378;&#26080;&#26631;&#31614;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAnchor&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;anchor head&#21644;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#65292;&#22686;&#24378;&#20102;&#26080;&#26631;&#31614;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;FedAnchor&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#21452;&#22836;&#32467;&#26500;&#65292;&#31216;&#20026;anchor head&#65292;&#24182;&#19982;&#20165;&#22312;&#26381;&#21153;&#22120;&#19978;&#26631;&#35760;&#30340;&#38170;&#23450;&#25968;&#25454;&#35757;&#32451;&#30340;&#20998;&#31867;&#22836;&#30456;&#37197;&#12290;&#38170;&#23450;&#22836;&#36890;&#36807;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10191v1 Announce Type: new  Abstract: Federated learning (FL) is a distributed learning paradigm that facilitates collaborative training of a shared global model across devices while keeping data localized. The deployment of FL in numerous real-world applications faces delays, primarily due to the prevalent reliance on supervised tasks. Generating detailed labels at edge devices, if feasible, is demanding, given resource constraints and the imperative for continuous data updates. In addressing these challenges, solutions such as federated semi-supervised learning (FSSL), which relies on unlabeled clients' data and a limited amount of labeled data on the server, become pivotal. In this paper, we propose FedAnchor, an innovative FSSL method that introduces a unique double-head structure, called anchor head, paired with the classification head trained exclusively on labeled anchor data on the server. The anchor head is empowered with a newly designed label contrastive loss base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10189</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#28436;&#31034;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#37197;&#32622;&#30340;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#28436;&#31034;&#26469;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;LLM&#21709;&#24212;&#20013;&#30340;&#21487;&#20449;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#65292;&#20063;&#34987;&#31215;&#26497;&#35752;&#35770;&#12290;&#29616;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#37327;&#21270;LLM&#21709;&#24212;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;LLM&#30340;&#22797;&#26434;&#24615;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29420;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#20851;&#30340;LLM&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#26469;&#33258;&#20110;&#25552;&#20379;&#30340;&#28436;&#31034;&#65288;aleatoric&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#19982;&#27169;&#22411;&#37197;&#32622;&#30456;&#20851;&#30340;&#27169;&#31946;&#24615;&#65288;epistemic&#19981;&#30830;&#23450;&#24615;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#21644;&#30456;&#24212;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20026;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#37324;&#30340;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10189v1 Announce Type: new  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-cont
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#30005;&#23376;&#32467;&#26500;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#33258;&#27965;&#22330;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#20302;&#39564;&#35777;&#25104;&#26412;&#21644;&#24378;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#33021;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#26469;&#25506;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10186</link><description>&lt;p&gt;
&#33258;&#27965;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#30005;&#23376;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Self-consistent Validation for Machine Learning Electronic Structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10186
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#30005;&#23376;&#32467;&#26500;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#33258;&#27965;&#22330;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#20302;&#39564;&#35777;&#25104;&#26412;&#21644;&#24378;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#33021;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#26469;&#25506;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26377;&#25928;&#35299;&#20915;&#30005;&#23376;&#32467;&#26500;&#38382;&#39064;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#32570;&#20047;&#20445;&#35777;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#26469;&#20272;&#35745;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#33258;&#27965;&#22330;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#26082;&#33021;&#23454;&#29616;&#20302;&#39564;&#35777;&#25104;&#26412;&#65292;&#21448;&#33021;&#35299;&#37322;&#24615;&#24378;&#12290;&#36825;&#21453;&#36807;&#26469;&#20351;&#24471;&#33021;&#22815;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#26469;&#25506;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#23558;&#20854;&#38598;&#25104;&#21040;&#23454;&#38469;&#30740;&#31350;&#20013;&#36171;&#20104;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10186v1 Announce Type: new  Abstract: Machine learning has emerged as a significant approach to efficiently tackle electronic structure problems. Despite its potential, there is less guarantee for the model to generalize to unseen data that hinders its application in real-world scenarios. To address this issue, a technique has been proposed to estimate the accuracy of the predictions. This method integrates machine learning with self-consistent field methods to achieve both low validation cost and interpret-ability. This, in turn, enables exploration of the model's ability with active learning and instills confidence in its integration into real-world studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22823;&#35268;&#27169;&#21463;&#38480;&#21046;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22120;&#29983;&#25104;&#26082;&#21487;&#34892;&#21448;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#36164;&#28304;&#20998;&#37197;&#21644;&#20351;&#29992;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10177</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21463;&#38480;&#21046;&#32858;&#31867;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Scale Constrained Clustering With Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22823;&#35268;&#27169;&#21463;&#38480;&#21046;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22120;&#29983;&#25104;&#26082;&#21487;&#34892;&#21448;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#36164;&#28304;&#20998;&#37197;&#21644;&#20351;&#29992;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#32593;&#32476;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#22312;&#32858;&#31867;&#32423;&#21035;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#65292;&#21487;&#20197;&#22686;&#24378;&#36164;&#28304;&#20998;&#37197;&#21644;&#20351;&#29992;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26368;&#23567;&#21270;&#32858;&#31867;&#20869;&#37096;&#36317;&#31163;&#21644;&#26368;&#22823;&#21270;&#20998;&#37197;&#32473;&#32858;&#31867;&#30340;&#33410;&#28857;&#25968;&#37327;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#32858;&#31867;&#20869;&#37096;&#27809;&#26377;&#20004;&#20010;&#33410;&#28857;&#30340;&#36317;&#31163;&#36229;&#36807;&#38408;&#20540;&#30340;&#20840;&#36830;&#25509;&#19981;&#30456;&#20132;&#32858;&#31867;&#38382;&#39064;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#20108;&#36827;&#21046;&#32447;&#24615;&#27169;&#22411;&#36731;&#26494;&#22320;&#24418;&#25104;&#38382;&#39064;&#65292;&#20294;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#23454;&#20363;&#26102;&#65292;&#20256;&#32479;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#24456;&#38590;&#24212;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#32858;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#22120;&#29983;&#25104;&#26082;&#21487;&#34892;&#21448;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20195;&#29702;&#22120;&#23398;&#20064;&#29305;&#23450;&#20110;&#35813;&#20219;&#21153;&#25152;&#36935;&#21040;&#30340;&#23454;&#20363;&#30340;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22312;&#32467;&#26524;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#20063;&#33021;&#25214;&#21040;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10177v1 Announce Type: cross  Abstract: Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage. In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance. While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances. We propose an approach to solve this constrained clustering problem via reinforcement learning. Our method involves training an agent to generate both feasible and (near) optimal solutions. The agent learns problem-specific heuristics, tailored to the instances encountered in this task. In the results section, we show that our algorithm finds near optimal solutions, even for l
&lt;/p&gt;</description></item><item><title>OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10176</link><description>&lt;p&gt;
OpenMathInstruct-1: &#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10176
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20026;&#20102;&#33719;&#24471;&#29305;&#23450;&#30340;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#22914;MetaMathQA&#21644;MAmmoTH&#65292;&#26159;&#20351;&#29992;&#26469;&#33258;&#21830;&#19994;&#38480;&#21046;&#35768;&#21487;&#30340;&#38381;&#28304;LLM&#30340;&#36755;&#20986;&#26500;&#24314;&#30340;&#12290;&#38480;&#21046;&#22312;&#36825;&#20123;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#28304;LLM&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#30446;&#21069;&#26368;&#22909;&#30340;&#38381;&#28304;LLM&#65288;&#22914;GPT-4&#65289;&#21644;&#26368;&#22909;&#30340;&#24320;&#28304;LLM&#20043;&#38388;&#22312;&#25968;&#23398;&#25216;&#33021;&#19978;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#26041;&#24335;&#21644;&#19968;&#20123;&#24378;&#21147;&#32553;&#25918;&#65292;&#26500;&#24314;&#20102;OpenMathInstruct-1&#65292;&#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#38382;&#39064;-&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20351;&#29992;GSM8K&#21644;MATH&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
&lt;/p&gt;</description></item><item><title>DeepSRGM&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Raga&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#22320;&#20301;&#12290;</title><link>https://arxiv.org/abs/2402.10168</link><description>&lt;p&gt;
DeepSRGM -- &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;&#20013;&#30340;&#24207;&#21015;&#20998;&#31867;&#21644;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10168
&lt;/p&gt;
&lt;p&gt;
DeepSRGM&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM-RNN&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Raga&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;arXiv:2402.10168v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#12299; &#25688;&#35201;&#65306;&#21360;&#24230;&#21476;&#20856;&#38899;&#20048;(ICM)&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;Raga&#65292;&#23427;&#20316;&#20026;&#20316;&#26354;&#21644;&#21363;&#20852;&#28436;&#22863;&#30340;&#26059;&#24459;&#26694;&#26550;&#12290;Raga&#30340;&#35782;&#21035;&#26159;ICM&#20013;&#19968;&#39033;&#37325;&#35201;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20174;&#38899;&#20048;&#25512;&#33616;&#21040;&#32452;&#32455;&#22823;&#22411;&#38899;&#20048;&#25910;&#34255;&#31561;&#22810;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Raga&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#65292;&#20351;&#29992;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#37319;&#26679;&#33258;&#21407;&#22987;&#38899;&#39057;&#30340;&#36739;&#23567;&#24207;&#21015;&#19978;&#36827;&#34892;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#32780;&#26368;&#32456;&#30340;&#25512;&#29702;&#21017;&#26159;&#22312;&#25972;&#20010;&#38899;&#39057;&#19978;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Comp Music Carnatic&#25968;&#25454;&#38598;&#21644;&#20854;10&#20010;Raga&#23376;&#38598;&#19978;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;88.1%&#21644;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#20351;&#20854;&#25104;&#20026;Raga&#35782;&#21035;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20351;&#24207;&#21015;&#25490;&#24207;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10168v1 Announce Type: cross  Abstract: A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#20855;&#26377;&#39640;&#26031;&#25968;&#25454;&#30340;&#19968;&#33324;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23558;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#26144;&#23556;&#21040;&#31561;&#25928;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#20005;&#26684;&#30028;&#38480;&#21644;&#25968;&#20540;&#23454;&#39564;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10164</link><description>&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#21644;&#22810;&#39033;&#24335;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Random features and polynomial rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#20855;&#26377;&#39640;&#26031;&#25968;&#25454;&#30340;&#19968;&#33324;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23558;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#26144;&#23556;&#21040;&#31561;&#25928;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#20005;&#26684;&#30028;&#38480;&#21644;&#25968;&#20540;&#23454;&#39564;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25551;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#25509;&#36817;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#26102;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#39640;&#26031;&#25968;&#25454;&#30340;&#19968;&#33324;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#21033;&#29992;&#26080;&#24207;&#31995;&#32479;&#30340;&#32479;&#35745;&#21147;&#23398;&#24037;&#20855;&#23558;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#26144;&#23556;&#21040;&#31561;&#25928;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24182;&#32472;&#21046;&#20102;&#24179;&#22343;&#27867;&#21270;&#26354;&#32447;&#20316;&#20026;&#38382;&#39064;&#30340;&#20004;&#20010;&#20027;&#35201;&#25511;&#21046;&#21442;&#25968;&#30340;&#20989;&#25968;&#65306;&#38543;&#26426;&#29305;&#24449;&#30340;&#25968;&#37327;N&#21644;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;P&#65292;&#20551;&#35774;&#23427;&#20204;&#37117;&#25353;&#29031;&#36755;&#20837;&#32500;&#24230;D&#30340;&#24130;&#36827;&#34892;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#20102;N&#65292;P&#21644;D&#20043;&#38388;&#27604;&#20363;&#32553;&#25918;&#30340;&#24773;&#20917;&#12290;&#23427;&#20204;&#19982;&#29305;&#23450;&#23398;&#20064;&#20219;&#21153;&#24050;&#30693;&#30340;&#20005;&#26684;&#30028;&#38480;&#19968;&#33268;&#65292;&#24182;&#19982;&#25968;&#20540;&#23454;&#39564;&#23450;&#37327;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10164v1 Announce Type: cross  Abstract: Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$f$-MICL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#25512;&#24191;&#22522;&#20110;InfoNCE&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;$f$-divergences&#23558;&#22522;&#20110;KL&#30340;&#20114;&#20449;&#24687;&#25512;&#24191;&#20026;$f$-Mutual Information in Contrastive Learning ($f$-MICL)&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#36229;&#36234;&#22522;&#20110;KL&#30340;&#30446;&#26631;&#20989;&#25968;&#20197;&#21450;&#35774;&#35745;&#26356;&#22909;&#30456;&#20284;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10150</link><description>&lt;p&gt;
$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$f$-MICL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#25512;&#24191;&#22522;&#20110;InfoNCE&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;$f$-divergences&#23558;&#22522;&#20110;KL&#30340;&#20114;&#20449;&#24687;&#25512;&#24191;&#20026;$f$-Mutual Information in Contrastive Learning ($f$-MICL)&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#36229;&#36234;&#22522;&#20110;KL&#30340;&#30446;&#26631;&#20989;&#25968;&#20197;&#21450;&#35774;&#35745;&#26356;&#22909;&#30456;&#20284;&#24230;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#19968;&#31181;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;InfoNCE&#65292;&#23427;&#20351;&#29992;&#21551;&#21457;&#24335;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#36827;&#34892;&#34920;&#31034;&#27604;&#36739;&#65292;&#24182;&#19988;&#19982;&#26368;&#22823;&#21270;&#22522;&#20110;KL&#30340;&#20114;&#20449;&#24687;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#20004;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;(1)&#25105;&#20204;&#33021;&#21542;&#36229;&#36234;&#22522;&#20110;KL&#30340;&#30446;&#26631;&#65311;(2)&#38500;&#20102;&#27969;&#34892;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30456;&#20284;&#24230;&#20989;&#25968;&#65311;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#20110;KL&#30340;&#20114;&#20449;&#24687;&#25512;&#24191;&#20026;$f$-Mutual Information in Contrastive Learning ($f$-MICL)&#65292;&#20351;&#29992;$f$-divergences&#26469;&#22238;&#31572;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#38024;&#23545;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;$f$-MICL&#30446;&#26631;&#20989;&#25968;&#65292;&#23427;&#20204;&#20855;&#26377;&#19982;InfoNCE&#30456;&#20284;&#30340;&#33391;&#22909;&#29305;&#24615;&#65288;&#22914;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#65289;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#31867;&#20284;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#20551;&#35774;&#32852;&#21512;&#29305;&#24449;&#20998;&#24067;&#19982;&#39640;&#26031;&#26680;&#25104;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10150v1 Announce Type: new  Abstract: In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#27788;&#26144;&#23556;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#21644;&#22522;&#20110;&#28151;&#27788;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#23618;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10145</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#27788;&#26144;&#23556;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#19981;&#23436;&#25972;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#27788;&#26144;&#23556;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#21644;&#22522;&#20110;&#28151;&#27788;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#23618;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#24471;&#22810;&#20010;&#20855;&#26377;&#25935;&#24863;&#25968;&#25454;&#20294;&#24076;&#26395;&#20849;&#20139;&#30693;&#35782;&#30340;&#21442;&#19982;&#32773;&#33021;&#22815;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32780;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#19982;&#22522;&#20110;&#28151;&#27788;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#23618;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#35780;&#20272;&#20102;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#27599;&#20010;&#23454;&#39564;&#20013;&#65292;&#38598;&#20013;&#24335;&#23398;&#20064;&#36807;&#31243;&#37117;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#22343;&#24615;&#33021;&#25351;&#26631;&#65292;&#21363;&#20351;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10145v1 Announce Type: new  Abstract: Federated Learning is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data. In this research, the authors employ a secured Federated Learning method with an additional layer of privacy and proposes a method for addressing the non-IID challenge. Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy. The experimental approach assesses the performance of the federated deep learning model with differential privacy using both IID and non-IID data. In each experiment, the Federated Learning process improves the average performance metrics of the deep neural network, even in the case of non-IID data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;</title><link>https://arxiv.org/abs/2402.10142</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Changing Probabilities via Dynamic Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#39044;&#27979;&#22120;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#22120;&#65292;&#20854;&#36755;&#20837;&#26159;&#19968;&#31995;&#21015;&#31163;&#25955;&#39033;&#30446;&#12290;&#39044;&#27979;&#22120;&#30340;&#20219;&#21153;&#26159;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#27010;&#29575;&#22810;&#31867;&#21035;&#39044;&#27979;&#65292;&#21363;&#36890;&#36807;&#36755;&#20986;&#26377;&#38646;&#20010;&#25110;&#22810;&#20010;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#25509;&#19979;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#39033;&#30446;&#65292;&#28982;&#21518;&#25581;&#31034;&#23454;&#38469;&#39033;&#30446;&#24182;&#20174;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36755;&#20986;&#27010;&#29575;&#65292;&#39044;&#27979;&#22120;&#20250;&#36319;&#36394;&#20854;&#25152;&#35265;&#39033;&#30446;&#30340;&#27604;&#20363;&#12290;&#39044;&#27979;&#22120;&#20855;&#26377;&#24658;&#23450;&#65288;&#26377;&#38480;&#65289;&#30340;&#31354;&#38388;&#65292;&#25105;&#20204;&#23547;&#27714;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#26356;&#26032;&#25216;&#26415;&#65306;&#27969;&#26159;&#26080;&#30028;&#30340;&#65292;&#39033;&#30446;&#30340;&#38598;&#21512;&#23545;&#39044;&#27979;&#22120;&#26159;&#26410;&#30693;&#30340;&#65292;&#23427;&#20204;&#30340;&#24635;&#25968;&#20063;&#21487;&#33021;&#26080;&#38480;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#38750;&#24179;&#31283;&#24615;&#65306;&#39033;&#30446;&#30340;&#28508;&#22312;&#39057;&#29575;&#21487;&#33021;&#20250;&#19981;&#26102;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#26032;&#39033;&#30446;&#21487;&#33021;&#24320;&#22987;&#20986;&#29616;&#65292;&#19968;&#20123;&#24403;&#21069;&#39057;&#32321;&#20986;&#29616;&#30340;&#39033;&#30446;&#21487;&#33021;&#20877;&#27425;&#20572;&#27490;&#20986;&#29616;&#12290;&#30001;&#20110;&#26377;&#31354;&#38388;&#38480;&#21046;&#65292;&#39044;&#27979;&#22120;&#21482;&#38656;&#35201;&#25552;&#20379;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10142v1 Announce Type: cross  Abstract: Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide pro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23545;&#31561;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#24179;&#22343;&#32858;&#21512;&#65292;&#20197;&#30830;&#23450;&#26368;&#24378;&#22823;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10135</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23545;&#31561;&#32852;&#37030;&#23398;&#20064;&#30340;&#31574;&#30053;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23545;&#31561;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#27979;&#35797;&#20102;&#21508;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#24179;&#22343;&#32858;&#21512;&#65292;&#20197;&#30830;&#23450;&#26368;&#24378;&#22823;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20445;&#25252;&#21644;&#38544;&#31169;&#35201;&#27714;&#30340;&#19981;&#26029;&#22686;&#21152;&#24341;&#36215;&#20102;&#23545;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24040;&#22823;&#30740;&#31350;&#20852;&#36259;&#65292;&#23588;&#20854;&#26159;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30001;&#25345;&#26377;&#33258;&#24049;&#31169;&#26377;&#25968;&#25454;&#30340;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#24314;&#31435;&#12290;&#22312;&#26368;&#21021;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#26696;&#20013;&#65292;&#26550;&#26500;&#26159;&#38598;&#20013;&#24335;&#30340;&#65292;&#32858;&#21512;&#26159;&#36890;&#36807;&#32852;&#37030;&#24179;&#22343;&#21270;&#26469;&#23436;&#25104;&#30340;&#65292;&#24847;&#21619;&#30528;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#23558;&#20351;&#29992;&#26368;&#30452;&#25509;&#30340;&#24179;&#22343;&#31574;&#30053;&#26469;&#21327;&#35843;&#32852;&#37030;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#22312;&#23545;&#31561;&#29615;&#22659;&#20013;&#27979;&#35797;&#19981;&#21516;&#30340;&#32852;&#37030;&#31574;&#30053;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#21253;&#25324;&#21152;&#26435;&#24179;&#22343;&#32858;&#21512;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#22240;&#32032;&#21644;&#22522;&#20110;&#21442;&#19982;&#32773;&#36129;&#29486;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;&#26368;&#24378;&#22823;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10135v1 Announce Type: cross  Abstract: The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy. This research is focused on testing different federated strategies in a peer-to-peer environment. The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution. The strategies are tested with varying data sizes to identify the most robust ones. This resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#23454;&#38469;&#25361;&#25112;&#19981;&#21305;&#37197;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10130</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#21542;&#36866;&#24212;&#29616;&#23454;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Continual Learning Ready for Real-world Challenges?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#23454;&#38469;&#25361;&#25112;&#19981;&#21305;&#37197;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36830;&#32493;&#23398;&#20064;&#22312;&#23398;&#26415;&#30028;&#26377;&#30528;&#24736;&#20037;&#32780;&#33391;&#22909;&#30340;&#21382;&#21490;&#65292;&#20294;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#30456;&#23545;&#26377;&#38480;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#24046;&#36317;&#26159;&#30001;&#20110;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#19982;&#36830;&#32493;&#23398;&#20064;&#30340;&#23454;&#38469;&#25361;&#25112;&#19981;&#21305;&#37197;&#65292;&#23548;&#33268;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#26377;&#25928;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#26032;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;OCL-3DSS&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#33258;&#24049;&#30340;&#20551;&#35774;&#24182;&#35780;&#20272;&#20102;&#36807;&#21435;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26356;&#21152;&#29616;&#23454;&#30340;&#21327;&#35758;&#26469;&#30740;&#31350;&#25991;&#29486;&#20013;&#30340;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#38656;&#35201;&#22312;&#32447;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#24212;&#23545;&#21160;&#24577;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26426;&#22120;&#20154;&#21644;&#19977;&#32500;&#35270;&#35273;&#24212;&#29992;&#65289;&#12290;&#32467;&#26524;&#20196;&#20154;&#27822;&#20007;&#65306;&#25152;&#26377;&#32771;&#34385;&#30340;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#65292;&#26126;&#26174;&#20559;&#31163;&#32852;&#21512;&#31163;&#32447;&#35757;&#32451;&#30340;&#19978;&#38480;&#12290;&#36825;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10130v1 Announce Type: cross  Abstract: Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS. We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in rea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;GES&#65288;&#24191;&#20041;&#25351;&#25968;&#21943;&#27922;&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#24191;&#20041;&#25351;&#25968;&#20989;&#25968;&#26469;&#24314;&#27169;3D&#22330;&#26223;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;3D&#37325;&#24314;&#21644;&#29983;&#25104;&#30340;&#25928;&#29575;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#39640;&#26031;&#21943;&#27922;&#26041;&#27861;&#65292;GES&#25152;&#38656;&#30340;&#31890;&#23376;&#25968;&#37327;&#26356;&#23569;&#65292;&#33021;&#26356;&#20934;&#30830;&#22320;&#34920;&#31034;&#20855;&#26377;&#38160;&#21033;&#36793;&#32536;&#30340;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.10128</link><description>&lt;p&gt;
GES&#65306;&#29992;&#20110;&#39640;&#25928;&#36752;&#23556;&#22330;&#28210;&#26579;&#30340;&#24191;&#20041;&#25351;&#25968;&#21943;&#27922;
&lt;/p&gt;
&lt;p&gt;
GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;GES&#65288;&#24191;&#20041;&#25351;&#25968;&#21943;&#27922;&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#24191;&#20041;&#25351;&#25968;&#20989;&#25968;&#26469;&#24314;&#27169;3D&#22330;&#26223;&#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;3D&#37325;&#24314;&#21644;&#29983;&#25104;&#30340;&#25928;&#29575;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#39640;&#26031;&#21943;&#27922;&#26041;&#27861;&#65292;GES&#25152;&#38656;&#30340;&#31890;&#23376;&#25968;&#37327;&#26356;&#23569;&#65292;&#33021;&#26356;&#20934;&#30830;&#22320;&#34920;&#31034;&#20855;&#26377;&#38160;&#21033;&#36793;&#32536;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10128v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;3D&#39640;&#26031;&#21943;&#27922;&#25216;&#26415;&#30340;&#36827;&#23637;&#26174;&#33879;&#21152;&#24555;&#20102;3D&#37325;&#24314;&#21644;&#29983;&#25104;&#30340;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#39640;&#26031;&#20989;&#25968;&#65292;&#23548;&#33268;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GES&#65288;&#24191;&#20041;&#25351;&#25968;&#21943;&#27922;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24191;&#20041;&#25351;&#25968;&#20989;&#25968;&#65288;GEF&#65289;&#26469;&#24314;&#27169;3D&#22330;&#26223;&#65292;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#31890;&#23376;&#26469;&#34920;&#31034;&#19968;&#20010;&#22330;&#26223;&#65292;&#22240;&#27492;&#22312;&#25928;&#29575;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#39640;&#26031;&#30340;&#21943;&#27922;&#26041;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#26031;-based utilities&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#25442;&#33021;&#21147;&#12290;GES&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#37117;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#22312;&#21407;&#21017;&#19978;&#30340;1D&#35774;&#32622;&#21644;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GEF&#27604;&#39640;&#26031;&#20989;&#25968;&#26356;&#20934;&#30830;&#22320;&#34920;&#31034;&#20855;&#26377;&#38160;&#21033;&#36793;&#32536;&#30340;&#20449;&#21495;&#65292;&#36825;&#22312;&#39640;&#26031;&#20989;&#25968;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20302;&#36890;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;GEF&#22312;&#25311;&#21512;&#33258;&#28982;&#21457;&#29983;&#30340;&#20449;&#21495;&#65288;&#20363;&#22914;&#27491;&#26041;&#24418;&#12289;&#19977;&#35282;&#24418;&#65289;&#26041;&#38754;&#20248;&#20110;&#39640;&#26031;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10128v1 Announce Type: cross  Abstract: Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#23574;&#23792;&#21327;&#26041;&#24046;&#30697;&#38453;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#23545;&#23574;&#23792;&#29305;&#24449;&#32467;&#26500;&#30340;&#23450;&#37327;&#25551;&#36848;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20302;&#32500;&#20449;&#21495;&#32467;&#26500;&#22914;&#20309;&#32463;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#24773;&#22659;&#65292;&#20854;&#20013;&#26435;&#37325;&#30697;&#38453;&#21457;&#23637;&#20986;&#19968;&#20010;&#31209;&#20026;&#19968;&#30340;&#20449;&#21495;&#20998;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10127</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#23574;&#23792;&#21327;&#26041;&#24046;&#30697;&#38453;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Nonlinear spiked covariance matrices and signal propagation in deep neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#23574;&#23792;&#21327;&#26041;&#24046;&#30697;&#38453;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#23545;&#23574;&#23792;&#29305;&#24449;&#32467;&#26500;&#30340;&#23450;&#37327;&#25551;&#36848;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20302;&#32500;&#20449;&#21495;&#32467;&#26500;&#22914;&#20309;&#32463;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#24773;&#22659;&#65292;&#20854;&#20013;&#26435;&#37325;&#30697;&#38453;&#21457;&#23637;&#20986;&#19968;&#20010;&#31209;&#20026;&#19968;&#30340;&#20449;&#21495;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#37117;&#30740;&#31350;&#20102;&#30001;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#26144;&#23556;&#23450;&#20041;&#30340;&#20849;&#36717;&#26680;&#65288;CK&#65289;&#30340;&#29305;&#24449;&#20540;&#35889;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32467;&#26524;&#21482;&#33021;&#24314;&#31435;&#32463;&#39564;&#29305;&#24449;&#20540;&#20998;&#24067;&#30340;&#24369;&#25910;&#25947;&#24615;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#23545;&#36890;&#24120;&#25429;&#25417;&#23398;&#20064;&#38382;&#39064;&#30340;&#20302;&#32500;&#20449;&#21495;&#32467;&#26500;&#30340;&#8220;&#23574;&#23792;&#8221;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#31934;&#30830;&#23450;&#37327;&#25551;&#36848;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#38750;&#32447;&#24615;&#29256;&#26412;&#30340;&#23574;&#23792;&#21327;&#26041;&#24046;&#27169;&#22411;&#65288;&#21253;&#25324;CK&#20316;&#20026;&#29305;&#20363;&#65289;&#36827;&#34892;&#20102;&#36825;&#20123;&#20449;&#21495;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#34920;&#24449;&#12290;&#21033;&#29992;&#36825;&#20010;&#19968;&#33324;&#32467;&#26524;&#65292;&#25105;&#20204;&#23450;&#37327;&#25551;&#36848;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#23574;&#23792;&#29305;&#24449;&#32467;&#26500;&#22914;&#20309;&#36890;&#36807;&#20855;&#26377;&#38543;&#26426;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#20256;&#25773;&#12290;&#20316;&#20026;&#31532;&#20108;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#19968;&#20010;&#31616;&#21333;&#24773;&#22659;&#65292;&#20854;&#20013;&#26435;&#37325;&#30697;&#38453;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21457;&#23637;&#20986;&#19968;&#20010;&#31209;&#20026;&#19968;&#30340;&#20449;&#21495;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10127v1 Announce Type: cross  Abstract: Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network. However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the ''spike'' eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case. Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. As a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#37325;&#29992;Softmax&#30828;&#20214;&#21333;&#20803;&#36827;&#34892;GELU&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#38477;&#20302;NLP&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10118</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#37325;&#29992;Softmax&#30828;&#20214;&#21333;&#20803;&#36827;&#34892;GELU&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Reusing Softmax Hardware Unit for GELU Computation in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#37325;&#29992;Softmax&#30828;&#20214;&#21333;&#20803;&#36827;&#34892;GELU&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#38477;&#20302;NLP&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22823;&#22823;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;Transformer&#30340;&#35745;&#31639;&#28041;&#21450;&#30697;&#38453;&#20056;&#27861;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;softmax&#21644;GELU&#65288;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65289;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#30452;&#25509;&#22312;&#30828;&#20214;&#20013;&#21152;&#36895;&#12290;&#30446;&#21069;&#65292;&#27599;&#20010;&#20989;&#25968;&#30340;&#35745;&#31639;&#37117;&#26159;&#20998;&#24320;&#23436;&#25104;&#30340;&#65292;&#24456;&#23569;&#33021;&#22815;&#37325;&#22797;&#20351;&#29992;&#30828;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;GELU&#35745;&#31639;&#26144;&#23556;&#21040;softmax&#36816;&#31639;&#31526;&#19978;&#12290;&#36825;&#26679;&#65292;&#24050;&#32463;&#35774;&#35745;&#29992;&#20110;softmax&#30340;&#39640;&#25928;&#30828;&#20214;&#21333;&#20803;&#20063;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;GELU&#12290;GELU&#30340;&#35745;&#31639;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;softmax&#30340;&#21521;&#37327;&#21270;&#29305;&#24615;&#65292;&#21516;&#26102;&#24182;&#34892;&#20135;&#29983;&#22810;&#20010;GELU&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#39044;&#20808;&#23384;&#22312;&#24182;&#36880;&#27493;&#20462;&#25913;&#30340;softmax&#30828;&#20214;&#21333;&#20803;&#35745;&#31639;GELU&#65288;a&#65289;&#19981;&#20250;&#38477;&#20302;&#20195;&#34920;&#24615;NLP&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#65292;&#65288;b&#65289;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10118v1 Announce Type: cross  Abstract: Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) all
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10115</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#20174;EEG&#35760;&#24405;&#20013;&#29983;&#25104;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24863;&#30693;&#24615;&#33041;&#35299;&#30721;&#39046;&#22495;&#30340;&#19968;&#20010;&#29616;&#20195;&#30740;&#31350;&#25361;&#25112;&#65292;&#21363;&#20351;&#29992;&#23545;&#25239;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20174;EEG&#20449;&#21495;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#20855;&#20307;&#30446;&#26631;&#26159;&#21033;&#29992;&#20027;&#20307;&#35266;&#30475;&#22270;&#20687;&#26102;&#33719;&#24471;&#30340;EEG&#35760;&#24405;&#37325;&#26032;&#21019;&#24314;&#23646;&#20110;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#29983;&#25104;EEG&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;GAN&#32593;&#32476;&#30340;&#29983;&#25104;&#22120;&#32452;&#20214;&#30340;&#36755;&#20837;&#12290;&#38500;&#20102;&#23545;&#25239;&#25439;&#22833;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10115v1 Announce Type: new  Abstract: In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;</title><link>https://arxiv.org/abs/2402.10110</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65306;LLM&#25351;&#20196;&#35843;&#33410;&#30340;&#23398;&#29983;&#36873;&#25321;&#25968;&#25454;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#19982;&#23398;&#29983;LLM&#30340;&#25968;&#25454;&#36873;&#25321;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#33410;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35828;&#38750;&#24120;&#20851;&#38190;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#36394;&#21644;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#20294;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#37117;&#33268;&#21147;&#20110;&#25913;&#36827;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#19982;&#27491;&#22312;&#24494;&#35843;&#30340;&#23398;&#29983;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#65292;&#36890;&#36807;&#32467;&#21512;&#25945;&#24072;LLM&#30340;&#21453;&#23556;&#21644;&#33258;&#30465;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#12290;&#36825;&#31181;&#24072;&#29983;&#21512;&#20316;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#19988;&#19982;&#23398;&#29983;LLM&#20860;&#23481;&#30340;&#25351;&#20196;&#21709;&#24212;&#23545;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#33410;&#21644;&#21331;&#36234;&#24615;&#33021;&#30340;LLM&#12290;&#36873;&#25321;&#24615;&#21453;&#23556;&#35843;&#33410;&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#21644;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#24120;&#33021;&#25913;&#21892;LLM&#24494;&#35843;&#21644;&#33258;&#25105;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10109</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#26469;&#38477;&#20302;&#35786;&#26029;&#38169;&#35823;&#12290;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#65292;&#24182;&#32473;&#20986;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#65292;&#29305;&#21035;&#38024;&#23545;&#35786;&#26029;&#24310;&#36831;&#21644;&#26469;&#33258;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35786;&#26029;&#38169;&#35823;&#21457;&#29983;&#26159;&#22240;&#20026;&#20020;&#24202;&#21307;&#29983;&#26080;&#27861;&#36731;&#26131;&#33719;&#21462;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#26041;&#27861;&#26469;&#35782;&#21035;&#30149;&#20154;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#20013;&#25351;&#31034;&#29305;&#23450;&#35786;&#26029;&#39118;&#38505;&#22686;&#21152;&#25110;&#20943;&#23569;&#30340;&#35777;&#25454;&#30340;&#26041;&#27861;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#22686;&#21152;&#35777;&#25454;&#30340;&#33719;&#21462;&#19982;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#26469;&#36827;&#34892;&#24102;&#26377;&#20010;&#20307;&#21270;&#39118;&#38505;&#20272;&#35745;&#30340;&#20197;&#35777;&#25454;&#20026;&#21518;&#30462;&#30340;&#39044;&#27979;&#65292;&#22312;&#20020;&#24202;&#21307;&#29983;&#20173;&#28982;&#19981;&#30830;&#23450;&#30340;&#26102;&#38388;&#28857;&#19978;&#65292;&#26088;&#22312;&#29305;&#21035;&#20943;&#36731;&#35786;&#26029;&#24310;&#36831;&#21644;&#28304;&#20110;&#19981;&#23436;&#25972;&#37492;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#65292;&#38656;&#35201;&#25512;&#26029;&#20986;&#20107;&#20214;&#24615;&#30340;&#8220;&#30495;&#23454;&#8221;&#35786;&#26029;&#30340;&#26102;&#38388;&#31890;&#24230;&#32454;&#33268;&#30340;&#22238;&#39038;&#24615;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#20445;&#35777;&#36755;&#20837;&#25991;&#26412;&#26159;&#22312;&#21487;&#20197;&#36827;&#34892;&#33258;&#20449;&#30340;&#35786;&#26029;&#20043;&#21069;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#26816;&#32034;&#21021;&#22987;&#30340;&#35777;&#25454;&#27744;&#65292;&#28982;&#21518;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#26080;&#20154;&#26426;&#25805;&#20316;&#21592;&#22312;&#22810;&#20010;&#23548;&#24377;&#23041;&#32961;&#19979;&#36827;&#34892;&#20915;&#31574;&#65292;&#36890;&#36807;&#23398;&#20064;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#26469;&#35780;&#20272;&#21508;&#31181;&#31574;&#30053;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#35758;&#26368;&#23433;&#20840;&#30340;&#34892;&#21160;&#26041;&#38024;&#12290;</title><link>https://arxiv.org/abs/2402.10101</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#23548;&#24377;&#36991;&#35753;&#24773;&#20917;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Situation Awareness for Multiple Missiles Evasion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#26080;&#20154;&#26426;&#25805;&#20316;&#21592;&#22312;&#22810;&#20010;&#23548;&#24377;&#23041;&#32961;&#19979;&#36827;&#34892;&#20915;&#31574;&#65292;&#36890;&#36807;&#23398;&#20064;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#26469;&#35780;&#20272;&#21508;&#31181;&#31574;&#30053;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#35758;&#26368;&#23433;&#20840;&#30340;&#34892;&#21160;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31354;&#23545;&#31354;&#23548;&#24377;&#30340;&#26377;&#25928;&#23556;&#31243;&#22686;&#21152;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#38590;&#20197;&#20445;&#25345;&#26080;&#20154;&#26426;&#25152;&#38656;&#30340;&#24773;&#20917;&#24863;&#30693;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#26080;&#20154;&#26426;&#25805;&#20316;&#21592;&#22312;&#35270;&#32447;&#22806;&#65288;BVR&#65289;&#31354;&#25112;&#24773;&#26223;&#20013;&#35780;&#20272;&#19981;&#21516;&#36873;&#25321;&#30340;&#39118;&#38505;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#36873;&#25321;&#20570;&#20986;&#20915;&#31574;&#12290;&#26089;&#26399;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#21333;&#19968;&#23548;&#24377;&#30340;&#23041;&#32961;&#65292;&#32780;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#24819;&#27861;&#25299;&#23637;&#21040;&#22810;&#20010;&#23548;&#24377;&#23041;&#32961;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#23398;&#20064;&#65292;&#20026;&#25805;&#20316;&#21592;&#25552;&#20379;&#19968;&#32452;&#19981;&#21516;&#31574;&#30053;&#30340;&#32467;&#26524;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#26469;&#34989;&#23548;&#24377;&#65292;&#35780;&#20272;&#19968;&#31995;&#21015;&#36873;&#39033;&#65292;&#24182;&#25512;&#33616;&#39118;&#38505;&#26368;&#23567;&#30340;&#34892;&#21160;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10101v1 Announce Type: cross  Abstract: As the effective range of air-to-air missiles increases, it becomes harder for human operators to maintain the situational awareness needed to keep a UAV safe. In this work, we propose a decision support tool to help UAV operators in Beyond Visual Range (BVR) air combat scenarios assess the risks of different options and make decisions based on those. Earlier work focused on the threat posed by a single missile, and in this work, we extend the ideas to several missile threats. The proposed method uses Deep Neural Networks (DNN) to learn from high-fidelity simulations to provide the operator with an outcome estimate for a set of different strategies. Our results demonstrate that the proposed system can manage multiple incoming missiles, evaluate a family of options, and recommend the least risky course of action.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.10100</link><description>&lt;p&gt;
&#35843;&#35856;&#65306;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#38480;&#21046;&#26465;&#20214;&#26159;&#20197;&#21453;&#26144;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#25910;&#38598;&#30340;&#23567;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;DenseNet&#21644;ConvNeXt&#22312;&#20869;&#30340;CNN&#27169;&#22411;&#65292;&#20197;&#21450;ViT&#12289;SWIN&#21644;AST&#31561;&#36716;&#25442;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35832;&#22914;YAMNet&#21644;VGGish&#30340;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#20020;&#24202;&#25968;&#25454;&#19978;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20174;&#21330;&#20013;&#24739;&#32773;&#20013;&#26032;&#25910;&#38598;&#20102;&#20004;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#24739;&#32773;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#21457;&#29616;&#22522;&#20110;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;RGB&#21644;&#28784;&#24230;&#35889;&#22270;&#36716;&#25442;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#21487;&#20197;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#65292;&#20854;&#20013;DenseNet-Contrastive&#21644;AST&#27169;&#22411;&#34920;&#29616;&#31361;&#20986;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#30340;&#25968;&#25454;&#36755;&#20837;&#38169;&#35823;unlearning&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#24615;&#31361;&#35302;&#25233;&#21046;&#65288;ASSD&#65289;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.10098</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#30340;&#25968;&#25454;&#36755;&#20837;&#38169;&#35823;unlearning&#26041;&#27861;&#19982;&#33258;&#36866;&#24212;&#36873;&#25321;&#24615;&#31361;&#35302;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
Parameter-tuning-free data entry error unlearning with adaptive selective synaptic dampening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#30340;&#25968;&#25454;&#36755;&#20837;&#38169;&#35823;unlearning&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#24615;&#31361;&#35302;&#25233;&#21046;&#65288;ASSD&#65289;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36755;&#20837;&#26159;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#32463;&#24120;&#20250;&#23548;&#33268;&#24341;&#20837;&#26631;&#31614;&#38169;&#35823;&#12290;&#24403;&#27169;&#22411;&#22312;&#21253;&#21547;&#36825;&#31181;&#38169;&#35823;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26377;&#25928;&#22320;&#21435;&#38500;&#38169;&#35823;&#25968;&#25454;&#30340;&#24433;&#21709;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#24050;&#30693;&#38169;&#35823;&#26465;&#30446;&#30340;&#27491;&#30830;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#26080;&#27861;&#30693;&#36947;&#38169;&#35823;&#25968;&#25454;&#30340;&#27491;&#30830;&#26631;&#31614;&#30340;&#25968;&#25454;&#36755;&#20837;&#38169;&#35823;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#36873;&#25321;&#24615;&#31361;&#35302;&#25233;&#21046;unlearning&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;unlearning&#26041;&#27861;&#23545;&#20174;&#19994;&#20154;&#21592;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#25193;&#23637;&#33258;&#36866;&#24212;&#36873;&#25321;&#24615;&#31361;&#35302;&#25233;&#21046;&#65288;ASSD&#65289;&#22312;&#19981;&#21516;&#30340;ResNet18&#21644;Visio&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10098v1 Announce Type: new  Abstract: Data entry constitutes a fundamental component of the machine learning pipeline, yet it frequently results in the introduction of labelling errors. When a model has been trained on a dataset containing such errors its performance is reduced. This leads to the challenge of efficiently unlearning the influence of the erroneous data to improve the model performance without needing to completely retrain the model. While model editing methods exist for cases in which the correct label for a wrong entry is known, we focus on the case of data entry errors where we do not know the correct labels for the erroneous data. Our contribution is twofold. First, we introduce an extension to the selective synaptic dampening unlearning method that removes the need for parameter tuning, making unlearning accessible to practitioners. We demonstrate the performance of this extension, adaptive selective synaptic dampening (ASSD), on various ResNet18 and Visio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#21644;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#36866;&#24212;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10097</link><description>&lt;p&gt;
&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#20013;&#20855;&#26377;&#29420;&#31435;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#21644;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#36866;&#24212;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#36827;&#34892;&#38543;&#26426;&#23376;&#38598;&#37319;&#26679;&#26469;&#35299;&#20915;&#36831;&#21040;&#32773;&#38382;&#39064;&#24182;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#32852;&#21512;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#26500;&#35774;&#35745;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21487;&#33021;&#19982;&#23454;&#38469;&#30340;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#26032;&#30340;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#38469;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#32771;&#34385;&#36890;&#20449;&#21644;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#24102;&#26377;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#30340;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#25910;&#25947;&#30028;&#38480;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#36718;&#25968;&#19978;&#30028;&#21644;&#27599;&#36718;&#39044;&#26399;&#35757;&#32451;&#26102;&#38388;&#30340;&#39640;&#25928;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#38469;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10097v1 Announce Type: new  Abstract: Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while consider
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#65288;CDMs&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#30340;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10095</link><description>&lt;p&gt;
&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Classification Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#65288;CDMs&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#30340;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.10095v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#19968;&#31181;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#20986;&#26041;&#27861;&#23478;&#26063;&#20381;&#36182;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#65288;DRE&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#34987;&#35757;&#32451;&#26469;$\textit{&#20998;&#31867;}$&#25968;&#25454;&#26679;&#26412;&#21644;&#26469;&#33258;&#26576;&#20010;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#31616;&#21333;&#30340;&#20302;&#32500;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#20013;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#23398;&#20064;&#20998;&#24067;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#23478;&#26063;&#26159;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#34987;&#35757;&#32451;&#26469;$\textit{&#21435;&#22122;}$&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;}$&#65288;CDMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#25216;&#26415;&#65292;&#23427;&#37319;&#29992;&#20102;DDM&#30340;&#21435;&#22122;&#22522;&#26412;&#24418;&#24335;&#65292;&#21516;&#26102;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#31867;&#20284;&#20110;DRE&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#65292;&#21363;MSE&#26368;&#20248;&#21270;&#30340;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10095v1 Announce Type: new  Abstract: A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal d
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#26102;&#38388;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#23458;&#25143;-&#36741;&#21161;&#22120;&#20998;&#37197;&#21644;&#35843;&#24230;&#20915;&#31574;&#30340;&#32852;&#21512;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.10092</link><description>&lt;p&gt;
&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Workflow Optimization for Parallel Split Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#20998;&#21106;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#26102;&#38388;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#23458;&#25143;-&#36741;&#21161;&#22120;&#20998;&#37197;&#21644;&#35843;&#24230;&#20915;&#31574;&#30340;&#32852;&#21512;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35753;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#35757;&#32451;&#22810;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#24182;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26041;&#27861;&#12290;SL&#23558;NN&#27169;&#22411;&#20998;&#21106;&#25104;&#37096;&#20998;&#65292;&#24182;&#20801;&#35768;&#23458;&#25143;&#31471;&#65288;&#35774;&#22791;&#65289;&#23558;&#26368;&#22823;&#37096;&#20998;&#20316;&#20026;&#22788;&#29702;&#20219;&#21153;&#21368;&#36733;&#32473;&#35745;&#31639;&#33021;&#21147;&#24378;&#22823;&#30340;&#36741;&#21161;&#22120;&#12290;&#22312;&#24182;&#34892;SL&#20013;&#65292;&#22810;&#20010;&#36741;&#21161;&#22120;&#21487;&#20197;&#22788;&#29702;&#19968;&#20010;&#25110;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#37096;&#20998;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#26102;&#38388;&#65288;makespan&#65289;&#12290;&#26412;&#25991;&#20851;&#27880;&#35813;&#25805;&#20316;&#30340;&#24037;&#20316;&#27969;&#32534;&#25490;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#24322;&#26500;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#23458;&#25143;-&#36741;&#21161;&#22120;&#20998;&#37197;&#21644;&#35843;&#24230;&#20915;&#31574;&#30340;&#32852;&#21512;&#38382;&#39064;&#24418;&#24335;&#21270;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#35757;&#32451;&#26102;&#38388;&#65288;makespan&#65289;&#65292;&#24182;&#35777;&#26126;&#35813;&#38382;&#39064;&#26159;NP&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#39064;&#20998;&#35299;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#22266;&#26377;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10092v1 Announce Type: cross  Abstract: Split learning (SL) has been recently proposed as a way to enable resource-constrained devices to train multi-parameter neural networks (NNs) and participate in federated learning (FL). In a nutshell, SL splits the NN model into parts, and allows clients (devices) to offload the largest part as a processing task to a computationally powerful helper. In parallel SL, multiple helpers can process model parts of one or more clients, thus, considerably reducing the maximum training time over all clients (makespan). In this paper, we focus on orchestrating the workflow of this operation, which is critical in highly heterogeneous systems, as our experiments show. In particular, we formulate the joint problem of client-helper assignments and scheduling decisions with the goal of minimizing the training makespan, and we prove that it is NP-hard. We propose a solution method based on the decomposition of the problem by leveraging its inherent sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20135;&#21697;&#21305;&#37197;&#30340;&#26032;&#24605;&#36335;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26080;&#30417;&#30563;&#21305;&#37197;&#19982;&#23569;&#37327;&#27880;&#37322;&#26679;&#26412;&#30340;&#20135;&#21697;&#38142;&#25509;&#21487;&#20197;&#25104;&#20026;&#20027;&#23548;&#30340;&#30417;&#30563;&#31574;&#30053;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10091</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#20135;&#21697;&#21305;&#37197;--&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text-Based Product Matching -- Semi-Supervised Clustering Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20135;&#21697;&#21305;&#37197;&#30340;&#26032;&#24605;&#36335;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26080;&#30417;&#30563;&#21305;&#37197;&#19982;&#23569;&#37327;&#27880;&#37322;&#26679;&#26412;&#30340;&#20135;&#21697;&#38142;&#25509;&#21487;&#20197;&#25104;&#20026;&#20027;&#23548;&#30340;&#30417;&#30563;&#31574;&#30053;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#65292;&#21305;&#37197;&#22810;&#20010;&#20135;&#21697;&#25552;&#20379;&#20013;&#30456;&#21516;&#30340;&#20135;&#21697;&#26159;&#19968;&#20010;&#20851;&#38190;&#35201;&#32032;&#65292;&#22914;&#27604;&#36739;&#20135;&#21697;&#20379;&#24212;&#12289;&#21160;&#24577;&#20215;&#26684;&#20248;&#21270;&#21644;&#36873;&#25321;&#20026;&#23458;&#25143;&#20010;&#24615;&#21270;&#23450;&#21046;&#30340;&#20135;&#21697;&#32452;&#21512;&#12290;&#23427;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#23454;&#20307;&#21305;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#20854;&#33258;&#36523;&#30340;&#29305;&#27530;&#24615;&#65292;&#22914;&#26080;&#22788;&#19981;&#22312;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#25110;&#19981;&#20934;&#30830;&#21644;&#19981;&#19968;&#33268;&#30340;&#20135;&#21697;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21322;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#20135;&#21697;&#21305;&#37197;&#30340;&#26032;&#24605;&#36335;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20027;&#35201;&#26159;&#25991;&#26412;&#29305;&#24449;&#21644;&#27169;&#31946;&#23383;&#31526;&#20018;&#21305;&#37197;&#30340;IDEC&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#26356;&#22810;&#26631;&#20934;&#26041;&#27861;&#20316;&#20026;&#21442;&#32771;&#65292;&#26469;&#30740;&#31350;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#30417;&#30563;&#21305;&#37197;&#32467;&#21512;&#23569;&#37327;&#27880;&#37322;&#26679;&#26412;&#30340;&#20135;&#21697;&#38142;&#25509;&#21487;&#33021;&#26159;&#19968;&#31181;&#21487;&#33021;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20027;&#23548;&#30340;&#30417;&#30563;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10091v1 Announce Type: cross  Abstract: Matching identical products present in multiple product feeds constitutes a crucial element of many tasks of e-commerce, such as comparing product offerings, dynamic price optimization, and selecting the assortment personalized for the client. It corresponds to the well-known machine learning task of entity matching, with its own specificity, like omnipresent unstructured data or inaccurate and inconsistent product descriptions. This paper aims to present a new philosophy to product matching utilizing a semi-supervised clustering approach. We study the properties of this method by experimenting with the IDEC algorithm on the real-world dataset using predominantly textual features and fuzzy string matching, with more standard approaches as a point of reference. Encouraging results show that unsupervised matching, enriched with a small annotated sample of product links, could be a possible alternative to the dominant supervised strategy,
&lt;/p&gt;</description></item><item><title>PICS&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#33258;&#21160;&#21270;&#22270;&#20687;&#25551;&#36848;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24773;&#24863;&#20998;&#26512;&#26469;&#22686;&#24378;&#20803;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#24211;&#30340;&#25628;&#32034;&#25928;&#29575;&#21644;&#35775;&#38382;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10090</link><description>&lt;p&gt;
PICS: &#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
PICS: Pipeline for Image Captioning and Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10090
&lt;/p&gt;
&lt;p&gt;
PICS&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#33258;&#21160;&#21270;&#22270;&#20687;&#25551;&#36848;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#24773;&#24863;&#20998;&#26512;&#26469;&#22686;&#24378;&#20803;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#24211;&#30340;&#25628;&#32034;&#25928;&#29575;&#21644;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22270;&#20687;&#30340;&#22686;&#38271;&#20351;&#24471;&#39640;&#25928;&#20998;&#31867;&#21644;&#26816;&#32034;&#30340;&#20808;&#36827;&#31995;&#32479;&#25104;&#20026;&#24517;&#38656;&#65292;&#36825;&#22312;&#25968;&#25454;&#24211;&#31649;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PICS&#65288;&#22270;&#20687;&#25551;&#36848;&#21644;&#25628;&#32034;&#30340;&#27969;&#27700;&#32447;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#35299;&#20915;&#32452;&#32455;&#22823;&#35268;&#27169;&#22270;&#20687;&#24211;&#20013;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;PICS&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#33258;&#21160;&#21270;&#22270;&#20687;&#25551;&#36848;&#30340;&#36807;&#31243;&#65292;&#25552;&#20379;&#19968;&#20010;&#36229;&#36234;&#20256;&#32479;&#25163;&#21160;&#27880;&#37322;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35748;&#35782;&#65292;&#21363;&#26377;&#24847;&#20041;&#30340;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25551;&#36848;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#22270;&#20687;&#30340;&#21487;&#25628;&#32034;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#36890;&#36807;&#23558;&#24773;&#24863;&#20998;&#26512;&#25972;&#21512;&#21040;&#27969;&#27700;&#32447;&#20013;&#65292;PICS&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20803;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#22522;&#26412;&#25551;&#36848;&#31526;&#30340;&#32454;&#33268;&#25628;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#31649;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10090v1 Announce Type: cross  Abstract: The growing volume of digital images necessitates advanced systems for efficient categorization and retrieval, presenting a significant challenge in database management and information retrieval. This paper introduces PICS (Pipeline for Image Captioning and Search), a novel approach designed to address the complexities inherent in organizing large-scale image repositories. PICS leverages the advancements in Large Language Models (LLMs) to automate the process of image captioning, offering a solution that transcends traditional manual annotation methods. The approach is rooted in the understanding that meaningful, AI-generated captions can significantly enhance the searchability and accessibility of images in large databases. By integrating sentiment analysis into the pipeline, PICS further enriches the metadata, enabling nuanced searches that extend beyond basic descriptors. This methodology not only simplifies the task of managing vas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10088</link><description>&lt;p&gt;
&#20998;&#23618;&#28151;&#21512;&#24314;&#27169;&#29992;&#20110;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical hybrid modeling for flexible tool use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#20013;&#65292;&#31163;&#25955;&#27169;&#22411;&#21487;&#20197;&#19982;&#36830;&#32493;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#31616;&#21333;&#30340;&#20195;&#29702;&#21487;&#20197;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19990;&#30028;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#30001;&#22810;&#20010;&#28151;&#21512; - &#36830;&#32493;&#21644;&#31163;&#25955; - &#21333;&#20803;&#32452;&#25104;&#65292;&#22797;&#21046;&#20195;&#29702;&#30340;&#37197;&#32622;&#65292;&#30001;&#39640;&#32423;&#31163;&#25955;&#27169;&#22411;&#25511;&#21046;&#65292;&#23454;&#29616;&#21160;&#24577;&#35268;&#21010;&#21644;&#21516;&#27493;&#34892;&#20026;&#12290;&#27599;&#20010;&#23618;&#27425;&#20869;&#37096;&#30340;&#36827;&#19968;&#27493;&#20998;&#35299;&#21487;&#20197;&#20197;&#20998;&#23618;&#26041;&#24335;&#34920;&#31034;&#19982;self&#30456;&#20851;&#30340;&#20854;&#20182;&#20195;&#29702;&#21644;&#23545;&#35937;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65306;&#22312;&#25342;&#21462;&#19968;&#20010;&#31227;&#21160;&#24037;&#20855;&#21518;&#21040;&#36798;&#19968;&#20010;&#31227;&#21160;&#29289;&#20307;&#12290;&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#25512;&#29702;&#20026;&#25511;&#21046;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10088v1 Announce Type: cross  Abstract: In a recent computational framework called active inference, discrete models can be linked to their continuous counterparts to perform decision-making in changing environments. From another perspective, simple agents can be combined to better capture the causal relationships of the world. How can we use these two features together to achieve efficient goal-directed behavior? We present an architecture composed of several hybrid -- continuous and discrete -- units replicating the agent's configuration, controlled by a high-level discrete model that achieves dynamic planning and synchronized behavior. Additional factorizations within each level allow to represent hierarchically other agents and objects in relation to the self. We evaluate this hierarchical hybrid model on a non-trivial task: reaching a moving object after having picked a moving tool. This study extends past work on control as inference and proposes an alternative directi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#38544;&#31192;&#36335;&#30001;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#20998;&#25955;&#30340;&#38544;&#31192;&#36335;&#30001;&#36890;&#20449;&#65292;&#20165;&#20351;&#29992;&#26412;&#22320;&#21453;&#39304;&#20449;&#24687;&#30830;&#23450;&#27599;&#20010;&#33410;&#28857;&#30340;&#19979;&#19968;&#36339;&#21644;&#36890;&#20449;&#27169;&#24335;&#12290;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#19982;&#26368;&#20248;&#38598;&#20013;&#24335;&#36335;&#30001;&#26041;&#26696;&#30456;&#27604;&#65292;&#24615;&#33021;&#25439;&#32791;&#21487;&#24573;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10087</link><description>&lt;p&gt;
&#24322;&#26500;&#32593;&#32476;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#25955;&#38544;&#31192;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Decentralized Covert Routing in Heterogeneous Networks Using Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#38544;&#31192;&#36335;&#30001;&#31639;&#27861;&#65292;&#22312;&#24322;&#26500;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#20998;&#25955;&#30340;&#38544;&#31192;&#36335;&#30001;&#36890;&#20449;&#65292;&#20165;&#20351;&#29992;&#26412;&#22320;&#21453;&#39304;&#20449;&#24687;&#30830;&#23450;&#27599;&#20010;&#33410;&#28857;&#30340;&#19979;&#19968;&#36339;&#21644;&#36890;&#20449;&#27169;&#24335;&#12290;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#19982;&#26368;&#20248;&#38598;&#20013;&#24335;&#36335;&#30001;&#26041;&#26696;&#30456;&#27604;&#65292;&#24615;&#33021;&#25439;&#32791;&#21487;&#24573;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24322;&#26500;&#32593;&#32476;&#20013;&#36827;&#34892;&#38544;&#31192;&#36335;&#30001;&#36890;&#20449;&#65292;&#20854;&#20013;&#28304;&#33410;&#28857;&#36890;&#36807;&#20013;&#32487;&#33410;&#28857;&#23558;&#26426;&#23494;&#25968;&#25454;&#20256;&#36755;&#21040;&#30446;&#26631;&#33410;&#28857;&#65292;&#27599;&#20010;&#20256;&#36755;&#33410;&#28857;&#37117;&#20250;&#26126;&#26234;&#22320;&#22312;&#22810;&#20010;&#36890;&#20449;&#27169;&#24335;&#20013;&#36873;&#25321;&#19968;&#31181;&#27169;&#24335;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#38544;&#31192;&#36335;&#30001;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#26681;&#25454;&#20174;&#20854;&#30456;&#37051;&#33410;&#28857;&#25509;&#25910;&#21040;&#30340;&#26412;&#22320;&#21453;&#39304;&#20449;&#24687;&#65292;&#20351;&#27599;&#20010;&#33410;&#28857;&#30830;&#23450;&#20854;&#19979;&#19968;&#20010;&#36339;&#21644;&#27169;&#24335;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#38544;&#31192;&#36335;&#30001;&#31574;&#30053;&#19982;&#26368;&#20248;&#38598;&#20013;&#24335;&#36335;&#30001;&#26041;&#26696;&#30456;&#27604;&#20165;&#26377;&#21487;&#24573;&#30053;&#30340;&#24615;&#33021;&#25439;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10087v1 Announce Type: cross  Abstract: This letter investigates covert routing communications in a heterogeneous network where a source transmits confidential data to a destination with the aid of relaying nodes where each transmitter judiciously chooses one modality among multiple communication modalities. We develop a novel reinforcement learning-based covert routing algorithm that finds a route from the source to the destination where each node identifies its next hop and modality only based on the local feedback information received from its neighboring nodes. We show based on numerical simulations that the proposed covert routing strategy has only negligible performance loss compared to the optimal centralized routing scheme.
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10086</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10086
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38590;&#20197;&#29702;&#35299;&#30340;AI&#31995;&#32479;&#21152;&#21095;&#20102;&#23545;AD&#23433;&#20840;&#20445;&#35777;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#26041;&#27861;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20840;&#38754;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#22312;AD&#32972;&#26223;&#19979;AI&#30340;&#35201;&#27714;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#26426;&#26500;&#36825;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;XAI&#23545;&#20110;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#20013;&#35299;&#37322;&#30340;&#26469;&#28304;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;XAI&#30340;&#20998;&#31867;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;XAI&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20116;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#65292;&#36741;&#21161;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#24320;&#21457;&#27969;&#31243;&#65292;&#21487;&#20197;&#35299;&#20915;&#32593;&#32476;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;"Lachesis"&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10085</link><description>&lt;p&gt;
&#24320;&#21457;&#31471;&#21040;&#31471;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Develop End-to-End Anomaly Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#24320;&#21457;&#27969;&#31243;&#65292;&#21487;&#20197;&#35299;&#20915;&#32593;&#32476;&#20013;&#24322;&#24120;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;"Lachesis"&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#30830;&#20445;&#32593;&#32476;&#40065;&#26834;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#20102;&#24694;&#24847;&#21644;&#38750;&#24694;&#24847;&#20107;&#20214;&#37117;&#21487;&#33021;&#24341;&#36215;&#24322;&#24120;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#26045;&#26234;&#33021;&#25253;&#35686;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38590;&#20197;&#30830;&#23450;&#24322;&#24120;&#27169;&#24335;&#12290;&#35745;&#31639;&#26426;&#32593;&#32476;&#39046;&#22495;&#20013;&#32570;&#20047;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#26102;&#38459;&#30861;&#20102;&#24320;&#21457;&#20855;&#26377;&#40065;&#26834;&#24615;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#24320;&#21457;&#27969;&#31243;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#25509;&#21463;&#29992;&#25143;&#21453;&#39304;&#65292;&#24182;&#23454;&#29616;&#25345;&#32493;&#30340;&#29992;&#25143;&#20013;&#24515;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#21644;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24182;&#22522;&#20934;&#27979;&#35797;&#19968;&#20010;&#21517;&#20026;"Lachesis"&#30340;&#26032;&#39044;&#27979;&#27169;&#22411;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#32593;&#32476;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10085v1 Announce Type: cross  Abstract: Anomaly detection plays a crucial role in ensuring network robustness. However, implementing intelligent alerting systems becomes a challenge when considering scenarios in which anomalies can be caused by both malicious and non-malicious events, leading to the difficulty of determining anomaly patterns. The lack of labeled data in the computer networking domain further exacerbates this issue, impeding the development of robust models capable of handling real-world scenarios. To address this challenge, in this paper, we propose an end-to-end anomaly detection model development pipeline. This framework makes it possible to consume user feedback and enable continuous user-centric model performance evaluation and optimization. We demonstrate the efficacy of the framework by way of introducing and bench-marking a new forecasting model -- named \emph{Lachesis} -- on a real-world networking problem. Experiments have demonstrated the robustnes
&lt;/p&gt;</description></item><item><title>FedRDF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.10082</link><description>&lt;p&gt;
FedRDF: &#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#27602;&#21270;&#25915;&#20987;&#30340;&#24378;&#22823;&#21644;&#21160;&#24577;&#32858;&#21512;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10082
&lt;/p&gt;
&lt;p&gt;
FedRDF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#37096;&#32626;&#25152;&#24102;&#26469;&#30340;&#20856;&#22411;&#38544;&#31169;&#38382;&#39064;&#12290;&#23613;&#31649;FL&#20855;&#26377;&#20247;&#25152;&#21608;&#30693;&#30340;&#20248;&#28857;&#65292;&#20294;&#23427;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#25915;&#20987;&#65292;&#22914;&#25308;&#21344;&#24237;&#34892;&#20026;&#21644;&#27602;&#21270;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#20250;&#20005;&#37325;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#25910;&#25947;&#12290;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#32531;&#35299;&#22797;&#26434;&#25915;&#20987;&#65288;&#22914;&#20013;&#20540;&#12289;&#20462;&#21098;&#22343;&#20540;&#25110;Krum&#32858;&#21512;&#20989;&#25968;&#65289;&#30340;&#25928;&#26524;&#20165;&#37096;&#20998;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#25915;&#20987;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FT&#65289;&#26469;&#26377;&#25928;&#22788;&#29702;&#22797;&#26434;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#25915;&#20987;&#32773;&#25968;&#37327;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#21033;&#29992;&#36825;&#31181;&#25968;&#25454;&#25216;&#26415;&#65292;FL&#23458;&#25143;&#31471;&#29983;&#25104;&#30340;&#26435;&#37325;&#34987;&#25237;&#24433;&#21040;&#39057;&#22495;&#20197;&#30830;&#23450;&#20854;&#23494;&#24230;&#20989;&#25968;&#65292;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#39057;&#29575;&#30340;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10082v1 Announce Type: new  Abstract: Federated Learning (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments. Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence. The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks. Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handling sophisticated attacks without prior knowledge of the number of attackers. Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency. Conseq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;</title><link>https://arxiv.org/abs/2402.10079</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of the Learning-based Camera and Lidar Simulation Methods for Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#20256;&#24863;&#22120;&#65292;&#23588;&#20854;&#26159;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#65292;&#26159;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;(Autonomous Driving Systems&#65292;ADS)&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#20197;&#20570;&#20986;&#26126;&#26234;&#30340;&#39550;&#39542;&#21644;&#25511;&#21046;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#36924;&#30495;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#22411;&#65292;&#23545;&#20110;&#26377;&#25928;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;ADS&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20419;&#36827;&#20102;&#24863;&#30693;&#20256;&#24863;&#22120;&#27169;&#22411;&#20316;&#20026;&#21512;&#25104;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#26222;&#21450;&#12290;&#20256;&#32479;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#31995;&#32479;&#22914;ADS&#20013;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#28508;&#21147;&#22312;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21463;&#21040;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#25512;&#21160;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10079v1 Announce Type: cross  Abstract: Perception sensors, particularly camera and Lidar, are key elements of Autonomous Driving Systems (ADS) that enable them to comprehend their surroundings for informed driving and control decisions. Therefore, developing realistic camera and Lidar simulation methods, also known as camera and Lidar models, is of paramount importance to effectively conduct simulation-based testing for ADS. Moreover, the rise of deep learning-based perception models has propelled the prevalence of perception sensor models as valuable tools for synthesising diverse training datasets. The traditional sensor simulation methods rely on computationally expensive physics-based algorithms, specifically in complex systems such as ADS. Hence, the current potential resides in learning-based models, driven by the success of deep generative models in synthesising high-dimensional data. This paper reviews the current state-of-the-art in learning-based sensor simulation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;AER-SNN&#23545;&#35937;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#38598;&#25104;&#20102;&#24322;&#27493;&#22788;&#29702;&#12289;&#31070;&#32463;&#24418;&#24577;&#20860;&#23481;&#24615;&#21644;&#31232;&#30095;&#23574;&#23792;&#30340;&#29305;&#24615;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.10078</link><description>&lt;p&gt;
EventF2S&#65306;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#21451;&#22909;&#31639;&#27861;&#30340;&#24322;&#27493;&#21644;&#31232;&#30095;&#23574;&#23792;AER&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EventF2S: Asynchronous and Sparse Spiking AER Framework using Neuromorphic-Friendly Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10078
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;AER-SNN&#23545;&#35937;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#38598;&#25104;&#20102;&#24322;&#27493;&#22788;&#29702;&#12289;&#31070;&#32463;&#24418;&#24577;&#20860;&#23481;&#24615;&#21644;&#31232;&#30095;&#23574;&#23792;&#30340;&#29305;&#24615;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21551;&#21457;&#24335;&#30340;&#22320;&#22336;&#20107;&#20214;&#34920;&#31034;&#65288;AER&#65289;&#20256;&#24863;&#22120;&#22240;&#20854;&#20302;&#21151;&#32791;&#12289;&#39640;&#31232;&#30095;&#24615;&#21644;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;AER&#25968;&#25454;&#22788;&#29702;&#30340;&#22266;&#26377;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;AER-SNN&#33539;&#24335;&#30340;&#38598;&#25104;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#24322;&#27493;&#22788;&#29702;&#12289;&#31070;&#32463;&#24418;&#24577;&#20860;&#23481;&#24615;&#21644;&#31232;&#30095;&#23574;&#23792;&#65292;&#36825;&#26159;&#36164;&#28304;&#21463;&#38480;&#24212;&#29992;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;AER-SNN&#23545;&#35937;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#19982;&#31532;&#19968;&#23574;&#23792;&#35782;&#21035;&#32593;&#32476;&#38598;&#25104;&#30340;&#25968;&#25454;&#32534;&#30721;&#22120;&#12290;&#21463;&#21040;&#35270;&#35273;&#30382;&#23618;&#20013;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24322;&#27493;&#30340;&#24182;&#19988;&#19982;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#20860;&#23481;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#22122;&#21644;&#31532;&#19968;&#23574;&#23792;&#32534;&#30721;&#30340;&#21407;&#29702;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#23574;&#23792;&#20449;&#21495;&#20256;&#36882;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10078v1 Announce Type: cross  Abstract: Bio-inspired Address Event Representation (AER) sensors have attracted significant popularity owing to their low power consumption, high sparsity, and high temporal resolution. Spiking Neural Network (SNN) has become the inherent choice for AER data processing. However, the integration of the AER-SNN paradigm has not adequately explored asynchronous processing, neuromorphic compatibility, and sparse spiking, which are the key requirements of resource-constrained applications. To address this gap, we introduce a brain-inspired AER-SNN object recognition solution, which includes a data encoder integrated with a First-To-Spike recognition network. Being fascinated by the functionality of neurons in the visual cortex, we designed the solution to be asynchronous and compatible with neuromorphic hardware. Furthermore, we have adapted the principle of denoising and First-To-Spike coding to achieve optimal spike signaling, significantly reduci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#21644;&#26631;&#35760;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;MOT17&#21644;NCLT&#65292;&#22635;&#34917;&#20102;&#20849;&#20139;&#22478;&#24066;&#21306;&#22495;&#20013;&#20154;&#26426;&#20132;&#20114;&#30340;&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.10077</link><description>&lt;p&gt;
&#22312;&#20849;&#20139;&#22478;&#24066;&#21306;&#22495;&#20013;&#19982;&#26426;&#22120;&#20154;&#20114;&#21160;&#30340;&#20154;&#20307;&#23039;&#21183;&#22823;&#35268;&#27169;&#34701;&#21512;&#21644;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards a large-scale fused and labeled dataset of human pose while interacting with robots in shared urban areas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#21644;&#26631;&#35760;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;MOT17&#21644;NCLT&#65292;&#22635;&#34917;&#20102;&#20849;&#20139;&#22478;&#24066;&#21306;&#22495;&#20013;&#20154;&#26426;&#20132;&#20114;&#30340;&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#38598;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#33258;&#20027;&#37197;&#36865;&#26426;&#22120;&#20154;&#65288;ADR&#65289;&#22312;&#22238;&#24212;&#19981;&#26029;&#22686;&#38271;&#30340;&#30005;&#23376;&#21830;&#21153;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#20063;&#25913;&#21464;&#20102;&#20256;&#32479;&#30340;&#37197;&#36865;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;ADR&#22312;&#20849;&#20139;&#22478;&#24066;&#21306;&#22495;&#20013;&#23433;&#20840;&#19982;&#34892;&#20154;&#20849;&#21516;&#20132;&#24448;&#30340;&#20934;&#22791;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;ADR&#19982;&#34892;&#20154;&#30340;&#30456;&#20114;&#20316;&#29992;&#23384;&#22312;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26159;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#65288;&#21253;&#25324;&#23039;&#21183;&#39044;&#27979;&#21644;&#20855;&#26377;&#31038;&#20250;&#24847;&#35782;&#30340;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#65289;&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#25429;&#33719;&#20849;&#20139;&#22478;&#24066;&#21306;&#22495;&#20013;&#20154;&#26426;&#20132;&#20114;&#30340;&#20016;&#23500;&#19988;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#38480;&#21046;&#20102;&#36825;&#20010;&#30446;&#26631;&#30340;&#23454;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;MOT17&#21644;NCLT&#65292;&#20998;&#21035;&#32858;&#28966;&#20110;&#34892;&#20154;&#36319;&#36394;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#22320;&#22270;&#26500;&#24314;&#65288;SLAM&#65289;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#32467;&#26524;&#24471;&#21040;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#21253;&#21547;&#25968;&#21315;&#20010;&#30495;&#23454;&#23460;&#20869;&#21644;&#25143;&#22806;&#22330;&#26223;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10077v1 Announce Type: cross  Abstract: Over the last decade, Autonomous Delivery Robots (ADRs) have transformed conventional delivery methods, responding to the growing e-commerce demand. However, the readiness of ADRs to navigate safely among pedestrians in shared urban areas remains an open question. We contend that there are crucial research gaps in understanding their interactions with pedestrians in such environments. Human Pose Estimation is a vital stepping stone for various downstream applications, including pose prediction and socially aware robot path-planning. Yet, the absence of an enriched and pose-labeled dataset capturing human-robot interactions in shared urban areas hinders this objective. In this paper, we bridge this gap by repurposing, fusing, and labeling two datasets, MOT17 and NCLT, focused on pedestrian tracking and Simultaneous Localization and Mapping (SLAM), respectively. The resulting unique dataset represents thousands of real-world indoor and o
&lt;/p&gt;</description></item><item><title>QUICK&#26159;&#19968;&#32452;&#38024;&#23545;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#25928;&#25512;&#29702;&#30340;&#20248;&#21270;CUDA&#20869;&#26680;&#12290;&#36890;&#36807;&#35299;&#20915;&#20849;&#20139;&#20869;&#23384;&#20914;&#31361;&#38382;&#39064;&#21644;&#20132;&#38169;&#37327;&#21270;&#26435;&#37325;&#30697;&#38453;&#65292;QUICK&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.10076</link><description>&lt;p&gt;
QUICK&#65306;&#38024;&#23545;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#37327;&#21270;&#24863;&#30693;&#20132;&#38169;&#21644;&#26080;&#20914;&#31361;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10076
&lt;/p&gt;
&lt;p&gt;
QUICK&#26159;&#19968;&#32452;&#38024;&#23545;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#25928;&#25512;&#29702;&#30340;&#20248;&#21270;CUDA&#20869;&#26680;&#12290;&#36890;&#36807;&#35299;&#20915;&#20849;&#20139;&#20869;&#23384;&#20914;&#31361;&#38382;&#39064;&#21644;&#20132;&#38169;&#37327;&#21270;&#26435;&#37325;&#30697;&#38453;&#65292;QUICK&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#36895;&#24230;&#25552;&#21319;&#21644;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;QUICK&#65292;&#19968;&#32452;&#29992;&#20110;&#39640;&#25928;&#25512;&#29702;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20248;&#21270;CUDA&#20869;&#26680;&#12290;QUICK&#35299;&#20915;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#20056;&#27861;&#20869;&#26680;&#30340;&#20849;&#20139;&#20869;&#23384;&#20914;&#31361;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#20132;&#38169;LLMs&#30340;&#37327;&#21270;&#26435;&#37325;&#30697;&#38453;&#65292;&#20174;&#32780;&#36339;&#36807;&#35299;&#37327;&#21270;&#21518;&#30340;&#20849;&#20139;&#20869;&#23384;&#20889;&#22238;&#12290;&#25105;&#20204;&#22312;&#36739;&#22823;&#25209;&#27425;&#19978;&#23637;&#31034;&#20102;&#19982;AutoAWQ&#29616;&#26377;&#20869;&#26680;&#30456;&#27604;&#22810;&#36798;1.91&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#24182;&#22312;&#21508;&#31181;NVIDIA GPU&#35774;&#22791;&#19978;&#30340;&#20195;&#34920;&#24615;LLM&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;1.94&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10076v1 Announce Type: cross  Abstract: We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of quantized Large Language Models (LLMs). QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels. Our method interleaves the quantized weight matrices of LLMs offline to skip the shared memory write-back after the dequantization. We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;GraphCBAL&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#36873;&#25321;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;GNNs&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10074</link><description>&lt;p&gt;
GraphCBAL: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;GraphCBAL&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#36873;&#25321;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;GNNs&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;GNNs&#30340;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#26597;&#35810;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;GNNs&#20013;&#30340;&#24378;&#21270;&#20027;&#21160;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#24230;&#20542;&#26012;&#30340;&#31867;&#21035;&#22330;&#26223;&#19979;&#12290;&#36825;&#36827;&#19968;&#27493;&#23545;&#20998;&#31867;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;GraphCBAL&#65292;&#29992;&#20110;GNNs&#12290;&#23427;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#20197;&#33719;&#21462;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#26631;&#35760;&#33410;&#28857;&#35757;&#32451;&#30340;GNNs&#30340;&#24615;&#33021;&#12290;GraphCBAL&#35774;&#35745;&#20102;&#31867;&#24179;&#34913;&#24863;&#30693;&#29366;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#31867;&#24179;&#34913;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;GraphCBAL&#65292;&#24471;&#21040;GraphCBAL++&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10074v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have recently demonstrated significant success. Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost. However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. This further adversely affects the classification performance. To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL. It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes. GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. We further upgrade GraphCBAL to GraphCBAL++ by intr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;(DJSCC)&#26041;&#26696;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#21487;&#38752;&#30340;&#36328;&#25216;&#26415;&#36890;&#20449;(CTC)&#12290;&#35813;&#26041;&#26696;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#23454;&#29616;&#20449;&#24687;&#21387;&#32553;&#21644;&#35821;&#20041;&#21547;&#20041;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10072</link><description>&lt;p&gt;
&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#20197;&#23454;&#29616;&#39640;&#25928;&#21487;&#38752;&#30340;&#24322;&#26500;&#25216;&#26415;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Deep Joint Source-Channel Coding for Efficient and Reliable Cross-Technology Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;(DJSCC)&#26041;&#26696;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#21487;&#38752;&#30340;&#36328;&#25216;&#26415;&#36890;&#20449;(CTC)&#12290;&#35813;&#26041;&#26696;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#23454;&#29616;&#20449;&#24687;&#21387;&#32553;&#21644;&#35821;&#20041;&#21547;&#20041;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#25216;&#26415;&#36890;&#20449;(CTC)&#26159;&#19968;&#31181;&#26377;&#25928;&#23454;&#29616;&#19981;&#20860;&#23481;&#26080;&#32447;&#25216;&#26415;&#20043;&#38388;&#30452;&#25509;&#36890;&#20449;&#32780;&#19981;&#38656;&#35201;&#30828;&#20214;&#20462;&#25913;&#30340;&#26377;&#26395;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20302;&#25928;&#24615;&#21644;&#19981;&#21487;&#38752;&#24615;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;(DJSCC)&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#21487;&#38752;&#30340;CTC&#12290;&#35813;&#26041;&#26696;&#22312;&#21457;&#36865;&#31471;&#21644;&#25509;&#25910;&#31471;&#20998;&#21035;&#24314;&#31435;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#23454;&#29616;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;1)&#23558;&#20449;&#24687;&#21387;&#32553;&#21040;&#20165;&#20445;&#30041;&#20854;&#22522;&#26412;&#35821;&#20041;&#21547;&#20041;&#30340;&#31243;&#24230;; 2)&#30830;&#20445;&#22312;&#36328;&#19981;&#20860;&#23481;&#25216;&#26415;&#20256;&#36755;&#35821;&#20041;&#21547;&#20041;&#26102;&#30340;&#31283;&#20581;&#24615;&#12290;&#35813;&#26041;&#26696;&#23558;&#29616;&#26377;&#30340;CTC&#32534;&#30721;&#31639;&#27861;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#65292;&#24341;&#23548;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#23398;&#20064;CTC&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10072v1 Announce Type: cross  Abstract: Cross-technology communication (CTC) is a promising technique that enables direct communications among incompatible wireless technologies without needing hardware modification. However, it has not been widely adopted in real-world applications due to its inefficiency and unreliability. To address this issue, this paper proposes a deep joint source-channel coding (DJSCC) scheme to enable efficient and reliable CTC. The proposed scheme builds a neural-network-based encoder and decoder at the sender side and the receiver side, respectively, to achieve two critical tasks simultaneously: 1) compressing the messages to the point where only their essential semantic meanings are preserved; 2) ensuring the robustness of the semantic meanings when they are transmitted across incompatible technologies. The scheme incorporates existing CTC coding algorithms as domain knowledge to guide the encoder-decoder pair to learn the characteristics of CTC l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23454;&#29616;&#30340;&#29983;&#29289;&#23398;&#21487;&#34892;&#26041;&#27861;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#20943;&#36731;&#20102;&#24378;&#21270;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#22122;&#22768;&#24341;&#20837;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.10069</link><description>&lt;p&gt;
&#23398;&#20064;&#24555;&#36895;&#21464;&#21270;&#30340;&#24930;&#24615;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Learning fast changing slow in spiking neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10069
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23454;&#29616;&#30340;&#29983;&#29289;&#23398;&#21487;&#34892;&#26041;&#27861;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#20943;&#36731;&#20102;&#24378;&#21270;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#22122;&#22768;&#24341;&#20837;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#38754;&#20020;&#30528;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#19982;&#29615;&#22659;&#30340;&#26377;&#38480;&#20132;&#20114;&#23548;&#33268;&#30340;&#21487;&#29992;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290; RL&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;&#23398;&#20064;&#65292;&#36825;&#20351;&#24471;&#22797;&#26434;&#24615;&#36827;&#19968;&#27493;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#24490;&#29615;&#33033;&#20914;&#32593;&#32476;&#23454;&#29616;RL&#26102;&#65292;&#33033;&#20914;&#24341;&#20837;&#30340;&#22266;&#26377;&#22122;&#22768;&#22686;&#21152;&#20102;&#38590;&#24230;&#12290;&#32456;&#36523;&#23398;&#20064;&#26426;&#22120;&#22312;&#26412;&#36136;&#19978;&#24517;&#39035;&#35299;&#20915;&#21487;&#22609;&#24615;-&#31283;&#23450;&#24615;&#24726;&#35770;&#12290;&#22312;&#33719;&#24471;&#26032;&#30693;&#35782;&#21644;&#20445;&#25345;&#31283;&#23450;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#29289;&#21487;&#34892;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23454;&#29616;&#65292;&#35748;&#20026;&#23427;&#26174;&#33879;&#20943;&#36731;&#20102;&#27492;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36827;&#23637;&#65306;&#39318;&#20808;&#65292;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10069v1 Announce Type: cross  Abstract: Reinforcement learning (RL) faces substantial challenges when applied to real-life problems, primarily stemming from the scarcity of available data due to limited interactions with the environment. This limitation is exacerbated by the fact that RL often demands a considerable volume of data for effective learning. The complexity escalates further when implementing RL in recurrent spiking networks, where inherent noise introduced by spikes adds a layer of difficulty. Life-long learning machines must inherently resolve the plasticity-stability paradox. Striking a balance between acquiring new knowledge and maintaining stability is crucial for artificial agents. In this context, we take inspiration from machine learning technology and introduce a biologically plausible implementation of proximal policy optimization, arguing that it significantly alleviates this challenge. Our approach yields two notable advancements: first, the ability t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#24212;&#29992;&#24847;&#22270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29983;&#25104;&#36880;&#27493;&#20998;&#35299;&#24847;&#22270;&#25152;&#38656;&#30340;&#21160;&#20316;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;API&#65292;&#23454;&#29616;&#20102;&#38381;&#25511;&#21046;&#24490;&#29615;&#26469;&#33258;&#21160;&#21270;&#31574;&#30053;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.10067</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#24847;&#22270;&#31649;&#29702;&#30340;&#31574;&#30053;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-based policy generation for intent-based management of applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10067
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#24212;&#29992;&#24847;&#22270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29983;&#25104;&#36880;&#27493;&#20998;&#35299;&#24847;&#22270;&#25152;&#38656;&#30340;&#21160;&#20316;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;API&#65292;&#23454;&#29616;&#20102;&#38381;&#25511;&#21046;&#24490;&#29615;&#26469;&#33258;&#21160;&#21270;&#31574;&#30053;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#31649;&#29702;&#38656;&#35201;&#23558;&#39640;&#32423;&#29992;&#25143;&#35831;&#27714;&#65292;&#20363;&#22914;&#24847;&#22270;&#65292;&#20998;&#35299;&#25104;&#31995;&#32479;&#21487;&#20197;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#25277;&#35937;&#12290;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#24847;&#22270;&#20063;&#38656;&#35201;&#25191;&#34892;&#19968;&#31995;&#21015;&#26377;&#24207;&#30340;&#27493;&#39588;&#12290;&#32780;&#35782;&#21035;&#21644;&#36866;&#24212;&#36825;&#20123;&#27493;&#39588;&#65288;&#38543;&#30528;&#26465;&#20214;&#30340;&#21464;&#21270;&#65289;&#30340;&#20219;&#21153;&#38656;&#35201;&#19968;&#31181;&#26080;&#27861;&#20107;&#20808;&#23436;&#20840;&#23450;&#20041;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#30340;&#24847;&#22270;&#20998;&#35299;&#21644;&#25191;&#34892;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31649;&#36947;&#65292;&#36890;&#36807;&#29983;&#25104;&#25152;&#38656;&#30340;&#21160;&#20316;&#65292;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#30340;&#25277;&#35937;&#36880;&#27493;&#20998;&#35299;&#24847;&#22270;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21019;&#24314;&#29992;&#20110;&#24847;&#22270;&#37096;&#32626;&#30340;&#38381;&#25511;&#21046;&#24490;&#29615;&#26469;&#33258;&#21160;&#21270;&#31574;&#30053;&#25191;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#29983;&#25104;&#24182;&#23558;&#31574;&#30053;&#26144;&#23556;&#21040;API&#65292;&#24182;&#24418;&#25104;&#25191;&#34892;&#25152;&#38656;&#30340;&#30417;&#25511;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#30340;&#24212;&#29992;&#31649;&#29702;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10067v1 Announce Type: cross  Abstract: Automated management requires decomposing high-level user requests, such as intents, to an abstraction that the system can understand and execute. This is challenging because even a simple intent requires performing a number of ordered steps. And the task of identifying and adapting these steps (as conditions change) requires a decomposition approach that cannot be exactly pre-defined beforehand. To tackle these challenges and support automated intent decomposition and execution, we explore the few-shot capability of Large Language Models (LLMs). We propose a pipeline that progressively decomposes intents by generating the required actions using a policy-based abstraction. This allows us to automate the policy execution by creating a closed control loop for the intent deployment. To do so, we generate and map the policies to APIs and form application management loops that perform the necessary monitoring, analysis, planning and executi
&lt;/p&gt;</description></item><item><title>NYCTALE&#26159;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#32954;&#32467;&#33410;&#20405;&#34989;&#24615;&#39044;&#27979;&#12290;&#19982;&#20256;&#32479;&#30340;CT&#22522;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;NYCTALE&#20165;&#22312;&#32047;&#31215;&#36275;&#22815;&#25968;&#37327;&#30340;&#35777;&#25454;&#26102;&#25165;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.10066</link><description>&lt;p&gt;
NYCTALE: &#29992;&#20110;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#32954;&#32467;&#33410;&#20405;&#34989;&#24615;&#39044;&#27979;&#30340;&#31070;&#32463;&#35777;&#25454;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10066
&lt;/p&gt;
&lt;p&gt;
NYCTALE&#26159;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#32954;&#32467;&#33410;&#20405;&#34989;&#24615;&#39044;&#27979;&#12290;&#19982;&#20256;&#32479;&#30340;CT&#22522;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;NYCTALE&#20165;&#22312;&#32047;&#31215;&#36275;&#22815;&#25968;&#37327;&#30340;&#35777;&#25454;&#26102;&#25165;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28789;&#38271;&#31867;&#21160;&#29289;&#22823;&#33041;&#24341;&#21457;&#30340;&#35777;&#25454;&#32047;&#31215;&#36807;&#31243;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#20511;&#37492;&#20102;&#35748;&#30693;&#24515;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;NYCTALE&#26694;&#26550;&#65292;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22522;&#20110;&#35777;&#25454;&#32047;&#31215;&#30340;Transformer&#26550;&#26500;&#12290;&#25552;&#20986;&#30340;&#31070;&#32463;&#21551;&#21457;&#30340;NYCTALE&#22312;&#20010;&#24615;&#21270;&#21307;&#23398;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#32954;&#30284;&#35786;&#26029;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36884;&#24452;&#12290;&#20316;&#20026;&#33258;&#28982;&#30028;&#20013;&#30340;&#19968;&#31181;&#23567;&#22411;&#29483;&#22836;&#40560;&#65292;Nyctales&#20197;&#20854;&#22812;&#38388;&#34892;&#20026;&#32780;&#38395;&#21517;&#65292;&#20027;&#35201;&#22312;&#22812;&#26202;&#36827;&#34892;&#25429;&#29454;&#12290;NYCTALE&#20197;&#31867;&#20284;&#35686;&#24789;&#30340;&#26041;&#24335;&#36816;&#20316;&#65292;&#21363;&#20197;&#22522;&#20110;&#35777;&#25454;&#30340;&#26041;&#24335;&#22788;&#29702;&#25968;&#25454;&#24182;&#21160;&#24577;/&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#19981;&#21516;&#65292;NYCTALE&#20165;&#22312;&#32047;&#31215;&#36275;&#22815;&#25968;&#37327;&#30340;&#35777;&#25454;&#26102;&#25165;&#36827;&#34892;&#39044;&#27979;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23545;&#20110;&#27599;&#20010;&#20154;&#65292;&#21482;&#22788;&#29702;&#25152;&#26377;&#25110;&#39044;&#23450;&#20041;&#30340;CT&#20999;&#29255;&#30340;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20840;&#37096;&#20999;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10066v1 Announce Type: cross  Abstract: Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture. The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis. In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively. Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated. In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#37327;&#21270;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#38544;&#31169;&#38450;&#24481;&#25514;&#26045;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10065</link><description>&lt;p&gt;
&#27599;&#20010;&#25968;&#25454;&#28857;&#27844;&#38706;&#24744;&#38544;&#31169;&#30340;&#31243;&#24230;&#26377;&#22810;&#22823;&#65311;&#37327;&#21270;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
How Much Does Each Datapoint Leak Your Privacy? Quantifying the Per-datum Membership Leakage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#37327;&#21270;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#38544;&#31169;&#38450;&#24481;&#25514;&#26045;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#25512;&#26029;&#20986;&#19968;&#20010;&#22266;&#23450;&#30446;&#26631;&#25968;&#25454;&#26159;&#21542;&#24050;&#21253;&#21547;&#22312;&#31639;&#27861;&#30340;&#36755;&#20837;&#25968;&#25454;&#38598;&#20013;&#65292;&#20174;&#32780;&#20405;&#29359;&#38544;&#31169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#20026;&#26368;&#20248;&#23545;&#25163;&#36776;&#35782;&#23427;&#30340;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#32463;&#39564;&#22343;&#20540;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#65292;&#24182;&#34920;&#26126;&#23427;&#21462;&#20915;&#20110;&#30446;&#26631;&#25968;&#25454;&#28857;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#30340;&#39532;&#27663;&#36317;&#31163;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20004;&#31181;&#38544;&#31169;&#38450;&#24481;&#25514;&#26045;&#30340;&#25928;&#26524;&#65292;&#21363;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#21644;&#23376;&#37319;&#26679;&#12290;&#25105;&#20204;&#20934;&#30830;&#22320;&#37327;&#21270;&#20102;&#23427;&#20204;&#37117;&#22914;&#20309;&#38477;&#20302;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#19968;&#20010;&#32467;&#21512;&#20102;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;Edgeworth&#23637;&#24320;&#21644;Lindeberg-Feller&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#30340;&#26032;&#22411;&#35777;&#26126;&#25216;&#26415;&#19978;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36830;&#25509;&#20102;&#29616;&#26377;&#30340;&#20284;&#28982;&#27604;&#21644;&#26631;&#37327;&#20056;&#31215;&#25915;&#20987;&#65292;&#24182;&#23545;&#36825;&#20123;&#25915;&#20987;&#36827;&#34892;&#20102;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10065v1 Announce Type: new  Abstract: We study the per-datum Membership Inference Attacks (MIAs), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy. First, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it. Then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the Mahalanobis distance between the target datum and the data-generating distribution. We further assess the effect of two privacy defences, i.e. adding Gaussian noise and sub-sampling. We quantify exactly how both of them decrease the per-datum membership leakage. Our analysis builds on a novel proof technique that combines an Edgeworth expansion of the likelihood ratio test and a Lindeberg-Feller central limit theorem. Our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#27169;&#25311;&#30340;&#24490;&#29615;&#21644;&#26465;&#20214;&#35745;&#31639;&#22270;&#30340;&#24037;&#20316;&#27969;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#36890;&#20449;&#23454;&#29616;&#20219;&#24847;&#22270;&#32467;&#26500;&#30340;&#25191;&#34892;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10064</link><description>&lt;p&gt;
&#23548;&#33322;&#29577;&#31859;&#65306;&#20998;&#23376;&#27169;&#25311;&#30340;&#24490;&#29615;&#21644;&#26465;&#20214;&#35745;&#31639;&#22270;
&lt;/p&gt;
&lt;p&gt;
Navigating the Maize: Cyclic and conditional computational graphs for molecular simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#27169;&#25311;&#30340;&#24490;&#29615;&#21644;&#26465;&#20214;&#35745;&#31639;&#22270;&#30340;&#24037;&#20316;&#27969;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#36890;&#20449;&#23454;&#29616;&#20219;&#24847;&#22270;&#32467;&#26500;&#30340;&#25191;&#34892;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#21270;&#23398;&#21644;&#20998;&#23376;&#27169;&#25311;&#24037;&#20316;&#27969;&#31243;&#21487;&#20197;&#34920;&#31034;&#20026;&#35745;&#31639;&#22270;&#12290;&#36825;&#31181;&#25277;&#35937;&#26377;&#21161;&#20110;&#27169;&#22359;&#21270;&#21644;&#28508;&#22312;&#22320;&#37325;&#29992;&#29616;&#26377;&#32452;&#20214;&#65292;&#24182;&#25552;&#20379;&#24182;&#34892;&#21270;&#21644;&#26131;&#20110;&#22797;&#21046;&#12290;&#29616;&#26377;&#24037;&#20855;&#23558;&#35745;&#31639;&#34920;&#31034;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#20174;&#32780;&#36890;&#36807;&#24182;&#34892;&#21270;&#24182;&#21457;&#20998;&#25903;&#26469;&#23454;&#29616;&#39640;&#25928;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#34920;&#31034;&#24490;&#29615;&#21644;&#26465;&#20214;&#24037;&#20316;&#27969;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Maize&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#31243;&#32534;&#31243;&#21407;&#29702;&#30340;&#12289;&#29992;&#20110;&#24490;&#29615;&#21644;&#26465;&#20214;&#22270;&#30340;&#24037;&#20316;&#27969;&#31649;&#29702;&#22120;&#12290;&#36890;&#36807;&#22312;&#21333;&#29420;&#30340;&#36827;&#31243;&#20013;&#21516;&#26102;&#36816;&#34892;&#22270;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20219;&#20309;&#26102;&#38388;&#36890;&#36807;&#19987;&#29992;&#30340;&#33410;&#28857;&#38388;&#36890;&#36947;&#36827;&#34892;&#36890;&#20449;&#65292;&#21487;&#20197;&#25191;&#34892;&#20219;&#24847;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#35745;&#31639;&#33647;&#29289;&#35774;&#35745;&#20013;&#36827;&#34892;&#21160;&#24577;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#26469;&#23637;&#31034;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#28041;&#21450;&#20351;&#29992;&#23567;&#20998;&#23376; gen
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10064v1 Announce Type: cross  Abstract: Many computational chemistry and molecular simulation workflows can be expressed as graphs. This abstraction is useful to modularize and potentially reuse existing components, as well as provide parallelization and ease reproducibility. Existing tools represent the computation as a directed acyclic graph (DAG), thus allowing efficient execution by parallelization of concurrent branches. These systems can, however, generally not express cyclic and conditional workflows. We therefore developed Maize, a workflow manager for cyclic and conditional graphs based on the principles of flow-based programming. By running each node of the graph concurrently in separate processes and allowing communication at any time through dedicated inter-node channels, arbitrary graph structures can be executed. We demonstrate the effectiveness of the tool on a dynamic active learning task in computational drug design, involving the use of a small molecule gen
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#26032;&#26087;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10063</link><description>&lt;p&gt;
&#24179;&#34913;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Balancing the Causal Effects in Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10063
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#26032;&#26087;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26159;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;PTMs&#39034;&#24207;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#20294;&#22823;&#37327;&#24037;&#20316;&#34920;&#26126;&#20102;&#32531;&#35299;PTMs&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24517;&#35201;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#21021;&#27493;&#30740;&#31350;&#21644;CIL&#30340;&#22240;&#26524;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#26032;&#26087;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#22240;&#26524;&#25928;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26032;&#25968;&#25454;&#20419;&#20351;&#27169;&#22411;&#36866;&#24212;&#26032;&#31867;&#21035;&#65292;&#21516;&#26102;&#38459;&#30861;&#20102;&#23545;&#26087;&#31867;&#21035;&#30340;&#36866;&#24212;&#12290;&#21516;&#26679;&#65292;&#26087;&#25968;&#25454;&#20419;&#20351;&#27169;&#22411;&#36866;&#24212;&#26087;&#31867;&#21035;&#65292;&#21516;&#26102;&#38459;&#30861;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#36866;&#24212;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#26032;&#26087;&#31867;&#21035;&#20043;&#38388;&#30340;&#36866;&#24212;&#36807;&#31243;&#23384;&#22312;&#20914;&#31361;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24179;&#34913;&#22240;&#26524;&#25928;&#24212;&#8221;&#65288;BCE&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10063v1 Announce Type: new  Abstract: Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence. Recently, Pre-Trained Models (PTMs) have led to breakthroughs in both visual and natural language processing tasks. Despite recent studies showing PTMs' potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we reveal that the crux lies in the imbalanced causal effects between new and old data. Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes. Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes. In other words, the adaptation process between new and old classes conflicts from the causal perspective. To alleviate this problem, we propose Balancing the Causal Effects (B
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#26410;&#30693;&#20998;&#24067;&#30340;&#26368;&#20248;&#21442;&#25968;&#21644;&#31070;&#32463;&#20803;&#21098;&#26525;&#26041;&#27861;&#65288;OPNP&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#21442;&#25968;&#21644;&#31070;&#32463;&#20803;&#30340;&#25935;&#24863;&#24615;&#26469;&#35299;&#20915;OOD&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10062</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#26410;&#30693;&#20998;&#24067;&#30340;&#26368;&#20248;&#21442;&#25968;&#21644;&#31070;&#32463;&#20803;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#26410;&#30693;&#20998;&#24067;&#30340;&#26368;&#20248;&#21442;&#25968;&#21644;&#31070;&#32463;&#20803;&#21098;&#26525;&#26041;&#27861;&#65288;OPNP&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#21442;&#25968;&#21644;&#31070;&#32463;&#20803;&#30340;&#25935;&#24863;&#24615;&#26469;&#35299;&#20915;OOD&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35782;&#21035;&#26410;&#30693;&#20998;&#24067;&#65288;OOD&#65289;&#26679;&#26412;&#30340;&#33021;&#21147;&#26159;&#19981;&#21487;&#25110;&#32570;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#20851;&#27880;&#20110;&#25506;&#32034;&#39640;&#32423;&#35757;&#32451;&#25216;&#24039;&#25110;&#35757;&#32451;&#26080;&#20851;&#30340;&#25216;&#24039;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#23545;&#26410;&#30693;&#26679;&#26412;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#24182;&#38750;&#22987;&#32456;&#21487;&#29992;&#30340;OOD&#26679;&#26412;&#65292;&#32780;&#22823;&#22810;&#25968;&#22522;&#20110;&#35757;&#32451;&#26080;&#20851;&#30340;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPNP&#65288;Optimal Parameter and Neuron Pruning&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#24182;&#21024;&#38500;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#30340;&#21442;&#25968;&#21644;&#31070;&#32463;&#20803;&#12290;&#20027;&#35201;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#26799;&#24230;&#24179;&#22343;&#26469;&#35780;&#20272;&#27169;&#22411;&#21442;&#25968;&#21644;&#31070;&#32463;&#20803;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10062v1 Announce Type: new  Abstract: For a machine learning model deployed in real world scenarios, the ability of detecting out-of-distribution (OOD) samples is indispensable and challenging. Most existing OOD detection methods focused on exploring advanced training skills or training-free tricks to prevent the model from yielding overconfident confidence score for unknown samples. The training-based methods require expensive training cost and rely on OOD samples which are not always available, while most training-free methods can not efficiently utilize the prior information from the training data. In this work, we propose an \textbf{O}ptimal \textbf{P}arameter and \textbf{N}euron \textbf{P}runing (\textbf{OPNP}) approach, which aims to identify and remove those parameters and neurons that lead to over-fitting. The main method is divided into two steps. In the first step, we evaluate the sensitivity of the model parameters and neurons by averaging gradients over all train
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23545;&#25968;&#24179;&#28369;&#65292;&#25506;&#35752;&#20102;ECE&#30340;&#32570;&#38519;&#20197;&#21450;&#23545;&#29616;&#26377;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#12289;&#26131;&#20110;&#20272;&#35745;&#30340;&#35823;&#24046;&#27979;&#24230;LS-ECE&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;LS-ECE&#19982;&#20998;&#31665;ECE&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2402.10046</link><description>&lt;p&gt;
ECE&#26377;&#22810;&#22823;&#30340;&#32570;&#38519;&#65311;&#36890;&#36807;&#23545;&#25968;&#24179;&#28369;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Flawed is ECE? An Analysis via Logit Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23545;&#25968;&#24179;&#28369;&#65292;&#25506;&#35752;&#20102;ECE&#30340;&#32570;&#38519;&#20197;&#21450;&#23545;&#29616;&#26377;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#12289;&#26131;&#20110;&#20272;&#35745;&#30340;&#35823;&#24046;&#27979;&#24230;LS-ECE&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;LS-ECE&#19982;&#20998;&#31665;ECE&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#19982;&#32622;&#20449;&#24230;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20010;&#27169;&#22411;&#23601;&#26159;&#26657;&#20934;&#30340;&#12290;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#34913;&#37327;&#26657;&#20934;&#24615;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#20102;ECE&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#23427;&#22312;&#39044;&#27979;&#32773;&#31354;&#38388;&#20013;&#26159;&#19981;&#36830;&#32493;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#38382;&#39064;&#26377;&#22810;&#26412;&#36136;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#23545;&#29616;&#26377;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23436;&#20840;&#25551;&#36848;&#20102;ECE&#23545;&#27874;&#20848;&#31354;&#38388;&#19978;&#30340;&#19968;&#33324;&#27010;&#29575;&#27979;&#24230;&#30340;&#19981;&#36830;&#32493;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#19981;&#36830;&#32493;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#12289;&#26131;&#20110;&#20272;&#35745;&#30340;&#35823;&#24046;&#27979;&#24230;&#65292;&#31216;&#20026;Logit-Smoothed ECE&#65288;LS-ECE&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;ECE&#21644;LS-ECE&#65292;&#25105;&#20204;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20998;&#31665;ECE&#19982;LS-ECE&#38750;&#24120;&#25509;&#36817;&#65292;&#34920;&#26126;&#29702;&#35770;&#26041;&#38754;&#26159;&#30456;&#31526;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10046v1 Announce Type: new  Abstract: Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2402.10045</link><description>&lt;p&gt;
&#30701;&#35270;&#39057;&#21644;&#24515;&#29702;&#20581;&#24247;&#65306;&#22522;&#20110;&#30693;&#35782;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10045
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35270;&#39057;&#27491;&#35797;&#22270;&#37325;&#26032;&#22609;&#36896;&#25972;&#20010;&#31038;&#20132;&#23186;&#20307;&#26223;&#35266;&#65292;&#28982;&#32780;&#19987;&#23478;&#20204;&#23545;&#20854;&#23545;&#35266;&#20247;&#30340;&#25233;&#37057;&#24433;&#21709;&#24863;&#21040;&#26497;&#24230;&#25285;&#24551;&#65292;&#36825;&#19968;&#28857;&#24050;&#30001;&#21307;&#23398;&#30740;&#31350;&#35777;&#26126;&#12290;&#20026;&#20102;&#38450;&#27490;&#24191;&#27867;&#24433;&#21709;&#65292;&#21508;&#24179;&#21488;&#28212;&#26395;&#39044;&#27979;&#36825;&#20123;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#37319;&#21462;&#24178;&#39044;&#25514;&#26045;&#65292;&#27604;&#22914;&#20462;&#35746;&#25512;&#33616;&#31639;&#27861;&#21644;&#26174;&#31034;&#35266;&#20247;&#24910;&#37325;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#32570;&#20047;&#19982;&#25233;&#37057;&#30151;&#30340;&#20020;&#24202;&#35777;&#23454;&#30340;&#22806;&#37096;&#29615;&#22659;&#22240;&#32032;&#30456;&#20851;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;&#20026;&#20102;&#32771;&#34385;&#36825;&#26679;&#30340;&#21307;&#23398;&#30693;&#35782;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#26041;&#27861;&#35770;&#23398;&#31185;&#8212;&#8212;&#31181;&#23376;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31181;&#23376;NTMs&#23384;&#22312;&#21333;&#19968;&#26469;&#28304;&#20027;&#39064;&#12289;&#26410;&#30693;&#20027;&#39064;&#26469;&#28304;&#12289;&#27169;&#31946;&#30340;&#31181;&#23376;&#30417;&#30563;&#21644;&#27425;&#20248;&#30340;&#25910;&#25947;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;Knowledg...&#65288;&#24453;&#34917;&#20805;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10045v1 Announce Type: cross  Abstract: While short-form videos head to reshape the entire social media landscape, experts are exceedingly worried about their depressive impacts on viewers, as evidenced by medical studies. To prevent widespread consequences, platforms are eager to predict these videos' impact on viewers' mental health. Subsequently, they can take intervention measures, such as revising recommendation algorithms and displaying viewer discretion. Nevertheless, applicable predictive methods lack relevance to well-established medical knowledge, which outlines clinically proven external and environmental factors of depression. To account for such medical knowledge, we resort to an emergent methodological discipline, seeded Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the limitations of single-origin topics, unknown topic sources, unclear seed supervision, and suboptimal convergence. To address those challenges, we develop a novel Knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10043</link><description>&lt;p&gt;
&#22914;&#20309;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to validate average calibration for machine learning regression tasks ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#27979;&#35797;&#12290;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#26657;&#20934;&#35823;&#24046;&#65288;CE&#65289;&#20272;&#35745;&#20026;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MSE&#65289;&#19982;&#24179;&#22343;&#26041;&#24046;&#65288;MV&#65289;&#25110;&#24179;&#22343;&#24179;&#26041;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#20540;&#12290;&#21478;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#25110;&#32553;&#25918;&#35823;&#24046;&#65288;ZMS&#65289;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#24471;&#20986;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#27491;&#22914;&#26469;&#33258;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25991;&#29486;&#20013;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#25152;&#31034;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;CE&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31163;&#32676;&#19981;&#30830;&#23450;&#24615;&#30340;&#23384;&#22312;&#65292;&#22240;&#27492;&#26080;&#27861;&#21487;&#38752;&#22320;&#29992;&#20110;&#26657;&#20934;&#27979;&#35797;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ZMS&#32479;&#35745;&#37327;&#19981;&#20855;&#26377;&#36825;&#31181;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#23545;&#26465;&#20214;&#26657;&#20934;&#39564;&#35777;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;</title><link>https://arxiv.org/abs/2402.10038</link><description>&lt;p&gt;
RS-DPO&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;RLHF&#26377;&#26102;&#19981;&#31283;&#23450;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#36229;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;DPO&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26367;&#20195;LLM&#29983;&#25104;&#30340;&#23545;&#27604;&#22238;&#22797;&#65292;&#32780;&#19981;&#26159;&#31574;&#30053;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;RLHF&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#32467;&#21512;&#25298;&#32477;&#37319;&#26679;&#65288;RS&#65289;&#21644;DPO&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;RS-DPO&#65292;&#39318;&#20808;&#24320;&#21457;&#20986;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65288;SFT&#65289;&#12290;&#28982;&#21518;&#30452;&#25509;&#20174;SFT&#27169;&#22411;&#20013;&#37319;&#26679;&#27599;&#20010;&#25552;&#31034;&#30340;k&#20010;&#21709;&#24212;&#12290;RS-DPO&#22522;&#20110;&#20854;&#30456;&#20284;&#24230;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#20351;&#29992;&#25152;&#23398;&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.10036</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Predictive Linear Online Tracking for Unknown Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#20351;&#29992;&#25152;&#23398;&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#36861;&#36394;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36319;&#38543;&#19968;&#20010;&#31227;&#21160;&#30340;&#30446;&#26631;&#12290;&#19982;&#32463;&#20856;&#30340;&#36861;&#36394;&#25511;&#21046;&#19981;&#21516;&#65292;&#30446;&#26631;&#26159;&#26410;&#30693;&#30340;&#12289;&#38750;&#24179;&#31283;&#30340;&#65292;&#24182;&#19988;&#23427;&#30340;&#29366;&#24577;&#36880;&#27493;&#25581;&#31034;&#65292;&#22240;&#27492;&#36866;&#21512;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#27425;&#25104;&#26412;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#12290;&#25152;&#23398;&#27169;&#22411;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PLOT&#30340;&#21160;&#24577;&#36951;&#25022;&#19982;$\mathcal{O}(\sqrt{TV_T})$&#25104;&#27604;&#20363;&#65292;&#20854;&#20013;$V_T$&#26159;&#30446;&#26631;&#21160;&#21147;&#23398;&#30340;&#24635;&#21464;&#21270;&#37327;&#65292;$T$&#26159;&#26102;&#38388;&#38271;&#24230;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#22235;&#26059;&#32764;&#26426;&#19978;&#23454;&#29616;&#20102;PLOT&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10036v1 Announce Type: cross  Abstract: In this paper, we study the problem of online tracking in linear control systems, where the objective is to follow a moving target. Unlike classical tracking control, the target is unknown, non-stationary, and its state is revealed sequentially, thus, fitting the framework of online non-stochastic control. We consider the case of quadratic costs and propose a new algorithm, called predictive linear online tracking (PLOT). The algorithm uses recursive least squares with exponential forgetting to learn a time-varying dynamic model of the target. The learned model is used in the optimal policy under the framework of receding horizon control. We show the dynamic regret of PLOT scales with $\mathcal{O}(\sqrt{TV_T})$, where $V_T$ is the total variation of the target dynamics and $T$ is the time horizon. Unlike prior work, our theoretical results hold for non-stationary targets. We implement PLOT on a real quadrotor and provide open-source so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10028</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#19982;&#22823;&#21160;&#20316;&#31354;&#38388;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models Meet Contextual Bandits with Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#65292;&#26377;&#25928;&#30340;&#25506;&#32034;&#26159;&#24773;&#22659;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35774;&#35745;&#20102;&#25193;&#25955;&#27748;&#26222;&#26862;&#37319;&#26679;&#65288;dTS&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#20026;dTS&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#31639;&#27861;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10028v1 Announce Type: cross  Abstract: Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10024</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#23545;&#20110;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-Augmented In-Context Learning for Unsupervised Word Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#35789;&#27719;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#35774;&#32622;&#20013;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#35789;&#27719;&#32763;&#35793;&#21644;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;(BLI)&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#27809;&#26377;&#31181;&#23376;&#32763;&#35793;&#23545;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#8220;&#20256;&#32479;&#8221;&#30340;&#22522;&#20110;&#26144;&#23556;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861; (SAIL) &#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;BLI&#65306;&#20174;&#38646;&#26679;&#26412;&#25552;&#31034;&#24320;&#22987;&#65292;SAIL&#36890;&#36807;&#36845;&#20195;&#22320;&#20174;LLM&#20013;&#24341;&#20986;&#19968;&#32452;&#39640;&#32622;&#20449;&#24230;&#30340;&#35789;&#27719;&#32763;&#35793;&#23545;&#65292;&#28982;&#21518;&#22312;ICL&#30340;&#26041;&#24335;&#19979;&#20877;&#27425;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;LLM&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#30340;BLI&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;LLM&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20063;&#22312;&#21508;&#20010;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26144;&#23556;&#30340;&#22522;&#32447;&#12290;&#38500;&#20102;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10024v1 Announce Type: cross  Abstract: Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.10009</link><description>&lt;p&gt;
&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38646;&#26679;&#26412;&#26080;&#30417;&#30563;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#32534;&#36753;&#24050;&#32463;&#22312;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#29467;&#30340;&#21457;&#23637;&#65292;&#20294;&#22312;&#38899;&#39057;&#39046;&#22495;&#23578;&#26410;&#20986;&#29616;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#22522;&#20110;DDPM&#21453;&#36716;&#30340;&#38899;&#39057;&#20449;&#21495;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#12290;&#31532;&#19968;&#31181;&#26159;&#20174;&#22270;&#20687;&#39046;&#22495;&#37319;&#29992;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22522;&#20110;&#25991;&#26412;&#36827;&#34892;&#32534;&#36753;&#12290;&#31532;&#20108;&#31181;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21457;&#29616;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#24403;&#24212;&#29992;&#20110;&#38899;&#20048;&#20449;&#21495;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23637;&#29616;&#20986;&#19968;&#31995;&#21015;&#20855;&#26377;&#38899;&#20048;&#20852;&#36259;&#30340;&#20462;&#25913;&#65292;&#20174;&#25511;&#21046;&#29305;&#23450;&#20048;&#22120;&#30340;&#21442;&#19982;&#21040;&#23545;&#26059;&#24459;&#36827;&#34892;&#21363;&#20852;&#28436;&#22863;&#12290;&#31034;&#20363;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#20363;&#23376;&#39029;&#38754;&#20013;&#25214;&#21040;&#65306;https://hilamanor.github.io/AudioEditing/ &#65292;&#20195;&#30721;&#21487;&#20197;&#22312; https://github.com/hilamanor/AudioEditing/ &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10009v1 Announce Type: cross  Abstract: Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22768;&#23398;&#20449;&#21495;&#22788;&#29702;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22797;&#26434;&#22768;&#23398;&#29616;&#35937;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10005</link><description>&lt;p&gt;
ML-ASPA: &#26426;&#22120;&#23398;&#20064;&#22312;&#22768;&#23398;&#20449;&#21495;&#22788;&#29702;&#20998;&#26512;&#20013;&#30340;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal Processing Analysis for Sounds, &amp; Strains Emerging Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22768;&#23398;&#20449;&#21495;&#22788;&#29702;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22797;&#26434;&#22768;&#23398;&#29616;&#35937;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#25968;&#25454;&#22312;&#25512;&#21160;&#31185;&#23398;&#21644;&#24037;&#31243;&#29702;&#35299;&#26041;&#38754;&#36215;&#30528;&#22522;&#26412;&#30340;&#22522;&#30707;&#20316;&#29992;&#65292;&#28041;&#21450;&#29983;&#29289;&#23398;&#12289;&#36890;&#20449;&#23398;&#20197;&#21450;&#28023;&#27915;&#21644;&#22320;&#29699;&#31185;&#23398;&#31561;&#22810;&#20010;&#23398;&#31185;&#12290;&#26412;&#25991;&#35814;&#32454;&#25506;&#35752;&#20102;&#22768;&#23398;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#21644;&#21464;&#38761;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#30340;&#22768;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#30456;&#27604;&#65292;ML&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;&#19982;&#26399;&#26395;&#26631;&#31614;&#25110;&#21160;&#20316;&#20043;&#38388;&#20197;&#21450;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#32473;&#23450;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23558;ML&#24212;&#29992;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#21457;&#29616;&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#35821;&#38899;&#21644;&#28151;&#21709;&#31561;&#22797;&#26434;&#22768;&#23398;&#29616;&#35937;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10005v1 Announce Type: cross  Abstract: Acoustic data serves as a fundamental cornerstone in advancing scientific and engineering understanding across diverse disciplines, spanning biology, communications, and ocean and Earth science. This inquiry meticulously explores recent advancements and transformative potential within the domain of acoustics, specifically focusing on machine learning (ML) and deep learning. ML, comprising an extensive array of statistical techniques, proves indispensable for autonomously discerning and leveraging patterns within data. In contrast to traditional acoustics and signal processing, ML adopts a data-driven approach, unveiling intricate relationships between features and desired labels or actions, as well as among features themselves, given ample training data. The application of ML to expansive sets of training data facilitates the discovery of models elucidating complex acoustic phenomena such as human speech and reverberation. The dynamic 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;D-GD&#65289;&#25552;&#20986;&#20102;&#39318;&#20010;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#25143;&#37325;&#24314;&#20854;&#37051;&#22495;&#20043;&#22806;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10001</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Privacy Attacks in Decentralized Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#65292;&#38024;&#23545;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;D-GD&#65289;&#25552;&#20986;&#20102;&#39318;&#20010;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#25143;&#37325;&#24314;&#20854;&#37051;&#22495;&#20043;&#22806;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;D-GD&#65289;&#20801;&#35768;&#19968;&#32452;&#29992;&#25143;&#22312;&#32593;&#32476;&#22270;&#20013;&#36890;&#36807;&#36845;&#20195;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#19982;&#20854;&#37051;&#23621;&#21512;&#20316;&#23398;&#20064;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#38750;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#30340;&#32570;&#22833;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#26080;&#27861;&#25512;&#26029;&#20986;&#20851;&#20110;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#30340;&#31934;&#30830;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;D-GD&#30340;&#25915;&#20987;&#65292;&#20351;&#19968;&#20010;&#29992;&#25143;&#65288;&#25110;&#19968;&#32452;&#29992;&#25143;&#65289;&#33021;&#22815;&#37325;&#24314;&#20854;&#37051;&#22495;&#20043;&#22806;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#20256;&#38395;&#24179;&#22343;&#21327;&#35758;&#30340;&#37325;&#24314;&#25915;&#20987;&#65292;&#28982;&#21518;&#23558;&#20854;&#25193;&#23637;&#20197;&#22788;&#29702;D-GD&#25552;&#20986;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#22270;&#21644;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#21333;&#20010;&#25110;&#23569;&#25968;&#25915;&#20987;&#32773;&#25152;&#23041;&#32961;&#21040;&#30340;&#29992;&#25143;&#25968;&#37327;&#36890;&#24120;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#26041;&#26696;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10001v1 Announce Type: new  Abstract: Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36816;&#33829;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38754;&#23545;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#23454;&#38469;&#24615;&#33021;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.09992</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#36719;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#40065;&#26834;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36816;&#33829;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38754;&#23545;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#23454;&#38469;&#24615;&#33021;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36816;&#33829;&#30740;&#31350;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#22810;&#38454;&#27573;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#20998;&#24067;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#40065;&#26834;&#30340;&#31574;&#30053;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#23545;&#24378;&#21270;&#23398;&#20064;&#31038;&#21306;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#30528;&#37325;&#20110;&#29702;&#35770;&#32467;&#26524;&#32780;&#19981;&#26159;&#23454;&#38469;&#24615;&#33021;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#27491;&#24335;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20854;&#26377;&#25928;&#24615;&#30340;&#25968;&#20540;&#35777;&#25454;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#30456;&#24212;&#30340;Q&#20540;&#30340;Bellman&#26041;&#31243;&#30340;&#29256;&#26412;&#65292;&#24341;&#20837;&#20102;&#31163;&#25955;&#24335;&#30340;&#36719;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#29109;&#39118;&#38505;&#24230;&#37327;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#30456;&#24212;&#30340;&#31574;&#30053;&#25913;&#36827;&#32467;&#26524;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#23454;&#38469;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20856;&#22411;&#30340;&#19978;&#19979;&#25991;&#22810;&#38454;&#27573;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09992v1 Announce Type: new  Abstract: We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-sta
&lt;/p&gt;</description></item><item><title>TIAViz&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#27169;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#20132;&#20114;&#24335;&#22320;&#26174;&#31034;&#22270;&#34920;&#12289;&#28909;&#22270;&#12289;&#20998;&#21106;&#12289;&#26631;&#27880;&#21644;&#20854;&#20182;&#20449;&#24687;&#22312;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.09990</link><description>&lt;p&gt;
TIAViz&#65306;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#27169;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
TIAViz: A Browser-based Visualization Tool for Computational Pathology Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09990
&lt;/p&gt;
&lt;p&gt;
TIAViz&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#27169;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#20132;&#20114;&#24335;&#22320;&#26174;&#31034;&#22270;&#34920;&#12289;&#28909;&#22270;&#12289;&#20998;&#21106;&#12289;&#26631;&#27880;&#21644;&#20854;&#20182;&#20449;&#24687;&#22312;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#22312;&#29616;&#20195;&#21307;&#30103;&#31995;&#32479;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#36825;&#31181;&#20174;&#20809;&#23398;&#26174;&#24494;&#38236;&#21040;&#25968;&#23383;&#22270;&#20687;&#30340;&#36716;&#21464;&#24102;&#26469;&#20102;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#21644;&#23558;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#25972;&#21512;&#21040;&#30149;&#29702;&#23398;&#23478;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#21487;&#35270;&#21270;&#12290;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#65292;&#25317;&#26377;&#28789;&#27963;&#12289;&#24320;&#25918;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#20197;&#21487;&#35270;&#21270;&#27169;&#22411;&#30340;&#36755;&#20986;&#12289;&#39044;&#27979;&#20197;&#21450;&#29992;&#20110;&#35757;&#32451;&#25110;&#27979;&#35797;&#27169;&#22411;&#30340;&#24213;&#23618;&#27880;&#37322;&#21644;&#22270;&#20687;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;TIAViz&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#20869;&#32622;&#20110;TIAToolbox&#20013;&#65292;&#20801;&#35768;&#22312;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#19978;&#28789;&#27963;&#12289;&#20132;&#20114;&#24335;&#12289;&#23436;&#20840;&#21487;&#32553;&#25918;&#22320;&#21472;&#21152;&#21508;&#31181;&#20449;&#24687;&#65292;&#21253;&#25324;&#22270;&#34920;&#12289;&#28909;&#22270;&#12289;&#20998;&#21106;&#12289;&#26631;&#27880;&#21644;&#20854;&#20182;WSI&#12290;&#29992;&#25143;&#30028;&#38754;&#22522;&#20110;&#27983;&#35272;&#22120;&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#36828;&#31243;&#35745;&#31639;&#26426;&#19978;&#25110;&#26381;&#21153;&#22120;&#19978;&#20351;&#29992;&#65292;&#25552;&#20379;&#20844;&#24320;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09990v1 Announce Type: cross  Abstract: Digital pathology has gained significant traction in modern healthcare systems. This shift from optical microscopes to digital imagery brings with it the potential for improved diagnosis, efficiency, and the integration of AI tools into the pathologists workflow. A critical aspect of this is visualization. Throughout the development of a machine learning (ML) model in digital pathology, it is crucial to have flexible, openly available tools to visualize models, from their outputs and predictions to the underlying annotations and images used to train or test a model. We introduce TIAViz, a Python-based visualization tool built into TIAToolbox which allows flexible, interactive, fully zoomable overlay of a wide variety of information onto whole slide images, including graphs, heatmaps, segmentations, annotations and other WSIs. The UI is browser-based, allowing use either locally, on a remote machine, or on a server to provide publicly a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09984</link><description>&lt;p&gt;
&#23545;&#20110;&#20020;&#26102;&#22242;&#38431;&#21512;&#20316;&#30340;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Breaking Augmentations for Ad Hoc Teamwork
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21327;&#20316;&#29615;&#22659;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#36866;&#24212;&#20351;&#29992;&#26410;&#30693;&#25110;&#20808;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#31574;&#30053;&#30340;&#26032;&#38431;&#21451;&#12290;&#23545;&#20110;AI&#20195;&#29702;&#26469;&#35828;&#65292;&#36825;&#36890;&#24120;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#31616;&#21333;&#65292;&#20294;&#21364;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;AI&#20195;&#29702;&#22312;&#35757;&#32451;&#38598;&#20013;&#23398;&#20250;&#20102;&#19982;&#21482;&#22312;&#19968;&#20391;&#36947;&#36335;&#19978;&#34892;&#39542;&#30340;&#20854;&#20182;&#36710;&#36742;&#24182;&#34892;&#39542;&#65292;&#37027;&#20040;&#21363;&#20351;&#36825;&#20123;&#36710;&#36742;&#30340;&#34892;&#20026;&#21482;&#26159;&#22312;&#24038;&#21491;&#23545;&#31216;&#19978;&#36827;&#34892;&#20102;&#32763;&#36716;&#65292;&#23427;&#20063;&#21487;&#33021;&#38590;&#20197;&#36866;&#24212;&#19982;&#30456;&#21453;&#26041;&#21521;&#19978;&#34892;&#39542;&#30340;&#39550;&#39542;&#21592;&#36827;&#34892;&#21327;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#65288;SBA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#23545;&#31216;&#32763;&#36716;&#25805;&#20316;&#26469;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#22686;&#24378;&#21518;&#30340;&#38431;&#21451;&#30340;&#26368;&#20339;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#33021;&#22815;&#25509;&#35302;&#21040;&#26356;&#24191;&#27867;&#30340;&#34892;&#20026;&#32422;&#23450;&#65292;&#20174;&#32780;&#25552;&#39640;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09984v1 Announce Type: cross  Abstract: In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies. While often simple for humans, this can be challenging for AI agents. For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry. To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation. By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates. We demonstrate this experimentally in two settings, and show that our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;85%&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09982</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#24212;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;85%&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#34920;&#24773;&#26159;&#25105;&#20204;&#22312;&#29702;&#35299;&#19968;&#20010;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#26102;&#39318;&#20808;&#20851;&#27880;&#30340;&#20107;&#29289;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#38754;&#37096;&#34920;&#24773;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#30001;&#20110;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#35782;&#21035;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#24212;&#29992;&#20960;&#20309;&#21464;&#25442;&#65292;&#24182;&#20174;&#22836;&#26500;&#24314;&#20102;&#33021;&#22815;&#20026;&#27599;&#31181;&#24773;&#32490;&#31867;&#22411;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#30340;GAN&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#34913;&#37327;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#39069;&#22806;&#25968;&#25454;&#24211;&#21327;&#35758;&#26041;&#27861;&#65292;&#21363;&#25105;&#20204;&#22312;&#32463;&#36807;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#20351;&#24471;&#21487;&#20197;&#36798;&#21040;&#24179;&#22343;&#20934;&#30830;&#24230;&#32422;&#20026;85%&#30340;&#25968;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09982v1 Announce Type: cross  Abstract: The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\% for 
&lt;/p&gt;</description></item><item><title>&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#38750;&#21380;&#31859;&#25299;&#30005;&#36335;&#65292;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38750;&#21380;&#31859;&#21704;&#23494;&#39039;&#37327;&#30340;&#26412;&#24449;&#20540;&#65292;&#21033;&#29992;DenseNet&#31639;&#27861;&#35774;&#35745;&#39640;&#32500;&#25299;&#25169;&#30005;&#36335;&#65292;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#22312;&#25429;&#25417;&#20840;&#23616;&#25299;&#25169;&#29305;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09978</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#21380;&#31859;&#25299;&#30005;&#36335;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep learning for the design of non-Hermitian topolectrical circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09978
&lt;/p&gt;
&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#38750;&#21380;&#31859;&#25299;&#30005;&#36335;&#65292;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38750;&#21380;&#31859;&#21704;&#23494;&#39039;&#37327;&#30340;&#26412;&#24449;&#20540;&#65292;&#21033;&#29992;DenseNet&#31639;&#27861;&#35774;&#35745;&#39640;&#32500;&#25299;&#25169;&#30005;&#36335;&#65292;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#22312;&#25429;&#25417;&#20840;&#23616;&#25299;&#25169;&#29305;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21380;&#31859;&#25299;&#25169;&#30456;&#30456;&#23545;&#20110;&#20854;&#21380;&#31859;&#23545;&#24212;&#29289;&#20855;&#26377;&#19968;&#20123;&#20986;&#33394;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#20256;&#32479;&#30340;&#20307;-&#36793;&#23545;&#24212;&#30340;&#30772;&#35010;&#20197;&#21450;&#38750;&#21380;&#31859;&#25299;&#25169;&#36793;&#27169;&#24577;&#12290;&#25105;&#20204;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24341;&#20837;&#20102;&#20960;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#21380;&#31859;&#21704;&#23494;&#39039;&#37327;&#30340;&#26412;&#24449;&#20540;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21608;&#26399;&#24615;&#30005;&#36335;&#30340;&#26368;&#23567;&#27169;&#22359;&#20316;&#20026;&#19968;&#20010;&#21333;&#20803;&#26469;&#26500;&#24314;&#39640;&#32500;&#30005;&#36335;&#25968;&#25454;&#29305;&#24449;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;DenseNet&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21033;&#29992;&#23618;&#20043;&#38388;&#30340;&#23494;&#38598;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35774;&#35745;&#38750;&#21380;&#31859;&#25299;&#30005;&#38472;&#30005;&#36335;&#65292;&#22240;&#20026;DenseNet&#31639;&#27861;&#26356;&#36866;&#21512;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#22312;&#25429;&#25417;&#20840;&#23616;&#25299;&#25169;&#29305;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09978v1 Announce Type: cross  Abstract: Non-Hermitian topological phases can produce some remarkable properties, compared with their Hermitian counterpart, such as the breakdown of conventional bulk-boundary correspondence and the non-Hermitian topological edge mode. Here, we introduce several algorithms with multi-layer perceptron (MLP), and convolutional neural network (CNN) in the field of deep learning, to predict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we use the smallest module of the periodic circuit as one unit to construct high-dimensional circuit data features. Further, we use the Dense Convolutional Network (DenseNet), a type of convolutional neural network that utilizes dense connections between layers to design a non-Hermitian topolectrical Chern circuit, as the DenseNet algorithm is more suitable for processing high-dimensional data. Our results demonstrate the effectiveness of the deep learning network in capturing the global topol
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#24615;&#33021;&#30053;&#26377;&#22949;&#21327;&#12290;</title><link>https://arxiv.org/abs/2402.09977</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#24555;&#36895;&#35789;&#27719;&#36716;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Vocabulary Transfer for Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#24615;&#33021;&#30053;&#26377;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19994;&#21153;&#24212;&#29992;&#38656;&#35201;&#22312;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#22823;&#23567;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#27719;&#36716;&#31227;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#22402;&#30452;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#27719;&#36716;&#31227;&#21487;&#20197;&#19982;&#20854;&#20182;&#21387;&#32553;&#25216;&#26415;&#26377;&#25928;&#32467;&#21512;&#20351;&#29992;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#30053;&#26377;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09977v1 Announce Type: cross  Abstract: Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21270;&#33258;&#22238;&#24402;&#36807;&#31243;&#26469;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;ParaTAA&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#24182;&#34892;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2402.09970</link><description>&lt;p&gt;
&#21152;&#36895;&#24182;&#34892;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Accelerating Parallel Sampling of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21270;&#33258;&#22238;&#24402;&#36807;&#31243;&#26469;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;ParaTAA&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#24182;&#34892;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#37319;&#26679;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#36890;&#24120;&#32791;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#33258;&#22238;&#24402;&#36807;&#31243;&#26469;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#37319;&#26679;&#36807;&#31243;&#37325;&#26032;&#26500;&#24314;&#20026;&#36890;&#36807;&#22266;&#23450;&#28857;&#36845;&#20195;&#35299;&#20915;&#19977;&#35282;&#38750;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#21019;&#26032;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#31995;&#32479;&#21270;&#30340;&#25216;&#26415;&#65292;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#27714;&#35299;&#36807;&#31243;&#25152;&#38656;&#30340;&#36845;&#20195;&#27493;&#39588;&#12290;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ParaTAA&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#24182;&#34892;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#26469;&#22686;&#21152;&#37319;&#26679;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ParaTAA&#21487;&#20197;&#20943;&#23569;&#24120;&#35265;&#30340;&#39034;&#24207;&#37319;&#26679;&#25152;&#38656;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09970v1 Announce Type: new  Abstract: Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21487;&#35270;&#21270;&#30446;&#26631;&#38388;&#23618;&#27425;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#27169;&#22411;&#25913;&#36827;&#20855;&#26377;&#28508;&#22312;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.09965</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Hierarchy Representation of Data in Machine Learnings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21487;&#35270;&#21270;&#30446;&#26631;&#38388;&#23618;&#27425;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#27169;&#22411;&#25913;&#36827;&#20855;&#26377;&#28508;&#22312;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23384;&#22312;&#22810;&#20010;&#25968;&#25454;&#28857;&#30340;&#27169;&#22411;&#20855;&#26377;&#26126;&#30830;&#30340;&#21028;&#26029;&#32467;&#26524;&#26102;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#20986;&#19968;&#31181;&#20851;&#31995;&#65292;&#21363;&#22914;&#26524;&#23427;&#20204;&#27491;&#30830;&#21028;&#26029;&#19968;&#20010;&#30446;&#26631;&#65292;&#21017;&#23427;&#20204;&#20063;&#20250;&#27491;&#30830;&#21028;&#26029;&#21478;&#19968;&#20010;&#30446;&#26631;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#22823;&#22810;&#25968;&#27169;&#22411;&#38169;&#35823;&#22320;&#21028;&#26029;&#19968;&#20010;&#30446;&#26631;&#65292;&#23427;&#20204;&#21487;&#33021;&#20063;&#20250;&#38169;&#35823;&#22320;&#21028;&#26029;&#21478;&#19968;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#30446;&#26631;&#20043;&#38388;&#23618;&#27425;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20449;&#24687;&#26377;&#26395;&#23545;&#27169;&#22411;&#25913;&#36827;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09965v1 Announce Type: cross  Abstract: When there are models with clear-cut judgment results for several data points, it is possible that most models exhibit a relationship where if they correctly judge one target, they also correctly judge another target. Conversely, if most models incorrectly judge one target, they may also incorrectly judge another target. We propose a method for visualizing this hierarchy among targets. This information is expected to be beneficial for model improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.09963</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#22256;&#38590;?
&lt;/p&gt;
&lt;p&gt;
Why are Sensitive Functions Hard for Transformers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;Transformer&#23545;&#25935;&#24863;&#20989;&#25968;&#30340;&#22256;&#38590;&#12290;&#36825;&#19968;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#23384;&#22312;&#19968;&#31995;&#21015;&#30340;&#23398;&#20064;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#22914;&#22312;&#23398;&#20064;&#35745;&#31639;&#31616;&#21333;&#24418;&#24335;&#35821;&#35328;&#65288;&#22914;PARITY&#65289;&#26102;&#30340;&#25345;&#20037;&#22256;&#38590;&#65292;&#20197;&#21450;&#23545;&#20302;&#38454;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#29702;&#35770;&#35201;&#20040;&#36807;&#24230;&#39044;&#27979;&#65292;&#35201;&#20040;&#20302;&#20272;&#20102;&#23454;&#38469;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;Transformer&#26550;&#26500;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#31354;&#38388;&#21463;&#21040;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#38480;&#21046;&#65306;&#36755;&#20986;&#23545;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#22810;&#20010;&#37096;&#20998;&#25935;&#24863;&#30340;Transformer&#23384;&#22312;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#23396;&#31435;&#28857;&#65292;&#23548;&#33268;&#27867;&#21270;&#20013;&#30340;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32479;&#19968;&#20102;&#20851;&#20110;Transformer&#23398;&#20064;&#33021;&#21147;&#21644;&#20559;&#35265;&#30340;&#24191;&#27867;&#35266;&#23519;&#65292;&#22914;&#23427;&#20204;&#23545;&#20302;&#25935;&#24863;&#24615;&#21644;&#20302;&#38454;&#30340;&#27867;&#21270;&#20559;&#24046;&#65292;&#20197;&#21450;&#22312;&#38271;&#24230;&#27867;&#21270;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09963v1 Announce Type: new  Abstract: Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#31163;&#32447;&#35745;&#21010;&#36827;&#34892;&#21160;&#24577;&#35843;&#25972;&#26469;&#20248;&#21270;&#20247;&#21253;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#20013;&#24555;&#36882;&#21592;&#35843;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20247;&#21253;&#24179;&#21488;&#30340;&#21033;&#28070;&#12290;</title><link>https://arxiv.org/abs/2402.09961</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#25193;&#23637;&#29677;&#27425;&#30340;&#26041;&#24335;&#25552;&#21319;&#20247;&#21253;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#20013;&#30340;&#24555;&#36882;&#21592;&#35843;&#24230;&#65306;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#31163;&#32447;&#35745;&#21010;&#36827;&#34892;&#21160;&#24577;&#35843;&#25972;&#26469;&#20248;&#21270;&#20247;&#21253;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#20013;&#24555;&#36882;&#21592;&#35843;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20247;&#21253;&#24179;&#21488;&#30340;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#37197;&#36865;&#24179;&#21488;&#38754;&#20020;&#30528;&#21305;&#37197;&#24555;&#36882;&#21592;&#21644;&#39038;&#23458;&#35746;&#21333;&#30340;&#22797;&#26434;&#35843;&#24230;&#25361;&#25112;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20247;&#21253;&#24555;&#36882;&#21592;&#65292;&#21363;&#25215;&#35834;&#22411;&#21644;&#20598;&#21457;&#22411;&#24555;&#36882;&#21592;&#65292;&#27599;&#31181;&#31867;&#22411;&#30340;&#24555;&#36882;&#21592;&#26377;&#19981;&#21516;&#30340;&#34917;&#20607;&#26041;&#26696;&#12290;&#20247;&#21253;&#37197;&#36865;&#24179;&#21488;&#36890;&#24120;&#26681;&#25454;&#39044;&#27979;&#30340;&#38656;&#27714;&#20026;&#25215;&#35834;&#22411;&#24555;&#36882;&#21592;&#23433;&#25490;&#29677;&#27425;&#12290;&#22240;&#27492;&#65292;&#24179;&#21488;&#21487;&#33021;&#20250;&#22312;&#35745;&#21010;&#21608;&#26399;&#20043;&#21069;&#20026;&#25215;&#35834;&#22411;&#24555;&#36882;&#21592;&#21046;&#23450;&#31163;&#32447;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#27714;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#26377;&#26102;&#38656;&#35201;&#23545;&#31163;&#32447;&#35745;&#21010;&#36827;&#34892;&#22312;&#32447;&#35843;&#25972;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36890;&#36807;&#20026;&#25215;&#35834;&#22411;&#24555;&#36882;&#21592;&#25552;&#20379;&#29677;&#27425;&#25193;&#23637;&#26469;&#21160;&#24577;&#35843;&#25972;&#31163;&#32447;&#35745;&#21010;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#12290;&#30446;&#26631;&#26159;&#36890;&#36807;&#30830;&#23450;&#24555;&#36882;&#21592;&#30340;&#29677;&#27425;&#25193;&#23637;&#21644;&#35831;&#27714;&#20998;&#37197;&#32473;&#24555;&#36882;&#21592;&#30340;&#26041;&#24335;&#26469;&#26368;&#22823;&#21270;&#24179;&#21488;&#21033;&#28070;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#28145;&#24230;Q&#32593;&#32476;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09961v1 Announce Type: new  Abstract: Crowdsourced delivery platforms face complex scheduling challenges to match couriers and customer orders. We consider two types of crowdsourced couriers, namely, committed and occasional couriers, each with different compensation schemes. Crowdsourced delivery platforms usually schedule committed courier shifts based on predicted demand. Therefore, platforms may devise an offline schedule for committed couriers before the planning period. However, due to the unpredictability of demand, there are instances where it becomes necessary to make online adjustments to the offline schedule. In this study, we focus on the problem of dynamically adjusting the offline schedule through shift extensions for committed couriers. This problem is modeled as a sequential decision process. The objective is to maximize platform profit by determining the shift extensions of couriers and the assignments of requests to couriers. To solve the model, a Deep Q-Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#30452;&#26041;&#22270;&#29702;&#35770;&#35774;&#35745;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36755;&#20837;&#29305;&#24449;&#25277;&#21462;&#26041;&#27861;&#65292;&#20026;&#26426;&#26800;&#29366;&#24577;&#35782;&#21035;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09957</link><description>&lt;p&gt;
&#35774;&#35745;&#26059;&#36716;&#26426;&#26800;&#29366;&#24577;&#30417;&#27979;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Designing Features for Condition Monitoring of Rotating Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#30452;&#26041;&#22270;&#29702;&#35770;&#35774;&#35745;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36755;&#20837;&#29305;&#24449;&#25277;&#21462;&#26041;&#27861;&#65292;&#20026;&#26426;&#26800;&#29366;&#24577;&#35782;&#21035;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#20351;&#29992;&#19968;&#32500;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35782;&#21035;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#35774;&#35745;&#36755;&#20837;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#22797;&#26434;&#65292;&#20381;&#36182;&#32463;&#39564;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#33021;&#22240;&#20351;&#29992;&#30340;&#26465;&#20214;&#30417;&#27979;&#25968;&#25454;&#32780;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#36755;&#20837;&#29305;&#24449;&#25277;&#21462;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#30340;&#29305;&#24449;&#35774;&#35745;/&#25277;&#21462;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#26041;&#22270;&#29702;&#35770;&#33719;&#24471;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25552;&#21462;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#36866;&#29992;&#20110;&#31616;&#21333;&#20998;&#31867;&#22120;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#12290;&#35774;&#35745;&#30340;&#36755;&#20837;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21333;&#19968;&#26694;&#26550;&#19979;&#30340;&#26426;&#26800;&#29366;&#24577;&#35782;&#21035;&#12290;&#36890;&#36807;&#19977;&#20010;&#23454;&#26102;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#35813;&#26041;&#26696;&#65306;a) &#22768;&#23398;&#25968;&#25454;&#38598;&#65292;b) CWRU&#25391;&#21160;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09957v1 Announce Type: new  Abstract: Various methods for designing input features have been proposed for fault recognition in rotating machines using one-dimensional raw sensor data. The available methods are complex, rely on empirical approaches, and may differ depending on the condition monitoring data used. Therefore, this article proposes a novel algorithm to design input features that unifies the feature extraction process for different time-series sensor data. This new insight for designing/extracting input features is obtained through the lens of histogram theory. The proposed algorithm extracts discriminative input features, which are suitable for a simple classifier to deep neural network-based classifiers. The designed input features are given as input to the classifier with end-to-end training in a single framework for machine conditions recognition. The proposed scheme has been validated through three real-time datasets: a) acoustic dataset, b) CWRU vibration da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09954</link><description>&lt;p&gt;
&#21046;&#23450;&#33391;&#22909;&#25552;&#31034;&#36824;&#26159;&#25552;&#20379;&#20986;&#33394;&#30340;&#23545;&#35805;&#65311;&#20851;&#20110;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#20998;&#31867;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#34920;&#26684;&#31561;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;ICL&#33021;&#21542;&#25913;&#36827;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#20154;&#31867;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#30340;ICL&#33021;&#21147;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#19977;&#20010;&#32467;&#35770;&#65306;1&#65289;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#26159;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#26368;&#30452;&#25509;&#12289;&#26377;&#25928;&#21644;&#32463;&#27982;&#30340;&#26041;&#27861;&#65307;2&#65289;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#21487;&#20197;&#21462;&#24471;&#26368;&#20339;&#30340;&#32467;&#26524;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#20855;&#26377;&#26356;&#22810;&#26679;&#21270;&#21644;&#26377;&#25928;&#20449;&#24687;&#30340;&#21407;&#22240;&#65307;&#19982;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#32467;&#26524;&#26368;&#24046;&#65307;3&#65289;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09954v1 Announce Type: new  Abstract: Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09949</link><description>&lt;p&gt;
&#22810;&#35789;&#26631;&#35760;&#21270;&#29992;&#20110;&#24207;&#21015;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Multi-Word Tokenization for Sequence Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24847;&#21619;&#30528;&#35745;&#31639;&#25104;&#26412;&#30340;&#22823;&#24133;&#22686;&#21152;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MWT&#30340;&#22810;&#35789;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;&#23558;&#39057;&#32321;&#20986;&#29616;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;&#21333;&#20010;&#26631;&#35760;&#65292;&#31361;&#30772;&#20102;&#35789;&#36793;&#30028;&#30340;&#38480;&#21046;&#12290;MWT&#20135;&#29983;&#20102;&#26356;&#32039;&#20945;&#21644;&#39640;&#25928;&#30340;&#26631;&#35760;&#21270;&#32467;&#26524;&#65292;&#24102;&#26469;&#20004;&#20010;&#22909;&#22788;&#65306;&#65288;1&#65289;&#22312;&#22266;&#23450;&#24207;&#21015;&#38271;&#24230;&#21644;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35206;&#30422;&#36755;&#20837;&#25968;&#25454;&#65307;&#65288;2&#65289;&#30001;&#20110;&#33021;&#22815;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#32780;&#23545;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#36895;&#21644;&#26356;&#36731;&#37327;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MWT&#22312;&#36739;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#26356;&#20026;&#31283;&#20581;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#26089;&#26399;&#24207;&#21015;&#25130;&#26029;&#23454;&#29616;&#37325;&#22823;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09949v1 Announce Type: new  Abstract: Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;IMU&#30417;&#30563;&#30340;&#31070;&#32463;&#32593;&#32476;5G&#23460;&#20869;&#23450;&#20301;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;IMU&#30340;&#20266;&#26631;&#31614;&#21644;&#23454;&#29992;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09948</link><description>&lt;p&gt;
&#24102;&#26377;IMU&#30417;&#30563;&#30340;&#31070;&#32463;&#32593;&#32476;5G&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Neural 5G Indoor Localization with IMU Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;IMU&#30417;&#30563;&#30340;&#31070;&#32463;&#32593;&#32476;5G&#23460;&#20869;&#23450;&#20301;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;IMU&#30340;&#20266;&#26631;&#31614;&#21644;&#23454;&#29992;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#20449;&#21495;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#29992;&#25143;&#23450;&#20301;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#22788;&#19981;&#22312;&#65292;&#21487;&#20197;&#22312;&#40657;&#26263;&#20013;&#36816;&#34892;&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#26159;&#36890;&#36807;&#20840;&#38754;&#30417;&#30563;&#23398;&#20064;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21644;&#20301;&#32622;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20301;&#32622;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#20351;&#29992;&#20174;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#35745;&#31639;&#20986;&#30340;&#20266;&#26631;&#31614;&#26469;&#25918;&#23485;&#36825;&#19968;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IMU&#21452;&#31215;&#20998;&#21644;&#23450;&#20301;&#31995;&#32479;&#35757;&#32451;&#30340;&#23454;&#29992;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;5G&#27979;&#37327;&#30340;&#27169;&#25311;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#20855;&#26377;&#20998;&#31859;&#32423;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;IMU&#30417;&#30563;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#23569;&#30340;&#37096;&#32626;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09948v1 Announce Type: cross  Abstract: Radio signals are well suited for user localization because they are ubiquitous, can operate in the dark and maintain privacy. Many prior works learn mappings between channel state information (CSI) and position fully-supervised. However, that approach relies on position labels which are very expensive to acquire. In this work, this requirement is relaxed by using pseudo-labels during deployment, which are calculated from an inertial measurement unit (IMU). We propose practical algorithms for IMU double integration and training of the localization system. We show decimeter-level accuracy on simulated and challenging real data of 5G measurements. Our IMU-supervised method performs similarly to fully-supervised, but requires much less effort to deploy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#20540;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#35814;&#32454;&#21644;&#26377;&#27934;&#23519;&#21147;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.09947</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#20540;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Explaining Probabilistic Models with Distributional Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#20540;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#35814;&#32454;&#21644;&#26377;&#27934;&#23519;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20998;&#25903;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#21338;&#24328;&#29702;&#35770;&#35299;&#37322;&#21487;&#33021;&#20250;&#35823;&#23548;&#25110;&#38590;&#20197;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#24120;&#23384;&#22312;&#30528;&#19968;&#20010;&#37325;&#35201;&#30340;&#19981;&#21305;&#37197;&#65292;&#21363;&#20154;&#20204;&#24076;&#26395;&#35299;&#37322;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#65289;&#19982;&#24403;&#21069;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;SHAP&#65289;&#25152;&#35299;&#37322;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#31867;&#21035;&#30340;&#27010;&#29575;&#65289;&#20043;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#24191;&#21512;&#20316;&#21338;&#24328;&#21644;&#20215;&#20540;&#31639;&#23376;&#65292;&#26469;&#35299;&#20915;&#27010;&#29575;&#27169;&#22411;&#30340;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#24067;&#20540;&#65292;&#36825;&#26159;&#19968;&#31181;&#38543;&#26426;&#21464;&#37327;&#65292;&#29992;&#20110;&#36861;&#36394;&#27169;&#22411;&#36755;&#20986;&#30340;&#21464;&#21270;&#65288;&#20363;&#22914;&#39044;&#27979;&#31867;&#21035;&#30340;&#21453;&#36716;&#65289;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#12289;&#20271;&#21162;&#21033;&#21644;&#20998;&#31867;&#25903;&#20184;&#30340;&#21338;&#24328;&#20013;&#30340;&#20998;&#24067;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#20960;&#20010;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#21644;&#26377;&#27934;&#23519;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09947v1 Announce Type: new  Abstract: A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models.
&lt;/p&gt;</description></item><item><title>FedLion&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38598;&#20013;&#24335;&#33258;&#36866;&#24212;&#31639;&#27861;Lion&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#65292;FedLion&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#22312;&#26412;&#22320;&#35757;&#32451;&#20013;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.09941</link><description>&lt;p&gt;
FedLion: &#26356;&#24555;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#20449;&#26356;&#23569;
&lt;/p&gt;
&lt;p&gt;
FedLion: Faster Adaptive Federated Optimization with Fewer Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09941
&lt;/p&gt;
&lt;p&gt;
FedLion&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38598;&#20013;&#24335;&#33258;&#36866;&#24212;&#31639;&#27861;Lion&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#65292;FedLion&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#22312;&#26412;&#22320;&#35757;&#32451;&#20013;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#19968;&#31181;&#36328;&#20998;&#24067;&#24335;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26694;&#26550;&#20013;&#65292;&#20687;FedAvg&#36825;&#26679;&#30340;&#30693;&#21517;&#31639;&#27861;&#24448;&#24448;&#20855;&#26377;&#36739;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#39640;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedLion&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#26080;&#32541;&#22320;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#38598;&#20013;&#24335;&#33258;&#36866;&#24212;&#31639;&#27861;Lion&#65288;Chen et al. 2023&#65289;&#30340;&#20851;&#38190;&#20803;&#32032;&#34701;&#20837;&#21040;FL&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;FL&#22522;&#20934;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FedLion&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21253;&#25324;FAFED&#65288;Wu et al. 2023&#65289;&#21644;FedDA&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22312;&#26412;&#22320;&#35757;&#32451;&#20013;&#20351;&#29992;&#20102;&#26377;&#31526;&#21495;&#26799;&#24230;&#65292;&#19982;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#30456;&#27604;&#65292;FedLion&#22312;&#19978;&#34892;&#36890;&#20449;&#36807;&#31243;&#20013;&#22823;&#22823;&#38477;&#20302;&#20102;&#25968;&#25454;&#20256;&#36755;&#35201;&#27714;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09941v1 Announce Type: cross  Abstract: In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training. To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into the FL framework. Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. Last but not least, this work also incl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.09939</link><description>&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#26368;&#26032;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Construction Industry: A State-of-the-art Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25552;&#20379;&#20102;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#26500;&#24314;&#23450;&#21046;&#21270;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#34892;&#19994;&#26159;&#20840;&#29699;&#32463;&#27982;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#20010;&#37096;&#38376;&#65292;&#20294;&#22312;&#35774;&#35745;&#12289;&#35268;&#21010;&#12289;&#37319;&#36141;&#12289;&#26816;&#26597;&#21644;&#32500;&#25252;&#31561;&#21508;&#20010;&#29615;&#33410;&#20013;&#38754;&#20020;&#30528;&#35768;&#22810;&#29983;&#20135;&#21147;&#25361;&#25112;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#22522;&#20110;&#26576;&#20123;&#36755;&#20837;&#25110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21019;&#36896;&#26032;&#39062;&#19988;&#36924;&#30495;&#30340;&#25968;&#25454;&#25110;&#20869;&#23481;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#20195;&#30721;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#21019;&#26032;&#21644;&#39072;&#35206;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#20110;&#24314;&#31569;&#34892;&#19994;&#20013;&#29983;&#25104;&#24335;AI&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#25991;&#29486;&#20013;&#23384;&#22312;&#30528;&#31354;&#30333;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#24314;&#31569;&#39046;&#22495;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#32570;&#65292;&#30740;&#31350;&#30446;&#26631;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23545;&#24314;&#31569;&#34892;&#19994;&#29616;&#26377;&#21644;&#26032;&#20852;&#30340;&#29983;&#25104;&#24335;AI&#26426;&#36935;&#21644;&#25361;&#25112;&#36827;&#34892;&#22238;&#39038;&#21644;&#20998;&#31867;&#65307;&#65288;2&#65289;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#24314;&#31569;&#20844;&#21496;&#21033;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#21644;&#38656;&#27714;&#26500;&#24314;&#23450;&#21046;&#21270;&#30340;&#29983;&#25104;&#24335;AI&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09939v1 Announce Type: new  Abstract: The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their ow
&lt;/p&gt;</description></item><item><title>BUSTER&#26159;&#19968;&#20010;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;3779&#20221;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#34701;&#20132;&#26131;&#25991;&#26723;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#20339;&#27169;&#22411;&#36824;&#29992;&#20110;&#33258;&#21160;&#26631;&#27880;6196&#20221;&#25991;&#26723;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#21457;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.09916</link><description>&lt;p&gt;
BUSTER:&#19968;&#20221;&#8220;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#8221;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BUSTER: a "BUSiness Transaction Entity Recognition" dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09916
&lt;/p&gt;
&lt;p&gt;
BUSTER&#26159;&#19968;&#20010;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;3779&#20221;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#34701;&#20132;&#26131;&#25991;&#26723;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#20339;&#27169;&#22411;&#36824;&#29992;&#20110;&#33258;&#21160;&#26631;&#27880;6196&#20221;&#25991;&#26723;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#23558;&#36825;&#20123;&#36827;&#23637;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#21830;&#19994;&#26696;&#20363;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#22312;&#20110;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#19982;&#23454;&#38469;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32570;&#20047;&#30417;&#30563;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#25968;&#25454;&#21644;&#38271;&#25991;&#26723;&#32463;&#24120;&#24433;&#21709;&#37329;&#34701;&#12289;&#27861;&#24459;&#21644;&#20581;&#24247;&#31561;&#22402;&#30452;&#39046;&#22495;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#20026;&#20102;&#25903;&#25345;&#38754;&#21521;&#34892;&#19994;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;BUSTER&#30340;&#8220;&#21830;&#19994;&#20132;&#26131;&#23454;&#20307;&#35782;&#21035;&#8221;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;3779&#20221;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#34701;&#20132;&#26131;&#25991;&#26723;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#36890;&#29992;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36824;&#34987;&#29992;&#20110;&#33258;&#21160;&#26631;&#27880;6196&#20221;&#25991;&#26723;&#65292;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#39069;&#22806;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#21457;&#24067;&#32473;BUSTER&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09916v1 Announce Type: new  Abstract: Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER.
&lt;/p&gt;</description></item><item><title>DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09910</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#26816;&#27979;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65306;DE-COP
&lt;/p&gt;
&lt;p&gt;
DE-COP: Detecting Copyrighted Content in Language Models Training Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09910
&lt;/p&gt;
&lt;p&gt;
DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#21040;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#20445;&#23494;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#20351;&#29992;&#20102;&#29256;&#26435;&#20869;&#23481;&#65311;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#22522;&#20110;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#24456;&#21487;&#33021;&#33021;&#22815;&#35782;&#21035;&#20986;&#20854;&#35757;&#32451;&#25991;&#26412;&#20013;&#30340;&#29420;&#25991;&#25688;&#24405;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DE-COP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#26159;&#21542;&#22312;&#35757;&#32451;&#20013;&#21253;&#21547;&#20102;&#19968;&#27573;&#29256;&#26435;&#20869;&#23481;&#12290;DE-COP&#30340;&#26680;&#24515;&#26041;&#27861;&#26159;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25506;&#27979;&#65292;&#36873;&#25321;&#39033;&#21253;&#25324;&#29420;&#25991;&#26412;&#21644;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;BookTection&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21069;&#21644;&#20043;&#21518;&#20986;&#29256;&#30340;165&#26412;&#20070;&#30340;&#25688;&#24405;&#20197;&#21450;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DE-COP&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#65292;&#26816;&#27979;&#24615;&#33021;&#65288;AUC&#65289;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20339;&#26041;&#27861;9.6%&#12290;&#27492;&#22806;&#65292;DE-COP&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#26816;&#27979;&#21487;&#30097;&#20070;&#31821;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;72%&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#26377;$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09910v1 Announce Type: new  Abstract: How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09906</link><description>&lt;p&gt;
&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Generative Representational Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#29983;&#25104;&#25110;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#21482;&#33021;&#22312;&#20854;&#20013;&#19968;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#26469;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;GritLM 8x7B&#22312;&#23581;&#35797;&#30340;&#25152;&#26377;&#24320;&#25918;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GRIT&#21487;&#20197;&#19982;&#20165;&#22312;&#29983;&#25104;&#25110;&#23884;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32479;&#19968;&#20004;&#32773;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;GRIT&#30340;&#32479;&#19968;&#21487;&#20197;&#23558;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;60%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09900</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Recurrent Reinforcement Learning with Memory Monoids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20687;RNN&#21644;transformers&#36825;&#26679;&#30340;&#35760;&#24518;&#27169;&#22411;&#36890;&#36807;&#23558;&#36712;&#36857;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#24207;&#21015;&#30340;&#35268;&#27169;&#21270;&#22788;&#29702;&#33021;&#21147;&#24182;&#19981;&#29305;&#21035;&#22909;&#65292;&#23588;&#20854;&#26159;&#19982;&#19968;&#31867;&#26032;&#20852;&#30340;&#35760;&#24518;&#27169;&#22411;&#65288;&#26377;&#26102;&#31216;&#20026;&#32447;&#24615;&#24490;&#29615;&#27169;&#22411;&#65289;&#30456;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#24490;&#29615;&#26356;&#26032;&#26159;&#19968;&#20010;&#21333;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20256;&#32479;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#21033;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#22686;&#21152;&#20102;&#22238;&#25253;&#65292;&#24182;&#31616;&#21270;&#20102;&#24490;&#29615;&#20002;&#22833;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;COVID-19&#30456;&#20851;&#35752;&#35770;&#12290;&#36890;&#36807;Twitter&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#21644;&#22810;&#31181;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;COVID-19&#30340;&#20581;&#24247;&#39118;&#38505;&#12289;&#39044;&#38450;&#12289;&#30151;&#29366;&#12289;&#20256;&#25773;&#21644;&#27835;&#30103;&#31561;&#26041;&#38754;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.09897</link><description>&lt;p&gt;
COVIDHealth&#65306;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;COVID-19&#35752;&#35770;&#30340;&#22522;&#20934;Twitter&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Web&#24212;&#29992;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;COVID-19&#30456;&#20851;&#35752;&#35770;&#12290;&#36890;&#36807;Twitter&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#21644;&#22810;&#31181;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;COVID-19&#30340;&#20581;&#24247;&#39118;&#38505;&#12289;&#39044;&#38450;&#12289;&#30151;&#29366;&#12289;&#20256;&#25773;&#21644;&#27835;&#30103;&#31561;&#26041;&#38754;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#23545;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#20135;&#29983;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#27425;&#30123;&#24773;&#20013;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#33719;&#21462;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#35266;&#28857;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;COVID-19&#30456;&#20851;&#35752;&#35770;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;Twitter API&#25910;&#38598;&#20102;&#25968;&#25454;&#65292;&#24182;&#23545;&#20849;6667&#26465;&#25512;&#25991;&#36827;&#34892;&#20102;&#20116;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#26631;&#35760;&#65306;&#20581;&#24247;&#39118;&#38505;&#12289;&#39044;&#38450;&#12289;&#30151;&#29366;&#12289;&#20256;&#25773;&#21644;&#27835;&#30103;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21253;&#25324;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;Adaboost&#12289;K-&#26368;&#36817;&#37051;&#23621;&#12289;&#36923;&#36753;&#22238;&#24402;&#21644;&#32447;&#24615;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#20869;&#30340;&#19971;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09897v1 Announce Type: new  Abstract: The COVID-19 pandemic has had adverse effects on both physical and mental health. During this pandemic, numerous studies have focused on gaining insights into health-related perspectives from social media. In this study, our primary objective is to develop a machine learning-based web application for automatically classifying COVID-19-related discussions on social media. To achieve this, we label COVID-19-related Twitter data, provide benchmark classification results, and develop a web application. We collected data using the Twitter API and labeled a total of 6,667 tweets into five different classes: health risks, prevention, symptoms, transmission, and treatment. We extracted features using various feature extraction methods and applied them to seven different traditional machine learning algorithms, including Decision Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest Neighbour, Logistic Regression, and Linear SVC. 
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#29305;&#24449;&#19981;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#65292;&#39044;&#27979;&#22120;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09891</link><description>&lt;p&gt;
&#39044;&#27979;&#22240;&#26524;&#29305;&#24449;&#19981;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Predictors from causal features do not generalize better to new domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09891
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#29305;&#24449;&#19981;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#65292;&#39044;&#27979;&#22120;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;&#22240;&#26524;&#29305;&#24449;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#28085;&#30422;&#20581;&#24247;&#12289;&#23601;&#19994;&#12289;&#25945;&#32946;&#12289;&#31038;&#20250;&#31119;&#21033;&#21644;&#25919;&#27835;&#31561;&#24212;&#29992;&#30340;16&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#26377;&#22810;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#27979;&#35797;&#19968;&#20010;&#22312;&#19968;&#20010;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#23545;&#20110;&#27599;&#20010;&#39044;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#36873;&#25321;&#23545;&#39044;&#27979;&#30446;&#26631;&#26377;&#22240;&#26524;&#24433;&#21709;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27979;&#35797;&#22522;&#20110;&#22240;&#26524;&#29305;&#24449;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#21542;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#65292;&#20351;&#29992;&#25152;&#26377;&#21487;&#29992;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#37117;&#27604;&#20351;&#29992;&#22240;&#26524;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#22312;&#39046;&#22495;&#20869;&#22806;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;&#26159;&#20174;&#19968;&#20010;&#39046;&#22495;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#32477;&#23545;&#19979;&#38477;&#23545;&#20110;&#22240;&#26524;&#39044;&#27979;&#22120;&#26469;&#35828;&#20063;&#19981;&#27604;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#22914;&#26524;&#30446;&#26631;&#26159;&#22312;&#26032;&#39046;&#22495;&#20013;&#27867;&#21270;&#65292;&#23454;&#36341;&#20013;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09891v1 Announce Type: new  Abstract: We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. If the goal is to generalize to new domains, prac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#30340;&#26680;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#20915;&#31574;&#26641;&#36817;&#20284;&#26680;k-means&#32858;&#31867;&#20998;&#21306;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#21512;&#36866;&#30340;&#29305;&#24449;&#36873;&#25321;&#23454;&#29616;&#20102;&#35299;&#37322;&#24615;&#21644;&#36817;&#20284;&#20445;&#35777;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.09881</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#26641;&#35299;&#37322;&#26680;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explaining Kernel Clustering via Decision Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#30340;&#26680;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#20915;&#31574;&#26641;&#36817;&#20284;&#26680;k-means&#32858;&#31867;&#20998;&#21306;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#21512;&#36866;&#30340;&#29305;&#24449;&#36873;&#25321;&#23454;&#29616;&#20102;&#35299;&#37322;&#24615;&#21644;&#36817;&#20284;&#20445;&#35777;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20851;&#20110;&#22266;&#26377;&#21487;&#35299;&#37322;&#32858;&#31867;&#26041;&#27861;&#30340;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#26368;&#36817;&#65292;&#35299;&#37322;&#32463;&#20856;k-means&#31639;&#27861;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#23548;&#33268;&#20102;&#20351;&#29992;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#36817;&#20284;k-means&#32858;&#31867;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#30340;k-means&#21464;&#31181;&#22312;&#23454;&#36341;&#20013;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#36890;&#24120;&#38656;&#35201;&#26356;&#28789;&#27963;&#30340;&#32858;&#31867;&#26041;&#27861;&#25165;&#33021;&#33719;&#24471;&#26377;&#29992;&#30340;&#25968;&#25454;&#20998;&#21306;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#30340;&#26680;&#32858;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#26500;&#24314;&#20915;&#31574;&#26641;&#26469;&#36817;&#20284;kernel k-means&#24341;&#23548;&#20998;&#21306;&#30340;&#31639;&#27861;&#65292;kernel k-means&#26159;k-means&#30340;&#38750;&#32447;&#24615;&#25193;&#23637;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20511;&#37492;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;k-means&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21512;&#36866;&#30340;&#29305;&#24449;&#36873;&#25321;&#22312;&#19981;&#25439;&#22833;&#35299;&#37322;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#36817;&#20284;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09881v1 Announce Type: new  Abstract: Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpret
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23884;&#20837;&#24335;&#22810;&#26680;&#24179;&#21488;&#19978;&#65292;&#37319;&#29992;&#30005;&#27744;&#20379;&#30005;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35843;&#25972;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#33021;&#37327;&#39044;&#31639;&#20869;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.09867</link><description>&lt;p&gt;
&#22312;&#23884;&#20837;&#24335;&#22810;&#26680;&#24179;&#21488;&#19978;&#34920;&#24449; EEG &#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23884;&#20837;&#24335;&#22810;&#26680;&#24179;&#21488;&#19978;&#65292;&#37319;&#29992;&#30005;&#27744;&#20379;&#30005;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35843;&#25972;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#33021;&#37327;&#39044;&#31639;&#20869;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30005;&#27744;&#20379;&#30005;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#65292;&#20197;&#30417;&#27979;&#33041;&#27963;&#21160;&#21644;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#38271;&#26102;&#38388;&#36830;&#32493;&#22788;&#29702;&#20197;&#29983;&#25104;&#21487;&#34892;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21487;&#31359;&#25140;&#35774;&#22791;&#30001;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#23567;&#23610;&#23544;&#32780;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#33021;&#37327;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#38480;&#21046;&#30340;&#33021;&#37327;&#39044;&#31639;&#20869;&#65292;&#23884;&#20837;&#24335;&#24322;&#26500;&#22810;&#26680;&#24179;&#21488;&#65288;HMPs&#65289;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#21033;&#29992; EEG &#24212;&#29992;&#31243;&#24207;&#27969;&#31243;&#30340;&#38169;&#35823;&#38887;&#24615;&#26469;&#26368;&#22823;&#21270; HMPs &#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;&#22312;&#23884;&#20837;&#24335; HMPs &#19978;&#35268;&#33539;&#35843;&#25972;&#36817;&#20284;&#38656;&#35201;&#23545;&#20934;&#30830;&#24615;-&#24615;&#33021;-&#21151;&#32791;&#26435;&#34913;&#31354;&#38388;&#36827;&#34892;&#24443;&#24213;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#19977;&#31181; EEG &#24212;&#29992;&#65288;&#21253;&#25324;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#12289;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#21644;&#21387;&#21147;&#26816;&#27979;&#65289;&#30340;&#38169;&#35823;&#38887;&#24615;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09867v1 Announce Type: cross  Abstract: Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders. These applications require long and continuous processing to generate feasible results. However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases. Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications. Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs. However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space. In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detecti
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22522;&#20934;&#32447;&#21644;&#22522;&#20934;&#27979;&#35797;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35757;&#32451;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#19981;&#38656;&#35201;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#26159;&#19968;&#20010;&#31526;&#21512;&#35201;&#27714;&#30340;&#24378;&#22823;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09849</link><description>&lt;p&gt;
&#23545;&#20110;&#22522;&#20934;&#32447;&#21644;&#22522;&#20934;&#27979;&#35797;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Recommendations for Baselines and Benchmarking Approximate Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09849
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20934;&#32447;&#21644;&#22522;&#20934;&#27979;&#35797;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#35757;&#32451;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#19981;&#38656;&#35201;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#26159;&#19968;&#20010;&#31526;&#21512;&#35201;&#27714;&#30340;&#24378;&#22823;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs)&#26159;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#31665;&#20013;&#25104;&#29087;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#32452;&#20214;&#12290;&#23427;&#20204;&#20855;&#26377;&#33258;&#21160;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#38656;&#29992;&#25143;&#24178;&#39044;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#35843;&#25972;&#35201;&#27714;&#20351;&#24471;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#65292;&#36825;&#23548;&#33268;&#32570;&#20047;&#23545;&#22312;&#21738;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;&#21738;&#31181;&#26041;&#27861;&#30340;&#26126;&#30830;&#24314;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;GP&#36817;&#20284;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#22522;&#20110;&#29992;&#25143;&#23545;&#26041;&#27861;&#30340;&#26399;&#26395;&#30340;&#35268;&#33539;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35757;&#32451;&#31243;&#24207;&#65292;&#29992;&#20110;Titsias [2009]&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#26159;&#31526;&#21512;&#25105;&#20204;&#35268;&#33539;&#30340;&#19968;&#20010;&#24378;&#22823;&#22522;&#20934;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25353;&#29031;&#25105;&#20204;&#30340;&#24314;&#35758;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#26356;&#28165;&#26224;&#22320;&#20102;&#35299;&#24403;&#21069;&#39046;&#22495;&#30340;&#29366;&#24577;&#65292;&#24182;&#21457;&#29616;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09849v1 Announce Type: new  Abstract: Gaussian processes (GPs) are a mature and widely-used component of the ML toolbox. One of their desirable qualities is automatic hyperparameter selection, which allows for training without user intervention. However, in many realistic settings, approximations are typically needed, which typically do require tuning. We argue that this requirement for tuning complicates evaluation, which has led to a lack of a clear recommendations on which method should be used in which situation. To address this, we make recommendations for comparing GP approximations based on a specification of what a user should expect from a method. In addition, we develop a training procedure for the variational method of Titsias [2009] that leaves no choices to the user, and show that this is a strong baseline that meets our specification. We conclude that benchmarking according to our suggestions gives a clearer view of the current state of the field, and uncovers 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#38477;&#27700;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21488;&#28286;&#22320;&#21306;&#30340;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#37327;&#21270;&#38477;&#27700;&#20272;&#35745;&#65292;&#24182;&#19982;&#20855;&#20307;&#38477;&#27700;&#20301;&#32622;&#30456;&#20851;&#32852;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;Z-R&#20851;&#31995;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#26816;&#27979;&#22825;&#27668;&#31995;&#32479;&#30340;&#28436;&#21464;&#21644;&#31227;&#21160;&#65292;&#24182;&#23558;&#20854;&#19982;&#29305;&#23450;&#22320;&#24418;&#23646;&#24615;&#30340;&#20301;&#32622;&#30456;&#32467;&#21512;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#21488;&#21271;&#22320;&#21306;&#30340;&#38477;&#27700;&#20272;&#35745;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09846</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#38477;&#27700;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Radar-based QPE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#38477;&#27700;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21488;&#28286;&#22320;&#21306;&#30340;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#37327;&#21270;&#38477;&#27700;&#20272;&#35745;&#65292;&#24182;&#19982;&#20855;&#20307;&#38477;&#27700;&#20301;&#32622;&#30456;&#20851;&#32852;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;Z-R&#20851;&#31995;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#26816;&#27979;&#22825;&#27668;&#31995;&#32479;&#30340;&#28436;&#21464;&#21644;&#31227;&#21160;&#65292;&#24182;&#23558;&#20854;&#19982;&#29305;&#23450;&#22320;&#24418;&#23646;&#24615;&#30340;&#20301;&#32622;&#30456;&#32467;&#21512;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#21488;&#21271;&#22320;&#21306;&#30340;&#38477;&#27700;&#20272;&#35745;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#38477;&#27700;&#20272;&#35745;&#21644;&#20351;&#29992;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#23450;&#37327;&#38477;&#27700;&#20272;&#35745;&#21644;&#20998;&#31163;&#65288;QPESUMS&#65289;&#39532;&#36187;&#20811;&#38647;&#36798;&#25968;&#25454;&#38598;&#30340;&#20307;&#31215;&#21040;&#28857;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#21488;&#28286;&#22320;&#21306;&#30340;&#26684;&#32593;&#21270;&#38647;&#36798;&#22238;&#27874;&#26102;&#38388;&#24207;&#21015;&#26500;&#24314;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#31449;&#30340;&#38477;&#27700;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#20174;&#36755;&#20837;&#25968;&#25454;&#20307;&#31215;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20855;&#20307;&#20301;&#32622;&#30340;&#38477;&#27700;&#20851;&#32852;&#36215;&#26469;&#12290;&#19982;&#22522;&#20110;Z-R&#20851;&#31995;&#30340;&#38477;&#27700;&#20272;&#35745;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#26816;&#27979;&#22825;&#27668;&#31995;&#32479;&#30340;&#28436;&#21464;&#21644;&#31227;&#21160;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#24335;&#19982;&#20855;&#26377;&#29305;&#23450;&#22320;&#24418;&#23646;&#24615;&#30340;&#20301;&#32622;&#20851;&#32852;&#36215;&#26469;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;2013&#24180;&#21488;&#21271;&#22320;&#21306;45&#20010;&#22825;&#27668;&#31449;&#30340;&#23567;&#26102;&#38477;&#27700;&#25968;&#25454;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09846v1 Announce Type: cross  Abstract: In this study, we propose a volume-to-point framework for quantitative precipitation estimation (QPE) based on the Quantitative Precipitation Estimation and Segregation Using Multiple Sensor (QPESUMS) Mosaic Radar data set. With a data volume consisting of the time series of gridded radar reflectivities over the Taiwan area, we used machine learning algorithms to establish a statistical model for QPE in weather stations. The model extracts spatial and temporal features from the input data volume and then associates these features with the location-specific precipitations. In contrast to QPE methods based on the Z-R relation, we leverage the machine learning algorithms to automatically detect the evolution and movement of weather systems and associate these patterns to a location with specific topographic attributes. Specifically, we evaluated this framework with the hourly precipitation data of 45 weather stations in Taipei during 2013
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20351;&#29992;&#24067;&#23616;&#22686;&#24378;&#26469;&#20351;&#29992;&#32431;&#25991;&#26412;LLM&#36827;&#34892;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09841</link><description>&lt;p&gt;
LAPDoc&#65306;&#38754;&#21521;&#25991;&#26723;&#30340;&#24067;&#23616;&#24863;&#30693;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
LAPDoc: Layout-Aware Prompting for Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20351;&#29992;&#24067;&#23616;&#22686;&#24378;&#26469;&#20351;&#29992;&#32431;&#25991;&#26412;LLM&#36827;&#34892;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#37327;&#32431;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35757;&#32451;&#38024;&#23545;&#25991;&#26723;&#29702;&#35299;&#30340;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26723;&#24067;&#23616;&#34701;&#21512;&#12290;&#36825;&#38656;&#35201;&#21333;&#29420;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30446;&#21069;&#65292;&#23578;&#27809;&#26377;&#20855;&#26377;&#19982;LLM&#30456;&#24403;&#27867;&#21270;&#33021;&#21147;&#30340;&#25991;&#26723;&#21464;&#21387;&#22120;&#21487;&#29992;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24067;&#23616;&#22686;&#24378;&#26469;&#35843;&#26597;&#20351;&#29992;&#32431;&#25991;&#26412;LLM&#29992;&#20110;&#25991;&#26723;&#29305;&#23450;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#28155;&#21152;&#20462;&#25913;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#32431;&#25991;&#26412;LLM&#25552;&#31034;&#20013;&#28155;&#21152;&#24067;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09841v1 Announce Type: new  Abstract: Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experimen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28176;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#37096;&#32626;&#31574;&#30053;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;MDRR&#26469;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.09838</link><description>&lt;p&gt;
&#28176;&#21464;&#29615;&#22659;&#20013;&#30340;&#34920;&#28436;&#24615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Performative Reinforcement Learning in Gradually Shifting Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28176;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#27169;&#25311;&#37096;&#32626;&#31574;&#30053;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;MDRR&#26469;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#24433;&#21709;&#29615;&#22659;&#24182;&#25913;&#21464;&#20854;&#21160;&#24577;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#24418;&#24335;&#21270;&#24314;&#27169;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#20998;&#26512;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#24403;&#21069;&#30340;&#29615;&#22659;&#21462;&#20915;&#20110;&#37096;&#32626;&#31574;&#30053;&#21450;&#20854;&#20808;&#21069;&#30340;&#21160;&#24577;&#12290;&#36825;&#26159;Performative RL&#65288;PRL&#65289;[Mandal et al., 2023]&#30340;&#19968;&#31181;&#27867;&#21270;&#12290;&#19982;PRL&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#23545;&#29615;&#22659;&#36880;&#28176;&#35843;&#25972;&#21040;&#37096;&#32626;&#31574;&#30053;&#30340;&#24773;&#26223;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#34920;&#28436;&#24615;&#39044;&#27979;&#25991;&#29486;&#20013;&#30340;&#20004;&#31181;&#31639;&#27861;&#36866;&#24212;&#21040;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31216;&#20026;&#28151;&#21512;&#24310;&#36831;&#37325;&#22797;&#35757;&#32451;&#65288;MDRR&#65289;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#25910;&#25947;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#65306;&#37325;&#35757;&#32451;&#27425;&#25968;&#65292;&#36924;&#36817;&#20445;&#35777;&#21644;&#27599;&#27425;&#37096;&#32626;&#30340;&#26679;&#26412;&#25968;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDRR&#32467;&#21512;&#20102;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09838v1 Announce Type: new  Abstract: When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines sample
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20307;&#21270;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#22791;&#20102;&#36229;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09834</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#19982;&#22810;&#21151;&#33021;&#24615;&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20307;&#21270;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#22791;&#20102;&#36229;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;LLMs&#26368;&#26174;&#33879;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#65292;&#22312;&#24191;&#27867;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#21333;&#19968;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#65292;&#36825;&#31181;&#33539;&#24335;&#34987;&#31216;&#20026;&#8220;&#19968;&#20307;&#21270;&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#20855;&#22791;&#20102;&#36229;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#12290;&#20511;&#21161;&#36825;&#20123;&#33021;&#21147;&#65292;&#21333;&#19968;&#30340;LLM&#22312;&#21508;&#31181;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36825;&#31181;&#33539;&#24335;&#34987;&#31216;&#20026;&#8220;&#22810;&#21151;&#33021;&#19968;&#20307;&#21270;&#8221;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20010;&#24819;&#27861;&#24212;&#29992;&#20110;&#22270;&#39046;&#22495;&#20173;&#28982;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#32463;&#24120;&#23548;&#33268;&#36127;&#36801;&#31227;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#21294;&#20047;&#38656;&#35201;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#28304;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09834v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#26500;&#24314;&#23545;&#25239;&#24615;&#39564;&#35777;&#22270;&#30340;&#38598;&#21512;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#30001;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#31995;&#32479;&#24341;&#36215;&#30340;&#27450;&#35784;&#65292;&#24182;&#30830;&#20445;&#20132;&#26131;&#20013;&#30340;&#29992;&#25143;&#26159;&#30495;&#23454;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09830</link><description>&lt;p&gt;
&#21033;&#29992;GAN&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65306;&#20351;&#29992;&#21512;&#25104;&#20132;&#26131;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#27604;&#36739;&#20102;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#26500;&#24314;&#23545;&#25239;&#24615;&#39564;&#35777;&#22270;&#30340;&#38598;&#21512;&#65292;&#26377;&#25928;&#38450;&#27490;&#20102;&#30001;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#31995;&#32479;&#24341;&#36215;&#30340;&#27450;&#35784;&#65292;&#24182;&#30830;&#20445;&#20132;&#26131;&#20013;&#30340;&#29992;&#25143;&#26159;&#30495;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#26088;&#22312;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#30340;&#23454;&#20363;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#24212;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30340;&#20248;&#21183;&#12290;GAN&#26159;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#31867;&#22411;&#65292;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65292;&#20351;&#20854;&#25104;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#25551;&#36848;&#20102;GAN&#21450;&#20854;&#34893;&#29983;&#27169;&#22411;&#30340;&#21407;&#21017;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27450;&#35784;&#26816;&#27979;&#24212;&#29992;&#12290;&#36890;&#36807;&#26500;&#24314;&#23545;&#25239;&#24615;&#39564;&#35777;&#22270;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#38450;&#27490;&#30001;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#31995;&#32479;&#24341;&#36215;&#30340;&#27450;&#35784;&#65292;&#24182;&#30830;&#20445;&#20132;&#26131;&#20013;&#30340;&#29992;&#25143;&#26159;&#30495;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09830v1 Announce Type: cross  Abstract: Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods. GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection. The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.09821</link><description>&lt;p&gt;
&#38899;&#39057;&#24674;&#22797;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Audio Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38899;&#39057;&#25773;&#25918;&#35774;&#22791;&#21644;&#24555;&#36895;&#25968;&#25454;&#20256;&#36755;&#30340;&#21457;&#23637;&#65292;&#23545;&#39640;&#38899;&#36136;&#30340;&#38656;&#27714;&#22312;&#23089;&#20048;&#21644;&#36890;&#20449;&#39046;&#22495;&#19981;&#26029;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24405;&#21046;&#36807;&#31243;&#20013;&#30340;&#22833;&#30495;&#21644;&#24178;&#25200;&#65292;&#25110;&#32773;&#30001;&#20110;&#19981;&#23436;&#21892;&#30340;&#20256;&#36755;&#31649;&#36947;&#65292;&#38899;&#39057;&#36136;&#37327;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38899;&#39057;&#24674;&#22797;&#26041;&#27861;&#26088;&#22312;&#20174;&#25439;&#22351;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#24674;&#22797;&#20986;&#28165;&#26224;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#25163;&#24037;&#35268;&#21017;&#21644;&#32479;&#35745;&#21551;&#21457;&#27861;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#25105;&#20204;&#23545;&#38899;&#39057;&#20449;&#21495;&#30340;&#35748;&#35782;&#12290;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#36716;&#21521;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24314;&#27169;&#33021;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09821v1 Announce Type: cross  Abstract: With the development of audio playback devices and fast data transmission, the demand for high sound quality is rising, for both entertainment and communications. In this quest for better sound quality, challenges emerge from distortions and interferences originating at the recording side or caused by an imperfect transmission pipeline. To address this problem, audio restoration methods aim to recover clean sound signals from the corrupted input data. We present here audio restoration algorithms based on diffusion models, with a focus on speech enhancement and music restoration tasks. Traditional approaches, often grounded in handcrafted rules and statistical heuristics, have shaped our understanding of audio signals. In the past decades, there has been a notable shift towards data-driven methods that exploit the modeling capabilities of deep neural networks (DNNs). Deep generative models, and among them diffusion models, have emerged 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.09820</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#26102;&#20195;&#65292;&#20154;&#20204;&#30340;&#29983;&#27963;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#20170;&#22825;&#30340;&#32593;&#32476;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#25216;&#26415;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20415;&#21033;&#30340;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#23433;&#20840;&#25361;&#25112;&#12290;&#20445;&#25345;&#32593;&#32476;&#23433;&#20840;&#21644;&#20445;&#25252;&#29992;&#25143;&#30340;&#21512;&#27861;&#21033;&#30410;&#26159;&#32593;&#32476;&#24314;&#35774;&#30340;&#26680;&#24515;&#12290;&#23041;&#32961;&#26816;&#27979;&#26159;&#19968;&#20010;&#23436;&#25972;&#26377;&#25928;&#30340;&#38450;&#24481;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#32593;&#32476;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#65292;&#32593;&#32476;&#25915;&#20987;&#21644;&#32593;&#32476;&#38450;&#25252;&#30340;&#25216;&#26415;&#26356;&#26032;&#26085;&#30410;&#36805;&#29467;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#26159;&#32593;&#32476;&#38450;&#25252;&#30340;&#20851;&#27880;&#28966;&#28857;&#20043;&#19968;&#12290;&#30446;&#21069;&#65292;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#36890;&#24120;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21019;&#24314;&#20154;&#24037;&#35268;&#21017;&#25110;&#25552;&#21462;&#24120;&#35265;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26410;&#30693;&#23041;&#32961;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#31995;&#32479;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09820v1 Announce Type: cross  Abstract: In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20449;&#36182;&#22495;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#24378;&#20985;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#22312;&#36845;&#20195;&#27425;&#25968;&#20026;$\mathcal{O}(\epsilon^{-1.5})$&#20869;&#25214;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.09807</link><description>&lt;p&gt;
&#35299;&#20915;&#38750;&#20984;&#24378;&#20985;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#30340;&#20004;&#31181;&#20449;&#36182;&#22495;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two trust region type algorithms for solving nonconvex-strongly concave minimax problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20449;&#36182;&#22495;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#24378;&#20985;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#65292;&#24182;&#21487;&#20197;&#22312;&#36845;&#20195;&#27425;&#25968;&#20026;$\mathcal{O}(\epsilon^{-1.5})$&#20869;&#25214;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#38750;&#20984;&#24378;&#20985;&#26368;&#23567;&#26368;&#22823;&#38382;&#39064;&#30340;&#26368;&#23567;&#26368;&#22823;&#20449;&#36182;&#22495;&#65288;MINIMAX-TR&#65289;&#31639;&#27861;&#21644;&#20855;&#26377;&#25910;&#32553;&#21644;&#25193;&#24352;&#30340;&#26368;&#23567;&#26368;&#22823;&#20449;&#36182;&#22495;&#31639;&#27861;&#65288;MINIMAX-TRACE&#65289;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;$\mathcal{O}(\epsilon^{-1.5})$&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;$(\epsilon, \sqrt{\epsilon})$-&#20108;&#38454;&#31283;&#23450;&#28857;(SSP)&#65292;&#36825;&#19982;&#24050;&#30693;&#26368;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09807v1 Announce Type: cross  Abstract: In this paper, we propose a Minimax Trust Region (MINIMAX-TR) algorithm and a Minimax Trust Region Algorithm with Contractions and Expansions(MINIMAX-TRACE) algorithm for solving nonconvex-strongly concave minimax problems. Both algorithms can find an $(\epsilon, \sqrt{\epsilon})$-second order stationary point(SSP) within $\mathcal{O}(\epsilon^{-1.5})$ iterations, which matches the best well known iteration complexity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#23545;&#20110;&#25439;&#22833;&#30340;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;CVaR&#21644;DRO&#30340;&#32467;&#26524;&#36828;&#36229;&#20986;&#29616;&#26377;&#30740;&#31350;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#21333;&#35843;&#20934;&#21017;&#22914;&#20542;&#26012;ERM&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;</title><link>https://arxiv.org/abs/2402.09802</link><description>&lt;p&gt;
&#20934;&#21017;&#23849;&#28291;&#21644;&#25439;&#22833;&#20998;&#24067;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Criterion collapse and loss distribution control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#23545;&#20110;&#25439;&#22833;&#30340;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;CVaR&#21644;DRO&#30340;&#32467;&#26524;&#36828;&#36229;&#20986;&#29616;&#26377;&#30740;&#31350;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#21333;&#35843;&#20934;&#21017;&#22914;&#20542;&#26012;ERM&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#21508;&#31181;&#23398;&#20064;&#20934;&#21017;&#19979;&#23849;&#28291;&#25104;&#35823;&#24046;&#27010;&#29575;&#26368;&#23567;&#21270;&#22120;&#30340;&#26465;&#20214;&#65292;&#20174;DRO&#21644;OCE&#39118;&#38505;&#65288;CVaR&#12289;&#20542;&#26012;ERM&#65289;&#21040;&#25991;&#29486;&#20013;&#25506;&#32034;&#30340;&#26368;&#26032;&#19978;&#21319;-&#19979;&#38477;&#31639;&#27861;&#30340;&#38750;&#21333;&#35843;&#20934;&#21017;&#65288;&#27946;&#27700;&#12289;SoftAD&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20271;&#21162;&#21033;&#20998;&#24067;&#25439;&#22833;&#30340;&#32972;&#26223;&#19979;&#65292;CVaR&#21644;DRO&#30340;&#29616;&#26377;&#32467;&#26524;&#36828;&#36828;&#36229;&#36234;&#20102;&#23849;&#28291;&#30340;&#33539;&#22260;&#65292;&#28982;&#21518;&#25193;&#22823;&#20102;&#25105;&#20204;&#30340;&#33539;&#22260;&#65292;&#21253;&#25324;&#20195;&#29702;&#25439;&#22833;&#65292;&#23637;&#31034;&#20102;&#20687;&#20542;&#26012;ERM&#36825;&#26679;&#30340;&#21333;&#35843;&#20934;&#21017;&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#30340;&#26465;&#20214;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09802v1 Announce Type: cross  Abstract: In this work, we consider the notion of "criterion collapse," in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD). We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;PSD&#27169;&#22411;&#30340;&#26032;&#22411;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#22312;&#36716;&#25442;&#21644;&#35266;&#27979;&#37117;&#26159;&#39640;&#26031;PSD&#27169;&#22411;&#26102;&#20197;&#38381;&#24335;&#24418;&#24335;&#39640;&#25928;&#22320;&#36827;&#34892;&#28388;&#27874;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36866;&#24212;&#36716;&#25442;&#27010;&#29575;&#30340;&#27491;&#21017;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09796</link><description>&lt;p&gt;
&#38381;&#24335;&#28388;&#27874;&#22120;&#22312;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Closed-form Filtering for Non-linear Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09796
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;PSD&#27169;&#22411;&#30340;&#26032;&#22411;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#22312;&#36716;&#25442;&#21644;&#35266;&#27979;&#37117;&#26159;&#39640;&#26031;PSD&#27169;&#22411;&#26102;&#20197;&#38381;&#24335;&#24418;&#24335;&#39640;&#25928;&#22320;&#36827;&#34892;&#28388;&#27874;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36866;&#24212;&#36716;&#25442;&#27010;&#29575;&#30340;&#27491;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#36125;&#21494;&#26031;&#28388;&#27874;&#26088;&#22312;&#20272;&#35745;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#20998;&#24067;&#65292;&#32473;&#23450;&#36807;&#21435;&#30340;&#35266;&#27979;&#20540;&#12290;&#23545;&#20110;&#22823;&#22810;&#25968;&#24212;&#29992;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#65292;&#38500;&#20102;&#20687;&#34920;&#26684;&#35774;&#32622;&#25110;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#36825;&#26679;&#30340;&#26126;&#26174;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;PSD&#27169;&#22411;&#30340;&#26032;&#22411;&#28388;&#27874;&#22120;&#65292;&#23427;&#22312;&#23494;&#24230;&#36817;&#20284;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#36716;&#25442;&#21644;&#35266;&#27979;&#37117;&#26159;&#39640;&#26031;PSD&#27169;&#22411;&#26102;&#65292;&#28388;&#27874;&#21487;&#20197;&#20197;&#38381;&#24335;&#24418;&#24335;&#39640;&#25928;&#22320;&#36827;&#34892;&#12290;&#24403;&#36716;&#25442;&#21644;&#35266;&#27979;&#34987;&#39640;&#26031;PSD&#27169;&#22411;&#36817;&#20284;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20272;&#35745;&#35823;&#24046;&#21462;&#20915;&#20110;&#36817;&#20284;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#36866;&#24212;&#36716;&#25442;&#27010;&#29575;&#30340;&#27491;&#21017;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#20854;&#20013;&#25105;&#20204;&#21487;&#20197;&#20197;&#38381;&#24335;&#24418;&#24335;&#39640;&#25928;&#22320;&#36827;&#34892;&#28388;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09796v1 Announce Type: cross  Abstract: Sequential Bayesian Filtering aims to estimate the current state distribution of a Hidden Markov Model, given the past observations. The problem is well-known to be intractable for most application domains, except in notable cases such as the tabular setting or for linear dynamical systems with gaussian noise. In this work, we propose a new class of filters based on Gaussian PSD Models, which offer several advantages in terms of density approximation and computational efficiency. We show that filtering can be efficiently performed in closed form when transitions and observations are Gaussian PSD Models. When the transition and observations are approximated by Gaussian PSD Models, we show that our proposed estimator enjoys strong theoretical guarantees, with estimation error that depends on the quality of the approximation and is adaptive to the regularity of the transition probabilities. In particular, we identify regimes in which our 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09782</link><description>&lt;p&gt;
MC-DBN&#65306;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MC-DBN: A Deep Belief Network-Based Model for Modality Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09782
&lt;/p&gt;
&lt;p&gt;
MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#39046;&#22495;&#12290;&#21033;&#29992;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#25968;&#25454;&#21487;&#33021;&#19981;&#24635;&#26159;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#21563;&#21512;&#12290;&#25554;&#20540;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20294;&#22312;&#31232;&#30095;&#20449;&#24687;&#24773;&#20917;&#19979;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#34917;&#20840;&#30340;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#27169;&#22411;&#65288;MC-DBN&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#33258;&#36523;&#19982;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#30830;&#20445;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29305;&#24615;&#23494;&#20999;&#30456;&#31526;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26469;&#33258;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;MC-DBN&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09782v1 Announce Type: cross  Abstract: Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate
&lt;/p&gt;</description></item><item><title>TinyCL&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#39640;&#25928;&#30828;&#20214;&#26550;&#26500;&#65292;&#22312;CL&#20013;&#25903;&#25345;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#26469;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#12290;</title><link>https://arxiv.org/abs/2402.09780</link><description>&lt;p&gt;
TinyCL:&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#39640;&#25928;&#30828;&#20214;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09780
&lt;/p&gt;
&lt;p&gt;
TinyCL&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#39640;&#25928;&#30828;&#20214;&#26550;&#26500;&#65292;&#22312;CL&#20013;&#25903;&#25345;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#26469;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#33539;&#24335;&#21253;&#25324;&#19981;&#26029;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36880;&#27493;&#23398;&#20064;&#25191;&#34892;&#26032;&#20219;&#21153;&#65292;&#32780;&#19981;&#38477;&#20302;&#20808;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#36991;&#20813;&#25152;&#35859;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;CL&#30340;&#33258;&#20027;&#31995;&#32479;&#20013;&#65292;DNN&#21442;&#25968;&#26356;&#26032;&#23545;&#36164;&#28304;&#35201;&#27714;&#26497;&#39640;&#12290;&#29616;&#26377;&#30340;DNN&#21152;&#36895;&#22120;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;CL&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#25903;&#25345;&#21069;&#21521;&#20256;&#25773;&#30340;&#25191;&#34892;&#12290;&#21482;&#26377;&#23569;&#25968;&#20808;&#21069;&#30340;&#26550;&#26500;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;CL&#30340;&#25511;&#21046;&#21644;&#31649;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30828;&#20214;&#26550;&#26500;TinyCL&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#20027;&#31995;&#32479;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#25191;&#34892;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#22788;&#29702;&#21333;&#20803;&#65292;&#20197;&#21450;&#19968;&#20010;&#31649;&#29702;&#22522;&#20110;&#20869;&#23384;&#30340;CL&#24037;&#20316;&#36127;&#36733;&#30340;&#25511;&#21046;&#21333;&#20803;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#20869;&#23384;&#35775;&#38382;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#28369;&#21160;&#31383;&#21475;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09780v1 Announce Type: new  Abstract: The Continuous Learning (CL) paradigm consists of continuously evolving the parameters of the Deep Neural Network (DNN) model to progressively learn to perform new tasks without reducing the performance on previous tasks, i.e., avoiding the so-called catastrophic forgetting. However, the DNN parameter update in CL-based autonomous systems is extremely resource-hungry. The existing DNN accelerators cannot be directly employed in CL because they only support the execution of the forward propagation. Only a few prior architectures execute the backpropagation and weight update, but they lack the control and management for CL. Towards this, we design a hardware architecture, TinyCL, to perform CL on resource-constrained autonomous systems. It consists of a processing unit that executes both forward and backward propagation, and a control unit that manages memory-based CL workload. To minimize the memory accesses, the sliding window of the con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2402.09766</link><description>&lt;p&gt;
&#20174;&#21464;&#21160;&#24615;&#21040;&#31283;&#23450;&#24615;&#65306;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#21270;&#23454;&#36341;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
From Variability to Stability: Advancing RecSys Benchmarking Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#26032;&#30340;&#31639;&#27861;&#32463;&#24120;&#36890;&#36807;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#20219;&#24847;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#22768;&#31216;&#33258;&#24049;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20840;&#38754;&#21453;&#26144;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#36827;&#35780;&#20272;&#23454;&#36341;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#25324;&#26412;&#25991;&#20171;&#32461;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;30&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;9&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;11&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23558;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#32858;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#25490;&#21517;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09766v1 Announce Type: cross  Abstract: In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#20154;&#31867;&#27493;&#24577;&#27169;&#24335;&#24182;&#20272;&#35745;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.09761</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#24815;&#24615;&#20256;&#24863;&#22120;&#36827;&#34892;&#27493;&#24577;&#29992;&#25143;&#20154;&#21475;&#32479;&#35745;&#20272;&#35745;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Gait-Based User Demography Estimation Using Inertial Sensors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09761
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#20154;&#31867;&#27493;&#24577;&#27169;&#24335;&#24182;&#20272;&#35745;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27493;&#24577;&#24050;&#34987;&#35777;&#26126;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#21160;&#20316;&#32447;&#32034;&#12290;&#35782;&#21035;&#20154;&#31867;&#27493;&#24577;&#30340;&#27169;&#24335;&#22312;&#23433;&#20840;&#12289;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#12289;&#21307;&#23398;&#24247;&#22797;&#21644;&#30142;&#30149;&#35782;&#21035;&#31561;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#27492;&#22806;&#65292;&#21487;&#31359;&#25140;&#24815;&#24615;&#20256;&#24863;&#22120;&#19981;&#20165;&#24191;&#27867;&#29992;&#20110;&#35760;&#24405;&#27493;&#24577;&#65292;&#36824;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12290;&#28145;&#24230;&#23398;&#20064;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;&#24815;&#24615;&#20256;&#24863;&#22120;&#20449;&#21495;&#30456;&#32467;&#21512;&#65292;&#22312;&#35782;&#21035;&#20154;&#31867;&#27493;&#24577;&#27169;&#24335;&#21644;&#20272;&#35745;&#29992;&#25143;&#20154;&#21475;&#32479;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#25581;&#31034;&#27169;&#22411;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#26469;&#35782;&#21035;&#22312;&#35782;&#21035;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#30340;&#37325;&#35201;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09761v1 Announce Type: cross  Abstract: Human gait has been shown to provide crucial motion cues for various applications. Recognizing patterns in human gait has been widely adopted in various application areas such as security, virtual reality gaming, medical rehabilitation, and ailment identification. Furthermore, wearable inertial sensors have been widely used for not only recording gait but also to predict users' demography. Machine Learning techniques such as deep learning, combined with inertial sensor signals, have shown promising results in recognizing patterns in human gait and estimate users' demography. However, the black-box nature of such deep learning models hinders the researchers from uncovering the reasons behind the model's predictions. Therefore, we propose leveraging deep learning and Layer-Wise Relevance Propagation (LRP) to identify the important variables that play a vital role in identifying the users' demography such as age and gender. To assess the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29699;&#24418;&#21333;&#20301;&#27491;&#21017;&#21270;SVD&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;SVD&#36924;&#36817;&#65292;&#35813;&#31639;&#27861;&#19981;&#21463;&#24322;&#24120;&#20540;&#24178;&#25200;&#65292;&#35745;&#31639;&#21487;&#20280;&#32553;&#65292;&#24182;&#33021;&#25552;&#20379;&#20934;&#30830;&#30340;&#22855;&#24322;&#21521;&#37327;&#36924;&#36817;&#12290;&#30456;&#27604;&#31454;&#20105;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#20351;&#29992;&#26631;&#20934;&#38477;&#31209;SVD&#31639;&#27861;&#20004;&#27425;&#24212;&#29992;&#20110;&#36866;&#24403;&#32553;&#25918;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#36895;&#24230;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.09754</link><description>&lt;p&gt;
Robust SVD&#21464;&#24471;&#31616;&#21333;&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#30340;&#24555;&#36895;&#21487;&#38752;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust SVD Made Easy: A fast and reliable algorithm for large-scale data analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29699;&#24418;&#21333;&#20301;&#27491;&#21017;&#21270;SVD&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;SVD&#36924;&#36817;&#65292;&#35813;&#31639;&#27861;&#19981;&#21463;&#24322;&#24120;&#20540;&#24178;&#25200;&#65292;&#35745;&#31639;&#21487;&#20280;&#32553;&#65292;&#24182;&#33021;&#25552;&#20379;&#20934;&#30830;&#30340;&#22855;&#24322;&#21521;&#37327;&#36924;&#36817;&#12290;&#30456;&#27604;&#31454;&#20105;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20165;&#20351;&#29992;&#26631;&#20934;&#38477;&#31209;SVD&#31639;&#27861;&#20004;&#27425;&#24212;&#29992;&#20110;&#36866;&#24403;&#32553;&#25918;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#36895;&#24230;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#25968;&#25454;&#30697;&#38453;&#20013;&#30340;&#24322;&#24120;&#20540;&#38750;&#24120;&#25935;&#24863;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;SVD&#31639;&#27861;&#24448;&#24448;&#22312;&#20445;&#35777;&#40065;&#26834;&#24615;&#26041;&#38754;&#29306;&#29298;&#20102;&#36895;&#24230;&#65292;&#25110;&#32773;&#22312;&#21482;&#26377;&#23569;&#25968;&#24322;&#24120;&#20540;&#23384;&#22312;&#26102;&#22833;&#25928;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#24230;&#19981;&#21463;&#24322;&#24120;&#20540;&#24178;&#25200;&#65292;&#35745;&#31639;&#21487;&#20280;&#32553;&#19988;&#25552;&#20379;&#20934;&#30830;&#22855;&#24322;&#21521;&#37327;&#36924;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#31216;&#20026;&#29699;&#24418;&#21333;&#20301;&#27491;&#21017;&#21270;SVD&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#26631;&#20934;&#38477;&#31209;SVD&#31639;&#27861;&#30340;&#20004;&#20010;&#24212;&#29992;&#20110;&#36866;&#24403;&#32553;&#25918;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#36895;&#24230;&#20248;&#21183;&#65292;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#31639;&#27861;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#20026;&#20102;&#35780;&#20272;&#36924;&#36817;&#22855;&#24322;&#21521;&#37327;&#21450;&#20854;&#23376;&#31354;&#38388;&#30340;&#25239;&#25968;&#25454;&#27745;&#26579;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30697;&#38453;&#20540;&#36755;&#20837;&#30340;&#26032;&#30340;&#22833;&#25928;&#28857;&#27010;&#24565;&#65292;&#21253;&#25324;&#36880;&#34892;&#65292;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09754v1 Announce Type: new  Abstract: The singular value decomposition (SVD) is a crucial tool in machine learning and statistical data analysis. However, it is highly susceptible to outliers in the data matrix. Existing robust SVD algorithms often sacrifice speed for robustness or fail in the presence of only a few outliers. This study introduces an efficient algorithm, called Spherically Normalized SVD, for robust SVD approximation that is highly insensitive to outliers, computationally scalable, and provides accurate approximations of singular vectors. The proposed algorithm achieves remarkable speed by utilizing only two applications of a standard reduced-rank SVD algorithm to appropriately scaled data, significantly outperforming competing algorithms in computation times. To assess the robustness of the approximated singular vectors and their subspaces against data contamination, we introduce new notions of breakdown points for matrix-valued input, including row-wise, c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26041;&#38754;&#12290;&#22823;&#27169;&#22411;&#30340;&#31361;&#20986;&#29305;&#28857;&#26159;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.09748</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Model Compression and Efficient Inference for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09748
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#31561;&#26041;&#38754;&#12290;&#22823;&#27169;&#22411;&#30340;&#31361;&#20986;&#29305;&#28857;&#26159;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#30456;&#20851;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25152;&#20135;&#29983;&#30340;&#26174;&#33879;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20174;&#31639;&#27861;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#25512;&#29702;&#26041;&#27861;&#12290;&#22312;&#20998;&#31867;&#26041;&#38754;&#65292;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#31867;&#20284;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#21644;&#21152;&#36895;&#31639;&#27861;&#20173;&#21487;&#20197;&#20998;&#20026;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#33976;&#39311;&#12289;&#32039;&#20945;&#26550;&#26500;&#35774;&#35745;&#21644;&#21160;&#24577;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#19982;&#36739;&#23567;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26377;&#20004;&#20010;&#31361;&#20986;&#30340;&#29305;&#28857;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#21387;&#32553;&#31639;&#27861;&#22312;&#21387;&#32553;&#21518;&#38656;&#35201;&#24494;&#35843;&#29978;&#33267;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22823;&#27169;&#22411;&#26368;&#26174;&#33879;&#30340;&#26041;&#38754;&#26159;&#19982;&#27169;&#22411;&#24494;&#35843;&#25110;&#35757;&#32451;&#30456;&#20851;&#30340;&#38750;&#24120;&#39640;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#38024;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#31639;&#27861;&#37117;&#38656;&#35201;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09748v1 Announce Type: cross  Abstract: Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#25552;&#39640;&#20102;&#35270;&#32593;&#33180;&#30142;&#30149;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#35270;&#32593;&#33180;OCT&#22270;&#20687;&#33719;&#21462;&#21644;&#26631;&#31614;&#36807;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.09747</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#38598;&#25104;&#23398;&#20064;&#22312;&#35270;&#32593;&#33180;&#30142;&#30149;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#25552;&#39640;&#20102;&#35270;&#32593;&#33180;&#30142;&#30149;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#35270;&#32593;&#33180;OCT&#22270;&#20687;&#33719;&#21462;&#21644;&#26631;&#31614;&#36807;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#22270;&#20687;&#23545;&#20110;&#21518;&#30524;&#27573;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25512;&#36827;&#33258;&#21160;&#21270;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#23545;&#20110;&#20026;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#37327;&#21270;&#25968;&#25454;&#12289;&#20419;&#36827;&#30693;&#24773;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26041;&#27861;&#22312;&#25191;&#34892;&#36825;&#20123;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#19982;&#32791;&#26102;&#32321;&#37325;&#30340;&#25163;&#21160;&#20998;&#26512;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35270;&#32593;&#33180;OCT&#22270;&#20687;&#30340;&#33719;&#21462;&#24448;&#24448;&#23384;&#22312;&#26469;&#33258;&#38544;&#31169;&#38382;&#39064;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#26631;&#31614;&#36807;&#31243;&#30340;&#25361;&#25112;&#65292;&#36825;&#19982;DL&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#25165;&#33021;&#21462;&#24471;&#20248;&#31168;&#24615;&#33021;&#30340;&#26222;&#36941;&#35266;&#24565;&#30456;&#30683;&#30462;&#12290;&#27492;&#22806;&#65292;&#21487;&#29992;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#39640;&#24615;&#33021;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#25968;&#36164;&#28304;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09747v1 Announce Type: cross  Abstract: Retinal optical coherence tomography (OCT) images provide crucial insights into the health of the posterior ocular segment. Therefore, the advancement of automated image analysis methods is imperative to equip clinicians and researchers with quantitative data, thereby facilitating informed decision-making. The application of deep learning (DL)-based approaches has gained extensive traction for executing these analysis tasks, demonstrating remarkable performance compared to labor-intensive manual analyses. However, the acquisition of Retinal OCT images often presents challenges stemming from privacy concerns and the resource-intensive labeling procedures, which contradicts the prevailing notion that DL models necessitate substantial data volumes for achieving superior performance. Moreover, limitations in available computational resources constrain the progress of high-performance medical artificial intelligence, particularly in less de
&lt;/p&gt;</description></item><item><title>QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09739</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;QuRating&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuRating: Selecting High-Quality Data for Training Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09739
&lt;/p&gt;
&lt;p&gt;
QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#21019;&#24314;&#33021;&#21147;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#24456;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QuRating&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#20010;&#29305;&#24449; - &#20889;&#20316;&#39118;&#26684;&#12289;&#25152;&#38656;&#19987;&#19994;&#30693;&#35782;&#12289;&#20107;&#23454;&#21644;&#29712;&#20107;&#20197;&#21450;&#25945;&#32946;&#20215;&#20540;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36776;&#21035;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#36827;&#34892;&#25991;&#26412;&#30340;&#37197;&#23545;&#21028;&#26029;&#26041;&#38754;&#27604;&#30452;&#25509;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26356;&#22909;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;QuRater&#27169;&#22411;&#65292;&#20174;&#37197;&#23545;&#21028;&#26029;&#20013;&#23398;&#20064;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#23427;&#20026;260B&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#27599;&#20010;&#26631;&#20934;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#27880;&#37322;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#36136;&#37327;&#35780;&#32423;&#36873;&#25321;&#20102;30B&#20010;&#20196;&#29260;&#65292;&#24182;&#22312;&#25152;&#36873;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;13&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09739v1 Announce Type: new  Abstract: Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts &amp; trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DFORM&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#27169;&#22411;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;DFORM&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#22352;&#26631;&#21464;&#25442;&#65292;&#22312;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#25552;&#20379;&#36830;&#32493;&#30340;&#12289;&#26368;&#22823;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20174;&#32780;&#27604;&#36739;&#23427;&#20204;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#36825;&#25193;&#23637;&#20102;&#24179;&#28369;&#36712;&#36947;&#21644;&#25299;&#25169;&#30340;&#27010;&#24565;&#65292;&#24182;&#35299;&#20915;&#20102;&#27169;&#22411;&#21160;&#24577;&#23545;&#27604;&#20013;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.09735</link><description>&lt;p&gt;
DFORM: &#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#27169;&#22411;&#21160;&#24577;&#30340;&#21487;&#24494;&#20998;&#30690;&#37327;&#22330;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DFORM: Diffeomorphic vector field alignment for assessing dynamics across learned models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DFORM&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#27169;&#22411;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;DFORM&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#22352;&#26631;&#21464;&#25442;&#65292;&#22312;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#25552;&#20379;&#36830;&#32493;&#30340;&#12289;&#26368;&#22823;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20174;&#32780;&#27604;&#36739;&#23427;&#20204;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#36825;&#25193;&#23637;&#20102;&#24179;&#28369;&#36712;&#36947;&#21644;&#25299;&#25169;&#30340;&#27010;&#24565;&#65292;&#24182;&#35299;&#20915;&#20102;&#27169;&#22411;&#21160;&#24577;&#23545;&#27604;&#20013;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#65288;&#20363;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#20316;&#20026;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;&#20551;&#35774;&#30340;&#24037;&#20855;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#30340;&#23398;&#20064;&#29983;&#25104;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#21644;&#22352;&#26631;&#31995;&#32479;&#30340;&#24046;&#24322;&#65292;&#36328;&#27169;&#22411;&#23545;&#27604;&#23398;&#20064;&#21160;&#24577;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DFORM&#65288;&#29992;&#20110;&#27604;&#36739;&#23398;&#20064;&#27169;&#22411;&#21160;&#24577;&#30340;&#21487;&#24494;&#20998;&#30690;&#37327;&#22330;&#23545;&#40784;&#26694;&#26550;&#65289;&#26041;&#27861;&#12290;DFORM&#23398;&#20064;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#22352;&#26631;&#21464;&#25442;&#65292;&#20026;&#23398;&#20064;&#27169;&#22411;&#30340;&#36712;&#36857;&#25552;&#20379;&#20102;&#36830;&#32493;&#30340;&#12289;&#26368;&#22823;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20174;&#32780;&#36817;&#20284;&#22320;&#30830;&#23450;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#24494;&#21516;&#32986;&#20851;&#31995;&#12290;DFORM-transformed&#30690;&#37327;&#22330;&#30340;&#19981;&#21305;&#37197;&#23450;&#20041;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#36712;&#36947;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#24179;&#28369;&#36712;&#36947;&#21644;&#25299;&#25169;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09735v1 Announce Type: new  Abstract: Dynamical system models such as Recurrent Neural Networks (RNNs) have become increasingly popular as hypothesis-generating tools in scientific research. Evaluating the dynamics in such networks is key to understanding their learned generative mechanisms. However, comparison of learned dynamics across models is challenging due to their inherent nonlinearity and because a priori there is no enforced equivalence of their coordinate systems. Here, we propose the DFORM (Diffeomorphic vector field alignment for comparing dynamics across learned models) framework. DFORM learns a nonlinear coordinate transformation which provides a continuous, maximally one-to-one mapping between the trajectories of learned models, thus approximating a diffeomorphism between them. The mismatch between DFORM-transformed vector fields defines the orbital similarity between two models, thus providing a generalization of the concepts of smooth orbital and topologica
&lt;/p&gt;</description></item><item><title>DOF&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#39640;&#38454;&#24494;&#20998;&#31639;&#23376;&#30340;&#35745;&#31639;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#25773;&#30340;&#26041;&#24335;&#65292;&#19981;&#20002;&#22833;&#31934;&#24230;&#65292;&#22312;&#25928;&#29575;&#21644;&#20869;&#23384;&#28040;&#32791;&#19978;&#26377;&#30528;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09730</link><description>&lt;p&gt;
DOF: &#20351;&#29992;&#21069;&#21521;&#20256;&#25773;&#21152;&#36895;&#39640;&#38454;&#24494;&#20998;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
DOF: Accelerating High-order Differential Operators with Forward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09730
&lt;/p&gt;
&lt;p&gt;
DOF&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#39640;&#38454;&#24494;&#20998;&#31639;&#23376;&#30340;&#35745;&#31639;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#25773;&#30340;&#26041;&#24335;&#65292;&#19981;&#20002;&#22833;&#31934;&#24230;&#65292;&#22312;&#25928;&#29575;&#21644;&#20869;&#23384;&#28040;&#32791;&#19978;&#26377;&#30528;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#39640;&#25928;&#26041;&#27861;&#23545;&#20110;&#20998;&#26512;&#22797;&#26434;&#30340;&#29289;&#29702;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#35299;&#20915;PDE&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#65292;&#22312;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;&#39640;&#38454;&#23548;&#25968;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#21463;&#21040;&#21069;&#21521;Laplacian&#30340;&#21551;&#21457;&#65292;&#19968;&#31181;&#21152;&#36895;Laplacian&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;Differential Operator with Forward-propagation&#65288;DOF&#65289;&#65292;&#29992;&#20110;&#35745;&#31639;&#19968;&#33324;&#30340;&#20108;&#38454;&#24494;&#20998;&#31639;&#23376;&#32780;&#19981;&#20002;&#22833;&#31934;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27604;&#29616;&#26377;&#26041;&#27861;&#20248;&#21183;&#30340;&#20005;&#26684;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#25552;&#39640;&#20102;&#20004;&#20493;&#65292;&#24182;&#19988;&#22312;&#20219;&#20309;&#26550;&#26500;&#19978;&#20943;&#23569;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#33258;&#21160;&#24494;&#20998;&#65288;AutoDiff&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09730v1 Announce Type: new  Abstract: Solving partial differential equations (PDEs) efficiently is essential for analyzing complex physical systems. Recent advancements in leveraging deep learning for solving PDE have shown significant promise. However, machine learning methods, such as Physics-Informed Neural Networks (PINN), face challenges in handling high-order derivatives of neural network-parameterized functions. Inspired by Forward Laplacian, a recent method of accelerating Laplacian computation, we propose an efficient computational framework, Differential Operator with Forward-propagation (DOF), for calculating general second-order differential operators without losing any precision. We provide rigorous proof of the advantages of our method over existing methods, demonstrating two times improvement in efficiency and reduced memory consumption on any architectures. Empirical results illustrate that our method surpasses traditional automatic differentiation (AutoDiff)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DPBalance&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#39044;&#31639;&#35843;&#24230;&#26426;&#21046;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#39033;&#26381;&#21153;&#65288;FLaaS&#65289;&#12290;&#35813;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#25968;&#25454;&#20998;&#26512;&#24072;&#32423;&#21035;&#30340;&#20027;&#23548;&#20221;&#39069;&#21644;FL&#29305;&#23450;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#39044;&#31639;&#30340;&#31934;&#30830;&#35843;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09715</link><description>&lt;p&gt;
DPBalance&#65306;&#39640;&#25928;&#21644;&#20844;&#24179;&#30340;&#38544;&#31169;&#39044;&#31639;&#35843;&#24230;&#26426;&#21046;&#29992;&#20110;&#20316;&#20026;&#19968;&#39033;&#26381;&#21153;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DPBalance&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#39044;&#31639;&#35843;&#24230;&#26426;&#21046;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#39033;&#26381;&#21153;&#65288;FLaaS&#65289;&#12290;&#35813;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#25968;&#25454;&#20998;&#26512;&#24072;&#32423;&#21035;&#30340;&#20027;&#23548;&#20221;&#39069;&#21644;FL&#29305;&#23450;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#39044;&#31639;&#30340;&#31934;&#30830;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#32858;&#21512;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#36827;&#19968;&#27493;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#39033;&#26381;&#21153;&#65288;FLaaS&#65289;&#65292;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#19978;&#25191;&#34892;&#20182;&#20204;&#30340;FL&#35757;&#32451;&#27969;&#31243;&#12290;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#23545;&#25968;&#25454;&#22359;&#24378;&#21046;&#25191;&#34892;&#30340;&#38544;&#31169;&#32423;&#21035;&#21487;&#20197;&#35270;&#20026;&#38656;&#35201;&#36827;&#34892;&#31934;&#24515;&#35843;&#24230;&#20197;&#28385;&#36275;&#19981;&#21516;&#35757;&#32451;&#27969;&#31243;&#30340;&#38544;&#31169;&#39044;&#31639;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#39044;&#31639;&#35843;&#24230;&#30740;&#31350;&#20998;&#21035;&#20248;&#20808;&#32771;&#34385;&#25928;&#29575;&#25110;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DPBalance&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#39044;&#31639;&#35843;&#24230;&#26426;&#21046;&#65292;&#21516;&#26102;&#20248;&#21270;&#20102;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#20998;&#26512;&#24072;&#32423;&#21035;&#30340;&#20027;&#23548;&#20221;&#39069;&#21644;FL&#29305;&#23450;&#30340;&#24615;&#33021;&#25351;&#26631;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39034;&#24207;&#20998;&#37197;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09715v1 Announce Type: cross  Abstract: Federated learning (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data. Cloud service providers further embrace Federated Learning as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data. Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines. Existing privacy budget scheduling studies prioritize either efficiency or fairness individually. In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and fairness. We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics. A sequential allocation mechanism is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38142;&#36335;&#39044;&#27979;&#20013;&#25913;&#36827;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NodeDup&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#22797;&#21046;&#20302;&#24230;&#33410;&#28857;&#24182;&#21019;&#24314;&#38142;&#25509;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09711</link><description>&lt;p&gt;
&#33410;&#28857;&#22797;&#21046;&#25913;&#21892;&#20919;&#21551;&#21160;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Node Duplication Improves Cold-start Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38142;&#36335;&#39044;&#27979;&#20013;&#25913;&#36827;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NodeDup&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#22797;&#21046;&#20302;&#24230;&#33410;&#28857;&#24182;&#21019;&#24314;&#38142;&#25509;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#31361;&#20986;&#65292;&#24182;&#22312;&#38142;&#36335;&#39044;&#27979;&#65288;LP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#21364;&#36739;&#24046;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#31561;LP&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25913;&#21892;&#20302;&#24230;&#33410;&#28857;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#31561;&#21516;&#20110;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#25552;&#39640;&#29992;&#25143;&#22312;&#23569;&#25968;&#35266;&#23519;&#30340;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#20307;&#39564;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;LP&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#39640;&#24230;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#31216;&#20026;NodeDup&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NodeDup&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;LP&#35757;&#32451;&#26041;&#26696;&#20013;&#65292;&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#22797;&#21046;&#33410;&#28857;&#24182;&#22312;&#33410;&#28857;&#21644;&#20854;&#21103;&#26412;&#20043;&#38388;&#21019;&#24314;&#38142;&#25509;&#12290;&#36890;&#36807;&#21033;&#29992;&#8220;&#22810;&#35270;&#22270;&#8221;&#35270;&#35282;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LP&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09711v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ''multi-view'' perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;5G&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#20013;&#20849;&#20139;&#25968;&#25454;&#24211;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27927;&#29260;&#30340;&#21487;&#23398;&#20064;&#21152;&#23494;&#25216;&#26415;&#26469;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.09710</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#20013;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#24212;&#29992;&#30340;&#25968;&#25454;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;5G&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#20013;&#20849;&#20139;&#25968;&#25454;&#24211;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27927;&#29260;&#30340;&#21487;&#23398;&#20064;&#21152;&#23494;&#25216;&#26415;&#26469;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#36827;&#39057;&#35889;&#35775;&#38382;&#25216;&#26415;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#31649;&#29702;&#21644;&#20849;&#20139;&#26377;&#38480;&#30340;&#39057;&#35889;&#36164;&#28304;&#65292;&#29992;&#20110;&#26032;&#20852;&#24212;&#29992;&#12290;&#23545;&#20110;&#20854;&#20013;&#20960;&#31181;&#24212;&#29992;&#65292;&#25935;&#24863;&#30340;&#26080;&#32447;&#25968;&#25454;&#65288;&#22914;&#39057;&#35889;&#22270;&#65289;&#23384;&#20648;&#22312;&#20849;&#20139;&#25968;&#25454;&#24211;&#25110;&#22810;&#26041;&#21033;&#30410;&#30456;&#20851;&#32773;&#20113;&#29615;&#22659;&#20013;&#65292;&#22240;&#27492;&#23481;&#26131;&#36896;&#25104;&#38544;&#31169;&#27844;&#28431;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;5G&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#20013;&#20849;&#20139;&#25968;&#25454;&#24211;&#22330;&#26223;&#30340;&#20856;&#22411;&#26696;&#20363;&#26469;&#35299;&#20915;&#27492;&#31867;&#38544;&#31169;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#22312;&#36817;&#23454;&#26102;&#65288;near-RT&#65289;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#26234;&#33021;&#25511;&#21046;&#22120;&#20013;&#26377;&#19968;&#20010;&#20849;&#20139;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#22914;&#20309;&#20445;&#25252;&#29992;&#20110;&#39057;&#35889;&#20849;&#20139;&#21644;&#24178;&#25200;&#32531;&#35299;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#21644;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#30340;&#22522;&#26412;&#24819;&#27861;&#26159;&#21033;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;&#21487;&#23398;&#20064;&#21152;&#23494;&#25216;&#26415;&#26469;&#21152;&#23494;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09710v1 Announce Type: cross  Abstract: Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications. For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks. This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller. We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances. The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encryp
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;(SEV)&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;&#21363;&#20351;&#27169;&#22411;&#19981;&#26159;&#31232;&#30095;&#30340;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20173;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09702</link><description>&lt;p&gt;
&#26080;&#38656;&#31232;&#30095;&#27169;&#22411;&#30340;&#31232;&#30095;&#19988;&#20934;&#30830;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Sparse and Faithful Explanations Without Sparse Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09702
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;(SEV)&#65292;&#29992;&#20110;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;&#21363;&#20351;&#27169;&#22411;&#19981;&#26159;&#31232;&#30095;&#30340;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20173;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#27169;&#22411;&#19981;&#28385;&#36275;&#20840;&#23616;&#30340;&#31232;&#30095;&#24615;&#65292;&#20915;&#31574;&#20173;&#28982;&#21487;&#20197;&#29992;&#23569;&#37327;&#30340;&#29305;&#24449;&#20934;&#30830;&#22320;&#25551;&#36848;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#26576;&#20154;&#32780;&#35328;&#65292;&#23613;&#31649;&#27809;&#26377;&#20449;&#29992;&#21382;&#21490;&#65292;&#20294;&#30003;&#35831;&#22823;&#31508;&#36151;&#27454;&#21487;&#33021;&#20250;&#34987;&#25298;&#32477;&#65292;&#36825;&#23601;&#24573;&#35270;&#20102;&#19982;&#20854;&#20449;&#29992;&#20215;&#20540;&#30456;&#20851;&#30340;&#20219;&#20309;&#35777;&#25454;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#35299;&#37322;&#20540;&#65288;SEV&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31232;&#30095;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20197;&#19978;&#36151;&#27454;&#25298;&#32477;&#30340;&#20363;&#23376;&#20013;&#65292;SEV&#20026;1&#65292;&#22240;&#20026;&#21482;&#38656;&#35201;&#19968;&#20010;&#22240;&#32032;&#26469;&#35299;&#37322;&#20026;&#20160;&#20040;&#36151;&#27454;&#34987;&#25298;&#32477;&#12290;SEV&#26159;&#23545;&#20915;&#31574;&#31232;&#30095;&#24615;&#30340;&#34913;&#37327;&#65292;&#32780;&#19981;&#26159;&#23545;&#25972;&#20307;&#27169;&#22411;&#31232;&#30095;&#24615;&#30340;&#34913;&#37327;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21363;&#20351;&#23427;&#20204;&#19981;&#26159;&#31232;&#30095;&#30340;&#8212;&#8212;&#23454;&#38469;&#19978;&#22312;SEV&#30340;&#34913;&#37327;&#19979;&#20855;&#26377;&#20302;&#20915;&#31574;&#31232;&#30095;&#24615;&#12290;SEV&#20351;&#29992;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#31227;&#21160;&#36827;&#34892;&#23450;&#20041;&#65292;&#20351;&#24471;SEV&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#21035;&#19978;&#19968;&#33268;&#22320;&#23450;&#20041;&#65292;&#20854;&#20013;&#31227;&#21160;&#38480;&#21046;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09702v1 Announce Type: new  Abstract: Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features. For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness. In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models. In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied. SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV. SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflectin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09698</link><description>&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#36807;&#28388;&#22120;&#20013;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Combining Evidence Across Filtrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#26102;&#21051;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#29702;&#20013;&#65292;&#24050;&#30693;&#20219;&#20309;&#21487;&#25509;&#21463;&#30340;&#25512;&#29702;&#26041;&#27861;&#24517;&#39035;&#22522;&#20110;&#27979;&#35797;&#38789;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24191;&#20041;&#21270;&#65292;&#31216;&#20026;e&#36827;&#31243;&#65292;&#23427;&#20204;&#26159;&#38750;&#36127;&#36827;&#31243;&#65292;&#20854;&#22312;&#20219;&#20309;&#20219;&#24847;&#20572;&#26102;&#30340;&#26399;&#26395;&#19978;&#30028;&#19981;&#36229;&#36807;&#19968;&#12290;e&#36827;&#31243;&#37327;&#21270;&#20102;&#38024;&#23545;&#22797;&#21512;&#38646;&#20551;&#35774;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#30340;&#32047;&#31215;&#35777;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#20449;&#24687;&#38598;&#65288;&#21363;&#36807;&#28388;&#22120;&#65289;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#21512;&#24182;&#26041;&#27861;&#65292;&#38024;&#23545;&#19968;&#20010;&#38646;&#20551;&#35774;&#12290;&#23613;&#31649;&#22312;&#30456;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#21487;&#20197;&#36731;&#26494;&#22320;&#21512;&#24182;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#24179;&#22343;&#65289;&#65292;&#20294;&#22312;&#19981;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#19981;&#33021;&#37027;&#20040;&#23481;&#26131;&#22320;&#21512;&#24182;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36739;&#31895;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#19981;&#33021;&#36716;&#25442;&#20026;&#22312;&#26356;&#32454;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25991;&#29486;&#20013;&#19977;&#20010;&#20855;&#20307;&#20363;&#23376;&#65306;&#21487;&#20132;&#25442;&#24615;&#27979;&#35797;&#65292;&#29420;&#31435;&#24615;&#27979;&#35797;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09698v1 Announce Type: cross  Abstract: In anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one. An e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes. This paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis. Even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration. We discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence te
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09695</link><description>&lt;p&gt;
&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reward Poisoning Attack Against Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#65292;&#25915;&#20987;&#32773;&#23545;&#23398;&#20064;&#31639;&#27861;&#23436;&#20840;&#19981;&#20102;&#35299;&#65292;&#24182;&#19988;&#20854;&#39044;&#31639;&#21463;&#21040;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#27745;&#26579;&#37327;&#20197;&#21450;&#24635;&#25200;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#20854;&#39640;&#23618;&#24605;&#24819;&#26159;&#20351;&#19968;&#20123;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#30340;&#40657;&#30418;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25915;&#20987;&#35774;&#35745;&#30340;&#29702;&#35770;&#27934;&#23519;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09695v1 Announce Type: cross  Abstract: We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation. We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation. We propose an attack strategy called `policy contrast attack'. The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing. To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting. We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22686;&#24378;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#36890;&#36807;&#39044;&#27979;&#35775;&#38382;&#39057;&#29575;&#22686;&#24378;&#36339;&#34920;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#25968;&#25454;&#32467;&#26500;&#30456;&#27604;&#65292;RobustSL&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.09687</link><description>&lt;p&gt;
&#40065;&#26834;&#23398;&#20064;&#22686;&#24378;&#23383;&#20856;
&lt;/p&gt;
&lt;p&gt;
Robust Learning-Augmented Dictionaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09687
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#22686;&#24378;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#36890;&#36807;&#39044;&#27979;&#35775;&#38382;&#39057;&#29575;&#22686;&#24378;&#36339;&#34920;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#25968;&#25454;&#32467;&#26500;&#30456;&#27604;&#65292;RobustSL&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#26469;&#23454;&#29616;&#23383;&#20856;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#26500;&#21517;&#20026;RobustSL&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#25968;&#25454;&#24207;&#21015;&#20013;&#20803;&#32032;&#30340;&#35775;&#38382;&#39057;&#29575;&#36827;&#34892;&#39044;&#27979;&#32780;&#22686;&#24378;&#30340;&#36339;&#34920;&#12290;&#36890;&#36807;&#24688;&#24403;&#30340;&#39044;&#27979;&#65292;RobustSL&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#19968;&#33268;&#24615;&#65288;&#23454;&#29616;&#38745;&#24577;&#26368;&#20248;&#24615;&#65289;&#12290;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#20445;&#25345;&#27599;&#20010;&#25805;&#20316;&#30340;&#23545;&#25968;&#36816;&#34892;&#26102;&#38388;&#65292;&#30830;&#20445;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#39044;&#27979;&#26159;&#20197;&#23545;&#25239;&#24615;&#26041;&#24335;&#29983;&#25104;&#30340;&#12290;&#22240;&#27492;&#65292;RobustSL&#20855;&#26377;&#26519;&#12289;&#32599;&#21644;&#20237;&#24503;&#27931;&#22827;&#65288;ICML 2022&#65289;&#20197;&#21450;&#26361;&#31561;&#20154;&#65288;arXiv 2023&#65289;&#26368;&#36817;&#25552;&#20986;&#30340;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#30340;&#25152;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22312;&#20043;&#21069;&#24037;&#20316;&#20013;&#32570;&#22833;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;RobustSL&#22312;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#26102;&#20248;&#20110;&#26367;&#20195;&#25968;&#25454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09687v1 Announce Type: cross  Abstract: We present the first learning-augmented data structure for implementing dictionaries with optimal consistency and robustness. Our data structure, named RobustSL, is a skip list augmented by predictions of access frequencies of elements in a data sequence. With proper predictions, RobustSL has optimal consistency (achieves static optimality). At the same time, it maintains a logarithmic running time for each operation, ensuring optimal robustness, even if predictions are generated adversarially. Therefore, RobustSL has all the advantages of the recent learning-augmented data structures of Lin, Luo, and Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness guarantees that are absent in the previous work. Numerical experiments show that RobustSL outperforms alternative data structures using both synthetic and real datasets.
&lt;/p&gt;</description></item><item><title>HyperMagNet&#26159;&#19968;&#31181;&#22522;&#20110;&#30913;&#24230;&#25289;&#26222;&#25289;&#26031;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#36229;&#22270;&#34920;&#31034;&#20026;&#38750;&#21487;&#36870;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#24182;&#26500;&#24314;&#30913;&#24230;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#23427;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09676</link><description>&lt;p&gt;
HyperMagNet:&#19968;&#31181;&#22522;&#20110;&#30913;&#24230;&#25289;&#26222;&#25289;&#26031;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09676
&lt;/p&gt;
&lt;p&gt;
HyperMagNet&#26159;&#19968;&#31181;&#22522;&#20110;&#30913;&#24230;&#25289;&#26222;&#25289;&#26031;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#36229;&#22270;&#34920;&#31034;&#20026;&#38750;&#21487;&#36870;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#24182;&#26500;&#24314;&#30913;&#24230;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#23427;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#36229;&#22270;&#26159;&#23545;&#23637;&#31034;&#22810;&#31181;&#20851;&#31995;&#30340;&#25968;&#25454;&#30340;&#33258;&#28982;&#27169;&#22411;&#65292;&#32780;&#22270;&#21482;&#33021;&#25429;&#25417;&#21040;&#20004;&#20004;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23545;&#31216;&#30697;&#38453;&#34920;&#31034;&#23558;&#36229;&#22270;&#26377;&#25928;&#22320;&#31616;&#21270;&#20026;&#26080;&#21521;&#22270;&#65292;&#21487;&#33021;&#20250;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#36229;&#22270;&#34920;&#31034;&#20026;&#38750;&#21487;&#36870;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#39532;&#23572;&#21487;&#22827;&#38142;&#26500;&#24314;&#20102;&#19968;&#20010;&#22797;&#25968;&#22467;&#23572;&#31859;&#29305;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453; - &#30913;&#24230;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#35813;&#30697;&#38453;&#20316;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;HyperMagNet&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22522;&#20110;&#22270;&#31616;&#21270;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09676v1 Announce Type: new  Abstract: In data science, hypergraphs are natural models for data exhibiting multi-way relations, whereas graphs only capture pairwise. Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected graphs via symmetrized matrix representations, potentially losing important information. We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain. We use this Markov chain to construct a complex Hermitian Laplacian matrix - the magnetic Laplacian - which serves as the input to our proposed hypergraph neural network. We study HyperMagNet for the task of node classification, and demonstrate its effectiveness over graph-reduction based hypergraph neural networks.
&lt;/p&gt;</description></item><item><title>PAL&#26159;&#31532;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09674</link><description>&lt;p&gt;
PAL&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#24341;&#23548;&#40657;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PAL: Proxy-Guided Black-Box Attack on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09674
&lt;/p&gt;
&lt;p&gt;
PAL&#26159;&#31532;&#19968;&#20010;&#40657;&#30418;&#26597;&#35810;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#20960;&#20010;&#26376;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#34987;&#25805;&#32437;&#26102;&#23427;&#20204;&#23637;&#31034;&#20986;&#30340;&#21361;&#38505;&#33021;&#21147;&#20196;&#20154;&#25285;&#24551;&#12290;&#23613;&#31649;&#23433;&#20840;&#24494;&#35843;&#31561;&#25216;&#26415;&#26088;&#22312;&#26368;&#23567;&#21270;&#26377;&#23475;&#20351;&#29992;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24341;&#21457;&#26377;&#27602;&#22238;&#24212;&#30340;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;LLMs&#30340;&#20195;&#29702;&#24341;&#23548;&#25915;&#20987;&#65288;PAL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#23545;LLMs&#30340;&#40657;&#30418;&#20165;&#26597;&#35810;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#37319;&#29992;&#20102;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;LLM API&#35774;&#35745;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;GPT-3.5-Turbo&#19978;&#36798;&#21040;84%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#22312;Llama-2-7B&#19978;&#36798;&#21040;48%&#65292;&#32780;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20165;&#20026;4%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GCG++&#65292;&#36825;&#26159;&#23545;GCG&#25915;&#20987;&#30340;&#25913;&#36827;&#65292;&#22312;&#30333;&#30418;Llama-2-7B&#19978;&#36798;&#21040;&#20102;94%&#30340;ASR&#65292;&#20197;&#21450;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#24378;&#26377;&#21147;&#20294;&#31616;&#21333;&#30340;&#22522;&#20934;&#26041;&#27861;&#8212;&#8212;LLMs&#19978;&#30340;&#38543;&#26426;&#25628;&#32034;&#25915;&#20987;&#65288;RAL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09674v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#21033;&#29992;PNG&#22270;&#20687;&#25991;&#20214;&#26684;&#24335;&#20013;&#30340;alpha&#36879;&#26126;&#23618;&#27450;&#39575;AI&#35270;&#35273;&#31995;&#32479;&#30340;&#26032;&#28431;&#27934;&#65292;&#23545;&#29616;&#26377;&#30340;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#35270;&#35273;&#31995;&#32479;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.09671</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;AI&#31995;&#32479;&#20013;&#30340;Alpha&#36879;&#26126;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Alpha Transparency In Language And Vision-Based AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09671
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#21033;&#29992;PNG&#22270;&#20687;&#25991;&#20214;&#26684;&#24335;&#20013;&#30340;alpha&#36879;&#26126;&#23618;&#27450;&#39575;AI&#35270;&#35273;&#31995;&#32479;&#30340;&#26032;&#28431;&#27934;&#65292;&#23545;&#29616;&#26377;&#30340;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#35270;&#35273;&#31995;&#32479;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;PNG&#22270;&#20687;&#25991;&#20214;&#26684;&#24335;&#30340;&#28431;&#27934;&#65292;&#20855;&#20307;&#26159;&#23427;&#20204;&#30340;alpha&#36879;&#26126;&#23618;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#36825;&#20010;&#28431;&#27934;&#23545;&#22810;&#20010;AI&#35270;&#35273;&#31995;&#32479;&#30340;&#27450;&#39575;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#20010;alpha&#36879;&#26126;&#23618;&#20316;&#20026;&#19968;&#20010;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#19981;&#21487;&#35265;&#20294;&#23436;&#20840;&#21487;&#25805;&#20316;&#30340;&#31192;&#23494;&#36890;&#36947;&#26469;&#27450;&#39575;AI&#22270;&#20687;&#22788;&#29702;&#22120;&#12290;&#21463;&#28431;&#27934;&#27979;&#35797;&#30340;&#33539;&#22260;&#21253;&#25324;&#33529;&#26524;&#12289;&#24494;&#36719;&#12289;&#35895;&#27468;&#12289;Salesforce&#12289;Nvidia&#21644;Facebook&#31561;&#20195;&#34920;&#24615;&#35270;&#35273;&#31995;&#32479;&#65292;&#31361;&#26174;&#20102;&#25915;&#20987;&#30340;&#28508;&#22312;&#24191;&#24230;&#12290;&#36825;&#20010;&#28431;&#27934;&#23545;&#29616;&#26377;&#30340;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#35270;&#35273;&#31995;&#32479;&#25552;&#20986;&#20102;&#23433;&#20840;&#21327;&#35758;&#30340;&#25361;&#25112;&#65292;&#20174;&#21307;&#23398;&#25104;&#20687;&#21040;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21463;&#21040;&#24433;&#21709;&#30340;&#31995;&#32479;&#65292;&#26080;&#35770;&#26159;&#20381;&#36182;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36824;&#26159;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#37117;&#19981;&#33021;&#36890;&#36807;&#31616;&#21333;&#30340;&#34917;&#19969;&#25110;&#26356;&#26032;&#24555;&#36895;&#22320;&#32531;&#35299;&#36825;&#20123;&#28431;&#27934;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21644;&#26550;&#26500;&#21464;&#21270;&#65292;&#34920;&#26126;&#36825;&#20123;&#28431;&#27934;&#26159;&#25345;&#20037;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09671v1 Announce Type: cross  Abstract: This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems. Our method uses this alpha layer as a clandestine channel invisible to human observers but fully actionable by AI image processors. The scope tested for the vulnerability spans representative vision systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook, highlighting the attack's potential breadth. This vulnerability challenges the security protocols of existing and fielded vision systems, from medical imaging to autonomous driving technologies. Our experiments demonstrate that the affected systems, which rely on convolutional neural networks or the latest multimodal language models, cannot quickly mitigate these vulnerabilities through simple patches or updates. Instead, they require retraining and architectural changes, indicating a persiste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Ask-LLM&#21644;Density&#20004;&#31181;&#20248;&#31168;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09668</link><description>&lt;p&gt;
&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Train Data-Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35757;&#32451;&#25968;&#25454;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;Ask-LLM&#21644;Density&#20004;&#31181;&#20248;&#31168;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#21313;&#20998;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#65292;&#21363;&#26088;&#22312;&#20248;&#21270;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#36164;&#28304;/&#25968;&#25454;&#28040;&#32791;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#22522;&#20110;&#65288;i&#65289;&#26114;&#36149;&#30340;&#25968;&#25454;&#36136;&#37327;&#20272;&#35745;&#21644;&#65288;ii&#65289;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30340;&#35206;&#30422;&#29575;&#21644;&#22810;&#26679;&#24615;&#27979;&#37327;&#30340;&#25968;&#25454;&#36873;&#25321;&#31243;&#24207;&#25152;&#24102;&#26469;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#25216;&#26415;&#8220;Ask-LLM&#8221;&#21033;&#29992;&#35843;&#33410;&#25351;&#20196;&#30340;LLM&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#26469;&#30452;&#25509;&#35780;&#20272;&#35757;&#32451;&#26679;&#20363;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#36798;&#21040;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23494;&#24230;&#37319;&#26679;&#65292;&#23427;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#36873;&#25321;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#22312;&#25105;&#20204;&#23545;19&#31181;&#37319;&#26679;&#22120;&#36827;&#34892;&#20102;&#25968;&#30334;&#20010;&#35780;&#20272;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#36816;&#34892;&#30340;&#23545;&#27604;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Ask-LLM&#21644;Density&#26159;&#21508;&#33258;&#31867;&#21035;&#20013;&#26368;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09668v1 Announce Type: cross  Abstract: The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09660</link><description>&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
User Modeling and User Profiling: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24050;&#32463;&#20419;&#20351;&#20808;&#36827;&#30340;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22522;&#20110;&#19982;&#36825;&#20123;&#31995;&#32479;&#30340;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#12290;&#26412;&#25991;&#23545;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21382;&#21490;&#27010;&#36848;&#65292;&#36861;&#28335;&#20102;&#20174;&#26089;&#26399;&#30340;&#21051;&#26495;&#27169;&#22411;&#21040;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#25152;&#26377;&#27963;&#21160;&#20027;&#39064;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#31361;&#20986;&#20102;&#21521;&#26356;&#22797;&#26434;&#30340;&#29992;&#25143;&#30011;&#20687;&#26041;&#27861;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#38544;&#24335;&#25968;&#25454;&#25910;&#38598;&#12289;&#22810;&#34892;&#20026;&#24314;&#27169;&#20197;&#21450;&#22270;&#25968;&#25454;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09660v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25968;&#23383;&#21644;&#27169;&#25311;&#20256;&#36755;&#22312;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#26412;&#36136;&#21306;&#21035;&#22312;&#20110;&#36890;&#20449;&#21644;&#35745;&#31639;&#26159;&#21542;&#21516;&#26102;&#35774;&#35745;&#65292;&#25968;&#23383;&#26041;&#26696;&#20998;&#31163;&#20102;&#36890;&#20449;&#35774;&#35745;&#21644;&#20855;&#20307;&#20219;&#21153;&#65292;&#32780;&#27169;&#25311;&#36890;&#20449;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#22823;&#35268;&#27169;&#35774;&#22791;&#30340;&#20256;&#36755;&#12290;</title><link>https://arxiv.org/abs/2402.09657</link><description>&lt;p&gt;
&#25968;&#23383;&#19982;&#27169;&#25311;&#20256;&#36755;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Digital versus Analog Transmissions for Federated Learning over Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25968;&#23383;&#21644;&#27169;&#25311;&#20256;&#36755;&#22312;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#26412;&#36136;&#21306;&#21035;&#22312;&#20110;&#36890;&#20449;&#21644;&#35745;&#31639;&#26159;&#21542;&#21516;&#26102;&#35774;&#35745;&#65292;&#25968;&#23383;&#26041;&#26696;&#20998;&#31163;&#20102;&#36890;&#20449;&#35774;&#35745;&#21644;&#20855;&#20307;&#20219;&#21153;&#65292;&#32780;&#27169;&#25311;&#36890;&#20449;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#22823;&#35268;&#27169;&#35774;&#22791;&#30340;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#37327;&#27604;&#36739;&#20102;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20004;&#31181;&#26377;&#25928;&#36890;&#20449;&#26041;&#26696;&#65292;&#21363;&#25968;&#23383;&#20256;&#36755;&#21644;&#27169;&#25311;&#20256;&#36755;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#26412;&#36136;&#21306;&#21035;&#20197;&#21450;&#21508;&#33258;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#25968;&#23383;&#21644;&#27169;&#25311;&#20256;&#36755;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#38469;&#32422;&#26463;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;&#32479;&#19968;&#20844;&#24179;&#30340;&#27604;&#36739;&#26041;&#26696;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#36890;&#29992;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#32447;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#20998;&#26512;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20004;&#31181;&#33539;&#20363;&#30340;&#26681;&#26412;&#21306;&#21035;&#22312;&#20110;&#36890;&#20449;&#21644;&#35745;&#31639;&#26159;&#21542;&#21516;&#26102;&#35774;&#35745;&#12290;&#25968;&#23383;&#26041;&#26696;&#23558;&#36890;&#20449;&#35774;&#35745;&#19982;&#20855;&#20307;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20998;&#31163;&#65292;&#20351;&#24471;&#25903;&#25345;&#22823;&#37327;&#35774;&#22791;&#21516;&#26102;&#19978;&#34892;&#20256;&#36755;&#25104;&#20026;&#22256;&#38590;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27169;&#25311;&#36890;&#20449;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#22823;&#35268;&#27169;&#35774;&#22791;&#30340;&#19978;&#34892;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09657v1 Announce Type: cross  Abstract: In this paper, we quantitatively compare these two effective communication schemes, i.e., digital and analog ones, for wireless federated learning (FL) over resource-constrained networks, highlighting their essential differences as well as their respective application scenarios. We first examine both digital and analog transmission methods, together with a unified and fair comparison scheme under practical constraints. A universal convergence analysis under various imperfections is established for FL performance evaluation in wireless networks. These analytical results reveal that the fundamental difference between the two paradigms lies in whether communication and computation are jointly designed or not. The digital schemes decouple the communication design from specific FL tasks, making it difficult to support simultaneous uplink transmission of massive devices with limited bandwidth. In contrast, the analog communication allows ove
&lt;/p&gt;</description></item><item><title>Atlassian&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;CI&#26500;&#24314;&#22833;&#36133;&#23545;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#21644;&#22242;&#38431;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;CI&#26500;&#24314;&#39044;&#27979;&#24037;&#20855;&#38598;&#25104;&#21040;Bitbucket&#29615;&#22659;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#26399;&#26395;&#12290;</title><link>https://arxiv.org/abs/2402.09651</link><description>&lt;p&gt;
Atlassian&#30340;CI&#26500;&#24314;&#22833;&#36133;&#39044;&#27979;&#30340;&#20174;&#19994;&#32773;&#25361;&#25112;&#21644;&#24863;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Practitioners' Challenges and Perceptions of CI Build Failure Predictions at Atlassian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09651
&lt;/p&gt;
&lt;p&gt;
Atlassian&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;CI&#26500;&#24314;&#22833;&#36133;&#23545;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#21644;&#22242;&#38431;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;CI&#26500;&#24314;&#39044;&#27979;&#24037;&#20855;&#38598;&#25104;&#21040;Bitbucket&#29615;&#22659;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#38598;&#25104;&#65288;CI&#65289;&#26500;&#24314;&#22833;&#36133;&#21487;&#33021;&#20250;&#23545;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#21644;&#22242;&#38431;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22914;&#24310;&#36831;&#21457;&#24067;&#26032;&#21151;&#33021;&#21644;&#38477;&#20302;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;Atlassian&#22312;&#20135;&#21697;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;CI&#26500;&#24314;&#22833;&#36133;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#20998;&#26512;&#21457;&#29616;&#65292;&#20195;&#30721;&#24211;&#32500;&#24230;&#26159;&#24433;&#21709;CI&#26500;&#24314;&#22833;&#36133;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23450;&#24615;&#35843;&#26597;&#21457;&#29616;&#65292;Atlassian&#24320;&#21457;&#20154;&#21592;&#35748;&#20026;CI&#26500;&#24314;&#22833;&#36133;&#26159;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;CI&#26500;&#24314;&#39044;&#27979;&#19981;&#20165;&#21487;&#20197;&#25552;&#20379;&#23545;CI&#26500;&#24314;&#22833;&#36133;&#30340;&#31215;&#26497;&#35265;&#35299;&#65292;&#36824;&#21487;&#20197;&#20419;&#36827;&#22242;&#38431;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#23558;CI&#26500;&#24314;&#39044;&#27979;&#24037;&#20855;&#38598;&#25104;&#21040;Bitbucket&#29615;&#22659;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#26399;&#26395;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;CI&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09651v1 Announce Type: cross  Abstract: Continuous Integration (CI) build failures could significantly impact the software development process and teams, such as delaying the release of new features and reducing developers' productivity. In this work, we report on an empirical study that investigates CI build failures throughout product development at Atlassian. Our quantitative analysis found that the repository dimension is the key factor influencing CI build failures. In addition, our qualitative survey revealed that Atlassian developers perceive CI build failures as challenging issues in practice. Furthermore, we found that the CI build prediction can not only provide proactive insight into CI build failures but also facilitate the team's decision-making. Our study sheds light on the challenges and expectations involved in integrating CI build prediction tools into the Bitbucket environment, providing valuable insights for enhancing CI processes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#35270;&#39057;&#25968;&#25454;&#12289;&#36793;&#30028;&#26694;&#20301;&#32622;&#12289;&#22270;&#20687;&#32454;&#33410;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#36275;&#29699;&#29359;&#35268;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.09650</link><description>&lt;p&gt;
&#21033;&#29992;&#36275;&#29699;&#24191;&#25773;&#35270;&#39057;&#20013;&#30340;&#20272;&#35745;&#23039;&#21183;&#26469;&#39044;&#27979;&#29359;&#35268;
&lt;/p&gt;
&lt;p&gt;
Foul prediction with estimated poses from soccer broadcast video
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09650
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#35270;&#39057;&#25968;&#25454;&#12289;&#36793;&#30028;&#26694;&#20301;&#32622;&#12289;&#22270;&#20687;&#32454;&#33410;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#36275;&#29699;&#29359;&#35268;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#36816;&#21160;&#21592;&#36319;&#36394;&#21644;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#36816;&#21160;&#20013;&#20351;&#29992;&#23039;&#21183;&#20272;&#35745;&#36827;&#34892;&#34892;&#20026;&#39044;&#27979;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36275;&#29699;&#29359;&#35268;&#30340;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#29699;&#21592;&#30340;&#22270;&#20687;&#23610;&#23544;&#36739;&#23567;&#65292;&#24182;&#19988;&#20351;&#29992;&#20363;&#22914;&#29699;&#21644;&#23039;&#21183;&#20449;&#24687;&#30340;&#22256;&#38590;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#36275;&#29699;&#29359;&#35268;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#29702;&#19968;&#20010;&#26032;&#39062;&#30340;&#36275;&#29699;&#29359;&#35268;&#25968;&#25454;&#38598;&#65292;&#23558;&#35270;&#39057;&#25968;&#25454;&#12289;&#36793;&#30028;&#26694;&#20301;&#32622;&#12289;&#22270;&#20687;&#32454;&#33410;&#21644;&#23039;&#21183;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#32467;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23436;&#25972;&#27169;&#22411;&#20248;&#20110;&#21093;&#31163;&#27169;&#22411;&#65292;&#20197;&#21450;&#25152;&#26377;&#30340;RNN&#27169;&#22359;&#12289;&#36793;&#30028;&#26694;&#20301;&#32622;&#21644;&#22270;&#20687;&#12289;&#20272;&#35745;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09650v1 Announce Type: cross  Abstract: Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players. However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information. In our research, we introduce an innovative deep learning approach for anticipating soccer fouls. This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset. Our model utilizes a combination of convolutional and recurrent neural networks (CNNs and RNNs) to effectively merge information from these four modalities. The experimental results show that our full model outperformed the ablated models, and all of the RNN modules, bounding box position and image, and estimated pose wer
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#20248;&#21270;(MFO)&#26159;&#19968;&#31181;&#20197;&#39640;&#27169;&#24577;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#24179;&#34913;&#30340;&#25104;&#26412;&#25928;&#30410;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#26367;&#20195;&#27169;&#22411;&#12289;&#24544;&#35802;&#24230;&#31649;&#29702;&#31574;&#30053;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#22797;&#26434;&#35745;&#31639;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;MFO&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#21644;&#31185;&#23398;&#21457;&#29616;&#31561;&#22810;&#20010;&#20851;&#38190;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09638</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20248;&#21270;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Methods for Optimization: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09638
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20248;&#21270;(MFO)&#26159;&#19968;&#31181;&#20197;&#39640;&#27169;&#24577;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#24179;&#34913;&#30340;&#25104;&#26412;&#25928;&#30410;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#26367;&#20195;&#27169;&#22411;&#12289;&#24544;&#35802;&#24230;&#31649;&#29702;&#31574;&#30053;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#22797;&#26434;&#35745;&#31639;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;MFO&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#21644;&#31185;&#23398;&#21457;&#29616;&#31561;&#22810;&#20010;&#20851;&#38190;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#40657;&#30418;&#20248;&#21270;&#24448;&#24448;&#28041;&#21450;&#32791;&#26102;&#25110;&#26114;&#36149;&#30340;&#23454;&#39564;&#21644;&#27169;&#25311;&#12290;&#22810;&#27169;&#24577;&#20248;&#21270;(MFO)&#36890;&#36807;&#19968;&#31181;&#20998;&#23618;&#30340;&#24544;&#35802;&#24230;&#26041;&#27861;&#65292;&#20197;&#39640;&#27169;&#24577;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#24179;&#34913;&#30340;&#25104;&#26412;&#25928;&#30410;&#31574;&#30053;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#26412;&#35843;&#26597;&#20197;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#25991;&#26412;&#25366;&#25496;&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#23545; MFO &#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102; MFO &#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#8212;&#8212;&#22810;&#27169;&#24577;&#20195;&#29702;&#27169;&#22411;&#12289;&#24544;&#35802;&#24230;&#31649;&#29702;&#31574;&#30053;&#21644;&#20248;&#21270;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#26597;&#36824;&#31361;&#20986;&#20102; MFO &#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#21644;&#31185;&#23398;&#21457;&#29616;&#31561;&#22810;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#19981;&#21516;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102; MFO &#22312;&#35299;&#20915;&#22797;&#26434;&#35745;&#31639;&#25361;&#25112;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#26395;&#20102;&#20960;&#31181;&#24212;&#29992; MFO &#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09638v1 Announce Type: new  Abstract: Real-world black-box optimization often involves time-consuming or costly experiments and simulations. Multi-fidelity optimization (MFO) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach. This survey presents a systematic exploration of MFO, underpinned by a novel text mining framework based on a pre-trained language model. We delve deep into the foundational principles and methodologies of MFO, focusing on three core components -- multi-fidelity surrogate models, fidelity management strategies, and optimization techniques. Additionally, this survey highlights the diverse applications of MFO across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of MFO in tackling complex computational challenges. Furthermore, we also envision several em
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#20132;&#25442;&#38382;&#39064;&#30340;&#26234;&#33021;&#20449;&#24687;&#20132;&#25442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26368;&#20248;&#22270;&#24418;&#26469;&#36873;&#25321;&#25968;&#25454;&#20256;&#36755;&#30340;&#38142;&#25509;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#24322;&#24120;&#35774;&#22791;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09629</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#20449;&#24687;&#20132;&#25442;
&lt;/p&gt;
&lt;p&gt;
Smart Information Exchange for Unsupervised Federated Learning via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#20132;&#25442;&#38382;&#39064;&#30340;&#26234;&#33021;&#20449;&#24687;&#20132;&#25442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26368;&#20248;&#22270;&#24418;&#26469;&#36873;&#25321;&#25968;&#25454;&#20256;&#36755;&#30340;&#38142;&#25509;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#24322;&#24120;&#35774;&#22791;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22914;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20998;&#24067;&#24335;&#35774;&#22791;&#20013;&#23384;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#35774;&#22791;&#38388;&#36890;&#20449;&#65288;D2D&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#19988;&#23545;&#20110;&#26377;&#24322;&#24120;&#35774;&#22791;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#23569;&#26631;&#31614;&#65292;&#22914;&#20309;&#36827;&#34892;&#25968;&#25454;&#20132;&#25442;&#24182;&#19981;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21019;&#24314;&#25968;&#25454;&#20256;&#36755;&#30340;&#26368;&#20248;&#22270;&#24418;&#30340;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#24418;&#25104;&#33021;&#22815;&#22312;&#26080;&#30417;&#30563; FL &#29615;&#22659;&#20013;&#32771;&#34385;&#29615;&#22659;&#32422;&#26463;&#12289;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#30340;&#38142;&#25509;&#12290;&#25968;&#20540;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#24322;&#24120;&#35774;&#22791;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#19981;&#21516;&#21487;&#29992; FL &#26041;&#26696;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09629v1 Announce Type: new  Abstract: One of the main challenges of decentralized machine learning paradigms such as Federated Learning (FL) is the presence of local non-i.i.d. datasets. Device-to-device transfers (D2D) between distributed devices has been shown to be an effective tool for dealing with this problem and robust to stragglers. In an unsupervised case, however, it is not obvious how data exchanges should take place due to the absence of labels. In this paper, we propose an approach to create an optimal graph for data transfer using Reinforcement Learning. The goal is to form links that will provide the most benefit considering the environment's constraints and improve convergence speed in an unsupervised FL environment. Numerical analysis shows the advantages in terms of convergence speed and straggler resilience of the proposed method to different available FL schemes and benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22312;&#32447;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#21644;&#35299;&#20915;&#22238;&#24402;&#20013;&#24322;&#26041;&#24046;&#24615;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#21516;&#26102;&#39044;&#27979;&#36793;&#30028;&#65292;&#24182;&#33021;&#22815;&#21487;&#38752;&#22320;&#35206;&#30422;&#26032;&#38543;&#26426;&#36712;&#36857;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#31934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#32780;&#19988;&#24448;&#24448;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09623</link><description>&lt;p&gt;
&#22810;&#20803;&#36712;&#36857;&#30340;&#31526;&#21512;&#24615;&#33258;&#36866;&#24212;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformalized Adaptive Forecasting of Heterogeneous Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22312;&#32447;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#21644;&#35299;&#20915;&#22238;&#24402;&#20013;&#24322;&#26041;&#24046;&#24615;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#21516;&#26102;&#39044;&#27979;&#36793;&#30028;&#65292;&#24182;&#33021;&#22815;&#21487;&#38752;&#22320;&#35206;&#30422;&#26032;&#38543;&#26426;&#36712;&#36857;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#31934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#32780;&#19988;&#24448;&#24448;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21516;&#26102;&#39044;&#27979;&#36793;&#30028;&#65292;&#20197;&#20855;&#26377;&#36275;&#22815;&#39640;&#30340;&#27010;&#29575;&#35206;&#30422;&#26032;&#38543;&#26426;&#36712;&#36857;&#30340;&#25972;&#20010;&#36335;&#24452;&#12290;&#37492;&#20110;&#22312;&#36816;&#21160;&#35268;&#21010;&#24212;&#29992;&#20013;&#38656;&#35201;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20854;&#20013;&#19981;&#21516;&#29289;&#20307;&#30340;&#34892;&#20026;&#21487;&#33021;&#26356;&#25110;&#26356;&#23569;&#21487;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#21333;&#20010;&#21644;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#20197;&#21450;&#35299;&#20915;&#22238;&#24402;&#20013;&#30340;&#24322;&#26041;&#24046;&#24615;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#34701;&#21512;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#26082;&#26377;&#21407;&#21017;&#24615;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#21448;&#26377;&#25928;&#65292;&#36890;&#24120;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09623v1 Announce Type: cross  Abstract: This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. Prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09611</link><description>&lt;p&gt;
&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Sign Language Translation at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#30446;&#21069;&#22312;&#32593;&#32476;&#19978;&#21487;&#29992;&#30340;&#22823;&#37096;&#20998;&#25163;&#35821;&#25968;&#25454;&#30001;&#20110;&#32570;&#20047;&#23545;&#40784;&#30340;&#23383;&#24149;&#32780;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#20854;&#20013;&#21253;&#21547;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#25216;&#26415;&#24212;&#35813;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SSVP-SLT&#65292;&#23427;&#21033;&#29992;&#21311;&#21517;&#21644;&#26410;&#27880;&#37322;&#30340;&#35270;&#39057;&#36827;&#34892;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#36807;&#31579;&#36873;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#24494;&#35843;&#12290; SSVP-SLT&#22312;How2Sign&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24494;&#35843;&#21644;&#38646;&#27425;gloss-free&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#65292;&#27604;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;3&#20010;BLEU-4&#12290;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#25163;&#35821;&#35789;&#27719;&#19978;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#24179;&#26041;&#31070;&#32463;&#32593;&#32476;&#26063;&#30340;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#34920;&#36798;&#24615;&#27850;&#26494;&#28857;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#26041;&#33539;&#25968;&#26469;&#21442;&#25968;&#21270;&#24378;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#31215;&#20998;&#24378;&#24230;&#20989;&#25968;&#26102;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#21644;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#33410;&#32422;&#20869;&#23384;&#21644;&#26102;&#38388;&#12290;&#36890;&#36807;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#33719;&#24471;&#23545;&#24378;&#24230;&#20989;&#25968;&#26368;&#32456;&#23618;&#30340;&#21442;&#25968;&#21270;&#37325;&#21442;&#25968;&#21270;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.09608</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#26041;&#31070;&#32463;&#32593;&#32476;&#26063;&#30340;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#34920;&#36798;&#24615;&#27850;&#26494;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#24179;&#26041;&#31070;&#32463;&#32593;&#32476;&#26063;&#30340;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#34920;&#36798;&#24615;&#27850;&#26494;&#28857;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#26041;&#33539;&#25968;&#26469;&#21442;&#25968;&#21270;&#24378;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#31215;&#20998;&#24378;&#24230;&#20989;&#25968;&#26102;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#21644;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#33410;&#32422;&#20869;&#23384;&#21644;&#26102;&#38388;&#12290;&#36890;&#36807;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#33719;&#24471;&#23545;&#24378;&#24230;&#20989;&#25968;&#26368;&#32456;&#23618;&#30340;&#21442;&#25968;&#21270;&#37325;&#21442;&#25968;&#21270;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#24378;&#24230;&#20989;&#25968;&#30340;&#21442;&#25968;&#21270;&#20026;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#26041;&#33539;&#25968;&#24341;&#20837;&#20102;&#24179;&#26041;&#31070;&#32463;&#27850;&#26494;&#28857;&#36807;&#31243;&#65288;SNEPPPs&#65289;&#12290;&#24403;&#38544;&#34255;&#23618;&#34987;&#22266;&#23450;&#19988;&#31532;&#20108;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31867;&#20284;&#20110;&#20043;&#21069;&#20351;&#29992;&#24179;&#26041;&#39640;&#26031;&#36807;&#31243;&#25110;&#26680;&#26041;&#27861;&#65292;&#20294;&#20801;&#35768;&#38544;&#34255;&#23618;&#23398;&#20064;&#33021;&#22815;&#25552;&#20379;&#39069;&#22806;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#24773;&#20917;&#19979;&#65292;&#31215;&#20998;&#24378;&#24230;&#20989;&#25968;&#21487;&#20197;&#24471;&#21040;&#23553;&#38381;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#20197;&#20108;&#27425;&#26102;&#38388;&#30456;&#23545;&#20110;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#27604;&#20197;&#21069;&#35752;&#35770;&#36807;&#30340;&#26356;&#22810;&#36825;&#26679;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#31616;&#21333;&#23454;&#29616;&#24179;&#26041;&#25110;&#25351;&#25968;&#26680;&#26041;&#27861;&#25110;&#39640;&#26031;&#36807;&#31243;&#26356;&#33410;&#32422;&#20869;&#23384;&#21644;&#26102;&#38388;&#12290;&#26368;&#22823;&#20284;&#28982;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#65288;&#20005;&#26684;&#65289;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#33719;&#24471;&#24378;&#24230;&#20989;&#25968;&#26368;&#32456;&#23618;&#30340;&#21442;&#25968;&#21270;&#37325;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09608v1 Announce Type: new  Abstract: We introduce squared neural Poisson point processes (SNEPPPs) by parameterising the intensity function by the squared norm of a two layer neural network. When the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared Gaussian process or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility. In many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons. We enumerate a far more extensive number of such cases than has previously been discussed. Our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or Gaussian processes. Maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20307;&#31215;&#26368;&#22823;&#21270;&#39033;&#20943;&#23569;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#21487;&#20197;&#38477;&#20302;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.09603</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Graph Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20307;&#31215;&#26368;&#22823;&#21270;&#39033;&#20943;&#23569;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#21487;&#20197;&#38477;&#20302;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#30340;&#27491;&#21017;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#33410;&#28857;&#25968;&#21644;&#23884;&#20837;&#32500;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#20026;&#20102;&#20943;&#36731;&#38750;&#23545;&#27604;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#31215;&#26368;&#22823;&#21270;&#39033;&#20943;&#23569;&#39044;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#36890;&#36807;&#22270;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#20943;&#23569;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32500;&#24230;&#37319;&#26679;&#20250;&#23548;&#33268;&#20934;&#30830;&#30340;&#25439;&#22833;&#35745;&#31639;&#65292;&#24182;&#29992;&#25968;&#23398;&#25512;&#23548;&#25903;&#25345;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#33410;&#28857;&#32423;&#22270;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#35268;&#27169;&#24456;&#22823;&#65292;&#25152;&#20197;&#22312;&#36825;&#26041;&#38754;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#33410;&#28857;&#25110;&#32500;&#24230;&#37319;&#26679;&#21487;&#20197;&#20943;&#23569;&#25439;&#22833;&#35745;&#31639;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09603v1 Announce Type: cross  Abstract: In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions. To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs. Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20302;&#31209;&#27491;&#35268;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.09600</link><description>&lt;p&gt;
&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Graph Contrastive Learning for Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20302;&#31209;&#27491;&#35268;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#22122;&#22768;&#65292;&#36825;&#20250;&#20005;&#37325;&#38477;&#20302;GNNs&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;GNN&#32534;&#30721;&#22120;&#65292;&#21363;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20302;&#31209;&#27491;&#24120;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#21517;&#20026;LR-GCL&#30340;&#20302;&#31209;GCL&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;LR-GCL&#29983;&#25104;&#30340;&#29305;&#24449;&#65292;&#20351;&#29992;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#31639;&#27861;&#23545;&#22270;&#20013;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;LR-GCL&#21463;&#21040;&#22270;&#25968;&#25454;&#21644;&#20854;&#26631;&#31614;&#30340;&#20302;&#39057;&#24615;&#36136;&#30340;&#21551;&#31034;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21463;&#21040;&#25105;&#20204;&#20851;&#20110;&#36716;&#23548;&#23398;&#20064;&#30340;&#23574;&#38160;&#27867;&#21270;&#30028;&#38480;&#30340;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09600v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies. In this work, we propose a novel and robust GNN encoder, Low-Rank Graph Contrastive Learning (LR-GCL). Our method performs transductive node classification in two steps. First, a low-rank GCL encoder named LR-GCL is trained by prototypical contrastive learning with low-rank regularization. Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph. Our LR-GCL is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our kno
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#32479;&#19968;&#35299;&#20915;MCMC&#21644;&#26426;&#22120;&#23398;&#20064;&#20132;&#21449;&#39046;&#22495;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#21253;&#25324;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#12289;&#33258;&#36866;&#24212;MCMC&#12289;&#27491;&#35268;&#27969;&#26500;&#24314;&#21644;&#20256;&#36755;&#36741;&#21161;MCMC&#12289;&#26367;&#20195;&#20284;&#28982;MCMC&#12289;&#22823;&#25968;&#25454;&#30340;MCMC&#26680;&#24515;&#38598;&#26500;&#24314;&#31561;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.09598</link><description>&lt;p&gt;
&#22522;&#20110;MCMC&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MCMC-driven learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09598
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#32479;&#19968;&#35299;&#20915;MCMC&#21644;&#26426;&#22120;&#23398;&#20064;&#20132;&#21449;&#39046;&#22495;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#21253;&#25324;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#12289;&#33258;&#36866;&#24212;MCMC&#12289;&#27491;&#35268;&#27969;&#26500;&#24314;&#21644;&#20256;&#36755;&#36741;&#21161;MCMC&#12289;&#26367;&#20195;&#20284;&#28982;MCMC&#12289;&#22823;&#25968;&#25454;&#30340;MCMC&#26680;&#24515;&#38598;&#26500;&#24314;&#31561;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20316;&#20026;&#12298;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#25163;&#20876;&#12299;&#30340;&#19968;&#31456;&#20986;&#29616;&#12290;&#35813;&#31456;&#30340;&#30446;&#26631;&#26159;&#22312;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#19978;&#32479;&#19968;&#21508;&#31181;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#12289;&#33258;&#36866;&#24212;MCMC&#12289;&#27491;&#35268;&#27969;&#26500;&#24314;&#21644;&#20256;&#36755;&#36741;&#21161;MCMC&#12289;&#26367;&#20195;&#20284;&#28982;MCMC&#12289;&#29992;&#20110;&#22823;&#25968;&#25454;&#30340;MCMC&#26680;&#24515;&#38598;&#26500;&#24314;&#12289;&#39532;&#23572;&#31185;&#22827;&#38142;&#26799;&#24230;&#19979;&#38477;&#12289;&#39532;&#23572;&#31185;&#22827;&#24471;&#20998;&#25856;&#29228;&#31561;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21487;&#20197;&#23558;&#20026;&#27599;&#20010;&#38382;&#39064;&#24320;&#21457;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#36827;&#34892;&#32763;&#35793;&#21644;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09598v1 Announce Type: cross  Abstract: This paper is intended to appear as a chapter for the Handbook of Markov Chain Monte Carlo. The goal of this chapter is to unify various problems at the intersection of Markov chain Monte Carlo (MCMC) and machine learning$\unicode{x2014}$which includes black-box variational inference, adaptive MCMC, normalizing flow construction and transport-assisted MCMC, surrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov chain gradient descent, Markovian score climbing, and more$\unicode{x2014}$within one common framework. By doing so, the theory and methods developed for each may be translated and generalized.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#65288;DES&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#20934;&#34880;&#28082;&#26679;&#26412;&#20998;&#26512;&#21644;&#21560;&#28895;&#21490;&#25968;&#25454;&#36827;&#34892;&#32954;&#30284;&#30340;&#32954;&#30149;&#23398;&#27700;&#24179;&#26816;&#27979;&#65292;&#22312;&#20025;&#40614;&#21335;&#37096;&#22320;&#21306;&#30340;&#22823;&#37327;&#24739;&#26377;&#39118;&#38505;&#30340;&#20154;&#32676;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27169;&#22411;&#22312;&#32954;&#30149;&#19987;&#23478;&#25552;&#20379;&#30340;&#35786;&#26029;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#65288;&#25688;&#35201;&#24635;&#32467;&#65289;</title><link>https://arxiv.org/abs/2402.09596</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#20934;&#34880;&#28082;&#26816;&#27979;&#32467;&#26524;&#21644;&#21560;&#28895;&#29366;&#20917;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32954;&#30284;&#30340;&#32954;&#30149;&#23398;&#27700;&#24179;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pulmonologists-Level lung cancer detection based on standard blood test results and smoking status using an explainable machine learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#65288;DES&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#20934;&#34880;&#28082;&#26679;&#26412;&#20998;&#26512;&#21644;&#21560;&#28895;&#21490;&#25968;&#25454;&#36827;&#34892;&#32954;&#30284;&#30340;&#32954;&#30149;&#23398;&#27700;&#24179;&#26816;&#27979;&#65292;&#22312;&#20025;&#40614;&#21335;&#37096;&#22320;&#21306;&#30340;&#22823;&#37327;&#24739;&#26377;&#39118;&#38505;&#30340;&#20154;&#32676;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27169;&#22411;&#22312;&#32954;&#30149;&#19987;&#23478;&#25552;&#20379;&#30340;&#35786;&#26029;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#65288;&#25688;&#35201;&#24635;&#32467;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#65288;LC&#65289;&#20173;&#28982;&#26159;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#26202;&#26399;&#35786;&#26029;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#30340;&#26089;&#26399;&#26816;&#27979;&#31574;&#30053;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#23637;&#31034;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#26816;&#27979;&#21508;&#31181;&#30142;&#30149;&#12290;&#22312;&#36825;&#20010;&#22238;&#39038;&#24615;&#24320;&#21457;&#21644;&#39564;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#65288;DES&#65289;&#30340;ML&#27169;&#22411;&#29992;&#20110;LC&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#26469;&#33258;&#20025;&#40614;&#24739;&#26377;&#39118;&#38505;&#30340;&#22823;&#37327;&#20154;&#21475;&#30340;&#26631;&#20934;&#34880;&#28082;&#26679;&#26412;&#20998;&#26512;&#21644;&#21560;&#28895;&#21490;&#25968;&#25454;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;2009&#24180;&#33267;2018&#24180;&#22312;&#20025;&#40614;&#21335;&#37096;&#22320;&#21306;&#30097;&#20284;LC&#30340;&#25152;&#26377;&#24739;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;DES&#27169;&#22411;&#39564;&#35777;&#21644;&#27604;&#36739;&#20102;&#20116;&#20301;&#32954;&#30149;&#19987;&#23478;&#25552;&#20379;&#30340;&#35786;&#26029;&#39044;&#27979;&#12290;&#22312;38944&#21517;&#24739;&#32773;&#20013;&#65292;9940&#21517;&#24739;&#32773;&#26377;&#23436;&#25972;&#25968;&#25454;&#65292;&#20854;&#20013;2505&#21517;&#65288;25%&#65289;&#24739;&#26377;LC&#12290;DES&#27169;&#22411;&#30340;roc&#26354;&#32447;&#19979;&#38754;&#31215;&#36798;&#21040;0.5&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09596v1 Announce Type: new  Abstract: Lung cancer (LC) remains the primary cause of cancer-related mortality, largely due to late-stage diagnoses. Effective strategies for early detection are therefore of paramount importance. In recent years, machine learning (ML) has demonstrated considerable potential in healthcare by facilitating the detection of various diseases. In this retrospective development and validation study, we developed an ML model based on dynamic ensemble selection (DES) for LC detection. The model leverages standard blood sample analysis and smoking history data from a large population at risk in Denmark. The study includes all patients examined on suspicion of LC in the Region of Southern Denmark from 2009 to 2018. We validated and compared the predictions by the DES model with diagnoses provided by five pulmonologists. Among the 38,944 patients, 9,940 had complete data of which 2,505 (25\%) had LC. The DES model achieved an area under the roc curve of 0.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#24213;&#23618;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#22270;&#26469;&#26377;&#25928;&#22320;&#37325;&#26500;&#38543;&#26426;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#21363;&#24213;&#23618;&#31354;&#38388;&#26159;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#19988;&#36830;&#25509;&#27010;&#29575;&#26159;&#23884;&#20837;&#22312;$\mathbb{R}^N$&#20013;&#30340;&#27969;&#24418;&#20013;&#28857;&#20043;&#38388;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#20005;&#26684;&#36882;&#20943;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.09591</link><description>&lt;p&gt;
&#37325;&#26500;&#38543;&#26426;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the Geometry of Random Geometric Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#24213;&#23618;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#22270;&#26469;&#26377;&#25928;&#22320;&#37325;&#26500;&#38543;&#26426;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#21363;&#24213;&#23618;&#31354;&#38388;&#26159;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#19988;&#36830;&#25509;&#27010;&#29575;&#26159;&#23884;&#20837;&#22312;$\mathbb{R}^N$&#20013;&#30340;&#27969;&#24418;&#20013;&#28857;&#20043;&#38388;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#20005;&#26684;&#36882;&#20943;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20960;&#20309;&#22270;&#26159;&#22312;&#24230;&#37327;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20174;&#24230;&#37327;&#31354;&#38388;&#20013;&#37319;&#26679;&#28857;&#65292;&#28982;&#21518;&#20197;&#20381;&#36182;&#20110;&#23427;&#20204;&#20043;&#38388;&#36317;&#31163;&#30340;&#27010;&#29575;&#29420;&#31435;&#22320;&#36830;&#25509;&#27599;&#23545;&#37319;&#26679;&#28857;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#26377;&#25928;&#22320;&#20174;&#37319;&#26679;&#30340;&#22270;&#20013;&#37325;&#26500;&#24213;&#23618;&#31354;&#38388;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21363;&#20551;&#35774;&#24213;&#23618;&#31354;&#38388;&#26159;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#19988;&#36830;&#25509;&#27010;&#29575;&#26159;&#23884;&#20837;&#22312;$\mathbb{R}^N$&#20013;&#30340;&#27969;&#24418;&#20013;&#28857;&#20043;&#38388;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#20005;&#26684;&#36882;&#20943;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34917;&#20805;&#20102;&#22823;&#37327;&#20851;&#20110;&#27969;&#24418;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#22312;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#28857;&#21450;&#20854;&#65288;&#36817;&#20284;&#30340;&#65289;&#36317;&#31163;&#20013;&#24674;&#22797;&#20986;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09591v1 Announce Type: new  Abstract: Random geometric graphs are random graph models defined on metric spaces. Such a model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs. In this work, we show how to efficiently reconstruct the geometry of the underlying space from the sampled graph under the manifold assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\mathbb{R}^N$. Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distances.
&lt;/p&gt;</description></item><item><title>MLTCP&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#20849;&#20139;GPU&#38598;&#32676;&#20013;&#30340;DNN&#35757;&#32451;&#20316;&#19994;&#30340;&#25317;&#22622;&#25511;&#21046;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#21457;&#36865;&#30340;&#23383;&#33410;&#25968;&#36827;&#34892;&#32553;&#25918;&#65292;&#20351;&#19981;&#21516;&#20316;&#19994;&#30340;&#27969;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#32593;&#32476;&#26497;&#22823;&#22320;&#21152;&#24555;&#35757;&#32451;&#20316;&#19994;&#30340;&#23436;&#25104;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.09589</link><description>&lt;p&gt;
MLTCP:&#29992;&#20110;DNN&#35757;&#32451;&#30340;&#25317;&#22622;&#25511;&#21046;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
MLTCP: Congestion Control for DNN Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09589
&lt;/p&gt;
&lt;p&gt;
MLTCP&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#20849;&#20139;GPU&#38598;&#32676;&#20013;&#30340;DNN&#35757;&#32451;&#20316;&#19994;&#30340;&#25317;&#22622;&#25511;&#21046;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#21457;&#36865;&#30340;&#23383;&#33410;&#25968;&#36827;&#34892;&#32553;&#25918;&#65292;&#20351;&#19981;&#21516;&#20316;&#19994;&#30340;&#27969;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#32593;&#32476;&#26497;&#22823;&#22320;&#21152;&#24555;&#35757;&#32451;&#20316;&#19994;&#30340;&#23436;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MLTCP&#65292;&#19968;&#31181;&#25216;&#26415;&#26469;&#22686;&#24378;&#24403;&#21069;&#30340;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#65292;&#20197;&#21152;&#36895;&#22312;&#20849;&#20139;GPU&#38598;&#32676;&#20013;&#36827;&#34892;&#30340;DNN&#35757;&#32451;&#20316;&#19994;&#12290;MLTCP&#20351;&#31454;&#20105;&#32593;&#32476;&#24102;&#23485;&#30340;&#20316;&#19994;&#30340;&#36890;&#20449;&#38454;&#27573;&#30456;&#20114;&#20132;&#38169;&#65292;&#20174;&#32780;&#39640;&#25928;&#21033;&#29992;&#32593;&#32476;&#12290;MLTCP&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;&#20851;&#38190;&#27010;&#24565;&#27934;&#23519;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#21407;&#21017;&#65306;DNN&#35757;&#32451;&#27969;&#24212;&#35813;&#26681;&#25454;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#21457;&#36865;&#30340;&#23383;&#33410;&#25968;&#26469;&#32553;&#25918;&#20854;&#25317;&#22622;&#31383;&#21475;&#22823;&#23567;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#36825;&#20010;&#21407;&#21017;&#25972;&#21512;&#21040;&#24403;&#21069;&#30340;&#25317;&#22622;&#25511;&#21046;&#21327;&#35758;&#20013;&#26159;&#30452;&#25509;&#30340;&#65306;&#36890;&#36807;&#22312;Reno&#12289;CUBIC&#25110;DCQCN&#20013;&#28155;&#21152;30-60&#34892;&#20195;&#30721;&#65292;MLTCP&#21487;&#20197;&#22312;&#20960;&#20010;&#35757;&#32451;&#36845;&#20195;&#20869;&#23558;&#19981;&#21516;&#20316;&#19994;&#30340;&#27969;&#31283;&#23450;&#22320;&#36716;&#21270;&#20026;&#20132;&#38169;&#29366;&#24577;&#65292;&#26080;&#35770;&#31454;&#20105;&#27969;&#30340;&#25968;&#37327;&#25110;&#27599;&#20010;&#27969;&#30340;&#24320;&#22987;&#26102;&#38388;&#22914;&#20309;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;DNN&#35757;&#32451;&#20316;&#19994;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21551;&#29992;MLTCP&#21487;&#20197;&#21152;&#24555;&#24179;&#22343;&#21644;99th pe&#30340;&#32467;&#26463;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09589v1 Announce Type: cross  Abstract: We present MLTCP, a technique to augment today's congestion control algorithms to accelerate DNN training jobs in shared GPU clusters. MLTCP enables the communication phases of jobs that compete for network bandwidth to interleave with each other, thereby utilizing the network efficiently. At the heart of MLTCP lies a very simple principle based on a key conceptual insight: DNN training flows should scale their congestion window size based on the number of bytes sent at each training iteration. We show that integrating this principle into today's congestion control protocols is straightforward: by adding 30-60 lines of code to Reno, CUBIC, or DCQCN, MLTCP stabilizes flows of different jobs into an interleaved state within a few training iterations, regardless of the number of competing flows or the start time of each flow. Our experiments with popular DNN training jobs demonstrate that enabling MLTCP accelerates the average and 99th pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26435;&#37325;&#27491;&#21017;&#21270;&#26041;&#27861;WERank&#65292;&#29992;&#20110;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#22604;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.09586</link><description>&lt;p&gt;
WERank: &#38024;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31561;&#32423;&#36864;&#21270;&#39044;&#38450;&#30340;&#26435;&#37325;&#27491;&#35268;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26435;&#37325;&#27491;&#21017;&#21270;&#26041;&#27861;WERank&#65292;&#29992;&#20110;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#22604;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#32500;&#24230;&#22349;&#22604;&#65288;&#20063;&#31216;&#20026;&#31561;&#32423;&#36864;&#21270;&#65289;&#65292;&#20854;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#34987;&#26144;&#23556;&#21040;&#34920;&#31034;&#31354;&#38388;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#12290;&#26368;&#26032;&#30340;&#38450;&#27490;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#25110;&#26550;&#26500;&#25216;&#24039;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#26435;&#37325;&#27491;&#21017;&#21270;&#26041;&#27861;WERank&#65292;&#29992;&#20110;&#39044;&#38450;&#32593;&#32476;&#19981;&#21516;&#23618;&#27425;&#30340;&#32500;&#24230;&#22349;&#22604;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#25968;&#23398;&#35770;&#35777;&#35777;&#26126;&#20102;&#35813;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#38450;&#27490;&#32500;&#24230;&#22349;&#22604;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#22312;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;WERank&#30340;&#24433;&#21709;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#32500;&#24230;&#22349;&#22604;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09586v1 Announce Type: new  Abstract: A common phenomena confining the representation quality in Self-Supervised Learning (SSL) is dimensional collapse (also known as rank degeneration), where the learned representations are mapped to a low dimensional subspace of the representation space. The State-of-the-Art SSL methods have shown to suffer from dimensional collapse and fall behind maintaining full rank. Recent approaches to prevent this problem have proposed using contrastive losses, regularization techniques, or architectural tricks. We propose WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network. We provide empirical evidence and mathematical justification to demonstrate the effectiveness of the proposed regularization method in preventing dimensional collapse. We verify the impact of WERank on graph SSL where dimensional collapse is more pronounced due to the lack of proper data augmentation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#23450;&#20301;&#31070;&#32463;&#32593;&#32476;&#65288;P-NN&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;&#38477;&#20302;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#20013;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.09580</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#20013;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#65306;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Complexity Reduction in Machine Learning-Based Wireless Positioning: Minimum Description Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#23450;&#20301;&#31070;&#32463;&#32593;&#32476;&#65288;P-NN&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;&#38477;&#20302;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#20013;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#65288;WP&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;WP&#31639;&#27861;&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#20102;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#29305;&#24449;&#65292;&#36825;&#23545;&#20110;&#31227;&#21160;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#31105;&#27490;&#30340;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#20301;&#31070;&#32463;&#32593;&#32476;&#65288;P-NN&#65289;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;WP&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#36873;&#25321;&#22522;&#20110;&#26368;&#22823;&#21151;&#29575;&#27979;&#37327;&#21450;&#20854;&#26102;&#38388;&#20301;&#32622;&#65292;&#20197;&#20256;&#36798;&#36827;&#34892;WP&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#20449;&#21495;&#20108;&#36827;&#21046;&#36873;&#25321;&#19978;&#20351;&#29992;&#20449;&#24687;&#35770;&#24230;&#37327;&#65292;&#20248;&#21270;&#20102;&#26399;&#26395;&#26377;&#29992;&#20449;&#24687;&#37327;&#21644;&#20998;&#31867;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09580v1 Announce Type: new  Abstract: A recent line of research has been investigating deep learning approaches to wireless positioning (WP). Although these WP algorithms have demonstrated high accuracy and robust performance against diverse channel conditions, they also have a major drawback: they require processing high-dimensional features, which can be prohibitive for mobile applications. In this work, we design a positioning neural network (P-NN) that substantially reduces the complexity of deep learning-based WP through carefully crafted minimum description features. Our feature selection is based on maximum power measurements and their temporal locations to convey information needed to conduct WP. We also develop a novel methodology for adaptively selecting the size of feature space, which optimizes over balancing the expected amount of useful information and classification capability, quantified using information-theoretic measures on the signal bin selection. Numeri
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09573</link><description>&lt;p&gt;
&#34676;&#34678;&#24341;&#36215;&#30340;&#21464;&#21270;&#65306;&#21033;&#29992;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#36827;&#34892;&#36828;&#35265;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09573
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#27788;&#20013;&#65292;&#20004;&#20010;&#21021;&#22987;&#26465;&#20214;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21576;&#25351;&#25968;&#32423;&#25918;&#22823;&#65292;&#23548;&#33268;&#36965;&#36828;&#30340;&#32467;&#26524;&#65292;&#20063;&#34987;&#31216;&#20026;&#34676;&#34678;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#36828;&#26399;&#20805;&#28385;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#38590;&#20197;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#26469;&#36890;&#36807;&#20811;&#26381;&#28151;&#27788;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65288;1&#65289;&#22823;&#37327;&#30340;&#21382;&#21490;&#24207;&#21015;&#21644;&#65288;2&#65289;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#26469;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#23558;&#19968;&#20010;&#20648;&#22791;&#35013;&#32622;&#36830;&#25509;&#21040;&#36716;&#25442;&#22120;&#19978;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#19968;&#32452;&#20648;&#22791;&#35013;&#32622;&#26469;&#20943;&#23569;&#30001;&#20110;&#21021;&#22987;&#21270;&#21464;&#21270;&#32780;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#65292;&#21253;&#25324;NLinear&#12289;Pyformer&#12289;Informer&#12289;Autoformer&#21644;&#22522;&#20934;Transformer&#65292;&#20854;&#35823;&#24046;&#20943;&#23569;&#39640;&#36798;-89.43&#65285;&#65292;&#36866;&#29992;&#20110;ETTh&#12289;ETTm&#21644;&#31354;&#27668;&#36136;&#37327;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09573v1 Announce Type: cross  Abstract: In Chaos, a minor divergence between two initial conditions exhibits exponential amplification over time, leading to far-away outcomes, known as the butterfly effect. Thus, the distant future is full of uncertainty and hard to forecast. We introduce Group Reservoir Transformer to predict long-term events more accurately and robustly by overcoming two challenges in Chaos: (1) the extensive historical sequences and (2) the sensitivity to initial conditions. A reservoir is attached to a Transformer to efficiently handle arbitrarily long historical lengths, with an extension of a group of reservoirs to reduce the uncertainty due to the initialization variations. Our architecture consistently outperforms state-of-the-art DNN models in multivariate time series, including NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an error reduction of up to -89.43\% in various fields such as ETTh, ETTm, and air quality, demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;Neyman-Pearson&#20998;&#31867;&#20013;&#26080;&#20998;&#24067;&#29575;&#30340;&#23436;&#25972;&#29305;&#24449;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20960;&#20309;&#26465;&#20214;&#65292;&#21363;&#19977;&#28857;&#20998;&#31163;&#26465;&#20214;&#65292;&#21051;&#30011;&#20102;&#30828;&#20998;&#31867;&#22120;&#21644;&#31616;&#21333;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20108;&#20998;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.09560</link><description>&lt;p&gt;
Neyman-Pearson&#20998;&#31867;&#20013;&#30340;&#26080;&#20998;&#24067;&#29575;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Rates in Neyman-Pearson Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;Neyman-Pearson&#20998;&#31867;&#20013;&#26080;&#20998;&#24067;&#29575;&#30340;&#23436;&#25972;&#29305;&#24449;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20960;&#20309;&#26465;&#20214;&#65292;&#21363;&#19977;&#28857;&#20998;&#31163;&#26465;&#20214;&#65292;&#21051;&#30011;&#20102;&#30828;&#20998;&#31867;&#22120;&#21644;&#31616;&#21333;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20108;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;Neyman-Pearson&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#27169;&#25311;&#20102;&#19981;&#24179;&#34913;&#20998;&#31867;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#26368;&#23567;&#21270;&#19982;&#20998;&#24067;$\mu_1$&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#21516;&#26102;&#20445;&#35777;&#19982;&#21478;&#19968;&#20010;&#20998;&#24067;$\mu_0$&#30456;&#20851;&#30340;&#38169;&#35823;&#36739;&#20302;&#12290;&#32473;&#23450;&#19968;&#20010;&#22266;&#23450;&#30340;VC&#20998;&#31867;&#22120;&#31867;$\mathcal{H}$&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#26080;&#20998;&#24067;&#29575;&#30340;&#23436;&#25972;&#29305;&#24449;&#65292;&#21363;&#25152;&#26377;&#37197;&#23545;$(\mu_0, \mu_1)$&#30340;&#26497;&#23567;&#21270;&#29575;&#12290;&#36825;&#20123;&#36895;&#29575;&#28041;&#21450;&#21040;&#20102;&#30828;&#20998;&#31867;&#22120;&#21644;&#31616;&#21333;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20108;&#20998;&#26465;&#20214;&#65292;&#23427;&#20204;&#26159;&#26681;&#25454;&#19968;&#20010;&#31616;&#21333;&#30340;&#20960;&#20309;&#26465;&#20214;&#65292;&#21363;&#19977;&#28857;&#20998;&#31163;&#26465;&#20214;&#26469;&#21051;&#30011;&#30340;&#65292;&#19982;VC&#32500;&#24230;&#30053;&#26377;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09560v1 Announce Type: new  Abstract: We consider the problem of Neyman-Pearson classification which models unbalanced classification settings where error w.r.t. a distribution $\mu_1$ is to be minimized subject to low error w.r.t. a different distribution $\mu_0$. Given a fixed VC class $\mathcal{H}$ of classifiers to be minimized over, we provide a full characterization of possible distribution-free rates, i.e., minimax rates over the space of all pairs $(\mu_0, \mu_1)$. The rates involve a dichotomy between hard and easy classes $\mathcal{H}$ as characterized by a simple geometric condition, a three-points-separation condition, loosely related to VC dimension.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.09558</link><description>&lt;p&gt;
&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21452;&#21521;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generative Pre-training for Improving Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20197;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#39033;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#21521;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#35201;&#20040;&#26159;&#38543;&#26426;&#23631;&#34109;&#26631;&#35760;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#21521;&#21450;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;BiTimelyGPT&#65289;&#65292;&#23427;&#36890;&#36807;&#20132;&#26367;&#30340;Transformer&#23618;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#21644;&#19978;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#21407;&#22987;&#20998;&#24067;&#21644;&#25968;&#25454;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#20840;&#31209;&#21069;&#21521;&#21644;&#21518;&#21521;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290; &#20351;&#29992;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#65292;BiTimelyGPT&#22312;&#39044;&#27979;&#31070;&#32463;&#21151;&#33021;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#27880;&#24847;&#21147;&#28909;&#22270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;BiTimelyGPT&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#21028;&#21035;&#24615;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#34892;&#20026;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#24863;&#30693;&#30340;&#28145;&#23618;&#32858;&#31867;&#26041;&#27861;&#65292;&#23558;&#22810;&#34892;&#20026;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#33509;&#24178;&#21333;&#19968;&#34892;&#20026;&#23376;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09550</link><description>&lt;p&gt;
&#25552;&#39640;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Dataset Clustering for Improved Offline Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#34892;&#20026;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#24863;&#30693;&#30340;&#28145;&#23618;&#32858;&#31867;&#26041;&#27861;&#65292;&#23558;&#22810;&#34892;&#20026;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#33509;&#24178;&#21333;&#19968;&#34892;&#20026;&#23376;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#39069;&#22806;&#30340;&#22312;&#32447;&#20132;&#20114;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#22266;&#23450;&#30340;&#65292;&#20854;&#36136;&#37327;&#25104;&#20026;&#24433;&#21709;&#23398;&#20064;&#31574;&#30053;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#34892;&#20026;&#30340;&#25968;&#25454;&#38598;&#29305;&#24449;&#65292;&#25351;&#31034;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;&#23637;&#29616;&#19981;&#21516;&#34892;&#20026;&#30340;&#22810;&#20010;&#31574;&#30053;&#25910;&#38598;&#30340;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#21333;&#19968;&#34892;&#20026;&#30340;&#25968;&#25454;&#38598;&#23558;&#20165;&#20351;&#29992;&#19968;&#20010;&#31574;&#30053;&#25910;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20174;&#21333;&#19968;&#34892;&#20026;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#31574;&#30053;&#36890;&#24120;&#27604;&#20174;&#22810;&#34892;&#20026;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#31574;&#30053;&#34920;&#29616;&#26356;&#22909;&#65292;&#23613;&#31649;&#21333;&#19968;&#34892;&#20026;&#25968;&#25454;&#38598;&#25317;&#26377;&#26356;&#23569;&#30340;&#31034;&#20363;&#21644;&#36739;&#20302;&#30340;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#24863;&#30693;&#30340;&#28145;&#23618;&#32858;&#31867;&#26041;&#27861;&#65292;&#23558;&#22810;&#34892;&#20026;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#33509;&#24178;&#21333;&#19968;&#34892;&#20026;&#23376;&#38598;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#19979;&#28216;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09550v1 Announce Type: new  Abstract: Offline policy learning aims to discover decision-making policies from previously-collected datasets without additional online interactions with the environment. As the training dataset is fixed, its quality becomes a crucial determining factor in the performance of the learned policy. This paper studies a dataset characteristic that we refer to as multi-behavior, indicating that the dataset is collected using multiple policies that exhibit distinct behaviors. In contrast, a uni-behavior dataset would be collected solely using one policy. We observed that policies learned from a uni-behavior dataset typically outperform those learned from multi-behavior datasets, despite the uni-behavior dataset having fewer examples and less diversity. Therefore, we propose a behavior-aware deep clustering approach that partitions multi-behavior datasets into several uni-behavior subsets, thereby benefiting downstream policy learning. Our approach is fl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#32463;&#39564;&#22238;&#25918;&#36896;&#25104;&#30340;&#20248;&#21270;&#19981;&#31283;&#23450;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#30340;&#20462;&#25913;&#26469;&#24179;&#34913;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22238;&#25918;&#24335;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09542</link><description>&lt;p&gt;
&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65306;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#36817;&#31471;&#28857;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#32463;&#39564;&#22238;&#25918;&#36896;&#25104;&#30340;&#20248;&#21270;&#19981;&#31283;&#23450;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#30340;&#20462;&#25913;&#26469;&#24179;&#34913;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22238;&#25918;&#24335;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#20174;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#37117;&#20351;&#29992;&#32463;&#39564;&#22238;&#25918;&#26469;&#21516;&#26102;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#24230;&#25311;&#21512;&#20808;&#21069;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#65306;&#20351;&#29992;&#32463;&#39564;&#22238;&#25918;&#35757;&#32451;&#30340;&#32593;&#32476;&#24448;&#24448;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#20248;&#21270;&#36712;&#36857;&#65292;&#24433;&#21709;&#20854;&#25972;&#20307;&#20934;&#30830;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22238;&#25918;&#32531;&#20914;&#21306;&#23384;&#20648;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#34920;&#26126;&#36825;&#20010;&#38382;&#39064;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20248;&#21270;&#20960;&#20309;&#30340;&#31616;&#21333;&#20462;&#25913;&#26469;&#26368;&#23567;&#21270;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#65292;&#22312;&#21482;&#20801;&#35768;&#36880;&#28176;&#25913;&#21464;&#36807;&#21435;&#25968;&#25454;&#30340;&#38544;&#34255;&#28608;&#27963;&#30340;&#21516;&#26102;&#65292;&#24179;&#34913;&#20102;&#20174;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#20013;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;LPR&#22312;&#22522;&#20110;&#22238;&#25918;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20013;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09542v1 Announce Type: new  Abstract: In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#22240;&#20026;&#23454;&#38469;&#25915;&#20987;&#32773;&#21487;&#33021;&#32570;&#20047;&#20934;&#30830;&#30340;&#31169;&#26377;&#25968;&#25454;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#38598;&#21487;&#33021;&#30456;&#23545;&#23481;&#26131;&#34987;&#38450;&#24481;&#12290;</title><link>https://arxiv.org/abs/2402.09540</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#22240;&#20026;&#23454;&#38469;&#25915;&#20987;&#32773;&#21487;&#33021;&#32570;&#20047;&#20934;&#30830;&#30340;&#31169;&#26377;&#25968;&#25454;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#38598;&#21487;&#33021;&#30456;&#23545;&#23481;&#26131;&#34987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36739;&#23567;&#30340;&#38544;&#31169;&#21442;&#25968;&#949;&#65292;&#949;-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#65292;&#21363;&#27809;&#26377;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#33021;&#22815;&#25104;&#21151;&#30830;&#23450;&#19968;&#20010;&#20154;&#30340;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;DP&#30340;&#20445;&#35777;&#26159;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#65292;&#22240;&#20026;&#65306;a&#65289;&#21363;&#20351;&#25915;&#20987;&#32773;&#24050;&#32463;&#30693;&#36947;&#25968;&#25454;&#38598;&#20013;&#38500;&#19968;&#20010;&#20154;&#30340;&#35760;&#24405;&#20043;&#22806;&#30340;&#25152;&#26377;&#35760;&#24405;&#65307;b&#65289;&#23427;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#22343;&#21248;&#36866;&#29992;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#26679;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65306;&#23454;&#38469;&#25915;&#20987;&#32773;&#21487;&#33021;&#32570;&#20047;&#65288;&#20960;&#20046;&#25152;&#26377;&#65289;&#31169;&#26377;&#25968;&#25454;&#30340;&#31934;&#30830;&#30693;&#35782;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#27604;&#26368;&#22351;&#24773;&#20917;&#30340;&#25968;&#25454;&#38598;&#26356;&#23481;&#26131;&#34987;&#38450;&#24481;&#12290;&#36825;&#20123;&#32771;&#34385;&#25512;&#21160;&#20102;&#20855;&#26377;&#22823;&#30340;&#38544;&#31169;&#21442;&#25968;&#65288;&#20363;&#22914;&#949;&#8805;7&#65289;&#30340;DP&#27169;&#22411;&#30340;&#24037;&#19994;&#37096;&#32626;&#65292;&#24182;&#19988;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#20855;&#26377;&#22823;&#949;&#30340;DP&#21487;&#20197;&#25104;&#21151;&#38450;&#24481;&#26368;&#20808;&#36827;&#30340;MIA&#12290;&#29616;&#26377;&#30340;DP&#27169;&#22411;&#30740;&#31350;&#19968;&#33324;&#38598;&#20013;&#20110;&#23567;&#949;&#65292;&#22240;&#27492;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#20855;&#26377;&#36739;&#22823;&#949;&#30340;DP&#21487;&#20197;&#38450;&#24481;&#23454;&#38469;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09540v1 Announce Type: cross  Abstract: For small privacy parameter $\epsilon$, $\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\epsilon \geq 7$), and it has been observed empirically that DP with large $\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP the
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24418;&#23494;&#24230;&#20989;&#25968;&#30340;&#20869;&#22312;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#40654;&#26364;&#27969;&#24418;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09529</link><description>&lt;p&gt;
&#27969;&#24418;&#23494;&#24230;&#20989;&#25968;&#65306;&#29992;&#20110;&#39564;&#35777;&#27969;&#24418;&#23398;&#20064;&#30340;&#20869;&#22312;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Manifold Density Function: An Intrinsic Method for the Validation of Manifold Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09529
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24418;&#23494;&#24230;&#20989;&#25968;&#30340;&#20869;&#22312;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#40654;&#26364;&#27969;&#24418;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27969;&#24418;&#23494;&#24230;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#30340;&#20869;&#22312;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;Ripley&#30340;K&#20989;&#25968;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#23558;&#27969;&#24418;&#23398;&#20064;&#31639;&#27861;&#30340;&#36755;&#20986;&#19982;&#28508;&#22312;&#27969;&#24418;&#30340;&#32467;&#26500;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#27969;&#24418;&#23494;&#24230;&#20989;&#25968;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#40654;&#26364;&#27969;&#24418;&#31867;&#21035;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;-&#21338;&#20869;&#23450;&#29702;&#23558;&#27969;&#24418;&#23494;&#24230;&#20989;&#25968;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#20108;&#32500;&#27969;&#24418;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#36229;&#26354;&#38754;&#65292;&#21487;&#20197;&#20351;&#29992;&#31532;&#19968;&#20010;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#20540;&#26469;&#36817;&#20284;&#27969;&#24418;&#23494;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09529v1 Announce Type: new  Abstract: We introduce the manifold density function, which is an intrinsic method to validate manifold learning techniques. Our approach adapts and extends Ripley's $K$-function, and categorizes in an unsupervised setting the extent to which an output of a manifold learning algorithm captures the structure of a latent manifold. Our manifold density function generalizes to broad classes of Riemannian manifolds. In particular, we extend the manifold density function to general two-manifolds using the Gauss-Bonnet theorem, and demonstrate that the manifold density function for hypersurfaces is well approximated using the first Laplacian eigenvalue. We prove desirable convergence and robustness properties.
&lt;/p&gt;</description></item><item><title>Higgs&#37492;&#21035;&#30340;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;&#27169;&#22411;&#23558;&#39044;&#22788;&#29702;&#21644;&#37327;&#23376;&#20998;&#31867;&#31639;&#27861;&#32479;&#19968;&#20026;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#37492;&#21035;LHC&#20013;&#30340;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.09524</link><description>&lt;p&gt;
Higgs&#37492;&#21035;&#30340;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Guided Quantum Compression for Higgs Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09524
&lt;/p&gt;
&lt;p&gt;
Higgs&#37492;&#21035;&#30340;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;&#27169;&#22411;&#23558;&#39044;&#22788;&#29702;&#21644;&#37327;&#23376;&#20998;&#31867;&#31639;&#27861;&#32479;&#19968;&#20026;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#37492;&#21035;LHC&#20013;&#30340;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.09524v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#26412;&#26032;&#39062;&#19988;&#26377;&#21069;&#26223;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#23545;&#24403;&#21069;&#21487;&#29992;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#19978;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#36890;&#36807;&#20351;&#29992;&#38477;&#32500;&#31639;&#27861;&#65288;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#22312;&#36890;&#36807;&#37327;&#23376;&#27169;&#22411;&#20043;&#21069;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#32463;&#20856;&#33258;&#21160;&#32534;&#30721;&#22120;&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23558;&#39044;&#22788;&#29702;&#21644;&#37327;&#23376;&#20998;&#31867;&#31639;&#27861;&#32479;&#19968;&#21040;&#21333;&#20010;&#21487;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26550;&#26500;&#65306;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#27169;&#22411;&#22312;LHC&#30340;&#36136;&#23376;-&#36136;&#23376;&#30896;&#25758;&#20013;&#37492;&#21035;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#30340;&#23454;&#29992;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#21017;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09524v1 Announce Type: cross  Abstract: Quantum machine learning provides a fundamentally novel and promising approach to analyzing data. However, many data sets are too complex for currently available quantum computers. Consequently, quantum machine learning applications conventionally resort to dimensionality reduction algorithms, e.g., auto-encoders, before passing data through the quantum models. We show that using a classical auto-encoder as an independent preprocessing step can significantly decrease the classification performance of a quantum machine learning algorithm. To ameliorate this issue, we design an architecture that unifies the preprocessing and quantum classification algorithms into a single trainable model: the guided quantum compression model. The utility of this model is demonstrated by using it to identify the Higgs boson in proton-proton collisions at the LHC, where the conventional approach proves ineffective. Conversely, the guided quantum compressio
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09497</link><description>&lt;p&gt;
&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Secure Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09497
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#26085;&#24120;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#23588;&#20854;&#22312;&#32534;&#31243;&#20013;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#35757;&#32451;LMs&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#21644;&#20154;&#31867;&#20559;&#22909;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#24378;&#20102;LMs&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#26696;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#25351;&#20196;&#35843;&#20248;&#30340;LMs&#20063;&#32463;&#24120;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SafeCoder&#26469;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;SafeCoder&#20351;&#29992;&#19968;&#20010;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23433;&#20840;&#20026;&#20013;&#24515;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#25910;&#38598;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#23433;&#20840;&#24494;&#35843;&#19982;&#26631;&#20934;&#30340;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#20197;&#20415;&#21516;&#26102;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;SafeCoder&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09497v1 Announce Type: cross  Abstract: Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.09495</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#28508;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Potential of Network-Based Features for Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20132;&#26131;&#27450;&#35784;&#32473;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#38590;&#20197;&#36319;&#19978;&#27450;&#35784;&#25112;&#26415;&#30340;&#28436;&#21464;&#65292;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#21644;&#28431;&#25253;&#29575;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#35782;&#21035;&#27450;&#35784;&#27169;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#65288;PPR&#65289;&#31639;&#27861;&#36890;&#36807;&#20998;&#26512;&#37329;&#34701;&#36134;&#25143;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#27604;&#36739;&#20256;&#32479;&#29305;&#24449;&#19982;&#28155;&#21152;PPR&#22312;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;PPR&#29305;&#24449;&#25552;&#20379;&#20102;&#29420;&#29305;&#32780;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#20854;&#39640;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#24471;&#20197;&#35777;&#26126;&#12290;&#29305;&#24449;&#31283;&#23450;&#24615;&#20998;&#26512;&#35777;&#23454;&#20102;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09495v1 Announce Type: cross  Abstract: Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consist
&lt;/p&gt;</description></item><item><title>PMGDA&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197; efficiently &#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25214;&#21040;&#19982;&#20915;&#31574;&#32773;&#20559;&#22909;&#23436;&#20840;&#21305;&#37197;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.09492</link><description>&lt;p&gt;
PMGDA: &#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PMGDA: A Preference-based Multiple Gradient Descent Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09492
&lt;/p&gt;
&lt;p&gt;
PMGDA&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197; efficiently &#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25214;&#21040;&#19982;&#20915;&#31574;&#32773;&#20559;&#22909;&#23436;&#20840;&#21305;&#37197;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#23547;&#25214;&#19982;&#20915;&#31574;&#32773;&#32473;&#23450;&#20559;&#22909;&#23436;&#20840;&#21305;&#37197;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#35268;&#27169;&#36739;&#22823;&#65292;&#34429;&#28982;&#26377;&#21487;&#29992;&#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#20294;&#29616;&#26377;&#30340;&#31639;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#39044;&#27979;-&#26657;&#27491;&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#25214;&#21040;&#20915;&#31574;&#32773;&#25152;&#38656;&#30340;&#31934;&#30830;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32422;&#26463;&#20989;&#25968;&#26469;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20351;&#35299;&#19982;&#29992;&#25143;&#29305;&#23450;&#20559;&#22909;&#23545;&#40784;&#65292;&#36825;&#20010;&#32422;&#26463;&#20989;&#25968;&#21487;&#20197;&#19982;&#22810;&#20010;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31934;&#30830;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09492v1 Announce Type: new  Abstract: It is desirable in many multi-objective machine learning applications, such as multi-task learning and multi-objective reinforcement learning, to find a Pareto optimal solution that can exactly match a given preference of decision-makers. These problems are often large-scale with available gradient information but cannot be handled very well by the existing algorithms. To tackle this critical issue, this paper proposes a novel predict-and-correct framework for locating the exact Pareto optimal solutions required by a decision maker. In the proposed framework, a constraint function is introduced in the search progress to align the solution with a user-specific preference, which can be optimized simultaneously with multiple objective functions. Experimental results show that our proposed method can efficiently find exact Pareto optimal solutions for standard benchmarks, multi-task, and multi-objective reinforcement learning problems with m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#32852;&#32593;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#28201;&#23460;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#30417;&#27979;&#21644;&#35843;&#25511;&#28201;&#23460;&#20869;&#29615;&#22659;&#26465;&#20214;&#65292;&#25552;&#39640;&#20316;&#29289;&#29983;&#38271;&#25928;&#29575;&#21644;&#20135;&#37327;&#65292;&#20943;&#23569;&#36164;&#28304;&#28010;&#36153;&#12290;</title><link>https://arxiv.org/abs/2402.09488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#32852;&#32593;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#28201;&#23460;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Intelligent Agricultural Greenhouse Control System Based on Internet of Things and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09488
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#32852;&#32593;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#28201;&#23460;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#30417;&#27979;&#21644;&#35843;&#25511;&#28201;&#23460;&#20869;&#29615;&#22659;&#26465;&#20214;&#65292;&#25552;&#39640;&#20316;&#29289;&#29983;&#38271;&#25928;&#29575;&#21644;&#20135;&#37327;&#65292;&#20943;&#23569;&#36164;&#28304;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35797;&#22270;&#23558;&#29289;&#32852;&#32593;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#19968;&#20010;&#20808;&#36827;&#30340;&#20892;&#19994;&#28201;&#23460;&#25511;&#21046;&#31995;&#32479;&#12290;&#36890;&#36807;&#23545;&#28201;&#23460;&#20869;&#22266;&#26377;&#29615;&#22659;&#21442;&#25968;&#30340;&#32454;&#33268;&#30417;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25972;&#21512;&#65292;&#33021;&#22815;&#36866;&#24403;&#35843;&#25511;&#28201;&#23460;&#20869;&#30340;&#26465;&#20214;&#12290;&#39044;&#26399;&#30340;&#32467;&#26524;&#26159;&#22686;&#21152;&#20316;&#29289;&#29983;&#38271;&#25928;&#29575;&#21644;&#20135;&#37327;&#65292;&#21516;&#26102;&#20943;&#23569;&#36164;&#28304;&#28010;&#36153;&#12290;&#22312;&#20840;&#29699;&#20154;&#21475;&#25345;&#32493;&#22686;&#38271;&#21644;&#27668;&#20505;&#21464;&#21270;&#19981;&#26029;&#21152;&#21095;&#30340;&#32972;&#26223;&#19979;&#65292;&#20892;&#19994;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#20892;&#19994;&#33539;&#24335;&#24050;&#32463;&#34987;&#35777;&#26126;&#26080;&#27861;&#28385;&#36275;&#39135;&#21697;&#23433;&#20840;&#21644;&#29983;&#20135;&#25928;&#29575;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#28201;&#23460;&#20892;&#19994;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#20316;&#29289;&#31181;&#26893;&#25552;&#20379;&#20102;&#19968;&#20010;&#21463;&#25511;&#30340;&#29615;&#22659;&#26469;&#22686;&#21152;&#20135;&#37327;&#65292;&#25913;&#21892;&#21697;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09488v1 Announce Type: cross  Abstract: This study endeavors to conceptualize and execute a sophisticated agricultural greenhouse control system grounded in the amalgamation of the Internet of Things (IoT) and machine learning. Through meticulous monitoring of intrinsic environmental parameters within the greenhouse and the integration of machine learning algorithms, the conditions within the greenhouse are aptly modulated. The envisaged outcome is an enhancement in crop growth efficiency and yield, accompanied by a reduction in resource wastage. In the backdrop of escalating global population figures and the escalating exigencies of climate change, agriculture confronts unprecedented challenges. Conventional agricultural paradigms have proven inadequate in addressing the imperatives of food safety and production efficiency. Against this backdrop, greenhouse agriculture emerges as a viable solution, proffering a controlled milieu for crop cultivation to augment yields, refin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;UMOEA/D&#26469;&#26500;&#24314;&#22343;&#21248;&#20998;&#24067;&#30340;Pareto&#30446;&#26631;&#65292;&#20197;&#35299;&#20915;&#20808;&#21069;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#20013;&#26377;&#38480;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09486</link><description>&lt;p&gt;
UMOEA/D&#65306;&#22522;&#20110;&#20998;&#35299;&#30340;&#22343;&#21248; Pareto &#30446;&#26631;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
UMOEA/D: A Multiobjective Evolutionary Algorithm for Uniform Pareto Objectives based on Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09486
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;UMOEA/D&#26469;&#26500;&#24314;&#22343;&#21248;&#20998;&#24067;&#30340;Pareto&#30446;&#26631;&#65292;&#20197;&#35299;&#20915;&#20808;&#21069;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#20013;&#26377;&#38480;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20854;&#20013;&#26500;&#24314; Pareto &#21069;&#27839;&#65288;PF&#65289;&#20197;&#26174;&#31034;&#21508;&#31181;&#20559;&#22909;&#19979;&#30340;&#26368;&#20248;&#35299;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992; Pareto &#30446;&#26631;&#38598;&#65288;PF &#19978;&#30340;&#31890;&#23376;&#65289;&#26469;&#34920;&#31034;&#25972;&#20010; PF&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752; PF &#19978; Pareto &#30446;&#26631;&#30340;&#32463;&#39564;&#20998;&#24067;&#65292;&#36825;&#38544;&#21547;&#22320;&#38480;&#21046;&#20102;&#20808;&#21069;&#26041;&#27861;&#20013;&#22810;&#26679;&#24615;&#21644;&#20195;&#34920;&#24615; Pareto &#30446;&#26631;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#26500;&#24314; PF &#19978;&#8220;&#22343;&#21248;&#20998;&#24067;&#8221;&#30340; Pareto &#30446;&#26631;&#65292;&#20197;&#20943;&#36731;&#20808;&#21069;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#20013;&#30340;&#26377;&#38480;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#27491;&#24335;&#23450;&#20041;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#8220;&#22343;&#21248;&#24615;&#8221;&#30340;&#30740;&#31350;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270; Pareto &#21069;&#27839;&#19978;&#30340;&#26368;&#22823;&#26368;&#23567;&#36317;&#31163;&#65292;&#24471;&#21040;&#28176;&#36817;&#21644;&#38750;&#28176;&#36817;&#22343;&#21248; Pareto &#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09486v1 Announce Type: new  Abstract: Multiobjective optimization (MOO) is prevalent in numerous applications, in which a Pareto front (PF) is constructed to display optima under various preferences. Previous methods commonly utilize the set of Pareto objectives (particles on the PF) to represent the entire PF. However, the empirical distribution of the Pareto objectives on the PF is rarely studied, which implicitly impedes the generation of diverse and representative Pareto objectives in previous methods. To bridge the gap, we suggest in this paper constructing \emph{uniformly distributed} Pareto objectives on the PF, so as to alleviate the limited diversity found in previous MOO approaches. We are the first to formally define the concept of ``uniformity" for an MOO problem. We optimize the maximal minimal distances on the Pareto front using a neural network, resulting in both asymptotically and non-asymptotically uniform Pareto objectives. Our proposed method is validated 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#35745;&#31639;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09483</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;Oracle-Efficient&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Oracle-Efficient Differentially Private Learning with Public Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09483
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#35745;&#31639;&#39640;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#31169;&#26377;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#35768;&#22810;&#20989;&#25968;&#31867;&#30340;&#21487;&#23398;&#20064;&#24615;&#30340;&#32479;&#35745;&#19979;&#38480;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#31169;&#26377;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#31639;&#27861;&#24517;&#39035;&#22987;&#32456;&#20445;&#35777;&#30456;&#23545;&#20110;&#31169;&#26377;&#26679;&#26412;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#22312;&#31169;&#26377;&#25968;&#25454;&#20998;&#24067;&#19982;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#36275;&#22815;&#25509;&#36817;&#26102;&#30830;&#20445;&#23398;&#20064;&#20445;&#35777;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#26377;&#36275;&#22815;&#30340;&#20844;&#20849;&#38750;&#26631;&#35760;&#25968;&#25454;&#26102;&#65292;&#21487;&#20197;&#20351;&#31169;&#26377;&#23398;&#20064;&#22312;&#32479;&#35745;&#19978;&#21487;&#20197;&#22788;&#29702;&#65292;&#20294;&#24471;&#21040;&#30340;&#31639;&#27861;&#37117;&#26159;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#21487;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20989;&#25968;&#31867;&#21487;&#38750;&#31169;&#26377;&#23398;&#20064;&#26102;&#26126;&#30830;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#36827;&#34892;&#31169;&#26377;&#23398;&#20064;&#65292;&#20854;&#20013;&#25105;&#20204;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#27010;&#24565;&#26159;&#30456;&#23545;&#20110;&#20248;&#21270;&#35843;&#29992;&#27425;&#25968;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09483v1 Announce Type: cross  Abstract: Due to statistical lower bounds on the learnability of many function classes under privacy constraints, there has been recent interest in leveraging public data to improve the performance of private learning algorithms. In this model, algorithms must always guarantee differential privacy with respect to the private samples while also ensuring learning guarantees when the private data distribution is sufficiently close to that of the public data. Previous work has demonstrated that when sufficient public, unlabelled data is available, private learning can be made statistically tractable, but the resulting algorithms have all been computationally inefficient. In this work, we present the first computationally efficient, algorithms to provably leverage public data to learn privately whenever a function class is learnable non-privately, where our notion of computational efficiency is with respect to the number of calls to an optimization o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#65292;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.09478</link><description>&lt;p&gt;
&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Data Reconstruction Attacks and Defenses: A Systematic Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#65292;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#25915;&#20987;&#21644;&#38450;&#24481;&#23545;&#20110;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#32463;&#39564;&#35266;&#23519;&#19978;&#65292;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#19988;&#26080;&#27861;&#21306;&#20998;&#38450;&#24481;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#19982;&#25915;&#20987;&#26041;&#27861;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#12290;&#35813;&#25915;&#20987;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19982;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#36825;&#31181;&#26356;&#24378;&#30340;&#25915;&#20987;&#19979;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#20840;&#38754;&#35843;&#26597;&#20102;&#26368;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#22914;&#26799;&#24230;&#21098;&#36753;&#12289;dropout&#12289;&#28155;&#21152;&#22122;&#38899;&#12289;&#23616;&#37096;&#32858;&#21512;&#31561;&#31561;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09478v1 Announce Type: cross  Abstract: Reconstruction attacks and defenses are essential in understanding the data leakage problem in machine learning. However, prior work has centered around empirical observations of gradient inversion attacks, lacks theoretical groundings, and was unable to disentangle the usefulness of defending methods versus the computational limitation of attacking methods. In this work, we propose a strong reconstruction attack in the setting of federated learning. The attack reconstructs intermediate features and nicely integrates with and outperforms most of the previous methods. On this stronger attack, we thoroughly investigate both theoretically and empirically the effect of the most common defense methods. Our findings suggest that among various defense mechanisms, such as gradient clipping, dropout, additive noise, local aggregation, etc., gradient pruning emerges as the most effective strategy to defend against state-of-the-art attacks.
&lt;/p&gt;</description></item><item><title>PANORAMIA&#26159;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#23457;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#8220;&#38750;&#25104;&#21592;&#8221;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#21487;&#20197;&#37327;&#21270;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#32780;&#26080;&#38656;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.09477</link><description>&lt;p&gt;
PANORAMIA: &#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
PANORAMIA: Privacy Auditing of Machine Learning Models without Retraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09477
&lt;/p&gt;
&lt;p&gt;
PANORAMIA&#26159;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#23457;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#8220;&#38750;&#25104;&#21592;&#8221;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#21487;&#20197;&#37327;&#21270;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#32780;&#26080;&#38656;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38544;&#31169;&#23457;&#35745;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20381;&#36182;&#20110;&#20351;&#29992;&#29983;&#25104;&#30340;&#8220;&#38750;&#25104;&#21592;&#8221;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23545;ML&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#23457;&#35745;&#12290;&#36825;&#20010;&#26041;&#26696;&#34987;&#31216;&#20026;PANORAMIA&#65292;&#23427;&#21487;&#20197;&#37327;&#21270;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#32780;&#26080;&#38656;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;ML&#39046;&#22495;&#36827;&#34892;&#20102;&#23457;&#35745;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20197;&#21450;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09477v1 Announce Type: cross  Abstract: We introduce a privacy auditing scheme for ML models that relies on membership inference attacks using generated data as "non-members". This scheme, which we call PANORAMIA, quantifies the privacy leakage for large-scale ML models without control of the training process or model re-training and only requires access to a subset of the training data. To demonstrate its applicability, we evaluate our auditing scheme across multiple ML domains, ranging from image and tabular data classification to large-scale language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#21464;&#21387;&#22120;&#26041;&#27861;&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65292;&#25552;&#39640;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09474</link><description>&lt;p&gt;
&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65306;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#24615;&#25151;&#39076;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#21464;&#21387;&#22120;&#26041;&#27861;&#35299;&#35835;&#24515;&#29575;&#20449;&#21495;&#65292;&#25552;&#39640;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21487;&#31359;&#25140;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#35774;&#22791;&#30340;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#22312;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24212;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#26041;&#27861;&#36827;&#34892;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#65292;&#20294;&#30001;&#20110;&#30446;&#21069;AI&#31639;&#27861;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#34987;&#24191;&#27867;&#25509;&#21463;&#20316;&#20026;&#20020;&#24202;&#35786;&#26029;&#30340;&#21487;&#38752;&#36741;&#21161;&#24037;&#20855;&#12290;&#23588;&#20854;&#38656;&#35201;&#30830;&#23450;ECG&#20449;&#21495;&#20013;&#36129;&#29486;&#20110;&#20934;&#30830;&#35786;&#26029;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#23548;&#32852;ECG&#25968;&#25454;&#35782;&#21035;&#25151;&#39076;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#26041;&#27861;&#20197;&#20316;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09474v1 Announce Type: cross  Abstract: Remote patient monitoring based on wearable single-lead electrocardiogram (ECG) devices has significant potential for enabling the early detection of heart disease, especially in combination with artificial intelligence (AI) approaches for automated heart disease detection. There have been prior studies applying AI approaches based on deep learning for heart disease detection. However, these models are yet to be widely accepted as a reliable aid for clinical diagnostics, in part due to the current black-box perception surrounding many AI algorithms. In particular, there is a need to identify the key features of the ECG signal that contribute toward making an accurate diagnosis, thereby enhancing the interpretability of the model. In the present study, we develop a vision transformer approach to identify atrial fibrillation based on single-lead ECG data. A residual network (ResNet) approach is also developed for comparison with the visi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21015;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#23545;&#22810;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#26694;&#26550;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#35299;&#37322;&#20013;&#21487;&#38598;&#20307;&#25913;&#21464;&#30340;&#29305;&#24449;&#25968;&#37327;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#23569;&#22320;&#20351;&#29992;&#35299;&#37322;&#26469;&#35299;&#37322;&#25152;&#26377;&#23454;&#20363;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#35745;&#31639;&#24615;&#33021;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.09473</link><description>&lt;p&gt;
&#21015;&#29983;&#25104;&#30340;&#19968;&#23545;&#22810;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
One-for-many Counterfactual Explanations by Column Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21015;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#23545;&#22810;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#26694;&#26550;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#35299;&#37322;&#20013;&#21487;&#38598;&#20307;&#25913;&#21464;&#30340;&#29305;&#24449;&#25968;&#37327;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#23569;&#22320;&#20351;&#29992;&#35299;&#37322;&#26469;&#35299;&#37322;&#25152;&#26377;&#23454;&#20363;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#35745;&#31639;&#24615;&#33021;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#29983;&#25104;&#19968;&#32452;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#37319;&#29992;&#19968;&#23545;&#22810;&#20998;&#37197;&#35268;&#21017;&#65292;&#20854;&#20013;&#19968;&#20010;&#35299;&#37322;&#34987;&#20998;&#37197;&#32473;&#19968;&#20010;&#23454;&#20363;&#23376;&#32452;&#12290;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#22312;&#32771;&#34385;&#31232;&#30095;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35299;&#37322;&#25152;&#38656;&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#35299;&#37322;&#20013;&#20801;&#35768;&#38598;&#20307;&#25913;&#21464;&#30340;&#29305;&#24449;&#25968;&#37327;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21015;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#25628;&#32034;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#20998;&#31867;&#22120;&#65292;&#22914;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#25991;&#29486;&#20013;&#30340;&#31616;&#21333;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20844;&#24335;&#30340;&#31616;&#21333;&#36866;&#24212;&#30456;&#27604;&#65292;&#21015;&#29983;&#25104;&#26694;&#26550;&#22312;&#21487;&#25193;&#23637;&#24615;&#12289;&#35745;&#31639;&#24615;&#33021;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#26041;&#38754;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09473v1 Announce Type: new  Abstract: In this paper, we consider the problem of generating a set of counterfactual explanations for a group of instances, with the one-for-many allocation rule, where one explanation is allocated to a subgroup of the instances. For the first time, we solve the problem of minimizing the number of explanations needed to explain all the instances, while considering sparsity by limiting the number of features allowed to be changed collectively in each explanation. A novel column generation framework is developed to efficiently search for the explanations. Our framework can be applied to any black-box classifier, like neural networks. Compared with a simple adaptation of a mixed-integer programming formulation from the literature, the column generation framework dominates in terms of scalability, computational performance and quality of the solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#38543;&#26426;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09471</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#21442;&#25968;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Stochastic Parametrisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#38543;&#26426;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#30340;&#22823;&#27668;&#27169;&#22411;&#36890;&#24120;&#20197;&#30830;&#23450;&#24615;&#26041;&#24335;&#26500;&#24314;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#32473;&#23450;&#24050;&#35299;&#26512;&#23610;&#24230;&#21464;&#37327;&#30340;&#29305;&#23450;&#29366;&#24577;&#19979;&#65292;&#20272;&#35745;&#24182;&#20351;&#29992;&#23376;&#32593;&#26684;&#23610;&#24230;&#36807;&#31243;&#30340;&#26368;&#21487;&#33021;&#24378;&#36843;&#39033;&#26469;&#39044;&#27979;&#22823;&#23610;&#24230;&#27969;&#30340;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#27668;&#20013;&#32570;&#20047;&#23610;&#24230;&#20998;&#31163;&#24847;&#21619;&#30528;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#35823;&#24046;&#26469;&#28304;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#24335;&#65306;&#20351;&#29992;&#38543;&#26426;&#25216;&#26415;&#26469;&#34920;&#24449;&#23567;&#23610;&#24230;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20123;&#25216;&#26415;&#29616;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#22825;&#27668;&#12289;&#27425;&#23395;&#33410;&#12289;&#23395;&#33410;&#21644;&#27668;&#20505;&#26102;&#38388;&#23610;&#24230;&#19978;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36817;&#24180;&#26469;&#20063;&#21462;&#24471;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26367;&#20195;&#21442;&#25968;&#21270;&#26041;&#26696;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#26377;&#28508;&#21147;&#21152;&#24555;&#24182;&#25913;&#36827;&#25105;&#20204;&#30340;&#25968;&#20540;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#37325;&#28857;&#20027;&#35201;&#26159;&#30830;&#23450;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09471v1 Announce Type: new  Abstract: Atmospheric models used for weather and climate prediction are traditionally formulated in a deterministic manner. In other words, given a particular state of the resolved scale variables, the most likely forcing from the sub-grid scale processes is estimated and used to predict the evolution of the large-scale flow. However, the lack of scale-separation in the atmosphere means that this approach is a large source of error in forecasts. Over recent years, an alternative paradigm has developed: the use of stochastic techniques to characterise uncertainty in small-scale processes. These techniques are now widely used across weather, sub-seasonal, seasonal, and climate timescales. In parallel, recent years have also seen significant progress in replacing parametrisation schemes using machine learning (ML). This has the potential to both speed up and improve our numerical models. However, the focus to date has largely been on deterministic a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#24182;&#26681;&#25454;&#24103;&#22312;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20808;&#21518;&#20998;&#37197;&#19981;&#21516;&#30340;&#22122;&#22768;&#37327;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35270;&#39057;&#39044;&#27979;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25193;&#25955;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09470</link><description>&lt;p&gt;
&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rolling Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#24182;&#26681;&#25454;&#24103;&#22312;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20808;&#21518;&#20998;&#37197;&#19981;&#21516;&#30340;&#22122;&#22768;&#37327;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35270;&#39057;&#39044;&#27979;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25193;&#25955;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26102;&#38388;&#25968;&#25454;&#65292;&#22914;&#35270;&#39057;&#12289;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#25110;&#27668;&#20505;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#21518;&#32493;&#24103;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#37327;&#35270;&#20026;&#30456;&#31561;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28378;&#21160;&#25193;&#25955;&#65306;&#19968;&#31181;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#30830;&#20445;&#25193;&#25955;&#36807;&#31243;&#36880;&#28176;&#36890;&#36807;&#26102;&#38388;&#36827;&#34892;&#30772;&#22351;&#65292;&#36890;&#36807;&#23558;&#26356;&#22810;&#30340;&#22122;&#22768;&#20998;&#37197;&#32473;&#24207;&#21015;&#20013;&#20986;&#29616;&#36739;&#26202;&#30340;&#24103;&#65292;&#21453;&#26144;&#20986;&#38543;&#30528;&#29983;&#25104;&#36807;&#31243;&#30340;&#23637;&#24320;&#65292;&#23545;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#36234;&#26469;&#36234;&#22823;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#26102;&#38388;&#21160;&#24577;&#22797;&#26434;&#26102;&#65292;&#28378;&#21160;&#25193;&#25955;&#20248;&#20110;&#26631;&#20934;&#25193;&#25955;&#12290;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;Kinetics-600&#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09470v1 Announce Type: new  Abstract: Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#27169;&#36816;&#31639;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21333;&#23618;Transformer&#22312;&#35299;&#20915;&#22797;&#26434;&#20195;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#12290;&#38416;&#26126;&#20102;&#36793;&#32536;&#26368;&#22823;&#21270;&#21407;&#21017;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09469</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20613;&#31435;&#21494;&#30005;&#36335;&#65306;&#35299;&#38145;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#27169;&#36816;&#31639;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#27169;&#36816;&#31639;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21333;&#23618;Transformer&#22312;&#35299;&#20915;&#22797;&#26434;&#20195;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#12290;&#38416;&#26126;&#20102;&#36793;&#32536;&#26368;&#22823;&#21270;&#21407;&#21017;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#25152;&#21033;&#29992;&#30340;&#20869;&#37096;&#34920;&#31034;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#22312;&#36817;&#26399;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#23545;&#32593;&#32476;&#37319;&#29992;&#29305;&#23450;&#35745;&#31639;&#31574;&#30053;&#32972;&#21518;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#28041;&#21450;k&#20010;&#36755;&#20837;&#30340;&#22797;&#26434;&#20195;&#25968;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#27169;&#36816;&#31639;&#30340;&#21152;&#27861;&#12290;&#25105;&#20204;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21333;&#23618;Transformer&#22312;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#29702;&#35770;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#26159;&#38416;&#26126;&#36793;&#32536;&#26368;&#22823;&#21270;&#21407;&#21017;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30340;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;p&#34920;&#31034;&#27169;&#25968;&#65292;Dp&#34920;&#31034;k&#20010;&#36755;&#20837;&#30340;&#27169;&#36816;&#31639;&#25968;&#25454;&#38598;&#65292;m&#34920;&#31034;&#32593;&#32476;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09469v1 Announce Type: new  Abstract: In the evolving landscape of machine learning, a pivotal challenge lies in deciphering the internal representations harnessed by neural networks and Transformers. Building on recent progress toward comprehending how networks execute distinct target functions, our study embarks on an exploration of the underlying reasons behind networks adopting specific computational strategies. We direct our focus to the complex algebraic learning task of modular addition involving $k$ inputs. Our research presents a thorough analytical characterization of the features learned by stylized one-hidden layer neural networks and one-layer Transformers in addressing this task.   A cornerstone of our theoretical framework is the elucidation of how the principle of margin maximization shapes the features adopted by one-hidden layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of modular arithmetic with $k$ inputs and $m$ denote the net
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22266;&#23450;&#32622;&#20449;&#24230;&#30340;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#949;-&#38408;&#20540;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28176;&#36817;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09467</link><description>&lt;p&gt;
&#26368;&#20248;&#38408;&#20540;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Optimal Thresholding Linear Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22266;&#23450;&#32622;&#20449;&#24230;&#30340;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#949;-&#38408;&#20540;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28176;&#36817;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65306;&#20855;&#26377;&#22266;&#23450;&#32622;&#20449;&#24230;&#30340;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#949;-&#38408;&#20540;&#36172;&#21338;&#26426;&#38382;&#39064;(TBP)&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#19979;&#30028;&#65292;&#24182;&#23558;&#35774;&#35745;&#29992;&#20110;&#32447;&#24615;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;TBP&#65292;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09467v1 Announce Type: cross  Abstract: We study a novel pure exploration problem: the $\epsilon$-Thresholding Bandit Problem (TBP) with fixed confidence in stochastic linear bandits. We prove a lower bound for the sample complexity and extend an algorithm designed for Best Arm Identification in the linear case to TBP that is asymptotically optimal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33041;&#30005;&#22270;&#30740;&#31350;&#20013;&#19981;&#21516;&#31639;&#27861;&#26159;&#21542;&#33021;&#19968;&#33268;&#25581;&#31034;&#20986;&#33041;&#40836;&#39044;&#27979;&#30340;&#20551;&#35774;&#65292;&#21457;&#29616;&#34429;&#28982;&#22823;&#22810;&#25968;&#27169;&#22411;&#25581;&#31034;&#20102;&#30456;&#20284;&#30340;&#21457;&#29616;&#65292;&#20294;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.09464</link><description>&lt;p&gt;
&#19981;&#21516;&#31639;&#27861;&#65288;&#21487;&#33021;&#65289;&#25581;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#65306;&#19968;&#20010;&#33041;&#40836;&#39044;&#27979;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Different Algorithms (Might) Uncover Different Patterns: A Brain-Age Prediction Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33041;&#30005;&#22270;&#30740;&#31350;&#20013;&#19981;&#21516;&#31639;&#27861;&#26159;&#21542;&#33021;&#19968;&#33268;&#25581;&#31034;&#20986;&#33041;&#40836;&#39044;&#27979;&#30340;&#20551;&#35774;&#65292;&#21457;&#29616;&#34429;&#28982;&#22823;&#22810;&#25968;&#27169;&#22411;&#25581;&#31034;&#20102;&#30456;&#20284;&#30340;&#21457;&#29616;&#65292;&#20294;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29983;&#29289;&#20449;&#21495;&#20998;&#26512;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#26032;&#39062;&#30340;&#31639;&#27861;&#36890;&#24120;&#33021;&#25913;&#21892;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23545;&#31639;&#27861;&#22810;&#26679;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#19981;&#21516;&#31639;&#27861;&#26159;&#21542;&#33021;&#19968;&#33268;&#25581;&#31034;&#20986;&#30456;&#20284;&#30340;&#21457;&#29616;&#65292;&#24456;&#23569;&#24471;&#21040;&#25506;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33041;&#30005;&#22270;&#30740;&#31350;&#20013;&#20174;&#24050;&#26377;&#30740;&#31350;&#20013;&#39564;&#35777;&#30340;&#33041;&#40836;&#39044;&#27979;&#20551;&#35774;&#26159;&#21542;&#36866;&#29992;&#20110;&#19981;&#21516;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#30740;&#20102;&#25991;&#29486;&#65292;&#24182;&#30830;&#23450;&#20102;&#21508;&#31181;&#24050;&#30693;&#23545;&#33041;&#40836;&#39044;&#27979;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#12289;&#22788;&#29702;&#27493;&#39588;&#21644;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20102;SHAP&#20540;&#30340;&#35299;&#37322;&#33021;&#21147;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#29616;&#26377;&#30740;&#31350;&#30456;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#24456;&#23569;&#26377;&#20960;&#20010;&#22312;&#25105;&#20204;&#20351;&#29992;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#34429;&#28982;&#25581;&#31034;&#20102;&#30456;&#20284;&#30340;&#21457;&#29616;&#65292;&#20294;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09464v1 Announce Type: cross  Abstract: Machine learning is a rapidly evolving field with a wide range of applications, including biological signal analysis, where novel algorithms often improve the state-of-the-art. However, robustness to algorithmic variability - measured by different algorithms, consistently uncovering similar findings - is seldom explored. In this paper we investigate whether established hypotheses in brain-age prediction from EEG research validate across algorithms. First, we surveyed literature and identified various features known to be informative for brain-age prediction. We employed diverse feature extraction techniques, processing steps, and models, and utilized the interpretative power of SHapley Additive exPlanations (SHAP) values to align our findings with the existing research in the field. Few of our models achieved state-of-the-art performance on the specific data-set we utilized. Moreover, analysis demonstrated that while most models do unc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;WaveNet&#26550;&#26500;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#25193;&#24352;&#21442;&#25968;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#23494;&#38598;RF&#39057;&#35889;&#20013;&#30340;&#20449;&#21495;&#20998;&#31163;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#26550;&#26500;&#21644;&#21019;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#22797;&#26434;&#20449;&#21495;&#28304;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.09461</link><description>&lt;p&gt;
WaveNet&#26550;&#26500;&#22312;&#20855;&#26377;&#21487;&#23398;&#20064;&#25193;&#24352;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;RF&#20449;&#21495;&#20998;&#31163;&#19978;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Approach to WaveNet Architecture for RF Signal Separation with Learnable Dilation and Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;WaveNet&#26550;&#26500;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#25193;&#24352;&#21442;&#25968;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#23494;&#38598;RF&#39057;&#35889;&#20013;&#30340;&#20449;&#21495;&#20998;&#31163;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#26550;&#26500;&#21644;&#21019;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#22797;&#26434;&#20449;&#21495;&#28304;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;WaveNet&#26550;&#26500;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#21487;&#23398;&#20064;&#30340;&#25193;&#24352;&#21442;&#25968;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#22312;&#23494;&#38598;RF&#39057;&#35889;&#20013;&#30340;&#20449;&#21495;&#20998;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#26550;&#26500;&#21644;&#21019;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#22797;&#26434;&#20449;&#21495;&#28304;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#25968;&#25454;&#20934;&#22791;&#25216;&#26415;&#21644;&#20851;&#38190;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36825;&#20123;&#23545;&#25105;&#20204;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#26174;&#33879;&#30340;&#25913;&#36827;&#24471;&#21040;&#20102;&#35777;&#26126;&#65306;&#22312;OFDM-QPSK&#19982;EMI&#20449;&#21495;1&#30340;BER&#20026;$10^{-3}$&#24773;&#20917;&#19979;&#65292;SINR&#22686;&#21152;&#20102;58.82&#65285;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;\cite{datadrivenrf2024}&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#24182;&#30830;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09461v1 Announce Type: cross  Abstract: In this paper, we address the intricate issue of RF signal separation by presenting a novel adaptation of the WaveNet architecture that introduces learnable dilation parameters, significantly enhancing signal separation in dense RF spectrums. Our focused architectural refinements and innovative data augmentation strategies have markedly improved the model's ability to discern complex signal sources. This paper details our comprehensive methodology, including the refined model architecture, data preparation techniques, and the strategic training strategy that have been pivotal to our success. The efficacy of our approach is evidenced by the substantial improvements recorded: a 58.82\% increase in SINR at a BER of $10^{-3}$ for OFDM-QPSK with EMI Signal 1, surpassing traditional benchmarks. Notably, our model achieved first place in the challenge \cite{datadrivenrf2024}, demonstrating its superior performance and establishing a new stand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26080;&#24310;&#36831;&#29983;&#25104;&#22266;&#23450;&#28388;&#27874;&#20027;&#21160;&#22122;&#22768;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21327;&#22788;&#29702;&#22120;&#21644;&#23454;&#26102;&#25511;&#21046;&#22120;&#38598;&#25104;&#21040;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;ANC&#31995;&#32479;&#20013;&#65292;&#19981;&#20165;&#30465;&#30053;&#20102;&#26631;&#27880;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#22122;&#22768;&#38477;&#20302;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09460</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26080;&#24310;&#36831;&#29983;&#25104;&#22266;&#23450;&#28388;&#27874;&#20027;&#21160;&#22122;&#22768;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning based end-to-end delayless generative fixed-filter active noise control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#26080;&#24310;&#36831;&#29983;&#25104;&#22266;&#23450;&#28388;&#27874;&#20027;&#21160;&#22122;&#22768;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21327;&#22788;&#29702;&#22120;&#21644;&#23454;&#26102;&#25511;&#21046;&#22120;&#38598;&#25104;&#21040;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;ANC&#31995;&#32479;&#20013;&#65292;&#19981;&#20165;&#30465;&#30053;&#20102;&#26631;&#27880;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#22122;&#22768;&#38477;&#20302;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20808;&#21069;&#30340;&#29983;&#25104;&#22266;&#23450;&#28388;&#27874;&#20027;&#21160;&#22122;&#22768;&#25511;&#21046;&#65288;GFANC&#65289;&#26694;&#26550;&#30340;&#39640;&#25928;&#21327;&#21516;&#65292;&#23454;&#29616;&#20102;&#26080;&#24310;&#36831;&#22122;&#22768;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#21327;&#22788;&#29702;&#22120;&#20013;&#30340;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#38656;&#35201;&#20351;&#29992;&#26631;&#27880;&#30340;&#22122;&#22768;&#25968;&#25454;&#38598;&#36827;&#34892;&#21021;&#22987;&#35757;&#32451;&#12290;&#26631;&#27880;&#22122;&#22768;&#25968;&#25454;&#21487;&#33021;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#65292;&#24182;&#19988;&#21487;&#33021;&#24341;&#20837;&#19968;&#20123;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;GFANC&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;1D CNN&#35757;&#32451;&#36807;&#31243;&#24182;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23558;&#21327;&#22788;&#29702;&#22120;&#21644;&#23454;&#26102;&#25511;&#21046;&#22120;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;ANC&#31995;&#32479;&#20013;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#32047;&#31215;&#30340;&#24179;&#26041;&#35823;&#24046;&#20449;&#21495;&#20316;&#20026;&#35757;&#32451;1D CNN&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#36825;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#26080;&#30417;&#30563;GFANC&#26041;&#27861;&#19981;&#20165;&#30465;&#30053;&#20102;&#26631;&#27880;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#22122;&#22768;&#38477;&#20302;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09460v1 Announce Type: cross  Abstract: Delayless noise control is achieved by our earlier generative fixed-filter active noise control (GFANC) framework through efficient coordination between the co-processor and real-time controller. However, the one-dimensional convolutional neural network (1D CNN) in the co-processor requires initial training using labelled noise datasets. Labelling noise data can be resource-intensive and may introduce some biases. In this paper, we propose an unsupervised-GFANC approach to simplify the 1D CNN training process and enhance its practicality. During training, the co-processor and real-time controller are integrated into an end-to-end differentiable ANC system. This enables us to use the accumulated squared error signal as the loss for training the 1D CNN. With this unsupervised learning paradigm, the unsupervised-GFANC method not only omits the labelling process but also exhibits better noise reduction performance compared to the supervise
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26500;&#24314;&#21487;&#36127;&#25285;&#30340;&#23450;&#21046;IMU&#26080;&#32447;&#21487;&#31359;&#25140;&#31995;&#32479;&#65292;&#22312;&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#20013;&#23454;&#29616;&#23545;&#36523;&#20307;&#37096;&#20301;&#23450;&#21521;&#36319;&#36394;&#21644;3D&#36816;&#21160;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09459</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#20307;&#37096;&#20301;&#23450;&#21521;&#36319;&#36394;&#21644;3D&#36816;&#21160;&#21487;&#35270;&#21270;&#30340;&#23450;&#21046;IMU&#26080;&#32447;&#21487;&#31359;&#25140;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body Parts Orientation Tracking and 3D Movement Visualization on an Avatar
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09459
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26500;&#24314;&#21487;&#36127;&#25285;&#30340;&#23450;&#21046;IMU&#26080;&#32447;&#21487;&#31359;&#25140;&#31995;&#32479;&#65292;&#22312;&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#20013;&#23454;&#29616;&#23545;&#36523;&#20307;&#37096;&#20301;&#23450;&#21521;&#36319;&#36394;&#21644;3D&#36816;&#21160;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#30830;&#35748;&#20102;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#30340;&#31995;&#32479;&#23545;&#20110;&#20154;&#20307;&#36816;&#21160;&#20998;&#26512;&#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#39640;&#31471;&#30340;&#21830;&#19994;&#21270;IMU&#35299;&#20915;&#26041;&#26696;&#20215;&#26684;&#26114;&#36149;&#19988;&#22797;&#26434;&#65292;&#26080;&#27861;&#26222;&#21450;&#22312;&#24191;&#22823;&#28508;&#22312;&#29992;&#25143;&#20013;&#30340;&#20351;&#29992;&#12290;&#24066;&#22330;&#19978;&#20986;&#29616;&#20102;&#19968;&#20123;&#21151;&#33021;&#36739;&#23569;&#30340;&#20302;&#31471;&#21830;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#20811;&#26381;&#30340;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#21307;&#30103;&#21644;&#36816;&#21160;&#24212;&#29992;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#35770;&#25991;&#20351;&#29992;&#30340;&#26159;&#38750;&#21830;&#19994;&#21270;&#30340;&#12289;&#33258;&#21046;&#30340;IMU&#31995;&#32479;&#12290;&#23613;&#31649;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20419;&#36827;&#36825;&#39033;&#25216;&#26415;&#30340;&#26222;&#21450;&#21270;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#21151;&#33021;&#26356;&#20026;&#26377;&#38480;&#65292;&#24182;&#19988;&#22914;&#20309;&#35774;&#35745;&#21644;&#26500;&#24314;&#23427;&#20204;&#30340;&#25551;&#36848;&#22312;&#25991;&#29486;&#20013;&#20173;&#28982;&#31232;&#32570;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#65306;&#65288;1&#65289;&#35777;&#26126;&#26500;&#24314;&#19968;&#31181;&#21487;&#36127;&#25285;&#30340;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#26159;&#21487;&#34892;&#30340;&#65292;&#26088;&#22312;&#21516;&#26102;&#36861;&#36394;&#22810;&#20010;&#20154;&#20307;&#37096;&#20301;&#30340;&#23450;&#21521;&#21644;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09459v1 Announce Type: cross  Abstract: Recent studies confirm the applicability of Inertial Measurement Unit (IMU)-based systems for human motion analysis. Notwithstanding, high-end IMU-based commercial solutions are yet too expensive and complex to democratize their use among a wide range of potential users. Less featured entry-level commercial solutions are being introduced in the market, trying to fill this gap, but still present some limitations that need to be overcome. At the same time, there is a growing number of scientific papers using not commercial, but custom do-it-yourself IMU-based systems in medical and sports applications. Even though these solutions can help to popularize the use of this technology, they have more limited features and the description on how to design and build them from scratch is yet too scarce in the literature. The aim of this work is two-fold: (1) Proving the feasibility of building an affordable custom solution aimed at simultaneous mu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.09456</link><description>&lt;p&gt;
&#26410;&#30693;&#21338;&#24328;&#20013;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#29992;&#20110;&#26080;&#36951;&#25022;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#21338;&#24328;&#20013;&#36827;&#34892;&#26080;&#36951;&#25022;&#23398;&#20064;&#30340;&#20048;&#35266;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#23558;&#29616;&#26377;&#31639;&#27861;&#19982;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28041;&#21450;&#22810;&#20010;&#20915;&#31574;&#32773;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#35266;&#27979;&#30340;&#26410;&#30693;&#21338;&#24328;&#12290;&#20026;&#20102;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#21644;&#22810;&#26426;&#26500;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27748;&#26222;&#26862;&#25277;&#26679;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#23545;&#25163;&#30340;&#34892;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#20132;&#36890;&#36335;&#30001;&#21644;&#38647;&#36798;&#24863;&#30693;&#20013;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#23454;&#39564;&#39044;&#31639;&#65292;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;&#20943;&#23569;&#20102;&#21313;&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#22870;&#21169;&#32467;&#26500;&#26377;&#19968;&#23450;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36951;&#25022;&#30028;&#38480;&#20165;&#23545;&#24635;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21576;&#23545;&#25968;&#20381;&#36182;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#22810;&#26426;&#26500;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20048;&#35266;-&#26080;&#36951;&#25022;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#39046;&#22495;&#20869;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09456v1 Announce Type: cross  Abstract: Many real-world problems involving multiple decision-makers can be modeled as an unknown game characterized by partial observations. Addressing the challenges posed by partial information and the curse of multi-agency, we developed Thompson sampling-type algorithms, leveraging information about opponent's action and reward structures. Our approach significantly reduces experimental budgets, achieving a more than tenfold reduction compared to baseline algorithms in practical applications like traffic routing and radar sensing. We demonstrate that, under certain assumptions about the reward structure, the regret bound exhibits merely a logarithmic dependence on the total action space size, effectively mitigating the curse of multi-agency. Additionally, this research introduces the Optimism-then-NoRegret framework, a novel contribution that integrates both our proposed methodologies and existing algorithms in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(WGAN)&#26469;&#25552;&#39640;EEG&#20449;&#21495;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290; WGAN&#22312;BCI2000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#21644;&#27979;&#37327;&#24471;&#20998;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;EEG&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.09453</link><description>&lt;p&gt;
&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#39640;EEG&#20449;&#21495;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving EEG Signal Classification Accuracy Using Wasserstein Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(WGAN)&#26469;&#25552;&#39640;EEG&#20449;&#21495;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290; WGAN&#22312;BCI2000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#21644;&#27979;&#37327;&#24471;&#20998;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;EEG&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#29983;&#24577;&#65288;EEG&#65289;&#22312;&#35760;&#24405;&#33041;&#37096;&#27963;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#23545;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;EEG&#20449;&#21495;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#39640;&#24230;&#21464;&#24322;&#24615;&#32473;&#21019;&#24314;&#21487;&#38752;&#30340;BCI&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;WGAN&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;WGAN&#26159;&#22312;BCI2000&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;45&#20010;&#20010;&#20307;&#30340;&#32422;1500&#20010;EEG&#35760;&#24405;&#21644;64&#20010;&#36890;&#36947;&#12290;&#36890;&#36807;&#19977;&#20010;&#20998;&#31867;&#22120;&#35780;&#20272;&#29983;&#25104;&#30340;EEG&#20449;&#21495;&#65292;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;&#20351;&#29992;Frechet Inception Distance&#65288;FID&#65289;&#27979;&#37327;&#30340;&#29983;&#25104;&#20449;&#21495;&#36136;&#37327;&#20026;1.345&#65288;&#30529;&#30524;&#65289;&#21644;11.565&#65288;&#38381;&#30524;&#65289;&#12290;&#21363;&#20351;&#27809;&#26377;&#39057;&#35889;&#25110;&#31354;&#38388;&#25439;&#22833;&#39033;&#65292;&#25105;&#20204;&#30340;WGAN&#27169;&#22411;&#20173;&#33021;&#27169;&#25311;&#20986;&#39057;&#35889;&#21644;&#31354;&#38388;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09453v1 Announce Type: cross  Abstract: Electroencephalography (EEG) plays a vital role in recording brain activities and is integral to the development of brain-computer interface (BCI) technologies. However, the limited availability and high variability of EEG signals present substantial challenges in creating reliable BCIs. To address this issue, we propose a practical solution drawing on the latest developments in deep learning and Wasserstein Generative Adversarial Network (WGAN). The WGAN was trained on the BCI2000 dataset, consisting of around 1500 EEG recordings and 64 channels from 45 individuals. The generated EEG signals were evaluated via three classifiers yielding improved average accuracies. The quality of generated signals measured using Frechet Inception Distance (FID) yielded scores of 1.345 and 11.565 for eyes-open and closed respectively. Even without a spectral or spatial loss term, our WGAN model was able to emulate the spectral and spatial properties of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#24237;&#21307;&#30103;&#22330;&#26223;&#20013;&#20351;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#26085;&#24120;&#27963;&#21160;&#30417;&#27979;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#37096;&#32626;&#31995;&#32479;&#21644;&#20998;&#26512;&#25968;&#25454;&#21464;&#21270;&#65292;&#25351;&#23548;&#20102;&#31283;&#20581;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;WiFi&#24863;&#30693;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#25552;&#39640;&#20102;&#32769;&#24180;&#25252;&#29702;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.09452</link><description>&lt;p&gt;
&#22522;&#20110;WiFi&#30340;&#23478;&#24237;&#21307;&#30103;&#20013;&#30495;&#23454;&#19990;&#30028;&#24739;&#32773;&#27963;&#21160;&#30417;&#27979;&#30340;&#25968;&#25454;&#20998;&#24067;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Dynamics in Real-World WiFi-Based Patient Activity Monitoring for Home Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#24237;&#21307;&#30103;&#22330;&#26223;&#20013;&#20351;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#26085;&#24120;&#27963;&#21160;&#30417;&#27979;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#37096;&#32626;&#31995;&#32479;&#21644;&#20998;&#26512;&#25968;&#25454;&#21464;&#21270;&#65292;&#25351;&#23548;&#20102;&#31283;&#20581;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;WiFi&#24863;&#30693;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#25552;&#39640;&#20102;&#32769;&#24180;&#25252;&#29702;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#24237;&#21307;&#30103;&#22330;&#26223;&#20013;&#20351;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#26085;&#24120;&#27963;&#21160;&#30417;&#27979;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#22522;&#20110;WiFi&#30340;&#27963;&#21160;&#35782;&#21035;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#38754;&#20020;&#29615;&#22659;&#12289;&#20027;&#20307;&#21644;&#31995;&#32479;&#37197;&#32622;&#21464;&#37327;&#31561;&#25361;&#25112;&#65292;&#24433;&#21709;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#30740;&#31350;&#21253;&#25324;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#37096;&#32626;&#31995;&#32479;&#21644;&#20998;&#26512;&#25968;&#25454;&#21464;&#21270;&#12290;&#30740;&#31350;&#26088;&#22312;&#25351;&#23548;&#30495;&#23454;&#29615;&#22659;&#19979;&#31283;&#20581;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;WiFi&#24863;&#30693;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#32769;&#24180;&#25252;&#29702;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;WiFi&#24863;&#30693;&#30340;&#27963;&#21160;&#26816;&#27979;&#27491;&#22312;&#21457;&#29983;&#21464;&#21270;&#65292;&#24357;&#21512;&#20102;&#23398;&#26415;&#30740;&#31350;&#19982;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#25216;&#26415;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09452v1 Announce Type: cross  Abstract: This paper examines the application of WiFi signals for real-world monitoring of daily activities in home healthcare scenarios. While the state-of-the-art of WiFi-based activity recognition is promising in lab environments, challenges arise in real-world settings due to environmental, subject, and system configuration variables, affecting accuracy and adaptability. The research involved deploying systems in various settings and analyzing data shifts. It aims to guide realistic development of robust, context-aware WiFi sensing systems for elderly care. The findings suggest a shift in WiFi-based activity sensing, bridging the gap between academic research and practical applications, enhancing life quality through technology.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09450</link><description>&lt;p&gt;
&#24341;&#23548;&#36974;&#34109;&#34920;&#31034;&#23398;&#20064;&#20197;&#25429;&#25417;&#24515;&#30005;&#22270;&#30340;&#26102;&#31354;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;ST-MEM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24191;&#27867;&#29992;&#20316;&#30417;&#27979;&#24515;&#33039;&#36215;&#28304;&#30340;&#30005;&#20449;&#21495;&#30340;&#35786;&#26029;&#24037;&#20855;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#20449;&#21495;&#36827;&#34892;&#21508;&#31181;&#30142;&#30149;&#31579;&#26597;&#30340;&#24212;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#30142;&#30149;&#31579;&#26597;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;ECG&#25968;&#25454;&#26377;&#38480;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23454;&#29616;&#36890;&#29992;&#34920;&#31034;&#26159;&#20811;&#26381;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24120;&#29992;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22312;ECG&#25968;&#25454;&#19978;&#32431;&#31929;&#24212;&#29992;SSL&#65292;&#32780;&#19981;&#32771;&#34385;ECG&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ST-MEM&#65288;&#26102;&#31354;&#36974;&#34109;&#24515;&#30005;&#22270;&#24314;&#27169;&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37325;&#26500;&#36974;&#34109;&#30340;12&#23548;&#32852;ECG&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#31354;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;ST-MEM&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;SSL&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09448</link><description>&lt;p&gt;
&#26222;&#36890;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09448
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#25552;&#21319;&#36816;&#21160;&#38556;&#30861;&#20010;&#20307;&#30340;BCI&#24212;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#26159;&#35299;&#35835;&#21644;&#35299;&#30721;&#21508;&#31181;&#25235;&#25569;&#21160;&#20316;&#65292;&#22914;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#12290;&#30446;&#26631;&#26159;&#30830;&#23450;&#21738;&#31181;EEG&#25216;&#26415;&#22312;&#22788;&#29702;&#21644;&#32763;&#35793;&#19982;&#25235;&#25569;&#30456;&#20851;&#30340;&#33041;&#30005;&#20449;&#21495;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;&#30740;&#31350;&#28041;&#21450;&#23545;&#21313;&#21517;&#20581;&#24247;&#21442;&#19982;&#32773;&#36827;&#34892;&#23454;&#39564;&#65292;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25569;&#25345;&#36816;&#21160;&#65306;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#65292;&#26080;&#36816;&#21160;&#26465;&#20214;&#20316;&#20026;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#35299;&#30721;&#25235;&#25569;&#21160;&#20316;&#26041;&#38754;&#23545;EEG&#21644;&#19977;&#26497;EEG&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#35813;&#27604;&#36739;&#28085;&#30422;&#20102;&#20960;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#12289;&#36890;&#36807;&#21151;&#33021;&#36830;&#25509;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#28041;&#21450;&#20174;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09448v1 Announce Type: cross  Abstract: This study aims to enhance BCI applications for individuals with motor impairments by comparing the effectiveness of tripolar EEG (tEEG) with conventional EEG. The focus is on interpreting and decoding various grasping movements, such as power grasp and precision grasp. The goal is to determine which EEG technology is more effective in processing and translating grasp related neural signals. The approach involved experimenting on ten healthy participants who performed two distinct grasp movements: power grasp and precision grasp, with a no movement condition serving as the baseline. Our research presents a thorough comparison between EEG and tEEG in decoding grasping movements. This comparison spans several key parameters, including signal to noise ratio (SNR), spatial resolution via functional connectivity, ERPs, and wavelet time frequency analysis. Additionally, our study involved extracting and analyzing statistical features from th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#23545;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#25104;&#21151;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23567;&#27874;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09447</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#23567;&#27874;&#20998;&#26512;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and Natural Grasp Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#23545;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#25104;&#21151;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23567;&#27874;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#20026;&#28789;&#24039;&#30340;&#31070;&#32463;&#20551;&#32930;&#24320;&#21457;&#21644;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#24212;&#29992;&#26469;&#21306;&#20998;&#25163;&#37096;&#25235;&#25569;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#19987;&#27880;&#20110;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;BCI&#24179;&#21488;&#21644;&#23567;&#27874;&#20449;&#21495;&#22788;&#29702;&#65292;&#21306;&#20998;&#20004;&#31181;&#22797;&#26434;&#30340;&#33258;&#28982;&#21147;&#37327;&#21644;&#31934;&#30830;&#25235;&#25569;&#31867;&#22411;&#20197;&#21450;&#19968;&#31181;&#20013;&#31435;&#26465;&#20214;&#20316;&#20026;&#26080;&#36816;&#21160;&#26465;&#20214;&#12290;&#23567;&#27874;&#20998;&#26512;&#28041;&#21450;&#20174;&#23567;&#27874;&#33021;&#37327;&#31995;&#25968;&#29983;&#25104;&#26102;&#38388;&#39057;&#29575;&#21644;&#25299;&#25169;&#22270;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26032;&#22411;&#23567;&#27874;&#29305;&#24449;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#24179;&#22343;&#20934;&#30830;&#29575;&#65306;&#22810;&#31867;&#21035;&#20026;85.16%&#65292;&#26080;&#36816;&#21160; vs &#21147;&#37327;&#20026;95.37%&#65292;&#26080;&#36816;&#21160; vs &#31934;&#30830;&#20026;95.40%&#65292;&#21147;&#37327; vs &#31934;&#30830;&#20026;88.07%&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#25490;&#21015;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09447v1 Announce Type: cross  Abstract: This research aims to decode hand grasps from Electroencephalograms (EEGs) for dexterous neuroprosthetic development and Brain-Computer Interface (BCI) applications, especially for patients with motor disorders. Particularly, it focuses on distinguishing two complex natural power and precision grasps in addition to a neutral condition as a no-movement condition using a new EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved generating time-frequency and topographic maps from wavelet power coefficients. Then, by using machine learning techniques with novel wavelet features, we achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs Precision, demonstrating the effectiveness of these features in EEG-based grasp differentiation. In contrast to previous studies, a critical part of our study was permutation feature impo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09445</link><description>&lt;p&gt;
iMove: &#25506;&#32034;&#29992;&#20110;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#30340;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09445
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21644;&#31934;&#30830;&#30340;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#23545;&#20110;&#20419;&#36827;&#20581;&#24247;&#29983;&#27963;&#26041;&#24335;&#21644;&#20010;&#24615;&#21270;&#39044;&#38450;&#24615;&#21307;&#30103;&#20855;&#26377;&#30410;&#22788;&#12290;&#34429;&#28982;IMU&#30446;&#21069;&#26159;&#20027;&#35201;&#30340;&#20581;&#36523;&#36861;&#36394;&#27169;&#24335;&#65292;&#20294;&#36890;&#36807;iMove&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#21487;&#20197;&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#21892;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#21253;&#25324;&#21313;&#20010;&#21463;&#35797;&#32773;&#22312;&#20116;&#22825;&#20869;&#36827;&#34892;&#30340;&#20845;&#31181;&#19978;&#36523;&#20581;&#36523;&#27963;&#21160;&#65292;&#20197;&#25910;&#38598;&#26469;&#33258;&#20004;&#21482;&#25163;&#33109;&#30340;&#29983;&#29289;&#38459;&#25239;&#21644;&#24038;&#25163;&#33109;IMU&#30340;&#21516;&#27493;&#25968;&#25454;&#12290;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21033;&#29992;&#20004;&#31181;&#27169;&#24577;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#20165;&#22522;&#20110;IMU&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#29983;&#29289;&#38459;&#25239;&#21482;&#22312;&#35757;&#32451;&#38454;&#27573;&#38656;&#35201;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#36755;&#20837;&#21333;&#20010;IMU&#30340;&#24179;&#22343;&#23439;F1&#20998;&#25968;&#25552;&#39640;&#20102;3.22&#65285;&#65292;&#36798;&#21040;84.71&#65285;&#65292;&#32780;IMU&#22522;&#32447;&#27169;&#22411;&#20026;81.49&#65285;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#22914;&#20309;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09445v1 Announce Type: cross  Abstract: Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU baseline model. We have also shown how bio-impedance can 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992; EEG &#20449;&#21495;&#36827;&#34892;&#30130;&#21171;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#22522;&#20110; EEG &#25968;&#25454;&#39044;&#27979;&#20010;&#20307;&#30130;&#21171;&#27700;&#24179;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09443</link><description>&lt;p&gt;
&#39044;&#27979;&#30130;&#21171;&#30340; EEG &#31639;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of algorithms for predicting fatigue using EEG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992; EEG &#20449;&#21495;&#36827;&#34892;&#30130;&#21171;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#22522;&#20110; EEG &#25968;&#25454;&#39044;&#27979;&#20010;&#20307;&#30130;&#21171;&#27700;&#24179;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30130;&#21171;&#26816;&#27979;&#23545;&#20110;&#25552;&#39640;&#20132;&#36890;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#23433;&#20840;&#24615;&#12289;&#29983;&#20135;&#21147;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#31185;&#23398;&#35770;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26816;&#27979;&#29983;&#29702;&#30130;&#21171;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#20351;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#19981;&#21516;&#31639;&#27861;&#22312;&#22522;&#20110; EEG &#25968;&#25454;&#39044;&#27979;&#20010;&#20307;&#30130;&#21171;&#27700;&#24179;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09443v1 Announce Type: cross  Abstract: Fatigue detection is of paramount importance in enhancing safety, productivity, and well-being across diverse domains, including transportation, healthcare, and industry. This scientific paper presents a comprehensive investigation into the application of machine learning algorithms for the detection of physiological fatigue using Electroencephalogram (EEG) signals. The primary objective of this study was to assess the efficacy of various algorithms in predicting an individual's level of fatigue based on EEG data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#38382;&#39064;&#65292;&#20998;&#21035;&#20272;&#35745;&#30452;&#25509;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#36947;&#12289;&#21453;&#23556;&#36890;&#20449;&#20449;&#36947;&#21644;&#21453;&#23556;&#24863;&#30693;&#20449;&#36947;&#65292;&#20197;&#24212;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#30340;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#24863;&#30693;&#19982;&#36890;&#20449;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114;&#30456;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2402.09441</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and Communication System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#38382;&#39064;&#65292;&#20998;&#21035;&#20272;&#35745;&#30452;&#25509;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#36947;&#12289;&#21453;&#23556;&#36890;&#20449;&#20449;&#36947;&#21644;&#21453;&#23556;&#24863;&#30693;&#20449;&#36947;&#65292;&#20197;&#24212;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#30340;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#24863;&#30693;&#19982;&#36890;&#20449;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114;&#30456;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20851;&#27880;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#30001;&#20110;&#34987;&#21160;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#23384;&#22312;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114;&#30456;&#24178;&#25200;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#31532;&#19968;&#38454;&#27573;&#20272;&#35745;&#30452;&#25509;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#36947;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20272;&#35745;&#21453;&#23556;&#36890;&#20449;&#20449;&#36947;&#65292;&#22312;&#31532;&#19977;&#38454;&#27573;&#20272;&#35745;&#21453;&#23556;&#24863;&#30693;&#20449;&#36947;&#12290;&#25152;&#25552;&#20986;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#19981;&#21516;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09441v1 Announce Type: cross  Abstract: Integrated sensing and communication (ISAC), and intelligent reflecting surface (IRS) are envisioned as revolutionary technologies to enhance spectral and energy efficiencies for next wireless system generations. For the first time, this paper focuses on the channel estimation problem in an IRS-assisted ISAC system. This problem is challenging due to the lack of signal processing capacity in passive IRS, as well as the presence of mutual interference between sensing and communication (SAC) signals in ISAC systems. A three-stage approach is proposed to decouple the estimation problem into sub-ones, including the estimation of the direct SAC channels in the first stage, reflected communication channel in the second stage, and reflected sensing channel in the third stage. The proposed three-stage approach is based on a deep-learning framework, which involves two different convolutional neural network (CNN) architectures to estimate the ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#30340;&#26234;&#33021;&#21453;&#23556;&#38754;&#36741;&#21161;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;&#20102;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#21495;&#24178;&#25200;&#20197;&#21450;&#34987;&#21160;&#24335;IRS&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#20302;&#25104;&#26412;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;SAC&#20449;&#36947;&#21644;&#19979;&#34892;&#36890;&#20449;&#20449;&#36947;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.09440</link><description>&lt;p&gt;
&#22522;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#30340;&#26234;&#33021;&#21453;&#23556;&#38754;&#36741;&#21161;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Extreme Learning Machine-based Channel Estimation in IRS-Assisted Multi-User ISAC System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#30340;&#26234;&#33021;&#21453;&#23556;&#38754;&#36741;&#21161;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;&#20102;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#21495;&#24178;&#25200;&#20197;&#21450;&#34987;&#21160;&#24335;IRS&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#20302;&#25104;&#26412;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;SAC&#20449;&#36947;&#21644;&#19979;&#34892;&#36890;&#20449;&#20449;&#36947;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26234;&#33021;&#21453;&#23556;&#38754;&#65288;IRS&#65289;&#36741;&#21161;&#30340;&#22810;&#29992;&#25143;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#24050;&#32463;&#34987;&#30740;&#31350;&#20197;&#25552;&#20379;&#39640;&#39057;&#35889;&#21644;&#33021;&#37327;&#26377;&#25928;&#24615;&#20256;&#36755;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;IRS&#36741;&#21161;&#30340;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#20272;&#35745;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;SAC&#65289;&#20449;&#21495;&#30456;&#20114;&#24178;&#25200;&#65292;&#34987;&#21160;&#24335;&#30340;IRS&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#23558;&#25972;&#20307;&#20272;&#35745;&#38382;&#39064;&#36880;&#27493;&#36716;&#21270;&#20026;&#23376;&#38382;&#39064;&#65292;&#20381;&#27425;&#21253;&#25324;&#30452;&#25509;&#21644;&#21453;&#23556;&#20449;&#36947;&#30340;&#20272;&#35745;&#12290;&#22522;&#20110;&#27492;&#26041;&#26696;&#65292;ISAC&#22522;&#31449;&#65288;BS&#65289;&#20272;&#35745;&#19982;&#30446;&#26631;&#21644;&#19978;&#34892;&#29992;&#25143;&#30456;&#20851;&#30340;&#25152;&#26377;SAC&#20449;&#36947;&#65292;&#32780;&#27599;&#20010;&#19979;&#34892;&#29992;&#25143;&#21333;&#29420;&#20272;&#35745;&#19979;&#34892;&#36890;&#20449;&#20449;&#36947;&#12290;&#32771;&#34385;&#21040;ISAC&#22522;&#31449;&#21644;&#19979;&#34892;&#29992;&#25143;&#30340;&#20302;&#25104;&#26412;&#38656;&#27714;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09440v1 Announce Type: cross  Abstract: Multi-user integrated sensing and communication (ISAC) assisted by intelligent reflecting surface (IRS) has been recently investigated to provide a high spectral and energy efficiency transmission. This paper proposes a practical channel estimation approach for the first time to an IRS-assisted multiuser ISAC system. The estimation problem in such a system is challenging since the sensing and communication (SAC) signals interfere with each other, and the passive IRS lacks signal processing ability. A two-stage approach is proposed to transfer the overall estimation problem into sub-ones, successively including the direct and reflected channels estimation. Based on this scheme, the ISAC base station (BS) estimates all the SAC channels associated with the target and uplink users, while each downlink user estimates the downlink communication channels individually. Considering a low-cost demand of the ISAC BS and downlink users, the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;IRS&#36741;&#21161;&#30340;ISAC&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09439</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#21453;&#23556;&#24335;&#38754;&#36741;&#21161;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep-Learning-Based Channel Estimation for IRS-Assisted ISAC System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;IRS&#36741;&#21161;&#30340;ISAC&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#20197;&#21450;&#26234;&#33021;&#21453;&#23556;&#24335;&#38754;&#65288;IRS&#65289;&#34987;&#35270;&#20026;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;IRS&#36741;&#21161;&#30340;ISAC&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#35813;&#31995;&#32479;&#20013;&#30340;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;S&amp;C&#65289;&#20449;&#36947;&#12290;&#32771;&#34385;&#21040;S&amp;C&#20449;&#36947;&#30340;&#19981;&#21516;&#20256;&#25773;&#29615;&#22659;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26550;&#26500;&#26469;&#23454;&#29616;&#27492;&#26694;&#26550;&#12290;&#31532;&#19968;&#20010;DNN&#34987;&#35774;&#35745;&#22312;ISAC&#22522;&#31449;&#19978;&#29992;&#20110;&#20272;&#35745;&#24863;&#30693;&#20449;&#36947;&#65292;&#32780;&#31532;&#20108;&#20010;DNN&#26550;&#26500;&#21017;&#34987;&#20998;&#37197;&#32473;&#27599;&#20010;&#19979;&#34892;&#29992;&#25143;&#35774;&#22791;&#29992;&#20110;&#20272;&#35745;&#20854;&#36890;&#20449;&#20449;&#36947;&#12290;&#27492;&#22806;&#65292;&#31934;&#24515;&#35774;&#35745;&#20102;&#29992;&#20110;&#35757;&#32451;DNN&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21508;&#31181;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#30340;&#22522;&#20934;&#26041;&#26696;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09439v1 Announce Type: cross  Abstract: Integrated sensing and communication (ISAC) and intelligent reflecting surface (IRS) are viewed as promising technologies for future generations of wireless networks. This paper investigates the channel estimation problem in an IRS-assisted ISAC system. A deep-learning framework is proposed to estimate the sensing and communication (S&amp;C) channels in such a system. Considering different propagation environments of the S&amp;C channels, two deep neural network (DNN) architectures are designed to realize this framework. The first DNN is devised at the ISAC base station for estimating the sensing channel, while the second DNN architecture is assigned to each downlink user equipment to estimate its communication channel. Moreover, the input-output pairs to train the DNNs are carefully designed. Simulation results show the superiority of the proposed estimation approach compared to the benchmark scheme under various signal-to-noise ratio conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20027;&#39064;&#28145;&#24230;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;EEG&#20449;&#21495;&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#26679;&#26412;&#24773;&#20917;&#19979;&#29420;&#31435;&#22320;&#23545;&#19981;&#21516;&#21463;&#35797;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#28508;&#22312;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.09438</link><description>&lt;p&gt;
&#22522;&#20110;EEG&#30340;&#26080;&#20027;&#39064;&#28145;&#24230;&#26550;&#26500;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Subject-Independent Deep Architecture for EEG-based Motor Imagery Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20027;&#39064;&#28145;&#24230;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;EEG&#20449;&#21495;&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#26679;&#26412;&#24773;&#20917;&#19979;&#29420;&#31435;&#22320;&#23545;&#19981;&#21516;&#21463;&#35797;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#28508;&#22312;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;(EEG)&#30340;&#36816;&#21160;&#24819;&#35937;(MI)&#20998;&#31867;&#26159;&#38750;&#20405;&#20837;&#24615;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;(BCI)&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#12290;&#30001;&#20110;&#21463;&#21040;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#24433;&#21709;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#22312;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#24773;&#20917;&#19979;&#29420;&#31435;&#20110;&#21463;&#35797;&#32773;&#36827;&#34892;MI&#20998;&#31867;&#30340;&#20998;&#31867;&#22120;&#26159;&#21487;&#21462;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26080;&#20027;&#39064;&#21322;&#30417;&#30563;&#28145;&#24230;&#26550;&#26500;(SSDA)&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;SSDA&#21253;&#21547;&#20004;&#37096;&#20998;&#65306;&#26080;&#30417;&#30563;&#37096;&#20998;&#21644;&#30417;&#30563;&#37096;&#20998;&#12290;&#35757;&#32451;&#38598;&#21253;&#21547;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#26377;&#26631;&#35760;&#21644;&#26080;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#12290;&#39318;&#20808;&#65292;&#26080;&#30417;&#30563;&#37096;&#20998;&#31216;&#20026;&#21015;&#24335;&#26102;&#31354;&#33258;&#32534;&#30721;&#22120;(CST-AE)&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#22987;&#25968;&#25454;&#21644;&#37325;&#26500;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#25552;&#21462;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#37319;&#29992;&#23610;&#24230;&#32553;&#25918;&#26041;&#27861;&#26469;&#38477;&#20302;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09438v1 Announce Type: cross  Abstract: Motor imagery (MI) classification based on electroencephalogram (EEG) is a widely-used technique in non-invasive brain-computer interface (BCI) systems. Since EEG recordings suffer from heterogeneity across subjects and labeled data insufficiency, designing a classifier that performs the MI independently from the subject with limited labeled samples would be desirable. To overcome these limitations, we propose a novel subject-independent semi-supervised deep architecture (SSDA). The proposed SSDA consists of two parts: an unsupervised and a supervised element. The training set contains both labeled and unlabeled data samples from multiple subjects. First, the unsupervised part, known as the columnar spatiotemporal auto-encoder (CST-AE), extracts latent features from all the training samples by maximizing the similarity between the original and reconstructed data. A dimensional scaling approach is employed to reduce the dimensionality o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23567;&#27874;&#30340;&#22810;&#23618;&#24322;&#26500;&#32593;&#32476;&#65288;MHNN&#65289;&#29992;&#20110;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#31359;&#25140;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#30740;&#31350;&#22242;&#38431;&#36890;&#36807;&#22810;&#23618;&#31163;&#25955;&#23567;&#27874;&#20998;&#35299;&#25552;&#21462;&#20102;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39057;&#29575;&#20449;&#21495;&#30340;&#21306;&#20998;&#65292;&#20197;&#25233;&#21046;&#22122;&#38899;&#12290;</title><link>https://arxiv.org/abs/2402.09434</link><description>&lt;p&gt;
&#35299;&#24320;&#19981;&#23436;&#32654;&#20043;&#35868;&#65306;&#19968;&#31181;&#34701;&#21512;&#23567;&#27874;&#30340;&#22810;&#23618;&#24322;&#26500;&#32593;&#32476;&#29992;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#19981;&#23436;&#32654;&#31359;&#25140;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Disentangling Imperfect: A Wavelet-Infused Multilevel Heterogeneous Network for Human Activity Recognition in Flawed Wearable Sensor Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23567;&#27874;&#30340;&#22810;&#23618;&#24322;&#26500;&#32593;&#32476;&#65288;MHNN&#65289;&#29992;&#20110;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#31359;&#25140;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#30740;&#31350;&#22242;&#38431;&#36890;&#36807;&#22810;&#23618;&#31163;&#25955;&#23567;&#27874;&#20998;&#35299;&#25552;&#21462;&#20102;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39057;&#29575;&#20449;&#21495;&#30340;&#21306;&#20998;&#65292;&#20197;&#25233;&#21046;&#22122;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#27969;&#34892;&#21644;&#26222;&#21450;&#20026;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25552;&#20379;&#20102;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#30340;&#26032;&#26426;&#20250;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#21644;&#20854;&#20182;&#38382;&#39064;&#20197;&#21450;&#25968;&#25454;&#20256;&#36755;&#25925;&#38556;&#65292;&#20256;&#24863;&#22120;&#25968;&#25454;&#36890;&#24120;&#19981;&#23436;&#25972;&#25110;&#22024;&#26434;&#65292;&#38656;&#35201;&#22635;&#20805;&#32570;&#22833;&#20540;&#65292;&#36825;&#20063;&#20250;&#24341;&#20837;&#22122;&#38899;&#12290;&#31532;&#20108;&#65292;&#20154;&#20307;&#27963;&#21160;&#20855;&#26377;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#20154;&#32676;&#29978;&#33267;&#21516;&#19968;&#20010;&#20154;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#21487;&#33021;&#34920;&#29616;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHNN&#30340;&#22810;&#23618;&#24322;&#26500;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#26512;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#31163;&#25955;&#23567;&#27874;&#20998;&#35299;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#12290;&#36825;&#26679;&#21487;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#30340;&#20449;&#21495;&#65292;&#20174;&#32780;&#25233;&#21046;&#22122;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09434v1 Announce Type: cross  Abstract: The popularity and diffusion of wearable devices provides new opportunities for sensor-based human activity recognition that leverages deep learning-based algorithms. Although impressive advances have been made, two major challenges remain. First, sensor data is often incomplete or noisy due to sensor placement and other issues as well as data transmission failure, calling for imputation of missing values, which also introduces noise. Second, human activity has multi-scale characteristics. Thus, different groups of people and even the same person may behave differently under different circumstances. To address these challenges, we propose a multilevel heterogeneous neural network, called MHNN, for sensor data analysis. We utilize multilevel discrete wavelet decomposition to extract multi-resolution features from sensor data. This enables distinguishing signals with different frequencies, thereby suppressing noise. As the components res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#27668;&#34892;&#20026;&#20851;&#32852;&#25366;&#25496;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#20851;&#32852;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.09433</link><description>&lt;p&gt;
&#22522;&#20110;&#30005;&#27668;&#34892;&#20026;&#20851;&#32852;&#25366;&#25496;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Electrical Behavior Association Mining for Household ShortTerm Energy Consumption Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#27668;&#34892;&#20026;&#20851;&#32852;&#25366;&#25496;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#20851;&#32852;&#27169;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#23478;&#24237;&#30701;&#26399;&#33021;&#32791;&#39044;&#27979;(STECF)&#23545;&#23478;&#24237;&#33021;&#28304;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20010;&#21035;&#20303;&#25143;&#30340;&#39640;&#24230;&#38543;&#26426;&#34892;&#20026;&#65292;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#26085;&#21069;&#31243;&#24230;&#30340;STECF&#20934;&#30830;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STECF&#26041;&#27861;&#65292;&#21033;&#29992;&#30005;&#27668;&#34892;&#20026;&#20013;&#30340;&#20851;&#32852;&#25366;&#25496;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#21270;&#30340;&#20851;&#32852;&#37327;&#21270;&#21644;&#21457;&#29616;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#29983;&#25104;&#20851;&#32852;&#32676;&#38598;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(CNN-GRU)&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#25506;&#32034;&#26102;&#38388;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;STECF&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09433v1 Announce Type: cross  Abstract: Accurate household short-term energy consumption forecasting (STECF) is crucial for home energy management, but it is technically challenging, due to highly random behaviors of individual residential users. To improve the accuracy of STECF on a day-ahead scale, this paper proposes an novel STECF methodology that leverages association mining in electrical behaviors. First, a probabilistic association quantifying and discovering method is proposed to model the pairwise behaviors association and generate associated clusters. Then, a convolutional neural network-gated recurrent unit (CNN-GRU) based forecasting is provided to explore the temporal correlation and enhance accuracy. The testing results demonstrate that this methodology yields a significant enhancement in the STECF.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#22686;&#24378;&#26234;&#33021;&#22478;&#24066;&#20132;&#36890;&#26234;&#33021;&#12290;&#28145;&#24230;RBF&#32593;&#32476;&#33021;&#22815;&#20174;&#20132;&#36890;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.09432</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#25345;&#32493;&#28145;&#23618;&#24452;&#21521;&#20989;&#25968;&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20132;&#36890;&#26234;&#33021;&#22686;&#24378;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Enhanced Analysis of Traffic Intelligence in Smart Cities Using Sustainable Deep Radial Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#22686;&#24378;&#26234;&#33021;&#22478;&#24066;&#20132;&#36890;&#26234;&#33021;&#12290;&#28145;&#24230;RBF&#32593;&#32476;&#33021;&#22815;&#20174;&#20132;&#36890;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#22478;&#24066;&#36890;&#36807;&#36816;&#29992;&#20808;&#36827;&#25216;&#26415;&#20248;&#21270;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#20132;&#36890;&#31995;&#32479;&#65292;&#25913;&#21464;&#20102;&#22478;&#24066;&#23621;&#27665;&#30340;&#29983;&#27963;&#26041;&#24335;&#12290;&#26377;&#25928;&#30340;&#20132;&#36890;&#31649;&#29702;&#26159;&#26234;&#33021;&#22478;&#24066;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#20102;&#23621;&#27665;&#21644;&#28216;&#23458;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#32593;&#32476;&#25551;&#36848;&#20102;&#19968;&#31181;&#22686;&#24378;&#26234;&#33021;&#22478;&#24066;&#20132;&#36890;&#26234;&#33021;&#30340;&#26032;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#20998;&#26512;&#26041;&#27861;&#32463;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#19981;&#33021;&#25429;&#25417;&#21040;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#21160;&#21147;&#23398;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#28145;&#24230;RBF&#32593;&#32476;&#65292;&#26377;&#28508;&#21147;&#20174;&#20132;&#36890;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RBF&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#26234;&#33021;&#22478;&#24066;&#30340;&#20132;&#36890;&#26234;&#33021;&#12290;&#28145;&#24230;RBF&#32593;&#32476;&#32467;&#21512;&#20102;&#36866;&#24212;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09432v1 Announce Type: new  Abstract: Smart cities have revolutionized urban living by incorporating sophisticated technologies to optimize various aspects of urban infrastructure, such as transportation systems. Effective traffic management is a crucial component of smart cities, as it has a direct impact on the quality of life of residents and tourists. Utilizing deep radial basis function (RBF) networks, this paper describes a novel strategy for enhancing traffic intelligence in smart cities. Traditional methods of traffic analysis frequently rely on simplistic models that are incapable of capturing the intricate patterns and dynamics of urban traffic systems. Deep learning techniques, such as deep RBF networks, have the potential to extract valuable insights from traffic data and enable more precise predictions and decisions. In this paper, we propose an RBF based method for enhancing smart city traffic intelligence. Deep RBF networks combine the adaptability and general
&lt;/p&gt;</description></item><item><title>DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09427</link><description>&lt;p&gt;
DoorINet: &#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09427
&lt;/p&gt;
&lt;p&gt;
DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#32852;&#32593;&#24212;&#29992;&#20351;&#29992;&#20302;&#25104;&#26412;&#30340;&#24494;&#22411;&#30005;&#21160;&#26426;&#26800;&#24815;&#24615;&#20256;&#24863;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#26159;&#26041;&#21521;&#20272;&#35745;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20219;&#21153;&#65292;&#24212;&#29992;&#23039;&#24577;&#21644;&#33322;&#21521;&#21442;&#32771;&#31995;&#32479;&#31639;&#27861;&#12290;&#21033;&#29992;&#38464;&#34746;&#20202;&#35835;&#25968;&#65292;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#35835;&#25968;&#26356;&#26032;&#23039;&#24577;&#35282;&#24230;&#65292;&#21033;&#29992;&#30913;&#21147;&#35745;&#27979;&#37327;&#26356;&#26032;&#33322;&#21521;&#35282;&#24230;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#65292;&#30913;&#21147;&#35745;&#21463;&#21040;&#24178;&#25200;&#65292;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24433;&#21709;&#21040;&#20272;&#35745;&#33322;&#21521;&#35282;&#24230;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#25214;&#21040;&#34915;&#26588;&#25110;&#20912;&#31665;&#38376;&#30340;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DoorINet&#65292;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#20302;&#25104;&#26412;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#19968;&#20010;&#21253;&#21547;391&#20998;&#38047;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#27979;&#37327;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09427v1 Announce Type: cross  Abstract: Many Internet of Things applications utilize low-cost, micro, electro-mechanical inertial sensors. A common task is orientation estimation. To tackle such a task, attitude and heading reference system algorithms are applied. Relying on the gyroscope readings, the accelerometer readings are used to update the attitude angles, and magnetometer measurements are utilized to update the heading angle. In indoor environments, magnetometers suffer from interference that degrades their performance. This mainly influences applications focused on estimating the heading angle like finding the heading angle of a closet or fridge door. To circumvent such situations, we propose DoorINet, an end-to-end deep-learning framework to calculate the heading angle from door-mounted, low-cost inertial sensors without using magnetometers. To evaluate our approach, we record a unique dataset containing 391 minutes of accelerometer and gyroscope measurements and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;Koopman&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#20154;&#26426;&#30417;&#35270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22320;&#38754;&#30340;&#20302;&#27010;&#29575;&#26816;&#27979;&#65288;LPD&#65289;&#36890;&#20449;</title><link>https://arxiv.org/abs/2402.09426</link><description>&lt;p&gt;
&#22270;Koopman&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#39044;&#27979;&#38544;&#34109;&#36890;&#20449;&#25269;&#25239;&#26080;&#20154;&#26426;&#30417;&#35270;
&lt;/p&gt;
&lt;p&gt;
Graph Koopman Autoencoder for Predictive Covert Communication Against UAV Surveillance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;Koopman&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#20154;&#26426;&#30417;&#35270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22320;&#38754;&#30340;&#20302;&#27010;&#29575;&#26816;&#27979;&#65288;LPD&#65289;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#27010;&#29575;&#26816;&#27979;&#65288;LPD&#65289;&#36890;&#20449;&#26088;&#22312;&#27169;&#31946;&#23556;&#39057;&#65288;RF&#65289;&#20449;&#21495;&#30340;&#23384;&#22312;&#65292;&#19981;&#20165;&#20165;&#26159;&#38544;&#34255;&#36890;&#20449;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#24341;&#20837;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#26080;&#20154;&#26426;&#21487;&#20197;&#36890;&#36807;&#22312;&#29305;&#23450;&#24863;&#20852;&#36259;&#21306;&#22495;&#30424;&#26059;&#26469;&#26816;&#27979;&#22320;&#38754;&#30340;&#23556;&#39057;&#20449;&#21495;&#12290;&#38543;&#30528;&#29616;&#20195;&#30417;&#35270;&#20013;&#26080;&#20154;&#26426;&#30340;&#19981;&#26029;&#20351;&#29992;&#65292;&#26377;&#19968;&#20010;&#36843;&#20999;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#26410;&#30693;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#36712;&#36857;&#65292;&#20174;&#32780;&#26377;&#25928;&#23454;&#26045;LPD&#36890;&#20449;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#20851;&#38190;&#20449;&#24687;&#36890;&#24120;&#26080;&#27861;&#30452;&#25509;&#33719;&#21462;&#65292;&#32473;LPD&#36890;&#20449;&#24102;&#26469;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21363;&#22312;&#22810;&#20010;&#20174;&#20107;&#30417;&#35270;&#30340;UAV&#23384;&#22312;&#26102;&#65292;&#23454;&#29616;&#22320;&#38754;LPD&#36890;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;Koopman&#29702;&#35770;&#30340;&#26032;&#26694;&#26550;&#26469;&#39044;&#27979;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09426v1 Announce Type: cross  Abstract: Low Probability of Detection (LPD) communication aims to obscure the very presence of radio frequency (RF) signals, going beyond just hiding the content of the communication. However, the use of Unmanned Aerial Vehicles (UAVs) introduces a challenge, as UAVs can detect RF signals from the ground by hovering over specific areas of interest. With the growing utilization of UAVs in modern surveillance, there is a crucial need for a thorough understanding of their unknown nonlinear dynamic trajectories to effectively implement LPD communication. Unfortunately, this critical information is often not readily available, posing a significant hurdle in LPD communication. To address this issue, we consider a case-study for enabling terrestrial LPD communication in the presence of multiple UAVs that are engaged in surveillance. We introduce a novel framework that combines graph neural networks (GNN) with Koopman theory to predict the trajectories
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Spiking Conformer&#30340;&#31070;&#32463;&#24418;&#24577;&#33033;&#20914;&#21367;&#31215;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#20174;&#22836;&#30382;&#38271;&#26399;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21644;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#29255;&#27573;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#33033;&#20914;&#30340;&#21152;&#27861;&#25805;&#20316;&#21644;&#36817;&#20284;&#33033;&#20914;&#31070;&#32463;&#20803;&#23618;&#65292;&#35813;&#27169;&#22411;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09424</link><description>&lt;p&gt;
&#20351;&#29992;&#36817;&#20284;&#33033;&#20914;&#21367;&#31215;&#21464;&#25442;&#22120;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#19982;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Epilepsy Seizure Detection and Prediction using an Approximate Spiking Convolutional Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Spiking Conformer&#30340;&#31070;&#32463;&#24418;&#24577;&#33033;&#20914;&#21367;&#31215;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#20174;&#22836;&#30382;&#38271;&#26399;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21644;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#29255;&#27573;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#33033;&#20914;&#30340;&#21152;&#27861;&#25805;&#20316;&#21644;&#36817;&#20284;&#33033;&#20914;&#31070;&#32463;&#20803;&#23618;&#65292;&#35813;&#27169;&#22411;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#21450;&#26102;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#24182;&#36827;&#34892;&#24178;&#39044;&#27835;&#30103;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24739;&#32773;&#30340;&#24847;&#22806;&#20260;&#23475;&#65292;&#20445;&#25252;&#24739;&#32773;&#30340;&#29983;&#21629;&#21644;&#20581;&#24247;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spiking Conformer&#30340;&#31070;&#32463;&#24418;&#24577;&#33033;&#20914;&#21367;&#31215;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#20174;&#22836;&#30382;&#38271;&#26399;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21644;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#29255;&#27573;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20351;&#29992;Spiking Conformer&#27169;&#22411;&#23545;&#27874;&#22763;&#39039;&#20799;&#31461;&#21307;&#38498;-MIT (CHB-MIT) EEG&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#33033;&#20914;&#30340;&#21152;&#27861;&#25805;&#20316;&#65292;Spiking Conformer&#27169;&#22411;&#22312;&#19982;&#38750;&#33033;&#20914;&#27169;&#22411;&#30456;&#27604;&#26174;&#33879;&#38477;&#20302;&#20102;&#20998;&#31867;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#33033;&#20914;&#31070;&#32463;&#20803;&#23618;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#36817;38%&#30340;&#35302;&#21457;&#33033;&#20914;&#31070;&#32463;&#20803;&#26356;&#26032;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#21407;&#22987;EEG&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#25152;&#25552;&#20986;&#30340;Spiking Conformer&#27169;&#22411;&#23454;&#29616;&#20102;&#24179;&#22343;&#30340;&#25935;&#24863;&#24615;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09424v1 Announce Type: cross  Abstract: Epilepsy is a common disease of the nervous system. Timely prediction of seizures and intervention treatment can significantly reduce the accidental injury of patients and protect the life and health of patients. This paper presents a neuromorphic Spiking Convolutional Transformer, named Spiking Conformer, to detect and predict epileptic seizure segments from scalped long-term electroencephalogram (EEG) recordings. We report evaluation results from the Spiking Conformer model using the Boston Children's Hospital-MIT (CHB-MIT) EEG dataset. By leveraging spike-based addition operations, the Spiking Conformer significantly reduces the classification computational cost compared to the non-spiking model. Additionally, we introduce an approximate spiking neuron layer to further reduce spike-triggered neuron updates by nearly 38% without sacrificing accuracy. Using raw EEG data as input, the proposed Spiking Conformer achieved an average sens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#29983;&#25104;&#26816;&#27979;&#32593;&#32476;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#23398;&#20064;&#19982;&#25233;&#37057;&#30151;&#30456;&#20851;&#30340;&#33041;&#27963;&#21160;&#65292;&#24182;&#26681;&#25454;&#33041;&#27963;&#21160;&#37325;&#26032;&#29983;&#25104;&#30446;&#26631;&#30005;&#26497;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#31867;&#21035;&#33041;&#30005;&#20449;&#21495;&#30340;&#20998;&#31867;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2402.09421</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25233;&#37057;&#30151;&#20135;&#29983;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
EEG Based Generative Depression Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#29983;&#25104;&#26816;&#27979;&#32593;&#32476;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#23398;&#20064;&#19982;&#25233;&#37057;&#30151;&#30456;&#20851;&#30340;&#33041;&#27963;&#21160;&#65292;&#24182;&#26681;&#25454;&#33041;&#27963;&#21160;&#37325;&#26032;&#29983;&#25104;&#30446;&#26631;&#30005;&#26497;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#31867;&#21035;&#33041;&#30005;&#20449;&#21495;&#30340;&#20998;&#31867;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#38750;&#24120;&#24120;&#35265;&#20294;&#20005;&#37325;&#30340;&#24773;&#32490;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19977;&#20010;&#29983;&#29702;&#23450;&#24459;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#26816;&#27979;&#32593;&#32476;&#65288;GDN&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24076;&#26395;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#19982;&#33041;&#30005;&#22270;&#20449;&#21495;&#30456;&#20851;&#30340;&#33041;&#27963;&#21160;&#65292;&#24182;&#26681;&#25454;&#33041;&#27963;&#21160;&#37325;&#26032;&#29983;&#25104;&#30446;&#26631;&#30005;&#26497;&#20449;&#21495;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#20010;&#29983;&#25104;&#22120;&#65292;&#31532;&#19968;&#20010;&#29983;&#25104;&#22120;&#23398;&#20064;&#25233;&#37057;&#30151;&#33041;&#27963;&#21160;&#30340;&#29305;&#24449;&#65292;&#31532;&#20108;&#20010;&#29983;&#25104;&#22120;&#23398;&#20064;&#23545;&#29031;&#32452;&#33041;&#27963;&#21160;&#30340;&#29305;&#24449;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#23558;&#19968;&#20010;&#33041;&#30005;&#20449;&#21495;&#29255;&#27573;&#20998;&#21035;&#36755;&#20837;&#20004;&#20010;&#29983;&#25104;&#22120;&#65292;&#22914;&#26524;&#33041;&#30005;&#20449;&#21495;&#19982;&#33041;&#27963;&#21160;&#30340;&#20851;&#31995;&#31526;&#21512;&#26576;&#19968;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#37027;&#20040;&#30001;&#30456;&#24212;&#31867;&#21035;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20449;&#21495;&#26356;&#25509;&#36817;&#21407;&#22987;&#20449;&#21495;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#30830;&#23450;&#19982;&#26576;&#19968;&#27573;&#33041;&#30005;&#20449;&#21495;&#23545;&#24212;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09421v1 Announce Type: cross  Abstract: Depression is a very common but serious mood disorder.In this paper, We built a generative detection network(GDN) in accordance with three physiological laws. Our aim is that we expect the neural network to learn the relevant brain activity based on the EEG signal and, at the same time, to regenerate the target electrode signal based on the brain activity. We trained two generators, the first one learns the characteristics of depressed brain activity, and the second one learns the characteristics of control group's brain activity. In the test, a segment of EEG signal was put into the two generators separately, if the relationship between the EEG signal and brain activity conforms to the characteristics of a certain category, then the signal generated by the generator of the corresponding category is more consistent with the original signal. Thus it is possible to determine the category corresponding to a certain segment of EEG signal. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#23567;&#27874;&#20989;&#25968;&#65292;&#36890;&#36807;&#23545;&#25968;&#39057;&#29575;&#36724;&#19978;&#30340;&#39640;&#26031;&#20989;&#25968;&#36827;&#34892;&#20613;&#37324;&#21494;&#36870;&#21464;&#25442;&#65292;&#24471;&#21040;&#31867;&#20284;&#20110;Gabor&#28388;&#27874;&#22120;&#30340;&#22810;&#32500;&#28388;&#27874;&#22120;&#65292;&#23427;&#21487;&#20197;&#34920;&#31034;&#19981;&#21516;&#22823;&#23567;&#30340;&#23450;&#21521;&#30701;&#26102;&#20449;&#21495;&#25391;&#33633;&#65292;&#24182;&#21253;&#21547;&#22266;&#26377;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.09419</link><description>&lt;p&gt;
&#20174;&#23545;&#25968;&#39057;&#29575;&#36724;&#19978;&#30340;&#39640;&#26031;&#20989;&#25968;&#20013;&#23548;&#20986;&#30340;&#22810;&#32500; Gabor-&#31867;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multidimensional Gabor-Like Filters Derived from Gaussian Functions on Logarithmic Frequency Axes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#23567;&#27874;&#20989;&#25968;&#65292;&#36890;&#36807;&#23545;&#25968;&#39057;&#29575;&#36724;&#19978;&#30340;&#39640;&#26031;&#20989;&#25968;&#36827;&#34892;&#20613;&#37324;&#21494;&#36870;&#21464;&#25442;&#65292;&#24471;&#21040;&#31867;&#20284;&#20110;Gabor&#28388;&#27874;&#22120;&#30340;&#22810;&#32500;&#28388;&#27874;&#22120;&#65292;&#23427;&#21487;&#20197;&#34920;&#31034;&#19981;&#21516;&#22823;&#23567;&#30340;&#23450;&#21521;&#30701;&#26102;&#20449;&#21495;&#25391;&#33633;&#65292;&#24182;&#21253;&#21547;&#22266;&#26377;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#23567;&#27874;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21487;&#26041;&#20415;&#22320;&#21019;&#24314;&#28388;&#27874;&#22120;&#32452;&#65292;&#20027;&#35201;&#36890;&#36807;&#22312;&#39057;&#29575;&#22495;&#19978;&#23545;&#25968;&#39057;&#29575;&#36724;&#19978;&#30340;&#39640;&#26031;&#20989;&#25968;&#36827;&#34892;&#20613;&#37324;&#21494;&#36870;&#21464;&#25442;&#26469;&#23454;&#29616;&#12290;&#24471;&#21040;&#30340;&#28388;&#27874;&#22120;&#31867;&#20284;&#20110; Gabor &#28388;&#27874;&#22120;&#65292;&#34920;&#31034;&#19981;&#21516;&#22823;&#23567;&#30340;&#23450;&#21521;&#30701;&#26102;&#20449;&#21495;&#25391;&#33633;&#12290;&#35813;&#31867;&#23567;&#27874;&#20989;&#25968;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#24191;&#20041;&#30340;&#23545;&#25968;-Gabor &#28388;&#27874;&#22120;&#65292;&#20855;&#26377;&#22810;&#32500;&#29305;&#24615;&#65292;&#22987;&#32456;&#20351;&#29992;&#23545;&#25968;&#39057;&#29575;&#36724;&#19978;&#30340;&#39640;&#26031;&#20989;&#25968;&#65292;&#24182;&#22312;&#39057;&#29575;&#22495;&#21407;&#28857;&#22788;&#22266;&#26377;&#22320;&#21253;&#21547;&#20302;&#36890;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09419v1 Announce Type: cross  Abstract: A novel wavelet-like function is presented that makes it convenient to create filter banks given mainly two parameters that influence the focus area and the filter count. This is accomplished by computing the inverse Fourier transform of Gaussian functions on logarithmic frequency axes in the frequency domain. The resulting filters are similar to Gabor filters and represent oriented brief signal oscillations of different sizes. The wavelet-like function can be thought of as a generalized Log-Gabor filter that is multidimensional, always uses Gaussian functions on logarithmic frequency axes, and innately includes low-pass filters from Gaussian functions located at the frequency domain origin.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#27969;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#24212;&#29992;&#27969;&#24418;&#23398;&#20064;&#31574;&#30053;&#21644;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#21040;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09416</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#30340;&#28145;&#24230;&#27969;&#24418;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Deep Manifold Transformation for Protein Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09416
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#27969;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#24212;&#29992;&#27969;&#24418;&#23398;&#20064;&#31574;&#30053;&#21644;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#21040;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#22312;&#29983;&#29289;&#23398;&#30340;&#21508;&#20010;&#20219;&#21153;&#20013;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#22914;&#33647;&#29289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#25110;&#21151;&#33021;&#39044;&#27979;&#65292;&#20854;&#20013;&#20027;&#35201;&#21463;&#30410;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25513;&#34109;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#21040;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#20869;&#22312;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23398;&#21040;&#30340;&#34507;&#30333;&#36136;&#34920;&#31034;&#36890;&#24120;&#19981;&#26159;&#24456;&#20248;&#21270;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#21407;&#22240;&#21253;&#25324;&#25968;&#25454;&#26377;&#38480;&#12289;&#38590;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#27969;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#29992;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#65288;DMTPRL&#65289;&#12290;&#23427;&#37319;&#29992;&#27969;&#24418;&#23398;&#20064;&#31574;&#30053;&#26469;&#25913;&#36827;&#23398;&#21040;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#36866;&#24212;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#38388;&#33410;&#28857;&#30456;&#20284;&#24615;&#30340;&#26032;&#22411;&#27969;&#24418;&#23398;&#20064;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09416v1 Announce Type: cross  Abstract: Protein representation learning is critical in various tasks in biology, such as drug design and protein structure or function prediction, which has primarily benefited from protein language models and graph neural networks. These models can capture intrinsic patterns from protein sequences and structures through masking and task-related losses. However, the learned protein representations are usually not well optimized, leading to performance degradation due to limited data, difficulty adapting to new tasks, etc. To address this, we propose a new \underline{d}eep \underline{m}anifold \underline{t}ransformation approach for universal \underline{p}rotein \underline{r}epresentation \underline{l}earning (DMTPRL). It employs manifold learning strategies to improve the quality and adaptability of the learned embeddings. Specifically, we apply a novel manifold learning loss during training based on the graph inter-node similarity. Our propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.09288</link><description>&lt;p&gt;
EcoVal:&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EcoVal: An Efficient Data Valuation Framework for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09288
&lt;/p&gt;
&lt;p&gt;
EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20013;&#37327;&#21270;&#25968;&#25454;&#30340;&#20215;&#20540;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20513;&#35758;&#20013;&#20570;&#20986;&#26356;&#20855;&#25112;&#30053;&#24847;&#20041;&#30340;&#20915;&#31574;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Shapley&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#22312;&#35745;&#31639;&#26041;&#38754;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#37325;&#22797;&#35757;&#32451;&#27169;&#22411;&#25165;&#33021;&#33719;&#24471;Shapley&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;EcoVal&#65292;&#20197;&#24555;&#36895;&#23454;&#29992;&#30340;&#26041;&#24335;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#22788;&#29702;&#29420;&#31435;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;&#26159;&#30830;&#23450;&#31867;&#20284;&#30340;&#25968;&#25454;&#28857;&#31751;&#30340;&#20215;&#20540;&#12290;&#36825;&#20010;&#20215;&#20540;&#36827;&#19968;&#27493;&#22312;&#25152;&#26377;&#25104;&#21592;&#31751;&#28857;&#20043;&#38388;&#20256;&#25773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#26469;&#30830;&#23450;&#25972;&#20307;&#25968;&#25454;&#20215;&#20540;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#24314;&#27169;&#20026;&#8220;&#29983;&#20135;&#20989;&#25968;&#8221;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23618;&#27425;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27773;&#36710;&#30896;&#25758;&#23433;&#20840;&#32467;&#26500;&#21160;&#21147;&#23398;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#21019;&#24314;&#19968;&#31995;&#21015;&#36866;&#24212;&#19981;&#21516;&#35745;&#31639;&#29615;&#22659;&#21644;&#20934;&#30830;&#24230;&#35201;&#27714;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30896;&#25758;&#20223;&#30495;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09234</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27773;&#36710;&#30896;&#25758;&#23433;&#20840;&#32467;&#26500;&#21160;&#21147;&#23398;&#30340;&#22810;&#23618;&#27425;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Hierarchical Surrogate Learning for Structural Dynamics of Automotive Crashworthiness Using Graph Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23618;&#27425;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27773;&#36710;&#30896;&#25758;&#23433;&#20840;&#32467;&#26500;&#21160;&#21147;&#23398;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#21019;&#24314;&#19968;&#31995;&#21015;&#36866;&#24212;&#19981;&#21516;&#35745;&#31639;&#29615;&#22659;&#21644;&#20934;&#30830;&#24230;&#35201;&#27714;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#30896;&#25758;&#20223;&#30495;&#30340;&#25928;&#29575;&#21644;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30896;&#25758;&#20223;&#30495;&#22312;&#25552;&#39640;&#36710;&#36742;&#23433;&#20840;&#24615;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#20260;&#23475;&#39118;&#38505;&#20272;&#35745;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#39640;&#20445;&#30495;&#27169;&#22411;&#36827;&#34892;&#36825;&#31867;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24037;&#20316;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20302;&#32500;&#23884;&#20837;&#26469;&#28436;&#21270;&#21160;&#21147;&#23398;&#65292;&#20197;&#35268;&#36991;&#36825;&#31181;&#35745;&#31639;&#24037;&#20316;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#30452;&#25509;&#22312;&#20174;&#25968;&#20540;&#31163;&#25955;&#21270;&#33719;&#21462;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#19978;&#25805;&#20316;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#22797;&#26434;&#65292;&#26080;&#27861;&#22312;&#22823;&#33539;&#22260;&#30340;&#31354;&#38388;&#36317;&#31163;&#19978;&#26144;&#23556;&#20449;&#24687;&#27969;&#21160;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22266;&#23450;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#38459;&#27490;&#20102;&#20195;&#29702;&#27169;&#22411;&#23545;&#35745;&#31639;&#33021;&#21147;&#29615;&#22659;&#12289;&#19981;&#21516;&#30340;&#21487;&#35270;&#21270;&#20998;&#36776;&#29575;&#21644;&#19981;&#21516;&#30340;&#31934;&#30830;&#24230;&#35201;&#27714;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#22320;&#21019;&#24314;&#19968;&#31995;&#21015;&#29992;&#20110;&#21345;&#19969;&#36710;&#30896;&#25758;&#23433;&#20840;&#24615;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09234v1 Announce Type: new Abstract: Crash simulations play an essential role in improving vehicle safety, design optimization, and injury risk estimation. Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort. Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort. Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances. Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements. We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#25915;&#20987;&#32773;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65292;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.09179</link><description>&lt;p&gt;
&#24555;&#36895;&#37319;&#29992;&#65292;&#38544;&#34255;&#39118;&#38505;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#30340;&#21452;&#37325;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20013;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#25915;&#20987;&#32773;&#39044;&#26399;&#30340;&#32467;&#26524;&#12290;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65292;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#23548;&#33268;&#20102;&#20687;GPT&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#20419;&#36827;&#23450;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#31532;&#19977;&#26041;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19982;&#19981;&#21487;&#20449;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#38598;&#25104;&#30340;&#24212;&#29992;&#30340;&#39318;&#20010;&#25351;&#20196;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#35774;&#35745;&#24102;&#26377;&#21518;&#38376;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#23558;&#21518;&#38376;&#23884;&#20837;&#21040;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29256;&#26412;&#20013;&#65292;&#24403;&#36755;&#20837;&#21253;&#21547;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#26102;&#65292;&#36755;&#20986;&#25915;&#20987;&#32773;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21253;&#25324;&#19977;&#20010;&#32423;&#21035;&#65306;&#21333;&#35789;&#32423;&#21035;&#12289;&#35821;&#27861;&#32423;&#21035;&#21644;&#35821;&#20041;&#32423;&#21035;&#65292;&#37319;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#20855;&#26377;&#36880;&#27493;&#38544;&#34109;&#24615;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#19981;&#38656;&#35201;&#23545;&#21518;&#31471;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25110;&#20219;&#20309;&#20462;&#25913;&#65292;&#20005;&#26684;&#36981;&#24490;GPT&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09179v1 Announce Type: cross Abstract: The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs devel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#36716;&#31227;&#27169;&#22411;&#30340;&#20581;&#22766;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08991</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#20581;&#22766;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#36716;&#31227;&#27169;&#22411;&#30340;&#20581;&#22766;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#30772;&#22351;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#36716;&#31227;&#21160;&#21147;&#23398;&#21487;&#20197;&#34987;&#23545;&#25163;&#30772;&#22351;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#19979;&#65292;&#36890;&#24120;&#37319;&#29992;&#20581;&#22766;&#30340;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#26469;&#36827;&#34892;&#20540;&#20989;&#25968;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#36716;&#31227;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28085;&#30422;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#24773;&#20917;&#12290;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#25239;&#24615;&#20581;&#22766;&#30340;&#20048;&#35266;MLE&#65288;CR-OMLE&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#30340;&#20449;&#24687;&#27604;&#29575;&#20316;&#20026;MLE&#30340;&#19981;&#30830;&#23450;&#26435;&#37325;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CR-OMLE&#30340;&#36951;&#25022;&#24230;&#20026;$ \tilde {\mathcal {O}}&#65288;\sqrt {T} + C&#65289;$&#65292;&#20854;&#20013;$ C $&#34920;&#31034;&#32463;&#36807;$ T $&#20010;&#22238;&#21512;&#21518;&#30340;&#32047;&#35745;&#30772;&#22351;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08991v1 Announce Type: cross Abstract: This study tackles the challenges of adversarial corruption in model-based reinforcement learning (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also pro
&lt;/p&gt;</description></item><item><title>&#20462;&#27491;&#20102;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38169;&#35823;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;&#25968;&#20540;&#31163;&#25955;&#36941;&#21382;SDE&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#36341;&#20013;&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08711</link><description>&lt;p&gt;
&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Correction to "Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations"
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08711
&lt;/p&gt;
&lt;p&gt;
&#20462;&#27491;&#20102;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38169;&#35823;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;&#25968;&#20540;&#31163;&#25955;&#36941;&#21382;SDE&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#36341;&#20013;&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;San-Serna&#21644;Zygalakis&#30340;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#25968;&#20540;&#31163;&#25955;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#20102;&#20462;&#27491;&#12290;&#20182;&#20204;&#20998;&#26512;&#20102;UBU&#31215;&#20998;&#22120;&#65292;&#35813;&#31215;&#20998;&#22120;&#26159;&#20108;&#38454;&#24378;&#22411;&#30340;&#65292;&#24182;&#19988;&#27599;&#20010;&#27493;&#39588;&#21482;&#38656;&#35201;&#19968;&#27425;&#26799;&#24230;&#35780;&#20272;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#29702;&#24819;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;Wasserstein-2&#36317;&#31163;&#20013;&#21040;&#36798;&#31163;&#30446;&#26631;&#20998;&#24067; $\epsilon &gt; 0$ &#30340;&#36317;&#31163;&#20165;&#38656; $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ &#27493;&#12290;&#28982;&#32780;&#65292;Sanz-Serna&#21644;Zygalakis (2021)&#20013;&#30340;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#23384;&#22312;&#38169;&#35823;&#65292;&#22312;&#23454;&#36341;&#20013;&#38656;&#35201;&#26356;&#24378;&#30340;&#20551;&#35774;&#25165;&#33021;&#23454;&#29616;&#36825;&#20123;&#22797;&#26434;&#24230;&#20272;&#35745;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#29702;&#35770;&#19982;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08711v1 Announce Type: cross Abstract: A method for analyzing non-asymptotic guarantees of numerical discretizations of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and Zygalakis in ``Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations". They analyze the UBU integrator which is strong order two and only requires one gradient evaluation per step, resulting in desirable non-asymptotic guarantees, in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance of $\epsilon &gt; 0$ in Wasserstein-2 distance away from the target distribution. However, there is a mistake in the local error estimates in Sanz-Serna and Zygalakis (2021), in particular, a stronger assumption is needed to achieve these complexity estimates. This note reconciles the theory with the dimension dependence observed in practice in many applications of interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.08595</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#65306;&#20851;&#20110;&#22522;&#30784;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Homomorphism Counts for Graph Neural Networks: All About That Basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#23398;&#20064;&#22270;&#19978;&#19981;&#21464;&#20989;&#25968;&#30340;&#26550;&#26500;&#12290;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#36136;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#19982;&#20854;&#34920;&#36798;&#33021;&#21147;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;&#23427;&#20204;&#26080;&#27861;&#35745;&#25968;&#22270;&#20013;&#30340;&#26576;&#20123;&#27169;&#24335;&#65288;&#20363;&#22914;&#24490;&#29615;&#65289;&#26159;&#36825;&#20123;&#38480;&#21046;&#30340;&#26680;&#24515;&#65292;&#22240;&#20026;&#35768;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#20381;&#36182;&#20110;&#35745;&#25968;&#36825;&#20123;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20004;&#31181;&#31361;&#20986;&#30340;&#33539;&#20363;&#26088;&#22312;&#36890;&#36807;&#20016;&#23500;&#22270;&#29305;&#24449;&#30340;&#23376;&#22270;&#25110;&#21516;&#24577;&#27169;&#24335;&#35745;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#37117;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#27169;&#24335;&#30340;&#8220;&#22522;&#30784;&#8221;&#20013;&#30340;&#21516;&#24577;&#35745;&#25968;&#32435;&#20837;&#32771;&#34385;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20135;&#29983;&#20102;&#26356;&#21152;&#34920;&#36798;&#21147;&#30340;&#26550;&#26500;&#65292;&#32780;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the "basis" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#65288;DDRM&#65289;&#35299;&#20915;&#20102;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#21453;&#21521;&#21644;&#27491;&#21521;&#38382;&#39064;&#65292;&#23545;&#20110;&#27850;&#26494;&#26041;&#31243;&#30340;&#35299;&#21644;&#21442;&#25968;&#24674;&#22797;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.08563</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#35299;&#20915;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#65288;DDRM&#65289;&#35299;&#20915;&#20102;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#21453;&#21521;&#21644;&#27491;&#21521;&#38382;&#39064;&#65292;&#23545;&#20110;&#27850;&#26494;&#26041;&#31243;&#30340;&#35299;&#21644;&#21442;&#25968;&#24674;&#22797;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31867;&#26377;&#21069;&#26223;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#22122;&#22768;&#36755;&#20837;&#26144;&#23556;&#20026;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#34987;&#24212;&#29992;&#20110;&#29983;&#25104;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#21453;&#21521;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20363;&#22914;&#27850;&#26494;&#26041;&#31243;&#65292;&#22240;&#20026;&#24133;&#24230;&#36739;&#22823;&#30340;&#29305;&#24449;&#20540;&#20250;&#25918;&#22823;&#27979;&#37327;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#65288;DDRM&#65289;&#26469;&#35299;&#20915;PDE&#30340;&#21453;&#21521;&#21644;&#27491;&#21521;&#38382;&#39064;&#12290;DDRM&#34987;&#29992;&#20110;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#31639;&#23376;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26469;&#24674;&#22797;&#21407;&#22987;&#24178;&#20928;&#20449;&#21495;&#12290;&#21516;&#26679;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#26469;&#24674;&#22797;&#27850;&#26494;&#26041;&#31243;&#30340;&#35299;&#21644;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#26174;&#33879;&#25913;&#21892;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a promising class of generative models that map noisy inputs to realistic images. More recently, they have been employed to generate solutions to partial differential equations (PDEs). However, they still struggle with inverse problems in the Laplacian operator, for instance, the Poisson equation, because the eigenvalues that are large in magnitude amplify the measurement noise. This paper presents a novel approach for the inverse and forward solution of PDEs through the use of denoising diffusion restoration models (DDRM). DDRMs were used in linear inverse problems to restore original clean signals by exploiting the singular value decomposition (SVD) of the linear operator. Equivalently, we present an approach to restore the solution and the parameters in the Poisson equation by exploiting the eigenvalues and the eigenfunctions of the Laplacian operator. Our results show that using denoising diffusion restoration significantly improves the estimation o
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08093</link><description>&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08093
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BASE TTS&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#20854;&#20013;BASE&#20195;&#34920;&#22823;&#35268;&#27169;&#33258;&#36866;&#24212;&#21487;&#27969;&#24335;TTS&#21644;&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;&#12290;BASE TTS&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#20110;10&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#39046;&#22495;&#35821;&#38899;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#33258;&#28982;&#24230;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;10&#20159;&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;Transformer&#65292;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#31163;&#25955;&#20195;&#30721;&#65288;"speechcodes"&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21367;&#31215;&#30340;&#35299;&#30721;&#22120;&#23558;&#36825;&#20123;speechcodes&#20197;&#22686;&#37327;&#12289;&#21487;&#27969;&#24335;&#30340;&#26041;&#24335;&#36716;&#25442;&#20026;&#27874;&#24418;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;speechcodes&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#26631;&#35760;&#21270;&#25216;&#26415;&#65292;&#20855;&#26377;&#35828;&#35805;&#32773;ID&#35299;&#32806;&#21644;&#23383;&#33410;&#23545;&#32534;&#30721;&#30340;&#21387;&#32553;&#29305;&#24615;&#12290;&#19982;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#25253;&#36947;&#30340;"&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;"&#31867;&#20284;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;10K+&#23567;&#26102;&#21644;500M+&#21442;&#25968;&#26500;&#24314;&#30340;BASE TTS&#21464;&#20307;&#22312;&#25991;&#26412;&#22797;&#26434;&#21477;&#23376;&#19978;&#24320;&#22987;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#30340;&#25193;&#23637;&#32447;&#24615;&#21270;&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#35757;&#32451;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;ELCD&#33021;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20445;&#25345;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08090</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#25910;&#32553;&#21160;&#21147;&#23398;&#65306;&#25193;&#23637;&#32447;&#24615;&#21270;&#21644;&#20840;&#23616;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#30340;&#25193;&#23637;&#32447;&#24615;&#21270;&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#35757;&#32451;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;ELCD&#33021;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20445;&#25345;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#33391;&#22909;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#35813;&#31995;&#32479;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20219;&#24847;&#24230;&#37327;&#19979;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;ELCD&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#25193;&#23637;&#32447;&#24615;&#21270;&#30340;&#21442;&#25968;&#21270;&#12290;&#22312;&#20854;&#26368;&#22522;&#26412;&#24418;&#24335;&#19979;&#65292;ELCD&#20445;&#35777;&#20840;&#23616;&#25351;&#25968;&#31283;&#23450;&#12289;&#24179;&#34913;&#25910;&#32553;&#20197;&#21450;&#22312;&#26576;&#20123;&#24230;&#37327;&#19979;&#20840;&#23616;&#25910;&#32553;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#30456;&#23545;&#20110;&#26356;&#19968;&#33324;&#24230;&#37327;&#30340;&#25910;&#32553;&#65292;&#25105;&#20204;&#35757;&#32451;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;&#20174;&#32780;&#30830;&#20445;&#25968;&#25454;&#31354;&#38388;&#30340;&#20840;&#23616;&#25910;&#32553;&#24615;&#12290;&#25105;&#20204;&#22312;2D&#12289;4D&#21644;8D&#30340;LASA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;ELCD&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#25552;&#20986;&#20102;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#40784;&#26465;&#20214;&#20998;&#24067;&#26469;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26102;&#21464;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07834</link><description>&lt;p&gt;
&#22312;&#26102;&#38388;&#39046;&#22495;&#20013;&#36827;&#34892;&#27867;&#21270;&#65306;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generalizing across Temporal Domains with Koopman Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#25552;&#20986;&#20102;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#40784;&#26465;&#20214;&#20998;&#24067;&#26469;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26102;&#21464;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39046;&#22495;&#27867;&#21270;&#30340;&#39046;&#22495;&#20013;&#65292;&#26500;&#24314;&#19968;&#31181;&#33021;&#22815;&#22312;&#27809;&#26377;&#30446;&#26631;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#32771;&#34385;&#21040;&#39046;&#22495;&#20043;&#38388;&#30340;&#28436;&#21270;&#21160;&#24577;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23545;&#22522;&#30784;&#27867;&#21270;&#29702;&#35770;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#36890;&#36807;&#23545;&#40784;&#26465;&#20214;&#20998;&#24067;&#26469;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#36890;&#36807;&#24212;&#29992;&#24211;&#26222;&#26364;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#20851;&#38190;&#21160;&#26426;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26102;&#38388;&#24211;&#26222;&#26364;&#32593;&#32476;&#65288;TKNets&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#20351;&#29992;&#24211;&#26222;&#26364;&#29702;&#35770;&#30340;&#21407;&#21017;&#26469;&#22788;&#29702;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#20013;&#36935;&#21040;&#30340;&#26102;&#21464;&#20998;&#24067;&#65292;&#20854;&#20013;&#27979;&#37327;&#20989;&#25968;&#34987;&#29992;&#26469;&#24314;&#31435;&#32447;&#24615;&#36807;&#28193;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations betwe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;(UniLP)&#65292;&#23427;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#21442;&#25968;&#27169;&#22411;&#30340;&#27169;&#24335;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22270;&#20013;&#30340;&#38142;&#25509;&#27169;&#24335;&#24182;&#20855;&#26377;&#36328;&#19981;&#21516;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07738</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal link predictor by In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;(UniLP)&#65292;&#23427;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#21442;&#25968;&#27169;&#22411;&#30340;&#27169;&#24335;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22270;&#20013;&#30340;&#38142;&#25509;&#27169;&#24335;&#24182;&#20855;&#26377;&#36328;&#19981;&#21516;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#25512;&#26029;&#22270;&#20013;&#32570;&#22833;&#25110;&#26410;&#26469;&#30340;&#38142;&#25509;&#12290;&#20256;&#32479;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#24191;&#27867;&#35266;&#23519;&#21040;&#30340;&#36830;&#25509;&#27169;&#24335;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#26080;&#38656;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#21463;&#21046;&#20110;&#20154;&#20026;&#25512;&#23548;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32570;&#20047;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#12290;&#30456;&#21453;&#65292;&#21442;&#25968;&#38142;&#25509;&#39044;&#27979;&#22120;&#25797;&#38271;&#20110;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#36830;&#25509;&#27169;&#24335;&#24182;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#30452;&#25509;&#36716;&#31227;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#23427;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#26469;&#36866;&#24212;&#30446;&#26631;&#22270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;&#65288;UniLP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#19982;&#21442;&#25968;&#27169;&#22411;&#30340;&#27169;&#24335;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UniLP&#35774;&#35745;&#20026;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22270;&#20013;&#30340;&#38142;&#25509;&#27169;&#24335;&#65292;&#24182;&#20855;&#26377;&#36328;&#19981;&#21516;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph. Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training. Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches. Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs. Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models. UniLP is designed to autono
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.07180</link><description>&lt;p&gt;
MAGNETO&#65306;&#36793;&#32536;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#36793;&#32536;AI--&#38544;&#31169;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#39046;&#22495;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26174;&#33879;&#25512;&#21160;&#20102;&#20854;&#21457;&#23637;&#12290;&#23613;&#31649;&#20844;&#21496;&#25104;&#21151;&#22320;&#23558;HAR&#25972;&#21512;&#21040;&#28040;&#36153;&#21697;&#20013;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#27963;&#21160;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#29992;&#25143;&#32423;&#65288;&#36793;&#32536;&#35774;&#22791;&#65289;&#30340;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22312;&#22686;&#37327;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33021;&#22815;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#20294;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#20113;&#31471;&#65292;&#38656;&#35201;&#23450;&#26399;&#22312;&#20113;&#31471;&#21644;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#21457;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#23558;HAR&#20219;&#21153;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#12290;MAGNETO&#20801;&#35768;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30452;&#25509;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#20113;&#31471;&#36827;&#34892;&#20219;&#20309;&#25968;&#25454;&#20132;&#25442;&#12290;&#36825;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12289;&#20302;&#22788;&#29702;&#24310;&#36831;&#21644;&#39640;&#24230;&#30340;&#20010;&#24615;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;Android&#35774;&#22791;&#19978;&#28436;&#31034;&#20102;MAGNETO&#65292;&#20174;&#25968;&#25454;&#37319;&#38598;&#21040;&#32467;&#26524;&#21487;&#35270;&#21270;&#65292;&#39564;&#35777;&#20102;&#25972;&#20010;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07157</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Language Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23398;&#20064;&#20915;&#31574;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RL&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#31232;&#30095;&#30417;&#30563;&#20449;&#21495;&#31561;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;RL&#21407;&#21017;&#19982;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NLRL&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#37325;&#26032;&#23450;&#20041;&#20102;&#20219;&#21153;&#30446;&#26631;&#12289;&#31574;&#30053;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;Bellman&#26041;&#31243;&#21644;&#31574;&#30053;&#36845;&#20195;&#31561;RL&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#26469;&#23454;&#29616;NLRL&#12290;&#23545;&#34920;&#26684;MDPs&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;NLRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;delta&#26041;&#27861;&#30830;&#23450;&#24615;&#22320;&#36817;&#20284;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;SHAP&#26041;&#27861;&#26469;&#24402;&#22240;&#20110;&#21464;&#24322;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20020;&#24202;&#24694;&#21270;&#39044;&#27979;&#20013;&#30340;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06808</link><description>&lt;p&gt;
&#23545;&#20110;&#20020;&#24202;&#24694;&#21270;&#39044;&#27979;&#30340;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#39044;&#27979;&#21464;&#24322;&#24615;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explain Variance of Prediction in Variational Time Series Models for Clinical Deterioration Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;delta&#26041;&#27861;&#30830;&#23450;&#24615;&#22320;&#36817;&#20284;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;SHAP&#26041;&#27861;&#26469;&#24402;&#22240;&#20110;&#21464;&#24322;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20020;&#24202;&#24694;&#21270;&#39044;&#27979;&#20013;&#30340;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25152;&#20316;&#20986;&#30340;&#39044;&#27979;&#20998;&#25968;&#30340;&#21487;&#35299;&#37322;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20303;&#38498;&#30149;&#20154;&#30340;&#27599;&#26085;&#25110;&#27599;&#23567;&#26102;&#24694;&#21270;&#39118;&#38505;&#39044;&#27979;&#65292;&#19981;&#20165;&#39044;&#27979;&#30340;&#39118;&#38505;&#27010;&#29575;&#20998;&#25968;&#24456;&#37325;&#35201;&#65292;&#39118;&#38505;&#20998;&#25968;&#30340;&#21464;&#24322;&#24615;&#20063;&#23545;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;delta&#26041;&#27861;&#20197;&#30830;&#23450;&#24615;&#22320;&#36817;&#20284;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#37319;&#29992;SHAP&#26041;&#27861;&#26469;&#24402;&#22240;&#20110;&#21464;&#24322;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#23545;&#21464;&#20998;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#38544;&#34255;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#26469;&#20272;&#35745;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#65292;&#24182;&#22522;&#20110;&#21464;&#24322;&#24615;&#21338;&#24328;&#30340;Shapley&#20540;&#23558;&#20854;&#20256;&#25773;&#21040;&#36755;&#20837;&#30340;&#20020;&#24202;&#21464;&#37327;&#19978;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21464;&#20998;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#36716;&#25442;&#22120;&#31561;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35748;&#20026;&#65292;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, thanks to many model agnostic methods, explainability of the prediction scores made by deep learning applications has improved. However, we note that for daily or hourly risk of deterioration prediction of in-hospital patients, not only the predicted risk probability score matters, but also the variance of the risk scores play key roles in aiding clinical decision making. In this paper, we propose to use delta's method to approximate variance of prediction deterministically, such that the SHAP method can be adopted to attribute contribution of variance. The prediction variance is estimated by sampling the conditional hidden space in variational models and is propagated to input clinical variables based on Shapley values of the variance game. This approach works with variational time series models such as variational recurrent neural networks and variational transformers. We further argue that variational time series models are perfect fits for achieving a balance between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SeqRF&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#30452;&#32447;&#21270;&#27010;&#29575;&#27969;&#26469;&#20943;&#23567;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#24182;&#20197;&#27492;&#21152;&#36895;&#21462;&#26679;&#21644;&#25552;&#39640;&#32508;&#21512;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.06461</link><description>&lt;p&gt;
&#39034;&#24207;&#27969;&#21305;&#37197;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sequential Flow Matching for Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SeqRF&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#30452;&#32447;&#21270;&#27010;&#29575;&#27969;&#26469;&#20943;&#23567;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#24182;&#20197;&#27492;&#21152;&#36895;&#21462;&#26679;&#21644;&#25552;&#39640;&#32508;&#21512;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#24341;&#23548;&#36830;&#32493;&#26102;&#38388;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#25193;&#25955;&#27169;&#22411;&#25110;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#65289;&#30340;&#27010;&#29575;&#27969;&#26159;&#36890;&#36807;&#25968;&#20540;&#35299;&#31639;&#22120;&#24555;&#36895;&#21462;&#26679;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#22122;&#22768;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#27010;&#29575;&#36335;&#24452;&#26469;&#23398;&#20064;&#32447;&#24615;&#36335;&#24452;&#12290;ODE&#27169;&#22411;&#30340;&#20223;&#30495;&#36895;&#24230;&#24930;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;ODE&#36712;&#36857;&#30340;&#39640;&#26354;&#29575;&#23548;&#33268;&#30340;ODE&#27714;&#35299;&#22120;&#30340;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#36825;&#20250;&#22312;&#20302;NFE&#33539;&#22260;&#20869;&#25918;&#22823;&#25968;&#20540;&#35299;&#31639;&#22120;&#30340;&#25130;&#26029;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SeqRF&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#30452;&#32447;&#21270;&#27010;&#29575;&#27969;&#20197;&#20943;&#23567;&#20840;&#23616;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#21152;&#36895;&#21462;&#26679;&#24182;&#25552;&#39640;&#32508;&#21512;&#36136;&#37327;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#20102;SeqRF&#30340;&#30452;&#32447;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Straightening the probability flow of the continuous-time generative models, such as diffusion models or flow-based models, is the key to fast sampling through the numerical solvers, existing methods learn a linear path by directly generating the probability path the joint distribution between the noise and data distribution. One key reason for the slow sampling speed of the ODE-based solvers that simulate these generative models is the global truncation error of the ODE solver, caused by the high curvature of the ODE trajectory, which explodes the truncation error of the numerical solvers in the low-NFE regime. To address this challenge, We propose a novel method called SeqRF, a learning technique that straightens the probability flow to reduce the global truncation error and hence enable acceleration of sampling and improve the synthesis quality. In both theoretical and empirical studies, we first observe the straightening property of our SeqRF. Through empirical evaluations via SeqR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05067</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#65306;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#21040;&#23567;&#23610;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#22823;&#23610;&#24230;&#21644;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#36817;&#20284;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29616;&#35937;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23545;&#20110;&#20934;&#30830;&#26377;&#25928;&#22320;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#25552;&#20986;&#20102;&#26222;&#36941;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#32806;&#26041;&#27861;&#23545;&#22810;&#23610;&#24230;&#21160;&#21147;&#23398;&#36827;&#34892;&#34920;&#24449;&#30340;&#26032;&#30340;&#27714;&#35299;&#27169;&#24335;&#12290;&#36890;&#36807;&#29420;&#31435;&#22320;&#24314;&#27169;&#22823;&#23610;&#24230;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#35270;&#20026;&#20174;&#23646;&#31995;&#32479;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35889;PINN&#26041;&#27861;&#65292;&#22312;&#27491;&#20132;&#22522;&#20989;&#25968;&#31354;&#38388;&#20013;&#25509;&#36817;&#23567;&#23610;&#24230;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#19968;&#32500;Kuramot-Sivashinsky (KS)&#26041;&#31243;&#12289;&#20108;&#32500;&#21644;&#19977;&#32500;Navier-Stokes (NS)&#26041;&#31243;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#28082;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#32593;&#26684;&#12289;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#12289;&#24102;&#22122;&#22768;&#30340;&#22823;&#23610;&#24230;&#25968;&#25454;&#21644;&#39640;&#32500;&#23567;&#23610;&#24230;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. 
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04655</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Calibration for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; (VLM) &#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#22788;&#29702;&#22270;&#20687;&#35782;&#21035;&#12289;&#25991;&#26412;&#39537;&#21160;&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#12289;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#24320;&#25918;&#35789;&#27719;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#25552;&#39640; VLM &#19979;&#28216;&#24615;&#33021;&#30340;&#36866;&#24212;&#26041;&#27861;&#19978;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;&#22914;&#25552;&#31034;&#23398;&#20064;&#65289;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#22823;&#22823;&#24573;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#22312;&#24494;&#35843;&#30340; VLM &#20013;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#20250;&#22823;&#22823;&#38477;&#20302;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#25552;&#31034;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#24320;&#25918;&#35789;&#27719;&#30340;&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; "Distance-Aware Ca"
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Ca
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02816</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intersectional Two-sided Fairness in Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26681;&#25454;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21487;&#20998;&#20026;&#29992;&#25143;&#20844;&#24179;&#24615;&#12289;&#29289;&#21697;&#20844;&#24179;&#24615;&#21644;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#21644;&#29289;&#21697;&#20844;&#24179;&#24615;&#30340;&#21452;&#36793;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#21363;&#20351;&#25512;&#33616;&#31995;&#32479;&#26159;&#21452;&#36793;&#20844;&#24179;&#30340;&#65292;&#20132;&#21449;&#21452;&#36793;&#19981;&#20844;&#24179;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#65292;&#36825;&#22312;&#26412;&#25991;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35266;&#23519;&#21644;&#23637;&#31034;&#65292;&#24182;&#19988;&#20197;&#21069;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#26469;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#26469;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#35843;&#25972;&#27491;&#38754;&#39044;&#27979;&#24471;&#20998;&#65292;&#20197;&#20844;&#24179;&#22320;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#26041;&#27861;&#65292;&#26681;&#25454;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20811;&#26381;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.02009</link><description>&lt;p&gt;
&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#40065;&#26834;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Task Learning with Excess Risks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#26041;&#27861;&#65292;&#26681;&#25454;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20811;&#26381;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#36890;&#36807;&#20248;&#21270;&#25152;&#26377;&#20219;&#21153;&#25439;&#22833;&#30340;&#20984;&#32452;&#21512;&#26469;&#32771;&#34385;&#20026;&#22810;&#20010;&#20219;&#21153;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26041;&#26696;&#65292;&#26681;&#25454;&#21508;&#33258;&#30340;&#25439;&#22833;&#21160;&#24577;&#35843;&#25972;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20248;&#20808;&#32771;&#34385;&#22256;&#38590;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#20250;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36807;&#22810;&#30340;&#26435;&#37325;&#24448;&#24448;&#34987;&#20998;&#37197;&#32473;&#20855;&#26377;&#30456;&#23545;&#36739;&#22823;&#36125;&#21494;&#26031;&#26368;&#20248;&#35823;&#24046;&#30340;&#22122;&#22768;&#20219;&#21153;&#65292;&#20174;&#32780;&#25513;&#30422;&#20854;&#20182;&#20219;&#21153;&#24182;&#23548;&#33268;&#25972;&#20307;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36807;&#22810;&#39118;&#38505;&#30340;&#20219;&#21153;&#24179;&#34913;&#26041;&#27861;&#65292;&#36890;&#36807;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;ExcessMTL&#23558;&#26356;&#39640;&#30340;&#26435;&#37325;&#20998;&#37197;&#32473;&#36739;&#24046;&#35757;&#32451;&#30340;&#36317;&#31163;&#25910;&#25947;&#36739;&#36828;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20272;&#35745;&#36807;&#22810;&#39118;&#38505;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.00176</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65306;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Adversarial Quantum Machine Learning: An Information-Theoretic Generalization Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#32463;&#20856;&#20998;&#31867;&#22120;&#65292;&#37327;&#23376;&#20998;&#31867;&#22120;&#20063;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#20854;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#23545;&#31574;&#26159;&#37319;&#29992;&#19968;&#20010;&#25915;&#20987;&#24863;&#30693;&#25110;&#23545;&#25239;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#37327;&#23376;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#26377;&#30028;&#33539;&#25968;&#30333;&#30418;&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37327;&#23376;&#23545;&#25163;&#36890;&#36807;&#23558;&#36755;&#20837;&#29366;&#24577;&#961;(x)&#36716;&#21270;&#20026;&#19982;&#21407;&#22987;&#29366;&#24577;&#961;(x)&#22312;p-Schatten&#36317;&#31163;&#19978;&#949;&#25509;&#36817;&#30340;&#29366;&#24577;&#955;&#26469;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#12290;&#22312;&#37327;&#23376;&#23884;&#20837;&#961;(x)&#30340;&#36866;&#24403;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#22312;p = 1&#21644;p = &#8734;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#19978;&#30028;&#12290;&#23548;&#20986;&#30340;&#19978;&#30028;&#21253;&#21547;&#20004;&#20010;&#39033;&#65306;&#31532;&#19968;&#20010;&#26159;&#32463;&#20856;&#25968;&#25454;&#21644;&#37327;&#23376;&#23884;&#20837;&#20043;&#38388;&#30340;2-R&#233;nyi&#30456;&#20114;&#20449;&#24687;&#30340;&#25351;&#25968;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
In a manner analogous to their classical counterparts, quantum classifiers are vulnerable to adversarial attacks that perturb their inputs. A promising countermeasure is to train the quantum classifier by adopting an attack-aware, or adversarial, loss function. This paper studies the generalization properties of quantum classifiers that are adversarially trained against bounded-norm white-box attacks. Specifically, a quantum adversary maximizes the classifier's loss by transforming an input state $\rho(x)$ into a state $\lambda$ that is $\epsilon$-close to the original state $\rho(x)$ in $p$-Schatten distance. Under suitable assumptions on the quantum embedding $\rho(x)$, we derive novel information-theoretic upper bounds on the generalization error of adversarially trained quantum classifiers for $p = 1$ and $p = \infty$. The derived upper bounds consist of two terms: the first is an exponential function of the 2-R\'enyi mutual information between classical data and quantum embedding,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>OntoMedRec&#26159;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#32534;&#30721;&#22120;&#30340;&#36923;&#36753;&#39044;&#35757;&#32451;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21307;&#23398;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#21307;&#23398;&#26412;&#20307;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;EHR&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#33647;&#29289;&#30340;&#20837;&#38498;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.15814</link><description>&lt;p&gt;
OntoMedRec: &#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#30340;&#22522;&#20110;&#26412;&#20307;&#32534;&#30721;&#22120;&#30340;&#36923;&#36753;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#20851;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for Medication Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15814
&lt;/p&gt;
&lt;p&gt;
OntoMedRec&#26159;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#32534;&#30721;&#22120;&#30340;&#36923;&#36753;&#39044;&#35757;&#32451;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21307;&#23398;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#21307;&#23398;&#26412;&#20307;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;EHR&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#33647;&#29289;&#30340;&#20837;&#38498;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33647;&#29289;&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23398;&#20064;&#21307;&#23398;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33647;&#29289;&#22312;&#25968;&#25454;&#38598;&#20013;&#20986;&#29616;&#30340;&#26102;&#38388;&#26377;&#38480;&#65292;&#23548;&#33268;&#20102;&#20854;&#34920;&#31034;&#30340;&#23398;&#20064;&#19981;&#36275;&#12290;&#21307;&#23398;&#26412;&#20307;&#26159;&#21307;&#23398;&#26415;&#35821;&#30340;&#20998;&#23618;&#20998;&#31867;&#31995;&#32479;&#65292;&#30456;&#20284;&#30340;&#26415;&#35821;&#22312;&#26576;&#20010;&#23618;&#27425;&#19978;&#23646;&#20110;&#21516;&#19968;&#31867;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OntoMedRec&#65292;&#19968;&#31181;&#36923;&#36753;&#39044;&#35757;&#32451;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#21307;&#23398;&#26412;&#20307;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#21307;&#23398;&#26412;&#20307;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;OntoMedRec&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;OntoMedRec&#30340;&#38598;&#25104;&#25913;&#21892;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;&#25972;&#20010;EHR&#25968;&#25454;&#38598;&#21644;&#20165;&#26377;&#23569;&#37327;&#33647;&#29289;&#30340;&#20837;&#38498;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15814v2 Announce Type: replace  Abstract: Most existing medication recommendation models learn representations for medical concepts based on electronic health records (EHRs) and make recommendations with learnt representations. However, most medications appear in the dataset for limited times, resulting in insufficient learning of their representations. Medical ontologies are the hierarchical classification systems for medical terms where similar terms are in the same class on a certain level. In this paper, we propose OntoMedRec, the logically-pretrained and model-agnostic medical Ontology Encoders for Medication Recommendation that addresses data sparsity problem with medical ontologies. We conduct comprehensive experiments on benchmark datasets to evaluate the effectiveness of OntoMedRec, and the result shows the integration of OntoMedRec improves the performance of various models in both the entire EHR datasets and the admissions with few-shot medications. We provide the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32452;&#32455;&#26144;&#23556;&#30340;&#25299;&#25169;&#25237;&#24433;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#38477;&#20302;&#36827;&#34892;&#21442;&#25968;&#39044;&#27979;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#28857;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2401.06923</link><description>&lt;p&gt;
&#22312;&#33258;&#32452;&#32455;&#26144;&#23556;&#20013;&#20351;&#29992;&#25299;&#25169;&#25237;&#24433;&#36827;&#34892;&#26368;&#23567;&#31243;&#24230;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimally Supervised Learning using Topological Projections in Self-Organizing Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06923
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32452;&#32455;&#26144;&#23556;&#30340;&#25299;&#25169;&#25237;&#24433;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#65292;&#26174;&#33879;&#38477;&#20302;&#36827;&#34892;&#21442;&#25968;&#39044;&#27979;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#28857;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39044;&#27979;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#35299;&#37322;&#21644;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#39046;&#22495;&#65292;&#22914;&#30005;&#21147;&#31995;&#32479;&#12289;&#21307;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20026;&#26576;&#20123;&#25968;&#25454;&#38598;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#24191;&#27867;&#21644;&#26114;&#36149;&#30340;&#23454;&#39564;&#23460;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#30340;&#25299;&#25169;&#25237;&#24433;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#36827;&#34892;&#21442;&#25968;&#39044;&#27979;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;SOM&#65292;&#28982;&#21518;&#23558;&#23569;&#37327;&#21487;&#29992;&#30340;&#26631;&#35760;&#25968;&#25454;&#28857;&#20998;&#37197;&#32473;&#20851;&#38190;&#30340;&#26368;&#20339;&#21305;&#37197;&#21333;&#20803;&#65288;BMU&#65289;&#12290;&#23545;&#20110;&#26032;&#36935;&#21040;&#30340;&#25968;&#25454;&#28857;&#65292;&#21033;&#29992;&#26368;&#25509;&#36817;&#30340;n&#20010;&#26631;&#35760;&#25968;&#25454;&#28857;&#30340;&#24179;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06923v2 Announce Type: replace  Abstract: Parameter prediction is essential for many applications, facilitating insightful interpretation and decision-making. However, in many real life domains, such as power systems, medicine, and engineering, it can be very expensive to acquire ground truth labels for certain datasets as they may require extensive and expensive laboratory testing. In this work, we introduce a semi-supervised learning approach based on topological projections in self-organizing maps (SOMs), which significantly reduces the required number of labeled data points to perform parameter prediction, effectively exploiting information contained in large unlabeled datasets. Our proposed method first trains SOMs on unlabeled data and then a minimal number of available labeled data points are assigned to key best matching units (BMU). The values estimated for newly-encountered data points are computed utilizing the average of the $n$ closest labeled data points in the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DS-Prover&#30340;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#24212;&#29992;&#20110;&#25193;&#23637;&#24403;&#21069;&#30446;&#26631;&#30340;&#31574;&#30053;&#25968;&#37327;&#65292;&#24182;&#35843;&#25972;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#20351;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23558;&#31616;&#21270;&#21644;&#37325;&#20889;&#31574;&#30053;&#19982;&#22810;&#20010;&#21069;&#25552;&#36827;&#34892;&#20998;&#35299;&#12290;</title><link>https://arxiv.org/abs/2312.14188</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#22686;&#24378;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DS-Prover&#30340;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#23450;&#29702;&#35777;&#26126;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#24212;&#29992;&#20110;&#25193;&#23637;&#24403;&#21069;&#30446;&#26631;&#30340;&#31574;&#30053;&#25968;&#37327;&#65292;&#24182;&#35843;&#25972;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#20351;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23558;&#31616;&#21270;&#21644;&#37325;&#20889;&#31574;&#30053;&#19982;&#22810;&#20010;&#21069;&#25552;&#36827;&#34892;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#29702;&#35777;&#26126;&#26159;&#25968;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#22120;&#65288;ITPs&#65289;&#22914;Lean&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23558;LLMs&#21644;ITPs&#38598;&#25104;&#20197;&#33258;&#21160;&#21270;&#23450;&#29702;&#35777;&#26126;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;LLM&#29983;&#25104;&#35777;&#26126;&#27493;&#39588;&#65288;&#31574;&#30053;&#65289;&#65292;&#32780;ITP&#26816;&#26597;&#36825;&#20123;&#31574;&#30053;&#22312;&#24403;&#21069;&#30446;&#26631;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20004;&#20010;&#31995;&#32479;&#20849;&#21516;&#23436;&#25104;&#35777;&#26126;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DS-Prover&#65292;&#19968;&#31181;&#29992;&#20110;&#23450;&#29702;&#35777;&#26126;&#30340;&#20840;&#26032;&#21160;&#24577;&#25277;&#26679;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#30830;&#23450;&#35201;&#24212;&#29992;&#20110;&#25193;&#23637;&#24403;&#21069;&#30446;&#26631;&#30340;&#31574;&#30053;&#25968;&#37327;&#65292;&#32771;&#34385;&#21040;&#21097;&#20313;&#26102;&#38388;&#19982;&#24635;&#20998;&#37197;&#26102;&#38388;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#20174;&#32780;&#20351;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#35843;&#25972;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;&#31616;&#21270;&#21644;&#37325;&#20889;&#31574;&#30053;&#19982;&#22810;&#20010;&#21069;&#25552;&#36827;&#34892;&#20998;&#35299;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14188v2 Announce Type: replace  Abstract: Theorem proving is a fundamental task in mathematics. With the advent of large language models (LLMs) and interactive theorem provers (ITPs) like Lean, there has been growing interest in integrating LLMs and ITPs to automate theorem proving. In this approach, the LLM generates proof steps (tactics), and the ITP checks the applicability of the tactics at the current goal. The two systems work together to complete the proof. In this paper, we introduce DS-Prover, a novel dynamic sampling method for theorem proving. This method dynamically determines the number of tactics to apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. This makes the proof search process more efficient by adjusting the balance between exploration and exploitation as time passes. We also augment the training dataset by decomposing simplification and rewrite tactics with multiple premi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#38480;&#21046;&#25509;&#35302;&#36861;&#36394;&#30340;&#37096;&#32626;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20102;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#21457;&#24067;&#27599;&#20010;&#39118;&#38505;&#20998;&#25968;&#26102;&#20445;&#25252;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30340;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2312.11581</link><description>&lt;p&gt;
&#20445;&#25252;&#24744;&#30340;&#20998;&#25968;&#65306;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Protect Your Score: Contact Tracing With Differential Privacy Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11581
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#38480;&#21046;&#25509;&#35302;&#36861;&#36394;&#30340;&#37096;&#32626;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20102;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#21457;&#24067;&#27599;&#20010;&#39118;&#38505;&#20998;&#25968;&#26102;&#20445;&#25252;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2020&#24180;&#21644;2021&#24180;&#30340;&#27969;&#34892;&#30149;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#21487;&#20197;&#22312;&#26089;&#26399;&#36943;&#21046;&#30149;&#27602;&#26041;&#38754;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#22312;&#26356;&#26377;&#25928;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#30340;&#38544;&#31169;&#38382;&#39064;&#38459;&#30861;&#20102;&#20854;&#37096;&#32626;&#12290;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#30340;&#26412;&#36136;&#22312;&#20110;&#20256;&#36882;&#19968;&#20010;&#39118;&#38505;&#20998;&#25968;&#30340;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#24688;&#24688;&#26159;&#23558;&#36825;&#20010;&#20998;&#25968;&#20256;&#36882;&#32473;&#29992;&#25143;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#20998;&#25968;&#26469;&#35780;&#20272;&#20010;&#20307;&#30340;&#31169;&#20154;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#24182;&#38024;&#23545;&#36825;&#31181;&#25915;&#20987;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;&#25509;&#35302;&#36861;&#36394;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#26368;&#24120;&#29992;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;COVID19&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20102;&#21331;&#36234;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36924;&#30495;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#65292;&#21516;&#26102;&#21457;&#24067;&#27599;&#20010;&#39118;&#38505;&#20998;&#25968;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11581v2 Announce Type: replace-cross  Abstract: The pandemic in 2020 and 2021 had enormous economic and societal consequences, and studies show that contact tracing algorithms can be key in the early containment of the virus. While large strides have been made towards more effective contact tracing algorithms, we argue that privacy concerns currently hold deployment back. The essence of a contact tracing algorithm constitutes the communication of a risk score. Yet, it is precisely the communication and release of this score to a user that an adversary can leverage to gauge the private health status of an individual. We pinpoint a realistic attack scenario and propose a contact tracing algorithm with differential privacy guarantees against this attack. The algorithm is tested on the two most widely used agent-based COVID19 simulators and demonstrates superior performance in a wide range of settings. Especially for realistic test scenarios and while releasing each risk score w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.11560</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#21457;&#29616;&#65306;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#33258;&#21457;&#29616;&#21463;&#21040;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#20551;&#35774;&#65292;&#21363;&#22312;&#35268;&#27169;&#25193;&#22823;&#30340;&#36807;&#31243;&#20013;&#39640;&#24230;&#20419;&#36827;&#24615;&#33021;&#30340;&#22240;&#32032;&#65306;&#20943;&#23569;&#21482;&#33021;&#19982;&#29305;&#23450;&#29305;&#24449;&#24418;&#25104;&#19968;&#23545;&#19968;&#20851;&#31995;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24448;&#24448;&#26356;&#31232;&#30095;&#65292;&#24182;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#24605;&#36335;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27809;&#26377;&#32479;&#19968;&#30340;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#31616;&#21333;&#22320;&#31105;&#27490;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24182;&#19981;&#33021;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#24847;&#24605;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#33258;&#21457;&#29616;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#24320;&#20102;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11560v2 Announce Type: replace-cross  Abstract: Recently, emergence has received widespread attention from the research community along with the success of large language models. Different from the literature, we hypothesize a key factor that highly promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we propose to learn from emergence and present a study on proactively inhibiting the monosemantic neurons in this paper. More specifically, we first propose a new
&lt;/p&gt;</description></item><item><title>GINN-LP&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21457;&#29616;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#24418;&#24335;&#21644;&#31995;&#25968;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24130;&#39033;&#36924;&#36817;&#22359;&#8221;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#31574;&#30053;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#26469;&#20248;&#21270;&#26041;&#31243;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.10913</link><description>&lt;p&gt;
GINN-LP&#65306;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10913
&lt;/p&gt;
&lt;p&gt;
GINN-LP&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21457;&#29616;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#24418;&#24335;&#21644;&#31995;&#25968;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24130;&#39033;&#36924;&#36817;&#22359;&#8221;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#31574;&#30053;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#26469;&#20248;&#21270;&#26041;&#31243;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#65292;&#19981;&#20250;&#20135;&#29983;&#23558;&#36755;&#20837;&#21644;&#36755;&#20986;&#36830;&#25509;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#24615;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#20989;&#25968;&#30340;&#33021;&#21147;&#26159;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GINN-LP&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#26041;&#31243;&#30340;&#24418;&#24335;&#21644;&#31995;&#25968;&#65292;&#24403;&#20551;&#35774;&#26041;&#31243;&#30340;&#24418;&#24335;&#26159;&#22810;&#20803;Laurent&#22810;&#39033;&#24335;&#26102;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#21517;&#20026;&#8220;&#24130;&#39033;&#36924;&#36817;&#22359;&#8221;&#65292;&#30001;&#23545;&#25968;&#21644;&#25351;&#25968;&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#26469;&#23454;&#29616;&#30340;&#12290;GINN-LP&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#65292;&#21487;&#20197;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#22686;&#38271;&#31574;&#30053;&#65292;&#33021;&#22815;&#25214;&#21040;&#20195;&#34920;&#25968;&#25454;&#30340;Laurent&#22810;&#39033;&#24335;&#20013;&#30340;&#21512;&#36866;&#39033;&#25968;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;&#26041;&#31243;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10913v2 Announce Type: replace-cross  Abstract: Traditional machine learning is generally treated as a black-box optimization problem and does not typically produce interpretable functions that connect inputs and outputs. However, the ability to discover such interpretable functions is desirable. In this work, we propose GINN-LP, an interpretable neural network to discover the form and coefficients of the underlying equation of a dataset, when the equation is assumed to take the form of a multivariate Laurent Polynomial. This is facilitated by a new type of interpretable neural network block, named the "power-term approximator block", consisting of logarithmic and exponential activation functions. GINN-LP is end-to-end differentiable, making it possible to use backpropagation for training. We propose a neural network growth strategy that will enable finding the suitable number of terms in the Laurent polynomial that represents the data, along with sparsity regularization to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.08724</link><description>&lt;p&gt;
&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Personalized Path Recourse for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#34917;&#25937;&#36335;&#24452;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#32534;&#36753;&#32473;&#23450;&#30340;&#21160;&#20316;&#36335;&#24452;&#20197;&#36798;&#21040;&#26399;&#26395;&#30340;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#27604;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65289;&#65292;&#21516;&#26102;&#30830;&#20445;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#39640;&#24230;&#30456;&#20284;&#24182;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#12290;&#20010;&#24615;&#21270;&#26159;&#25351;&#26032;&#36335;&#24452;&#22312;&#20174;&#31574;&#30053;&#20989;&#25968;&#20013;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#23450;&#21046;&#31243;&#24230;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#34917;&#25937;&#20195;&#29702;&#26469;&#29983;&#25104;&#36825;&#26679;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#65292;&#36825;&#20123;&#36335;&#24452;&#26159;&#20351;&#29992;&#32771;&#34385;&#30446;&#26631;&#12289;&#30456;&#20284;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#33719;&#24471;&#30340;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#20197;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#24207;&#21015;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#36798;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08724v2 Announce Type: replace-cross  Abstract: This paper introduces Personalized Path Recourse, a novel method that generates recourse paths for a reinforcement learning agent. The goal is to edit a given path of actions to achieve desired goals (e.g., better outcomes compared to the agent's original path) while ensuring a high similarity to the agent's original paths and being personalized to the agent. Personalization refers to the extent to which the new path is tailored to the agent's observed behavior patterns from their policy function. We train a personalized recourse agent to generate such personalized paths, which are obtained using reward functions that consider the goal, similarity, and personalization. The proposed method is applicable to both reinforcement learning and supervised learning settings for correcting or improving sequences of actions or sequences of data to achieve a pre-determined goal. The method is evaluated in various settings. Experiments show
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#27979;&#31639;&#27861;&#33539;&#24335;&#19979;&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#39044;&#22788;&#29702;&#26102;&#38388;&#21644;&#26597;&#35810;&#26102;&#38388;&#30340;&#22810;&#39033;&#24335;&#20851;&#31995;&#26469;&#22788;&#29702;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#12290;</title><link>https://arxiv.org/abs/2312.08489</link><description>&lt;p&gt;
&#39044;&#27979;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connectivity Oracles for Predictable Vertex Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08489
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#27979;&#31639;&#27861;&#33539;&#24335;&#19979;&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#39044;&#22788;&#29702;&#26102;&#38388;&#21644;&#26597;&#35810;&#26102;&#38388;&#30340;&#22810;&#39033;&#24335;&#20851;&#31995;&#26469;&#22788;&#29702;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#38382;&#39064;&#20043;&#19968;&#12290;&#24050;&#26377;&#30340;&#30740;&#31350;&#22312;&#26597;&#35810;&#26102;&#38388;&#26041;&#38754;&#24050;&#32463;&#26377;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65306;&#20197;&#21069;&#30340;&#20316;&#21697;[Duan-Pettie STOC'10; Long-Saranurak FOCS'22]&#23454;&#29616;&#20102;&#19982;&#22833;&#36133;&#39030;&#28857;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#30340;&#26597;&#35810;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#39044;&#22788;&#29702;&#21644;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#26356;&#26032;&#30340;&#26465;&#20214;&#19979;&#26159;&#26377;&#26465;&#20214;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#22312;&#39044;&#27979;&#31639;&#27861;&#30340;&#33539;&#24335;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#38382;&#65292;&#22914;&#26524;&#21487;&#20197;&#39044;&#27979;&#21040;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#65292;&#26597;&#35810;&#26102;&#38388;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#32467;&#26500;&#65292;&#32473;&#23450;&#19968;&#20010;&#22270;G=(V,E)&#21644;&#19968;&#20010;&#39044;&#27979;&#20250;&#22833;&#36133;&#30340;&#39030;&#28857;&#38598;&#21512;\widehat{D} \subseteq V&#65288;&#20854;&#20013;d=|\widehat{D}|&#65289;&#65292;&#23558;&#20854;&#39044;&#22788;&#29702;&#26102;&#38388;&#20026;$\tilde{O}(d|E|)$&#65292;&#28982;&#21518;&#21487;&#20197;&#25509;&#25910;&#19968;&#20010;&#26356;&#26032;&#65292;&#35813;&#26356;&#26032;&#20197;&#23545;&#31216;&#24046;&#20998;&#24418;&#24335;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08489v2 Announce Type: replace-cross  Abstract: The problem of designing connectivity oracles supporting vertex failures is one of the basic data structures problems for undirected graphs. It is already well understood: previous works [Duan--Pettie STOC'10; Long--Saranurak FOCS'22] achieve query time linear in the number of failed vertices, and it is conditionally optimal as long as we require preprocessing time polynomial in the size of the graph and update time polynomial in the number of failed vertices.   We revisit this problem in the paradigm of algorithms with predictions: we ask if the query time can be improved if the set of failed vertices can be predicted beforehand up to a small number of errors. More specifically, we design a data structure that, given a graph $G=(V,E)$ and a set of vertices predicted to fail $\widehat{D} \subseteq V$ of size $d=|\widehat{D}|$, preprocesses it in time $\tilde{O}(d|E|)$ and then can receive an update given as the symmetric differ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.00817</link><description>&lt;p&gt;
&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#29992;&#20110;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#65292;&#22914;BERT&#21644;GPT&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;PTMs&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#21457;&#23637;&#28382;&#21518;&#12290;&#36825;&#20984;&#26174;&#20102;&#29616;&#26377;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#25429;&#25417;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21363;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;TimelyGPT&#65289;&#12290;TimelyGPT&#37319;&#29992;&#21487;&#25512;&#24191;&#20301;&#32622;&#65288;xPos&#65289;&#23884;&#20837;&#23558;&#36235;&#21183;&#21644;&#21608;&#26399;&#27169;&#24335;&#32534;&#30721;&#21040;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20013;&#12290;&#23427;&#36824;&#38598;&#25104;&#20102;&#24490;&#29615;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TimelyGPT&#22312;&#24314;&#27169;&#36830;&#32493;&#30417;&#27979;&#30340;&#29983;&#29289;&#20449;&#21495;&#21644;&#32463;&#24120;&#20986;&#29616;&#22312;&#32437;&#21521;&#30005;&#30913;&#27874;&#39046;&#22495;&#20013;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36824;&#23637;&#31034;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.18237</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#30693;&#35782;&#36801;&#31227;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36824;&#23637;&#31034;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#35299;&#20915;&#22914;&#20309;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#36807;Task-Agnostic VFM&#33976;&#39311;&#12289;Web-Scale CLIP&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24335;ImageNet&#39044;&#35757;&#32451;&#21644;&#33258;&#30417;&#30563;DINO&#39044;&#35757;&#32451;29.8%&#12289;22.1%&#12289;13.7%&#21644;11.6%&#30340;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#23637;&#29616;&#20986;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18237v2 Announce Type: replace-cross  Abstract: Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, "How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.17978</link><description>&lt;p&gt;
AutArch&#65306;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
AutArch: An AI-assisted workflow for object detection and automated recording in archaeological catalogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#32972;&#26223;&#26159;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20174;&#24322;&#26500;&#30340;&#24050;&#21457;&#34920;&#36164;&#28304;&#20013;&#21019;&#24314;&#22823;&#35268;&#27169;&#32479;&#19968;&#30340;&#32771;&#21476;&#25968;&#25454;&#38598;&#65292;&#27604;&#22914;&#36951;&#29289;&#30446;&#24405;&#12290;&#35770;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#33268;&#32771;&#21476;&#25968;&#25454;&#32452;&#21512;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#29616;&#26377;&#35760;&#24405;&#22312;&#36136;&#37327;&#21644;&#35760;&#24405;&#26631;&#20934;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#26080;&#27861;&#31616;&#21333;&#22320;&#21512;&#24182;&#29616;&#26377;&#35760;&#24405;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#20174;&#24050;&#21457;&#34920;&#30340;&#32771;&#21476;&#25554;&#22270;&#20013;&#37325;&#26032;&#21019;&#24314;&#35760;&#24405;&#12290;&#21482;&#26377;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#36825;&#25165;&#26159;&#21487;&#34892;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#32771;&#21476;&#36951;&#29289;&#30446;&#24405;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#36825;&#20123;&#30446;&#24405;&#20316;&#20026;&#36951;&#30041;&#36164;&#28304;&#23384;&#22312;&#65292;&#27604;&#22914;&#22823;&#22411;&#26410;&#25490;&#24207;&#30340;PDF&#25991;&#20214;&#20013;&#30340;&#32771;&#21476;&#32472;&#22270;&#21644;&#29031;&#29255;&#65307;&#35813;&#24037;&#20316;&#27969;&#31243;&#20381;&#36182;&#20110;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;&#12289;&#29289;&#20307;&#26816;&#27979;&#20197;&#21450;&#39564;&#35777;&#21644;&#35843;&#25972;&#33258;&#21160;&#33719;&#21462;&#25968;&#25454;&#30340;&#20132;&#20114;&#25163;&#27573;&#30340;&#33258;&#23450;&#20041;&#36719;&#20214;&#65288;AutArch&#65289;&#12290;&#25105;&#20204;&#38598;&#25104;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17978v2 Announce Type: replace-cross  Abstract: The context of this paper is the creation of large uniform archaeological datasets from heterogeneous published resources, such as find catalogues - with the help of AI and Big Data. The paper is concerned with the challenge of consistent assemblages of archaeological data. We cannot simply combine existing records, as they differ in terms of quality and recording standards. Thus, records have to be recreated from published archaeological illustrations. This is only a viable path with the help of automation. The contribution of this paper is a new workflow for collecting data from archaeological find catalogues available as legacy resources, such as archaeological drawings and photographs in large unsorted PDF files; the workflow relies on custom software (AutArch) supporting image processing, object detection, and interactive means of validating and adjusting automatically retrieved data. We integrate artificial intelligence (
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;&#65288;ASI&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#20934;&#30830;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;ASI&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21487;&#35270;&#21270;ASI&#12289;&#24179;&#22343;&#20934;&#30830;&#24230;&#21644;&#21464;&#24322;&#31995;&#25968;&#30340;3D&#26354;&#38754;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23450;&#37327;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.15332</link><description>&lt;p&gt;
ASI:&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
ASI: Accuracy-Stability Index for Evaluating Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;&#65288;ASI&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#20934;&#30830;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;ASI&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21487;&#35270;&#21270;ASI&#12289;&#24179;&#22343;&#20934;&#30830;&#24230;&#21644;&#21464;&#24322;&#31995;&#25968;&#30340;3D&#26354;&#38754;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23450;&#37327;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#27169;&#22411;&#30340;&#19981;&#26029;&#24341;&#20837;&#20351;&#24471;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#35780;&#20272;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24378;&#35843;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#20934;&#30830;&#24615;-&#31283;&#23450;&#24615;&#25351;&#25968;&#65288;ASI&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#20934;&#30830;&#24230;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;ASI&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#21487;&#35270;&#21270;ASI&#12289;&#24179;&#22343;&#20934;&#30830;&#24230;&#21644;&#21464;&#24322;&#31995;&#25968;&#30340;3D&#26354;&#38754;&#27169;&#22411;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23450;&#37327;&#22522;&#20934;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#25991;&#31456;&#26368;&#21518;&#36824;&#23545;&#28508;&#22312;&#24369;&#28857;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15332v2 Announce Type: replace-cross  Abstract: In the context of deep learning research, where model introductions continually occur, the need for effective and efficient evaluation remains paramount. Existing methods often emphasize accuracy metrics, overlooking stability. To address this, the paper introduces the Accuracy-Stability Index (ASI), a quantitative measure incorporating both accuracy and stability for assessing deep learning models. Experimental results demonstrate the application of ASI, and a 3D surface model is presented for visualizing ASI, mean accuracy, and coefficient of variation. This paper addresses the important issue of quantitative benchmarking metrics for deep learning models, providing a new approach for accurately evaluating accuracy and stability of deep learning models. The paper concludes with discussions on potential weaknesses and outlines future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22522;&#20934;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#23454;&#35777;&#27604;&#36739;&#20102;&#31361;&#21464;&#39564;&#35777;&#65288;MV&#65289;&#21644;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;MV&#21644;CV&#22312;&#36873;&#25321;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#22522;&#26412;&#31561;&#25928;&#65292;&#20294;MV&#22312;&#36873;&#25321;&#31616;&#21333;&#27169;&#22411;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.14079</link><description>&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#21644;&#31361;&#21464;&#39564;&#35777;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#23454;&#35777;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Empirical Comparison between Cross-Validation and Mutation-Validation in Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22522;&#20934;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#23454;&#35777;&#27604;&#36739;&#20102;&#31361;&#21464;&#39564;&#35777;&#65288;MV&#65289;&#21644;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;MV&#21644;CV&#22312;&#36873;&#25321;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#22522;&#26412;&#31561;&#25928;&#65292;&#20294;MV&#22312;&#36873;&#25321;&#31616;&#21333;&#27169;&#22411;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31361;&#21464;&#39564;&#35777;&#65288;MV&#65289;&#26159;&#19968;&#31181;&#36817;&#26399;&#25552;&#20986;&#30340;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#22240;&#20854;&#29420;&#29305;&#29305;&#24615;&#21644;&#28508;&#22312;&#30410;&#22788;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#26041;&#27861;&#30456;&#27604;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;MV&#21644;k&#25240;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#36827;&#34892;&#20102;&#22522;&#20934;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#27979;&#35797;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20135;&#29983;&#19977;&#20010;&#21518;&#39564;&#27010;&#29575;&#30340;&#27867;&#21270;&#20272;&#35745;&#65306;&#23454;&#38469;&#31561;&#25928;&#24615;&#12289;CV&#20248;&#21183;&#21644;MV&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25152;&#36873;&#27169;&#22411;&#30340;&#33021;&#21147;&#24046;&#24322;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#22823;&#22810;&#25968;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;MV&#21644;CV&#37117;&#36873;&#25321;&#20855;&#26377;&#23454;&#38469;&#31561;&#25928;&#27867;&#21270;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;MV&#22312;&#36873;&#25321;&#36739;&#31616;&#21333;&#27169;&#22411;&#21644;&#36739;&#20302;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;MV&#36873;&#25321;&#36807;&#20110;&#31616;&#21333;&#30340;&#27169;&#22411;&#23548;&#33268;&#27424;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14079v2 Announce Type: replace  Abstract: Mutation validation (MV) is a recently proposed approach for model selection, garnering significant interest due to its unique characteristics and potential benefits compared to the widely used cross-validation (CV) method. In this study, we empirically compared MV and $k$-fold CV using benchmark and real-world datasets. By employing Bayesian tests, we compared generalization estimates yielding three posterior probabilities: practical equivalence, CV superiority, and MV superiority. We also evaluated the differences in the capacity of the selected models and computational efficiency. We found that both MV and CV select models with practically equivalent generalization performance across various machine learning algorithms and the majority of benchmark datasets. MV exhibited advantages in terms of selecting simpler models and lower computational costs. However, in some cases MV selected overly simplistic models leading to underfitting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20179;&#24211;&#25366;&#25496;&#21644;&#25991;&#26412;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#23545;Hugging Face&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;Hugging Face&#30340;&#25972;&#20307;&#22686;&#38271;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;ML&#39046;&#22495;&#12289;&#26694;&#26550;&#20351;&#29992;&#12289;&#20316;&#32773;&#20998;&#32452;&#31561;&#26041;&#38754;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#24320;&#21457;&#32773;&#31038;&#21306;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20027;&#39064;&#21644;&#35265;&#35299;&#20197;&#21450;&#27169;&#22411;&#30340;&#32500;&#25252;&#29366;&#24577;&#21644;&#28436;&#21270;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2311.13380</link><description>&lt;p&gt;
&#20998;&#26512;Hugging Face&#19978;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Evolution and Maintenance of ML Models on Hugging Face
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20179;&#24211;&#25366;&#25496;&#21644;&#25991;&#26412;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#23545;Hugging Face&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;Hugging Face&#30340;&#25972;&#20307;&#22686;&#38271;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;ML&#39046;&#22495;&#12289;&#26694;&#26550;&#20351;&#29992;&#12289;&#20316;&#32773;&#20998;&#32452;&#31561;&#26041;&#38754;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#24320;&#21457;&#32773;&#31038;&#21306;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20027;&#39064;&#21644;&#35265;&#35299;&#20197;&#21450;&#27169;&#22411;&#30340;&#32500;&#25252;&#29366;&#24577;&#21644;&#28436;&#21270;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hugging Face&#65288;HF&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24320;&#21457;&#21644;&#20998;&#20139;&#30340;&#37325;&#35201;&#24179;&#21488;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;HF Hub API&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#23545;&#36229;&#36807;380,000&#20010;&#27169;&#22411;&#36827;&#34892;&#20179;&#24211;&#25366;&#25496;&#65292;&#26088;&#22312;&#25506;&#32034;HF&#19978;&#25176;&#31649;&#30340;&#27169;&#22411;&#30340;&#31038;&#21306;&#21442;&#19982;&#12289;&#28436;&#21270;&#21644;&#32500;&#25252;&#31561;&#26041;&#38754;&#65292;&#36825;&#20123;&#26041;&#38754;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23578;&#26410;&#20840;&#38754;&#25506;&#35752;&#12290;&#25105;&#20204;&#39318;&#20808;&#23457;&#26597;&#20102;HF&#30340;&#25972;&#20307;&#22686;&#38271;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;ML&#39046;&#22495;&#12289;&#26694;&#26550;&#20351;&#29992;&#12289;&#20316;&#32773;&#20998;&#32452;&#20197;&#21450;&#26631;&#31614;&#21644;&#25968;&#25454;&#38598;&#30340;&#28436;&#21270;&#36235;&#21183;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#21345;&#29255;&#25551;&#36848;&#30340;&#25991;&#26412;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#35797;&#22270;&#30830;&#23450;&#24320;&#21457;&#32773;&#31038;&#21306;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20027;&#39064;&#21644;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#28085;&#30422;&#20102;&#27169;&#22411;&#32500;&#25252;&#26041;&#38754;&#65292;&#22312;&#36825;&#26041;&#38754;&#25105;&#20204;&#35780;&#20272;&#20102;ML&#27169;&#22411;&#30340;&#32500;&#25252;&#29366;&#24577;&#65292;&#23558;&#25552;&#20132;&#28040;&#24687;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65288;&#26657;&#27491;&#24615;&#12289;&#23436;&#21892;&#24615;&#21644;&#36866;&#24212;&#24615;&#65289;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#28436;&#21270;&#24773;&#20917;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13380v2 Announce Type: cross  Abstract: Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF, aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;AI&#20013;&#20171;&#24179;&#21488;&#38754;&#20020;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#19994;&#30028;&#30340;&#30456;&#20851;&#23454;&#36341;&#65292;&#21253;&#25324;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;</title><link>https://arxiv.org/abs/2311.12573</link><description>&lt;p&gt;
&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;: AI&#20013;&#20171;&#24179;&#21488;&#30340;&#24179;&#21488;&#27835;&#29702;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;AI&#20013;&#20171;&#24179;&#21488;&#38754;&#20020;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#19994;&#30028;&#30340;&#30456;&#20851;&#23454;&#36341;&#65292;&#21253;&#25324;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2311.12573v2 &#20844;&#21578;&#31867;&#22411;: replace-cross &#25688;&#35201;: AI&#24320;&#21457;&#31038;&#21306;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#25176;&#31649;&#20013;&#20171;&#24179;&#21488;&#65292;&#22914;Hugging Face&#65292;&#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#20415;&#25463;&#35775;&#38382;&#12290;&#36825;&#20123;&#27169;&#22411;&#24066;&#22330;&#38477;&#20302;&#20102;&#25104;&#21315;&#19978;&#19975;&#29992;&#25143;&#30340;&#25216;&#26415;&#37096;&#32626;&#38376;&#27099;&#65292;&#20294;&#20063;&#21487;&#33021;&#34987;&#29992;&#20110;&#35768;&#22810;&#28508;&#22312;&#26377;&#23475;&#21644;&#38750;&#27861;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#31995;&#32479;&#22914;&#20309;&#26082;&#33021;&#8220;&#21253;&#21547;&#8221;&#20869;&#23481;&#21448;&#33021;&#26159;&#24320;&#25918;&#24335;&#24037;&#20855;&#65292;&#20174;&#32780;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#26840;&#25163;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#20998;&#26512;&#27169;&#22411;&#24066;&#22330;&#22914;&#20309;&#31649;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#26696;&#20363;&#36328;&#36234;&#20102;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#24179;&#21488;&#65292;&#21363;Hugging Face&#12289;GitHub&#21644;Civitai&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19994;&#30028;&#27491;&#22312;&#21046;&#23450;&#30340;&#37325;&#35201;&#65288;&#20294;&#20173;&#28982;&#26377;&#38480;&#65289;&#24212;&#23545;&#35843;&#33410;&#38656;&#27714;&#30340;&#20570;&#27861;&#65306;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12573v2 Announce Type: replace-cross  Abstract: The AI development community is increasingly making use of hosting intermediaries such as Hugging Face provide easy access to user-uploaded models and training data. These model marketplaces lower technical deployment barriers for hundreds of thousands of users, yet can be used in numerous potentially harmful and illegal ways. In this article, we explain ways in which AI systems, which can both `contain' content and be open-ended tools, present one of the trickiest platform governance challenges seen to date. We provide case studies of several incidents across three illustrative platforms -- Hugging Face, GitHub and Civitai -- to examine how model marketplaces moderate models. Building on this analysis, we outline important (and yet nevertheless limited) practices that industry has been developing to respond to moderation demands: licensing, access and use restrictions, automated content moderation, and open policy development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#65288;RLP&#65289;&#25439;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#20851;&#31995;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;RLP&#25439;&#22833;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#22312;&#26356;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#28155;&#21152;&#22122;&#22768;&#34920;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.12356</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#24179;&#38754;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#65288;RLP&#65289;&#25439;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#20851;&#31995;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;RLP&#25439;&#22833;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#22312;&#26356;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#28155;&#21152;&#22122;&#22768;&#34920;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#32447;&#24615;&#25237;&#24433;&#65288;RLP&#65289;&#25439;&#22833;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#20851;&#31995;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#19982;&#20256;&#32479;&#30340;&#26088;&#22312;&#26368;&#23567;&#21270;&#36880;&#28857;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#21516;&#65292;RLP&#25439;&#22833;&#36890;&#36807;&#26368;&#23567;&#21270;&#36830;&#25509;&#22266;&#23450;&#22823;&#23567;&#30340;&#29305;&#24449;-&#39044;&#27979;&#23545;&#21644;&#29305;&#24449;-&#26631;&#31614;&#23545;&#30340;&#36229;&#24179;&#38754;&#38598;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#25805;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#31034;&#20363;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;RLP&#25439;&#22833;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#20110;&#20351;&#29992;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#25968;&#25454;&#26679;&#26412;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#28155;&#21152;&#22122;&#22768;&#34920;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#23454;&#35777;&#32467;&#26524;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12356v2 Announce Type: replace  Abstract: Advancing loss function design is pivotal for optimizing neural network training and performance. This work introduces Random Linear Projections (RLP) loss, a novel approach that enhances training efficiency by leveraging geometric relationships within the data. Distinct from traditional loss functions that target minimizing pointwise errors, RLP loss operates by minimizing the distance between sets of hyperplanes connecting fixed-size subsets of feature-prediction pairs and feature-label pairs. Our empirical evaluations, conducted across benchmark datasets and synthetic examples, demonstrate that neural networks trained with RLP loss outperform those trained with traditional loss functions, achieving improved performance with fewer data samples, and exhibiting greater robustness to additive noise. We provide theoretical analysis supporting our empirical findings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dual Input Stream Transformer&#65288;DIST&#65289;&#30340;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#30524;&#21160;&#38405;&#35835;&#25968;&#25454;&#20013;&#30001;&#20110;&#22402;&#30452;&#28418;&#31227;&#32780;&#20135;&#29983;&#30340;&#27880;&#35270;&#28857;&#20998;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#19982;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DIST&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#22810;&#20010;DIST&#27169;&#22411;&#23454;&#20363;&#32452;&#21512;&#25104;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#38405;&#35835;&#30740;&#31350;&#20013;&#25163;&#21160;&#20998;&#37197;&#25991;&#26412;&#34892;&#30340;&#29942;&#39048;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.06095</link><description>&lt;p&gt;
&#29992;&#20110;&#32416;&#27491;&#30524;&#21160;&#38405;&#35835;&#25968;&#25454;&#20013;&#30340;&#22402;&#30452;&#28418;&#31227;&#30340;&#21452;&#36755;&#20837;&#27969;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual input stream transformer for vertical drift correction in eye-tracking reading data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06095
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dual Input Stream Transformer&#65288;DIST&#65289;&#30340;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#30524;&#21160;&#38405;&#35835;&#25968;&#25454;&#20013;&#30001;&#20110;&#22402;&#30452;&#28418;&#31227;&#32780;&#20135;&#29983;&#30340;&#27880;&#35270;&#28857;&#20998;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#19982;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DIST&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#22810;&#20010;DIST&#27169;&#22411;&#23454;&#20363;&#32452;&#21512;&#25104;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#38405;&#35835;&#30740;&#31350;&#20013;&#25163;&#21160;&#20998;&#37197;&#25991;&#26412;&#34892;&#30340;&#29942;&#39048;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#36755;&#20837;&#27969;&#36716;&#25442;&#22120;&#65288;DIST&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30524;&#21160;&#25968;&#25454;&#20013;&#23558;&#27880;&#35270;&#28857;&#20998;&#37197;&#21040;&#23454;&#38469;&#25152;&#20851;&#27880;&#30340;&#25991;&#26412;&#34892;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#36825;&#19968;&#21518;&#22788;&#29702;&#27493;&#39588;&#23545;&#20110;&#38405;&#35835;&#25968;&#25454;&#30340;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#22402;&#30452;&#28418;&#31227;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#20061;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;DIST&#19982;&#21313;&#19968;&#31181;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#22810;&#20010;DIST&#27169;&#22411;&#23454;&#20363;&#32452;&#21512;&#25104;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#23558;DIST&#38598;&#25104;&#27169;&#22411;&#19982;&#26368;&#20339;&#30340;&#32463;&#20856;&#26041;&#27861;&#36827;&#19968;&#27493;&#32452;&#21512;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;98.17&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#38405;&#35835;&#30740;&#31350;&#20013;&#25163;&#21160;&#20998;&#37197;&#25991;&#26412;&#34892;&#30340;&#29942;&#39048;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20419;&#25104;DIST&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06095v2 Announce Type: replace-cross  Abstract: We introduce a novel Dual Input Stream Transformer (DIST) for the challenging problem of assigning fixation points from eye-tracking data collected during passage reading to the line of text that the reader was actually focused on. This post-processing step is crucial for analysis of the reading data due to the presence of noise in the form of vertical drift. We evaluate DIST against eleven classical approaches on a comprehensive suite of nine diverse datasets. We demonstrate that combining multiple instances of the DIST model in an ensemble achieves high accuracy across all datasets. Further combining the DIST ensemble with the best classical approach yields an average accuracy of 98.17 %. Our approach presents a significant step towards addressing the bottleneck of manual line assignment in reading research. Through extensive analysis and ablation studies, we identify key factors that contribute to DIST's success, including t
&lt;/p&gt;</description></item><item><title>ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;</title><link>https://arxiv.org/abs/2310.20431</link><description>&lt;p&gt;
&#25552;&#21319;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#30340;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Raising the ClaSS of Streaming Time Series Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20431
&lt;/p&gt;
&lt;p&gt;
ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#65292;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#21457;&#23556;&#39640;&#39057;&#25968;&#20540;&#27979;&#37327;&#27969;&#65292;&#21453;&#26144;&#20102;&#20154;&#31867;&#12289;&#21160;&#29289;&#12289;&#24037;&#19994;&#12289;&#21830;&#19994;&#21644;&#33258;&#28982;&#36807;&#31243;&#30340;&#29305;&#24615;&#12290;&#36825;&#20123;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#20363;&#22914;&#30001;&#22806;&#37096;&#20107;&#20214;&#25110;&#20869;&#37096;&#29366;&#24577;&#21464;&#21270;&#24341;&#36215;&#30340;&#65292;&#20250;&#34920;&#29616;&#20026;&#35760;&#24405;&#20449;&#21495;&#20013;&#30340;&#21464;&#21270;&#12290;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#65288;STSS&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#27969;&#20998;&#21106;&#20026;&#23545;&#24212;&#20110;&#25152;&#35266;&#23519;&#30340;&#36807;&#31243;&#25110;&#23454;&#20307;&#29366;&#24577;&#30340;&#36830;&#32493;&#21487;&#21464;&#22823;&#23567;&#30340;&#20998;&#27573;&#12290;&#20998;&#21106;&#25805;&#20316;&#26412;&#36523;&#24517;&#39035;&#33021;&#22815;&#24212;&#23545;&#36755;&#20837;&#20449;&#21495;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ClaSS&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;STSS&#31639;&#27861;&#12290;ClaSS&#20351;&#29992;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#28508;&#22312;&#20998;&#21106;&#30340;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#65288;CPs&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#35780;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22823;&#22411;&#22522;&#20934;&#21644;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26723;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20431v2 Announce Type: replace-cross  Abstract: Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#29616;&#35937;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26080;&#35770;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21463;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#20004;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#27169;&#24335;&#65306;&#35760;&#24518;&#21270;&#27169;&#24335;&#21644;&#27867;&#21270;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2310.05264</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Emergence of Reproducibility and Consistency in Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#29616;&#35937;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26080;&#35770;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21463;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#20004;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#27169;&#24335;&#65306;&#35760;&#24518;&#21270;&#27169;&#24335;&#21644;&#27867;&#21270;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#19988;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#19968;&#33268;&#30340;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#8221;&#65306;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#36215;&#22987;&#22122;&#22768;&#36755;&#20837;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#20135;&#29983;&#38750;&#24120;&#30456;&#20284;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#34920;&#26126;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#26080;&#35770;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#19978;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#21463;&#35757;&#25968;&#25454;&#35268;&#27169;&#24433;&#21709;&#19979;&#30340;&#19981;&#21516;&#20998;&#24067;&#12290;&#36825;&#19968;&#28857;&#24471;&#21040;&#20102;&#20004;&#31181;&#19981;&#21516;&#35757;&#32451;&#27169;&#24335;&#19979;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#30340;&#20307;&#29616;&#65306;&#65288;i&#65289;&#8220;&#35760;&#24518;&#21270;&#27169;&#24335;&#8221;&#65292;&#20854;&#20013;&#25193;&#25955;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#21644;&#65288;ii&#65289;&#8220;&#27867;&#21270;&#27169;&#24335;&#8221;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05264v2 Announce Type: replace  Abstract: In this work, we investigate an intriguing and prevalent phenomenon of diffusion models which we term as "consistent model reproducibility": given the same starting noise input and a deterministic sampler, different diffusion models often yield remarkably similar outputs. We confirm this phenomenon through comprehensive experiments, implying that different diffusion models consistently reach the same data distribution and scoring function regardless of diffusion model frameworks, model architectures, or training procedures. More strikingly, our further investigation implies that diffusion models are learning distinct distributions affected by the training data size. This is supported by the fact that the model reproducibility manifests in two distinct training regimes: (i) "memorization regime", where the diffusion model overfits to the training data distribution, and (ii) "generalization regime", where the model learns the underlyin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#31574;&#30053;&#21512;&#24182;&#35299;&#20915;&#26426;&#22120;&#20154;&#32676;&#20307;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;Meta-World&#29615;&#22659;&#20013;&#23558;50&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#34892;&#20026;&#25972;&#21512;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2310.01362</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#21512;&#24182;&#23454;&#29616;&#33328;&#38431;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fleet Learning via Policy Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#31574;&#30053;&#21512;&#24182;&#35299;&#20915;&#26426;&#22120;&#20154;&#32676;&#20307;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;Meta-World&#29615;&#22659;&#20013;&#23558;50&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#34892;&#20026;&#25972;&#21512;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#32676;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#20135;&#29983;&#30340;&#22823;&#37327;&#24322;&#26500;&#27969;&#25968;&#25454;&#23384;&#20648;&#25110;&#20256;&#36755;&#19978;&#30340;&#22256;&#38590;&#65292;&#26426;&#22120;&#20154;&#22242;&#38431;&#38656;&#35201;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#24322;&#26500;&#32463;&#39564;&#26469;&#20849;&#21516;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#25216;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36825;&#31181;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#31574;&#30053;&#21512;&#24182;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22312;&#33328;&#38431;&#29615;&#22659;&#20013;&#39640;&#25928;&#21512;&#24182;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLEET-MERGE&#65292;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#25511;&#21046;&#31574;&#30053;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#23454;&#20363;&#65292;&#32771;&#34385;&#20102;&#21442;&#25968;&#21270;&#25511;&#21046;&#31574;&#30053;&#20013;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;FLEET-MERGE&#22312;Meta-World&#29615;&#22659;&#20013;&#23545;50&#20010;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#31574;&#30053;&#34892;&#20026;&#36827;&#34892;&#20102;&#25972;&#21512;&#65292;&#24182;&#19988;&#20960;&#20046;&#22312;&#25152;&#26377;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01362v2 Announce Type: replace-cross  Abstract: Fleets of robots ingest massive amounts of heterogeneous streaming data silos generated by interacting with their environments, far more than what can be stored or transmitted with ease. At the same time, teams of robots should co-acquire diverse skills through their heterogeneous experiences in varied settings. How can we enable such fleet-level learning without having to transmit or centralize fleet-scale data? In this paper, we investigate policy merging (PoMe) from such distributed heterogeneous datasets as a potential solution. To efficiently merge policies in the fleet setting, we propose FLEET-MERGE, an instantiation of distributed learning that accounts for the permutation invariance that arises when parameterizing the control policies with recurrent neural networks. We show that FLEET-MERGE consolidates the behavior of policies trained on 50 tasks in the Meta-World environment, with good performance on nearly all train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;MDP&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#22686;&#24378;&#29615;&#22659;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#23618;&#25945;&#24072;&#26234;&#33021;&#20307;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#33021;&#21147;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2310.00301</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#36712;&#36857;&#27169;&#22411;&#22686;&#24378;&#23618;&#27425;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Hierarchical Environment Design via Generative Trajectory Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;MDP&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#22686;&#24378;&#29615;&#22659;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#23618;&#25945;&#24072;&#26234;&#33021;&#20307;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#20197;&#20419;&#36827;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#33021;&#21147;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26159;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#29615;&#22659;&#35838;&#31243;&#30340;&#33539;&#20363;&#65292;&#20351;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#23637;&#36890;&#29992;&#33021;&#21147;&#65292;&#21363;&#23454;&#29616;&#33391;&#22909;&#30340;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23545;&#24320;&#25918;&#24335;&#26234;&#33021;&#20307;&#35757;&#32451;&#30340;&#29615;&#22659;&#36827;&#34892;&#38543;&#26426;&#29983;&#25104;&#65292;&#36825;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23545;&#29983;&#25104;&#29615;&#22659;&#25968;&#37327;&#30340;&#38480;&#21046;&#26041;&#38754;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#27425;MDP&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#36827;&#34892;&#29615;&#22659;&#35774;&#35745;&#12290;&#23427;&#30001;&#19968;&#20010;&#19978;&#23618;&#24378;&#21270;&#23398;&#20064;&#25945;&#24072;&#26234;&#33021;&#20307;&#21644;&#19968;&#20010;&#19979;&#23618;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#21512;&#20316;&#32452;&#25104;&#12290;&#24378;&#21270;&#23398;&#20064;&#25945;&#24072;&#21487;&#20197;&#21033;&#29992;&#20808;&#21069;&#21457;&#29616;&#30340;&#29615;&#22659;&#32467;&#26500;&#65292;&#36890;&#36807;&#35266;&#23519;&#23398;&#29983;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#34920;&#31034;&#22312;&#23398;&#29983;&#33021;&#21147;&#30340;&#21069;&#27839;&#29983;&#25104;&#36866;&#24403;&#30340;&#35757;&#32451;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00301v2 Announce Type: replace-cross  Abstract: Unsupervised Environment Design (UED) is a paradigm for automatically generating a curriculum of training environments, enabling agents trained in these environments to develop general capabilities, i.e., achieving good zero-shot transfer performance. However, existing UED approaches focus primarily on the random generation of environments for open-ended agent training. This is impractical in scenarios with limited resources, such as the constraints on the number of generated environments. In this paper, we introduce a hierarchical MDP framework for environment design under resource constraints. It consists of an upper-level RL teacher agent that generates suitable training environments for a lower-level student agent. The RL teacher can leverage previously discovered environment structures and generate environments at the frontier of the student's capabilities by observing the student policy's representation. Moreover, to redu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#19981;&#25935;&#24863;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#26356;&#26032;&#27493;&#39588;&#30340;&#30701;&#23567;&#36845;&#20195;&#36807;&#31243;&#65292;&#20943;&#36731;&#20102;&#31163;&#32676;&#20540;&#23545;&#28388;&#27874;&#24615;&#33021;&#30340;&#26377;&#23475;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#28508;&#22312;&#30340;&#31163;&#32676;&#20540;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#30340;&#27491;&#24577;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#28388;&#27874;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#19988;&#23545;&#31163;&#32676;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.09505</link><description>&lt;p&gt;
&#24322;&#24120;&#19981;&#25935;&#24863;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#65306;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Outlier-Insensitive Kalman Filtering: Theory and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#19981;&#25935;&#24863;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#26356;&#26032;&#27493;&#39588;&#30340;&#30701;&#23567;&#36845;&#20195;&#36807;&#31243;&#65292;&#20943;&#36731;&#20102;&#31163;&#32676;&#20540;&#23545;&#28388;&#27874;&#24615;&#33021;&#30340;&#26377;&#23475;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#28508;&#22312;&#30340;&#31163;&#32676;&#20540;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#30340;&#27491;&#24577;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#28388;&#27874;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#19988;&#23545;&#31163;&#32676;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20174;&#22122;&#22768;&#35266;&#27979;&#20013;&#36827;&#34892;&#29366;&#24577;&#20272;&#35745;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#36890;&#24120;&#20351;&#29992;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;KF&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#20984;&#20108;&#27425;&#30446;&#26631;&#20989;&#25968;&#30340;&#25935;&#24863;&#24615;&#65292;&#24403;&#35266;&#27979;&#20013;&#23384;&#22312;&#31163;&#32676;&#20540;&#26102;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#21487;&#20197;&#24212;&#29992;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#23545;KF&#30340;&#26631;&#20934;&#26356;&#26032;&#27493;&#39588;&#36827;&#34892;&#30701;&#23567;&#30340;&#36845;&#20195;&#36807;&#31243;&#26102;&#65292;&#20943;&#36731;&#31163;&#32676;&#20540;&#30340;&#26377;&#23475;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#28508;&#22312;&#30340;&#31163;&#32676;&#20540;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#30340;&#27491;&#24577;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#25110;&#20132;&#26367;&#26368;&#22823;&#21270;&#31639;&#27861;&#36827;&#34892;&#22312;&#32447;&#20272;&#35745;&#12290;&#20223;&#30495;&#21644;&#23454;&#22320;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28388;&#27874;&#22330;&#26223;&#20013;&#23545;&#31163;&#32676;&#20540;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09505v2 Announce Type: replace-cross  Abstract: State estimation of dynamical systems from noisy observations is a fundamental task in many applications. It is commonly addressed using the linear Kalman filter (KF), whose performance can significantly degrade in the presence of outliers in the observations, due to the sensitivity of its convex quadratic objective function. To mitigate such behavior, outlier detection algorithms can be applied. In this work, we propose a parameter-free algorithm which mitigates the harmful effect of outliers while requiring only a short iterative process of the standard update step of the KF. To that end, we model each potential outlier as a normal process with unknown variance and apply online estimation through either expectation maximization or alternating maximization algorithms. Simulations and field experiment evaluations demonstrate competitive performance of our method, showcasing its robustness to outliers in filtering scenarios comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21608;&#26399;&#24615;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#21608;&#26399;&#25968;K&#25104;&#27604;&#29575;&#26368;&#20248;&#30340;&#36951;&#25022;&#25910;&#25947;&#29575;O(&#8730;K)&#12290;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#24102;&#26377;&#20048;&#35266;&#21453;&#39304;&#30340;&#38543;&#26426;&#35774;&#32622;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#20063;&#26159;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#23545;&#25239;&#35774;&#32622;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#30446;&#21069;&#23578;&#26410;&#25214;&#21040;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2308.14642</link><description>&lt;p&gt;
&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36895;&#29575;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rate-Optimal Policy Optimization for Linear Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21608;&#26399;&#24615;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#21608;&#26399;&#25968;K&#25104;&#27604;&#29575;&#26368;&#20248;&#30340;&#36951;&#25022;&#25910;&#25947;&#29575;O(&#8730;K)&#12290;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#24102;&#26377;&#20048;&#35266;&#21453;&#39304;&#30340;&#38543;&#26426;&#35774;&#32622;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#20063;&#26159;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#23545;&#25239;&#35774;&#32622;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#30446;&#21069;&#23578;&#26410;&#25214;&#21040;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#32447;&#21608;&#26399;&#24615;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#19982;$K$&#65288;&#34920;&#31034;&#21608;&#26399;&#25968;&#65289;&#25104;&#27604;&#29575;&#26368;&#20248;&#30340;$\widetilde{O}(\sqrt{K})$&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#22312;&#24102;&#26377;&#20048;&#35266;&#21453;&#39304;&#30340;&#38543;&#26426;&#35774;&#32622;&#20013;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19982;$K$&#26368;&#20248;&#65288;&#30456;&#23545;&#20110;$K$&#65289;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#20063;&#26159;&#39318;&#27425;&#24314;&#31435;&#22312;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#25932;&#23545;&#35774;&#32622;&#20013;&#19982;$K$&#26368;&#20248;&#30340;&#36895;&#29575;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30446;&#21069;&#27809;&#26377;&#24050;&#30693;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14642v2 Announce Type: replace  Abstract: We study regret minimization in online episodic linear Markov Decision Processes, and obtain rate-optimal $\widetilde O (\sqrt K)$ regret where $K$ denotes the number of episodes. Our work is the first to establish the optimal (w.r.t.~$K$) rate of convergence in the stochastic setting with bandit feedback using a policy optimization based approach, and the first to establish the optimal (w.r.t.~$K$) rate in the adversarial setup with full information feedback, for which no algorithm with an optimal rate guarantee is currently known.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#38544;&#24335;&#22270;&#31070;&#32463;&#25193;&#25955;&#32593;&#32476;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#35299;&#20915;&#20102;&#20854;&#25910;&#25947;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#20801;&#35768;&#23398;&#20064;&#24230;&#37327;&#21644;&#22270;&#25193;&#25955;&#24378;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26469;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.03306</link><description>&lt;p&gt;
&#38544;&#24335;&#22270;&#31070;&#32463;&#25193;&#25955;&#32593;&#32476;&#65306;&#25910;&#25947;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Implicit Graph Neural Diffusion Networks: Convergence, Generalization, and Over-Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.03306
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#38544;&#24335;&#22270;&#31070;&#32463;&#25193;&#25955;&#32593;&#32476;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#35299;&#20915;&#20102;&#20854;&#25910;&#25947;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#20801;&#35768;&#23398;&#20064;&#24230;&#37327;&#21644;&#22270;&#25193;&#25955;&#24378;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26469;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#22270;&#23398;&#20064;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19981;&#33391;&#30340;&#38544;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#21487;&#33021;&#23545;&#23398;&#20064;&#22270;&#24230;&#37327;&#20855;&#26377;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#65292;&#32463;&#39564;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#25110;&#32773;&#34920;&#29616;&#20986;&#27425;&#20248;&#25910;&#25947;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#21487;&#33021;&#38459;&#30861;&#23427;&#20204;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21442;&#25968;&#21270;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#38544;&#24335;&#22270;&#25193;&#25955;&#23618;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#39030;&#28857;&#21644;&#36793;&#32536;&#31354;&#38388;&#30340;&#24230;&#37327;&#65292;&#20197;&#21450;&#22270;&#25193;&#25955;&#24378;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38544;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#36845;&#20195;&#35299;&#26512;&#33021;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23450;&#28857;&#26041;&#31243;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#36973;&#21463;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38544;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.03306v2 Announce Type: replace  Abstract: Implicit Graph Neural Networks (GNNs) have achieved significant success in addressing graph learning problems recently. However, poorly designed implicit GNN layers may have limited adaptability to learn graph metrics, experience over-smoothing issues, or exhibit suboptimal convergence and generalization properties, potentially hindering their practical performance. To tackle these issues, we introduce a geometric framework for designing implicit graph diffusion layers based on a parameterized graph Laplacian operator. Our framework allows learning the metrics of vertex and edge spaces, as well as the graph diffusion strength from data. We show how implicit GNN layers can be viewed as the fixed-point equation of a Dirichlet energy minimization problem and give conditions under which it may suffer from over-smoothing during training (OST) and inference (OSI). We further propose a new implicit GNN model to avoid OST and OSI. We establi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FWin&#30340;&#24555;&#36895;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#21152;&#36895;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#19982;Softmax&#20840;&#27880;&#24847;&#21147;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2307.00493</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#28151;&#21512;&#31383;&#21475;&#27880;&#24847;&#21147;&#65306;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FWin&#30340;&#24555;&#36895;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#21152;&#36895;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#19982;Softmax&#20840;&#27880;&#24847;&#21147;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;Informer&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#31383;&#21475;&#27880;&#24847;&#21147;&#26159;&#23616;&#37096;&#30340;&#21644;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#20294;&#23427;&#32570;&#20047;&#25429;&#33719;&#20840;&#23616;&#20196;&#29260;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36825;&#36890;&#36807;&#21518;&#32493;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#22359;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;FWin&#65292;&#19981;&#20381;&#36182;&#20110;Informer&#30340;ProbSparse&#27880;&#24847;&#21147;&#20013;&#30340;&#26597;&#35810;&#31232;&#30095;&#24615;&#20551;&#35774;&#21644;&#32463;&#39564;&#24615;&#36817;&#20284;&#12290;&#36890;&#36807;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FWin transformers&#21487;&#20197;&#25552;&#39640;Informer&#30340;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#23558;&#20854;&#25512;&#26029;&#36895;&#24230;&#21152;&#36895;40%&#33267;50%&#12290;&#25105;&#20204;&#36824;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;FWin&#31867;&#22411;&#27880;&#24847;&#21147;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36890;&#36807;&#20174;Informer&#27169;&#22411;&#30340;&#20840;&#27880;&#24847;&#21147;&#23618;&#20013;&#25552;&#21462;&#30340;&#20851;&#38190;&#21521;&#37327;&#26469;&#36924;&#36817;&#29978;&#33267;&#32988;&#36807;&#22522;&#20110;Softmax&#20840;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00493v2 Announce Type: replace-cross  Abstract: We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention is local and a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 40 to 50 %. We also show in a nonlinear regression model that a learned FWin type attention approaches or even outperforms softmax full attention based on key vectors extracted from an Informer model's full attention layer acting on time series data.
&lt;/p&gt;</description></item><item><title>Hyp-OW&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;</title><link>https://arxiv.org/abs/2306.14291</link><description>&lt;p&gt;
Hyp-OW: &#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14291
&lt;/p&gt;
&lt;p&gt;
Hyp-OW&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;(OWOD)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#29616;&#23454;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#12290;&#23427;&#38656;&#35201;&#22312;&#26816;&#27979;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#30340;&#21516;&#26102;&#65292;&#25972;&#21512;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#29992;&#20110;&#26410;&#26469;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#8220;&#26410;&#30693;&#24615;&#8221;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#26641;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32972;&#26223;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#24212;&#35813;&#24050;&#32463;&#23884;&#20837;&#21040;&#24050;&#30693;&#31867;&#21035;&#20013;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#24050;&#30693;&#21644;&#26410;&#30693;&#39033;&#20043;&#38388;&#24212;&#35813;&#23384;&#22312;&#35821;&#20041;&#25110;&#28508;&#22312;&#30340;&#32467;&#26500;&#20851;&#31995;&#31561;&#24453;&#21457;&#29616;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hyp-OW&#65292;&#19968;&#31181;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#26469;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open World Object Detection (OWOD) is a challenging and realistic task that extends beyond the scope of standard Object Detection task. It involves detecting both known and unknown objects while integrating learned knowledge for future tasks. However, the level of "unknownness" varies significantly depending on the context. For example, a tree is typically considered part of the background in a self-driving scene, but it may be significant in a household context. We argue that this contextual information should already be embedded within the known classes. In other words, there should be a semantic or latent structure relationship between the known and unknown items to be discovered. Motivated by this observation, we propose Hyp-OW, a method that learns and models hierarchical representation of known items through a SuperClass Regularizer. Leveraging this representation allows us to effectively detect unknown objects using a similarity distance-based relabeling module. Extensive experi
&lt;/p&gt;</description></item><item><title>DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.17000</link><description>&lt;p&gt;
DistriBlock: &#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.17000
&lt;/p&gt;
&lt;p&gt;
DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#35823;&#23548;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20351;&#20854;&#39044;&#27979;&#20219;&#24847;&#30446;&#26631;&#25991;&#26412;&#65292;&#20174;&#32780;&#26500;&#25104;&#26126;&#26174;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistriBlock&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;ASR&#31995;&#32479;&#30340;&#39640;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#39044;&#27979;&#36755;&#20986;&#26631;&#35760;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#35813;&#20998;&#24067;&#30340;&#19968;&#32452;&#29305;&#24449;&#36827;&#34892;&#27979;&#37327;&#65306;&#36755;&#20986;&#27010;&#29575;&#30340;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#65292;&#20998;&#24067;&#30340;&#29109;&#65292;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#21644;Jensen-Shannon&#25955;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#20998;&#31867;&#12289;&#36825;&#31181;&#20998;&#31867;&#22120;&#30340;&#38598;&#21512;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.17000v2 Announce Type: replace-cross  Abstract: Adversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat. To prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step. We measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. Then, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. Through extensive analysis across different state-of-the-art ASR systems and language data sets, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36793;&#32536;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;&#23618;&#27425;&#25512;&#26029;&#38382;&#39064;&#65292;&#20197;&#22312;&#20445;&#35777;&#25512;&#26029;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20302;&#24310;&#36831;&#12289;&#24102;&#23485;&#33410;&#30465;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2304.00891</link><description>&lt;p&gt;
&#36793;&#32536;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#23618;&#27425;&#25512;&#26029;&#22312;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Algorithms for Hierarchical Inference in Deep Learning applications at the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.00891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36793;&#32536;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;&#23618;&#27425;&#25512;&#26029;&#38382;&#39064;&#65292;&#20197;&#22312;&#20445;&#35777;&#25512;&#26029;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20302;&#24310;&#36831;&#12289;&#24102;&#23485;&#33410;&#30465;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#65288;&#22914;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25110;&#24494;&#25511;&#21046;&#21333;&#20803;&#65289;&#65292;&#20854;&#20013;&#23884;&#20837;&#26377;&#19968;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;S-ML&#65289;&#29992;&#20110;&#36890;&#29992;&#20998;&#31867;&#24212;&#29992;&#65292;&#20197;&#21450;&#19968;&#20010;&#25176;&#31649;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;L-ML&#65289;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#65288;ES&#65289;&#12290;&#30001;&#20110;S-ML&#30340;&#25512;&#26029;&#20934;&#30830;&#24615;&#20302;&#20110;L-ML&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#37117;&#34987;&#21457;&#36865;&#21040;ES&#36827;&#34892;&#25512;&#26029;&#20250;&#23548;&#33268;&#36739;&#39640;&#30340;&#25512;&#26029;&#20934;&#30830;&#24615;&#65292;&#20294;&#36825;&#36829;&#32972;&#20102;&#22312;ED&#19978;&#23884;&#20837;S-ML&#30340;&#30446;&#30340;&#65292;&#21093;&#22842;&#20102;&#36827;&#34892;&#26412;&#22320;&#25512;&#26029;&#30340;&#20302;&#24310;&#36831;&#12289;&#24102;&#23485;&#33410;&#30465;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;ED&#25512;&#26029;&#21644;ES&#25512;&#26029;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23618;&#27425;&#25512;&#26029;&#65288;HI&#65289;&#30340;&#24605;&#24819;&#65292;&#21363;&#21482;&#26377;&#22312;S-ML&#25512;&#26029;&#27491;&#30830;&#26102;&#25165;&#25509;&#21463;&#65292;&#21542;&#21017;&#23558;&#25968;&#25454;&#26679;&#26412;&#21457;&#36865;&#21040;L-ML&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#29702;&#24819;&#30340;HI&#23454;&#29616;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#27491;&#30830;&#24615;&#30340;&#21028;&#26029;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.00891v2 Announce Type: replace  Abstract: We consider a resource-constrained Edge Device (ED), such as an IoT sensor or a microcontroller unit, embedded with a small-size ML model (S-ML) for a generic classification application and an Edge Server (ES) that hosts a large-size ML model (L-ML). Since the inference accuracy of S-ML is lower than that of the L-ML, offloading all the data samples to the ES results in high inference accuracy, but it defeats the purpose of embedding S-ML on the ED and deprives the benefits of reduced latency, bandwidth savings, and energy efficiency of doing local inference. In order to get the best out of both worlds, i.e., the benefits of doing inference on the ED and the benefits of doing inference on ES, we explore the idea of Hierarchical Inference (HI), wherein S-ML inference is only accepted when it is correct, otherwise the data sample is offloaded for L-ML inference. However, the ideal implementation of HI is infeasible as the correctness o
&lt;/p&gt;</description></item><item><title>&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;</title><link>https://arxiv.org/abs/2303.03751</link><description>&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#36935;&#21040;&#20154;&#24037;&#21453;&#39304;&#65306;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#23454;&#29616;&#21487;&#35777;&#26126;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.03751
&lt;/p&gt;
&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#30340;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;-&#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#65292;&#29305;&#21035;&#26159;&#24403;&#20989;&#25968;&#30001;&#20154;&#31867;&#35780;&#21028;&#21592;&#35780;&#20272;&#26102;&#12290;&#36825;&#31181;&#25361;&#25112;&#21463;&#21040;&#20102;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#24037;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#29992;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;ZO-RankSGD&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.03751v2 Announce Type: replace-cross  Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;cGAN&#30340;TheraGAN&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#24247;&#22797;&#27963;&#21160;&#30456;&#20851;&#30340;&#39640;&#32500;IMU&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#27963;&#21160;&#65292;&#31616;&#21270;&#20102;&#29983;&#25104;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#27963;&#21160;&#35782;&#21035;&#20998;&#31867;&#22120;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2302.07998</link><description>&lt;p&gt;
&#22522;&#20110;cGAN&#30340;&#22686;&#24378;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#39640;&#32500;IMU&#20256;&#24863;&#22120;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
cGAN-Based High Dimensional IMU Sensor Data Generation for Enhanced Human Activity Recognition in Therapeutic Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.07998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;cGAN&#30340;TheraGAN&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#24247;&#22797;&#27963;&#21160;&#30456;&#20851;&#30340;&#39640;&#32500;IMU&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#27963;&#21160;&#65292;&#31616;&#21270;&#20102;&#29983;&#25104;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#27963;&#21160;&#35782;&#21035;&#20998;&#31867;&#22120;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26159;&#24247;&#22797;&#12289;&#20581;&#24247;&#30417;&#27979;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#24212;&#29992;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23588;&#20854;&#26159;IMU&#20256;&#24863;&#22120;&#65292;&#20197;&#30456;&#23545;&#36739;&#20302;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20154;&#20307;&#36816;&#21160;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#12290;&#24320;&#21457;&#40065;&#26834;&#30340;&#27963;&#21160;&#35782;&#21035;&#20998;&#31867;&#22120;&#19968;&#30452;&#26159;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#26159;&#36890;&#24120;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#28145;&#24230;&#20998;&#31867;&#22120;&#21464;&#24471;&#22256;&#38590;&#65292;&#26377;&#26102;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#32593;&#32476;TheraGAN&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#24247;&#22797;&#27963;&#21160;&#30456;&#20851;&#30340;IMU&#20449;&#21495;&#12290;&#29983;&#25104;&#30340;&#20449;&#21495;&#21253;&#25324;&#26469;&#33258;6&#20010;&#36890;&#36947;&#30340;IMU&#25968;&#25454;&#65292;&#21363;&#35282;&#36895;&#24230;&#21644;&#32447;&#24615;&#21152;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#31616;&#21333;&#27963;&#21160;&#31616;&#21270;&#20102;&#19981;&#21516;&#38271;&#24230;&#27963;&#21160;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#35780;&#20272;&#29983;&#25104;&#30340;&#20449;&#21495;&#65292;&#36827;&#34892;&#20102;&#20960;&#20010;&#23450;&#24615;&#23454;&#39564;&#21644;&#23450;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.07998v2 Announce Type: replace-cross  Abstract: Human activity recognition is a core technology for applications such as rehabilitation, health monitoring, and human-computer interactions. Wearable devices, especially IMU sensors, provide rich features of human movements at a reasonable cost, which can be leveraged in activity recognition. Developing a robust classifier for activity recognition has always been of interest to researchers. One major problem is that there is usually a deficit of training data, which makes developing deep classifiers difficult and sometimes impossible. In this work, a novel GAN network called TheraGAN was developed to generate IMU signals associated with rehabilitation activities. The generated signal comprises data from a 6-channel IMU, i.e., angular velocities and linear accelerations. Also, introducing simple activities simplified the generation process for activities of varying lengths. To evaluate the generated signals, several qualitative 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#25552;&#20379;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#36827;&#34892;&#38598;&#20307;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#24182;&#22312;&#35299;&#20915;&#22797;&#26434;&#30446;&#26631;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#27493;&#25968;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2302.04944</link><description>&lt;p&gt;
&#20351;&#29992;&#32473;&#23450;&#30340;&#23376;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#22797;&#26434;&#30340;&#22242;&#38431;&#21512;&#20316;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.04944
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#25552;&#20379;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#36827;&#34892;&#38598;&#20307;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#24182;&#22312;&#35299;&#20915;&#22797;&#26434;&#30446;&#26631;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#27493;&#25968;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22242;&#38431;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#38754;&#20020;&#35832;&#22914;&#22312;&#22823;&#22411;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#20013;&#25628;&#32034;&#31574;&#30053;&#21644;&#22240;&#20114;&#30456;&#36866;&#24212;&#32780;&#23548;&#33268;&#30340;&#38750;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#20419;&#36827;&#23545;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19987;&#23478;&#25552;&#20379;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#22810;&#26234;&#33021;&#20307;&#23376;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#23545;&#25972;&#20010;&#22242;&#38431;&#30340;&#23376;&#38598;&#36827;&#34892;&#35757;&#32451;&#20197;&#33719;&#21462;&#29305;&#23450;&#20110;&#23376;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#23558;&#23376;&#22242;&#38431;&#21512;&#24182;&#24182;&#36801;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#65292;&#22312;&#37027;&#37324;&#20182;&#20204;&#30340;&#31574;&#30053;&#34987;&#38598;&#20307;&#35843;&#25972;&#20197;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35299;&#20915;&#22797;&#26434;&#30446;&#26631;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#23376;&#20219;&#21153;&#20998;&#35299;&#30340;&#22825;&#30495;&#23454;&#29616;&#26041;&#27861;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.04944v2 Announce Type: replace-cross  Abstract: Training a team to complete a complex task via multi-agent reinforcement learning can be difficult due to challenges such as policy search in a large joint policy space, and non-stationarity caused by mutually adapting agents. To facilitate efficient learning of complex multi-agent tasks, we propose an approach which uses an expert-provided decomposition of a task into simpler multi-agent sub-tasks. In each sub-task, a subset of the entire team is trained to acquire sub-task-specific policies. The sub-teams are then merged and transferred to the target task, where their policies are collectively fine-tuned to solve the more complex target task. We show empirically that such approaches can greatly reduce the number of timesteps required to solve a complex target task relative to training from-scratch. However, we also identify and investigate two problems with naive implementations of approaches based on sub-task decomposition, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65288;MPI&#65289;&#22312;&#39118;&#38505;&#25935;&#24863;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#24050;&#26377;&#32467;&#26524;&#19981;&#21516;&#30340;&#35777;&#26126;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2302.03811</link><description>&lt;p&gt;
&#20851;&#20110;&#39118;&#38505;&#25935;&#24863;&#25351;&#25968;&#25104;&#26412;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Modified Policy Iteration in Risk Sensitive Exponential Cost Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03811
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65288;MPI&#65289;&#22312;&#39118;&#38505;&#25935;&#24863;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#24050;&#26377;&#32467;&#26524;&#19981;&#21516;&#30340;&#35777;&#26126;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#25913;&#30340;&#31574;&#30053;&#36845;&#20195;&#65288;MPI&#65289;&#26159;&#19968;&#31181;&#23558;&#31574;&#30053;&#36845;&#20195;&#21644;&#20540;&#36845;&#20195;&#30456;&#32467;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;MPI&#30340;&#25910;&#25947;&#24615;&#22312;&#25240;&#25187;&#21644;&#24179;&#22343;&#25104;&#26412;MDP&#30340;&#32972;&#26223;&#19979;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#25104;&#26412;&#39118;&#38505;&#25935;&#24863;MDP&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#23545;&#27169;&#22411;&#21442;&#25968;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#34429;&#28982;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;MDP&#24050;&#32463;&#23545;&#31574;&#30053;&#36845;&#20195;&#21644;&#20540;&#36845;&#20195;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;MPI&#21364;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;MPI&#20063;&#23545;&#39118;&#38505;&#25935;&#24863;&#38382;&#39064;&#25910;&#25947;&#12290;&#30001;&#20110;&#25351;&#25968;&#25104;&#26412;&#24418;&#24335;&#28041;&#21450;&#20056;&#27861;&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#19982;&#25240;&#25187;&#21644;&#39118;&#38505;&#20013;&#31435;&#24179;&#22343;&#25104;&#26412;&#38382;&#39064;&#20197;&#21450;&#39118;&#38505;&#25935;&#24863;&#20540;&#21644;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#19981;&#21516;&#30340;&#25910;&#25947;&#35777;&#26126;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03811v2 Announce Type: replace-cross  Abstract: Modified policy iteration (MPI) is a dynamic programming algorithm that combines elements of policy iteration and value iteration. The convergence of MPI has been well studied in the context of discounted and average-cost MDPs. In this work, we consider the exponential cost risk-sensitive MDP formulation, which is known to provide some robustness to model parameters. Although policy iteration and value iteration have been well studied in the context of risk sensitive MDPs, MPI is unexplored. We provide the first proof that MPI also converges for the risk-sensitive problem in the case of finite state and action spaces. Since the exponential cost formulation deals with the multiplicative Bellman equation, our main contribution is a convergence proof which is quite different than existing results for discounted and risk-neutral average-cost problems as well as risk sensitive value and policy iteration approaches. We conclude our a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#21464;&#37327;&#31354;&#38388;&#30456;&#20851;&#24863;&#30693;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#24863;&#22120;&#25968;&#25454;&#20559;&#26012;&#19988;&#38750;&#39640;&#26031;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2301.00462</link><description>&lt;p&gt;
&#19968;&#31181;&#28508;&#21464;&#37327;&#31354;&#38388;&#30456;&#20851;&#24863;&#30693;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#20559;&#26012;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Correlation-Aware Autoencoder for Anomaly Detection in Skewed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.00462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#21464;&#37327;&#31354;&#38388;&#30456;&#20851;&#24863;&#30693;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#24863;&#22120;&#25968;&#25454;&#20559;&#26012;&#19988;&#38750;&#39640;&#26031;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21464;&#37327;&#31354;&#38388;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#21306;&#20998;&#24322;&#24120;&#25968;&#25454;&#21644;&#27491;&#24120;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#27492;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36807;&#21435;&#24050;&#32463;&#25506;&#32034;&#20102;&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#20013;&#26816;&#27979;&#24322;&#24120;&#30340;&#23494;&#24230;&#20272;&#35745;&#21644;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#35777;&#26126;&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#20013;&#20445;&#30041;&#36755;&#20837;&#25968;&#25454;&#30340;&#26377;&#20215;&#20540;&#23646;&#24615;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#37325;&#26500;&#27979;&#35797;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#26159;&#20559;&#26012;&#19988;&#38750;&#39640;&#26031;&#24615;&#30340;&#65292;&#20351;&#24471;&#22522;&#20110;&#24179;&#22343;&#30340;&#20272;&#35745;&#22120;&#23545;&#20110;&#20559;&#26012;&#25968;&#25454;&#19981;&#21487;&#38752;&#12290;&#20877;&#32773;&#65292;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#27431;&#27663;&#36317;&#31163;&#65292;&#19981;&#32771;&#34385;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#26377;&#29992;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#35813;&#25968;&#25454;&#20559;&#31163;&#35757;&#32451;&#20998;&#24067;&#26102;&#26080;&#27861;&#20934;&#30830;&#22320;&#37325;&#26500;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#20559;&#26012;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.00462v3 Announce Type: replace  Abstract: Unsupervised learning-based anomaly detection in latent space has gained importance since discriminating anomalies from normal data becomes difficult in high-dimensional space. Both density estimation and distance-based methods to detect anomalies in latent space have been explored in the past. These methods prove that retaining valuable properties of input data in latent space helps in the better reconstruction of test data. Moreover, real-world sensor data is skewed and non-Gaussian in nature, making mean-based estimators unreliable for skewed data. Again, anomaly detection methods based on reconstruction error rely on Euclidean distance, which does not consider useful correlation information in the feature space and also fails to accurately reconstruct the data when it deviates from the training distribution. In this work, we address the limitations of reconstruction error-based autoencoders and propose a kernelized autoencoder th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SimCS&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32447;&#39046;&#22495;&#22686;&#37327;&#32487;&#32493;&#20998;&#21106;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SimCS&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#23494;&#38598;&#26631;&#35760;&#22270;&#20687;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#65292;&#26080;&#38656;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2211.16234</link><description>&lt;p&gt;
SimCS&#65306;&#39046;&#22495;&#22686;&#37327;&#22312;&#32447;&#32487;&#32493;&#20998;&#21106;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
SimCS: Simulation for Domain Incremental Online Continual Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.16234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SimCS&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32447;&#39046;&#22495;&#22686;&#37327;&#32487;&#32493;&#20998;&#21106;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SimCS&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#23494;&#38598;&#26631;&#35760;&#22270;&#20687;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#65292;&#26080;&#38656;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#36808;&#21521;&#32456;&#36523;&#26234;&#33021;&#30340;&#19968;&#27493;&#65292;&#20854;&#20013;&#27169;&#22411;&#21487;&#20197;&#25345;&#32493;&#20174;&#26368;&#36817;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#19981;&#20250;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20110;&#20855;&#26377;&#28165;&#26224;&#20219;&#21153;&#36793;&#30028;&#21644;&#26080;&#38480;&#35745;&#31639;&#39044;&#31639;&#30340;&#20998;&#31867;&#35774;&#32622;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32447;&#39046;&#22495;&#22686;&#37327;&#32487;&#32493;&#20998;&#21106;&#65288;ODICS&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#23494;&#38598;&#26631;&#35760;&#22270;&#20687;&#25209;&#27425;&#19978;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#65292;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#24182;&#19988;&#27809;&#26377;&#20851;&#20110;&#20219;&#21153;&#36793;&#30028;&#30340;&#20449;&#24687;&#12290;ODICS&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#36825;&#21487;&#33021;&#23545;&#24212;&#20110;&#22312;&#26102;&#38388;&#19978;&#23545;&#19968;&#31995;&#21015;&#22478;&#24066;&#20013;&#30340;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#23613;&#31649;&#22312;&#31867;&#21035;&#22686;&#37327;&#20998;&#21106;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;SimCS&#65292;&#23427;&#37319;&#29992;&#22686;&#37327;&#39046;&#22495;&#23398;&#20064;&#21644;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.16234v2 Announce Type: replace-cross  Abstract: Continual Learning is a step towards lifelong intelligence where models continuously learn from recently collected data without forgetting previous knowledge. Existing continual learning approaches mostly focus on image classification in the class-incremental setup with clear task boundaries and unlimited computational budget. This work explores the problem of Online Domain-Incremental Continual Segmentation (ODICS), where the model is continually trained over batches of densely labeled images from different domains, with limited computation and no information about the task boundaries. ODICS arises in many practical applications. In autonomous driving, this may correspond to the realistic scenario of training a segmentation model over time on a sequence of cities. We analyze several existing continual learning methods and show that they perform poorly in this setting despite working well in class-incremental segmentation. We p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#22120;&#20013;&#30340;"&#20849;&#21516;&#35757;&#32451;"&#26041;&#27861;&#30340;&#20215;&#20540;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21462;&#24471;&#19982;&#20351;&#29992;&#20840;&#37096;&#25968;&#25454;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2211.05920</link><description>&lt;p&gt;
&#24403;&#23569;&#21363;&#26159;&#22810;&#65306;&#20851;&#20110;&#21322;&#30417;&#30563;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#22120;"&#20849;&#21516;&#35757;&#32451;"&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
When Less is More: On the Value of "Co-training" for Semi-Supervised Software Defect Predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.05920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#22120;&#20013;&#30340;"&#20849;&#21516;&#35757;&#32451;"&#26041;&#27861;&#30340;&#20215;&#20540;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21462;&#24471;&#19982;&#20351;&#29992;&#20840;&#37096;&#25968;&#25454;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#35760;&#27169;&#22359;&#20026;&#32570;&#38519;&#25110;&#38750;&#32570;&#38519;&#30340;&#20219;&#21153;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#21322;&#30417;&#30563;&#20998;&#31867;&#22120;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#26631;&#31614;&#12289;&#20849;&#21516;&#35757;&#32451;&#12289;&#26368;&#22823;&#38388;&#38548;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#31561;&#12290;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31181;&#26041;&#27861;&#34987;&#29992;&#20110;&#27979;&#35797;&#65288;&#20363;&#22914;&#39044;&#27979;&#32570;&#38519;&#65289;&#65292;&#32780;&#19988;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#23569;&#25968;&#20960;&#20010;&#39033;&#30446;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#26412;&#25991;&#23558;55&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#22120;&#24212;&#29992;&#20110;714&#20010;&#39033;&#30446;&#19978;&#65292;&#21457;&#29616;&#21322;&#30417;&#30563;&#30340;"&#20849;&#21516;&#35757;&#32451;&#26041;&#27861;"&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#26631;&#35760;&#20102;&#20165;2.5%&#30340;&#25968;&#25454;&#21518;&#65292;&#20351;&#29992;&#20849;&#21516;&#35757;&#32451;&#26041;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20351;&#29992;100%&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#35880;&#24910;&#20351;&#29992;&#20849;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#22240;&#20026;&#20849;&#21516;&#35757;&#32451;&#26041;&#27861;&#30340;&#36873;&#25321;&#20855;&#20307;&#21462;&#20915;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.05920v2 Announce Type: replace-cross  Abstract: Labeling a module defective or non-defective is an expensive task. Hence, there are often limits on how much-labeled data is available for training. Semi-supervised classifiers use far fewer labels for training models. However, there are numerous semi-supervised methods, including self-labeling, co-training, maximal-margin, and graph-based methods, to name a few. Only a handful of these methods have been tested in SE for (e.g.) predicting defects and even there, those methods have been tested on just a handful of projects.   This paper applies a wide range of 55 semi-supervised learners to over 714 projects. We find that semi-supervised "co-training methods" work significantly better than other approaches. Specifically, after labeling, just   2.5% of data, then make predictions that are competitive to those using 100% of the data.   That said, co-training needs to be used cautiously since the specific choice of co-training meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2210.02042</link><description>&lt;p&gt;
FedMT: &#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMT: Federated Learning with Mixed-type Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.02042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20998;&#31867;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;&#32593;&#32476;&#65289;&#22312;&#22810;&#20010;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#22312;&#36825;&#20123;&#20013;&#24515;&#20043;&#38388;&#20132;&#25442;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#22312;&#25152;&#26377;&#21442;&#19982;&#35757;&#32451;&#30340;&#20013;&#24515;&#20013;&#37319;&#29992;&#30456;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#36825;&#20010;&#38480;&#21046;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;FL&#30340;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24456;&#21487;&#33021;&#22312;&#20020;&#24202;&#20013;&#24515;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#19982;&#20256;&#32479;FL&#30340;&#35774;&#32622;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;FL&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;FL&#65292;&#20854;&#20013;&#21508;&#20010;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#65292;&#20174;&#32780;&#23548;&#33268;&#20013;&#24515;&#38388;&#26631;&#31614;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20026;&#20256;&#32479;&#35774;&#32622;&#35774;&#35745;&#30340;&#29616;&#26377;FL&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.02042v3 Announce Type: replace-cross Abstract: In federated learning (FL), classifiers (e.g., deep networks) are trained on datasets from multiple centers without exchanging data across them, and thus improves sample efficiency. In the classical setting of FL, the same labeling criterion is usually employed across all centers being involved in training. This constraint greatly limits the applicability of FL. For example, standards used for disease diagnosis are more likely to be different across clinical centers, which mismatches the classical FL setting. In this paper, we consider an important yet under-explored setting of FL, namely FL with mixed-type labels where different labeling criteria can be employed by various centers, leading to inter-center label space differences and challenging existing FL methods designed for the classical setting. To effectively and efficiently train models with mixed-type labels, we propose a theory-guided and model-agnostic approach that ca
&lt;/p&gt;</description></item><item><title>PixTrack&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#29289;&#20307;&#23039;&#24577;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;NeRF&#27169;&#26495;&#21644;&#29305;&#24449;&#24230;&#37327;&#23545;&#40784;&#26041;&#27861;&#65292;&#33021;&#22815;&#31934;&#30830;&#36319;&#36394;&#29289;&#20307;&#30340;6DoF&#23039;&#24577;&#65292;&#32780;&#19988;&#26080;&#38656;&#25968;&#25454;&#27880;&#37322;&#25110;&#36712;&#36857;&#24179;&#28369;&#12290;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#31934;&#30830;&#12289;&#40065;&#26834;&#19988;&#26080;&#25238;&#21160;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#29992;&#20110;&#22810;&#30446;&#26631;&#36319;&#36394;&#12290;</title><link>https://arxiv.org/abs/2209.03910</link><description>&lt;p&gt;
PixTrack&#65306;&#20351;&#29992;NeRF&#27169;&#26495;&#21644;&#29305;&#24449;&#24230;&#37327;&#23545;&#29289;&#20307;&#30340;6DoF&#23039;&#24577;&#36827;&#34892;&#31934;&#30830;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and Feature-metric Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.03910
&lt;/p&gt;
&lt;p&gt;
PixTrack&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#29289;&#20307;&#23039;&#24577;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;NeRF&#27169;&#26495;&#21644;&#29305;&#24449;&#24230;&#37327;&#23545;&#40784;&#26041;&#27861;&#65292;&#33021;&#22815;&#31934;&#30830;&#36319;&#36394;&#29289;&#20307;&#30340;6DoF&#23039;&#24577;&#65292;&#32780;&#19988;&#26080;&#38656;&#25968;&#25454;&#27880;&#37322;&#25110;&#36712;&#36857;&#24179;&#28369;&#12290;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#31934;&#30830;&#12289;&#40065;&#26834;&#19988;&#26080;&#25238;&#21160;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#29992;&#20110;&#22810;&#30446;&#26631;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PixTrack&#65292;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#29289;&#20307;&#23039;&#24577;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;&#21644;&#28145;&#24230;&#29305;&#24449;&#24230;&#37327;&#23545;&#40784;&#12290;&#25105;&#20204;&#36981;&#24490;&#22522;&#20110;SfM&#30340;&#37325;&#26032;&#23450;&#20301;&#33539;&#24335;&#65292;&#20351;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#35268;&#33539;&#22320;&#34920;&#31034;&#34987;&#36319;&#36394;&#30340;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#30446;RGB&#22270;&#20687;&#21644;RGB-D&#22270;&#20687;&#20013;&#20135;&#29983;&#39640;&#24230;&#31934;&#30830;&#12289;&#40065;&#26834;&#19988;&#26080;&#25238;&#21160;&#30340;&#29289;&#20307;6DoF&#23039;&#24577;&#20272;&#35745;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#27880;&#37322;&#25110;&#36712;&#36857;&#24179;&#28369;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29305;&#28857;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;CPU&#22810;&#36827;&#31243;&#21487;&#20197;&#23454;&#29616;&#22810;&#30446;&#26631;&#36319;&#36394;&#32780;&#26080;&#38656;&#25913;&#21464;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/GiantAI/pixtrack
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.03910v2 Announce Type: replace-cross  Abstract: We present PixTrack, a vision based object pose tracking framework using novel view synthesis and deep feature-metric alignment. We follow an SfM-based relocalization paradigm where we use a Neural Radiance Field to canonically represent the tracked object. Our evaluations demonstrate that our method produces highly accurate, robust, and jitter-free 6DoF pose estimates of objects in both monocular RGB images and RGB-D images without the need of any data annotation or trajectory smoothing. Our method is also computationally efficient making it easy to have multi-object tracking with no alteration to our algorithm through simple CPU multiprocessing. Our code is available at: https://github.com/GiantAI/pixtrack
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#26377;&#30028;&#30340;&#20010;&#24615;&#21270;PageRank&#31639;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#22270;&#23398;&#20064;&#30340;&#20960;&#31181;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2207.06944</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22270;&#23398;&#20064;&#30340;&#25935;&#24863;&#24615;&#26377;&#30028;&#20010;&#24615;&#21270;PageRank&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.06944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#26377;&#30028;&#30340;&#20010;&#24615;&#21270;PageRank&#31639;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#22270;&#23398;&#20064;&#30340;&#20960;&#31181;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;PageRank(PPR)&#26159;&#19968;&#31181;&#22522;&#26412;&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#34920;&#31034;&#65292;&#22914;&#33410;&#28857;&#25490;&#24207;&#12289;&#26631;&#27880;&#21644;&#22270;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#38544;&#31169;&#25104;&#20026;&#26368;&#36817;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#29616;&#26377;&#30340;PPR&#31639;&#27861;&#24182;&#26410;&#35774;&#35745;&#29992;&#20110;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;PPR&#23545;&#36755;&#20837;&#22270;&#30340;&#36793;&#38750;&#24120;&#25935;&#24863;&#65306;&#20165;&#24046;&#19968;&#20010;&#36793;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#23548;&#33268;PPR&#21521;&#37327;&#21457;&#29983;&#24040;&#22823;&#25913;&#21464;&#65292;&#20174;&#32780;&#21487;&#33021;&#27844;&#28431;&#29992;&#25143;&#31169;&#23494;&#25968;&#25454;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36755;&#20986;&#36817;&#20284;PPR&#65292;&#24182;&#23545;&#36755;&#20837;&#36793;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#25935;&#24863;&#24615;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36755;&#20837;&#22270;&#20855;&#26377;&#22823;&#24230;&#25968;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#19982;&#38750;&#31169;&#23494;&#31639;&#27861;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25935;&#24863;&#24615;&#26377;&#30028;PPR&#30452;&#25509;&#24847;&#21619;&#30528;&#22270;&#23398;&#20064;&#30340;&#20960;&#31181;&#31169;&#23494;&#31639;&#27861;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;(DP)PPR&#25490;&#24207;&#12289;DP&#33410;&#28857;&#20998;&#31867;&#21644;DP&#33410;&#28857;&#23884;&#20837;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of graph representations such as node ranking, labeling, and graph embedding. However, while data privacy is one of the most important recent concerns, existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges: the difference of only one edge may cause a big change in the PPR vector, potentially leaking private user data.   In this work, we propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. In addition, we prove that our algorithm achieves similar accuracy to non-private algorithms when the input graph has large degrees. Our sensitivity-bounded PPR directly implies private algorithms for several tools of graph learning, such as, differentially private (DP) PPR ranking, DP node classification, and DP node embedding. To complement our theoretical analysis, we also empirically verify the practical perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#24847;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#35774;&#35745;&#20986;&#20102;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2204.09092</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#24847;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Indiscriminate Data Poisoning Attacks on Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.09092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#24847;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#35774;&#35745;&#20986;&#20102;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26159;&#25351;&#24694;&#24847;&#23545;&#25163;&#36890;&#36807;&#23558;&#8220;&#27745;&#26579;&#8221;&#30340;&#25968;&#25454;&#27880;&#20837;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26469;&#24433;&#21709;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#30340;&#27745;&#26579;&#25915;&#20987;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#35299;&#20915;&#39034;&#24207;&#26031;&#22612;&#20811;&#20271;&#26684;&#21338;&#24328;&#30340;&#26032;&#32769;&#31639;&#27861;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#20026;&#25915;&#20987;&#32773;&#36873;&#25321;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#30340;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#30340;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#33258;&#21160;&#24494;&#20998;&#36719;&#20214;&#21253;&#21516;&#26102;&#12289;&#21327;&#35843;&#22320;&#29983;&#25104;&#25968;&#19975;&#20010;&#27745;&#26579;&#28857;&#65292;&#19982;&#29616;&#26377;&#30340;&#36880;&#20010;&#29983;&#25104;&#27745;&#26579;&#28857;&#30340;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.09092v2 Announce Type: replace  Abstract: Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting "poisoned" data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations that exploit modern auto-differentiation packages and allow simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks.
&lt;/p&gt;</description></item><item><title>CLASSIX&#26159;&#19968;&#31181;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#21518;&#30340;&#25968;&#25454;&#30340;&#36138;&#23146;&#32858;&#21512;&#21644;&#32676;&#32452;&#21512;&#24182;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24615;&#21644;&#36817;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2202.01456</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#24207;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and explainable clustering based on sorting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.01456
&lt;/p&gt;
&lt;p&gt;
CLASSIX&#26159;&#19968;&#31181;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#21518;&#30340;&#25968;&#25454;&#30340;&#36138;&#23146;&#32858;&#21512;&#21644;&#32676;&#32452;&#21512;&#24182;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24615;&#21644;&#36817;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;CLASSIX&#12290;&#23427;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21363;&#23558;&#25490;&#24207;&#21518;&#30340;&#25968;&#25454;&#32858;&#21512;&#25104;&#38468;&#36817;&#25968;&#25454;&#28857;&#32452;&#25104;&#30340;&#32676;&#32452;&#30340;&#36138;&#23146;&#32858;&#21512;&#38454;&#27573;&#65292;&#28982;&#21518;&#23558;&#32676;&#32452;&#21512;&#24182;&#25104;&#32858;&#31867;&#12290;&#35813;&#31639;&#27861;&#30001;&#20004;&#20010;&#26631;&#37327;&#21442;&#25968;&#25511;&#21046;&#65292;&#19968;&#20010;&#26159;&#32858;&#21512;&#30340;&#36317;&#31163;&#21442;&#25968;&#65292;&#21478;&#19968;&#20010;&#26159;&#25511;&#21046;&#26368;&#23567;&#32858;&#31867;&#22823;&#23567;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#32858;&#31867;&#24418;&#29366;&#21644;&#20302;&#21040;&#39640;&#30340;&#29305;&#24449;&#32500;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CLASSIX&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#31454;&#20105;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#22312;&#24191;&#27867;&#30340;&#38382;&#39064;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#25509;&#36817;&#32447;&#24615;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;&#20854;&#22266;&#26377;&#30340;&#31616;&#21333;&#24615;&#20351;&#24471;&#21487;&#20197;&#29983;&#25104;&#23545;&#35745;&#31639;&#30340;&#32858;&#31867;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.01456v2 Announce Type: replace  Abstract: We introduce a fast and explainable clustering method called CLASSIX. It consists of two phases, namely a greedy aggregation phase of the sorted data into groups of nearby data points, followed by the merging of groups into clusters. The algorithm is controlled by two scalar parameters, namely a distance parameter for the aggregation and another parameter controlling the minimal cluster size. Extensive experiments are conducted to give a comprehensive evaluation of the clustering performance on synthetic and real-world datasets, with various cluster shapes and low to high feature dimensionality. Our experiments demonstrate that CLASSIX competes with state-of-the-art clustering algorithms. The algorithm has linear space complexity and achieves near linear time complexity on a wide range of problems. Its inherent simplicity allows for the generation of intuitive explanations of the computed clusters.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#19990;&#30028;&#27169;&#22411;&#26500;&#24314;&#26694;&#26550;ED2&#65292;&#33021;&#22815;&#36890;&#36807;&#21457;&#29616;&#23376;&#21160;&#21147;&#23398;&#24182;&#36827;&#34892;&#20998;&#35299;&#39044;&#27979;&#65292;&#26356;&#20934;&#30830;&#22320;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2112.02817</link><description>&lt;p&gt;
ED2: &#36830;&#32493;&#25511;&#21046;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ED2: Environment Dynamics Decomposition World Models for Continuous Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.02817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#19990;&#30028;&#27169;&#22411;&#26500;&#24314;&#26694;&#26550;ED2&#65292;&#33021;&#22815;&#36890;&#36807;&#21457;&#29616;&#23376;&#21160;&#21147;&#23398;&#24182;&#36827;&#34892;&#20998;&#35299;&#39044;&#27979;&#65292;&#26356;&#20934;&#30830;&#22320;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL)&#22312;&#23454;&#36341;&#20013;&#30456;&#23545;&#20110;model-free RL&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#20854;&#24615;&#33021;&#24120;&#24120;&#21463;&#38480;&#20110;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#20943;&#23569;&#27169;&#22411;&#35823;&#24046;&#65292;&#26631;&#20934;&#30340;MBRL&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#32593;&#32476;&#26469;&#25311;&#21512;&#25972;&#20010;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#20294;&#36825;&#28010;&#36153;&#20102;&#21487;&#20197;&#20998;&#21035;&#24314;&#27169;&#30340;&#22810;&#20010;&#23376;&#21160;&#21147;&#23398;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#26356;&#20934;&#30830;&#22320;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29615;&#22659;&#21160;&#21147;&#23398;&#20998;&#35299;&#65288;ED2&#65289;&#30340;&#21019;&#26032;&#19990;&#30028;&#27169;&#22411;&#26500;&#24314;&#26694;&#26550;&#65292;&#20854;&#20197;&#19968;&#31181;&#20998;&#35299;&#30340;&#26041;&#24335;&#23545;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;ED2&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#23376;&#21160;&#21147;&#23398;&#21457;&#29616;&#65288;SD2&#65289;&#21644;&#21160;&#21147;&#23398;&#20998;&#35299;&#39044;&#27979;&#65288;D2P&#65289;&#12290;SD2&#33021;&#22815;&#33258;&#21160;&#21457;&#29616;&#29615;&#22659;&#20013;&#30340;&#23376;&#21160;&#21147;&#23398;&#65292;&#28982;&#21518;D2P&#26681;&#25454;&#36825;&#20123;&#23376;&#21160;&#21147;&#23398;&#26500;&#24314;&#20998;&#35299;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;ED2&#21487;&#20197;&#19982;&#29616;&#26377;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.02817v2 Announce Type: replace-cross  Abstract: Model-based reinforcement learning (MBRL) achieves significant sample efficiency in practice in comparison to model-free RL, but its performance is often limited by the existence of model prediction error. To reduce the model error, standard MBRL approaches train a single well-designed network to fit the entire environment dynamics, but this wastes rich information on multiple sub-dynamics which can be modeled separately, allowing us to construct the world model more accurately. In this paper, we propose the Environment Dynamics Decomposition (ED2), a novel world model construction framework that models the environment in a decomposing manner. ED2 contains two key components: sub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2 discovers the sub-dynamics in an environment automatically and then D2P constructs the decomposed world model following the sub-dynamics. ED2 can be easily combined with existing
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#38656;&#27491;&#21017;&#21270;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#20381;&#36182;&#28508;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#36827;&#34892;&#37319;&#26679;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#37325;&#26500;&#36136;&#37327;&#21644;&#29983;&#25104;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20986;&#19968;&#31181;&#26377;&#24207;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#25913;&#21892;&#20102;&#29983;&#25104;&#12289;&#35299;&#32544;&#21644;&#22806;&#25512;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2006.07796</link><description>&lt;p&gt;
&#32467;&#26500;&#36890;&#36807;&#26550;&#26500;&#65306;&#26080;&#38656;&#27491;&#21017;&#21270;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Structure by Architecture: Structured Representations without Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.07796
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#38656;&#27491;&#21017;&#21270;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#20381;&#36182;&#28508;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#36827;&#34892;&#37319;&#26679;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#37325;&#26500;&#36136;&#37327;&#21644;&#29983;&#25104;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20986;&#19968;&#31181;&#26377;&#24207;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#25913;&#21892;&#20102;&#29983;&#25104;&#12289;&#35299;&#32544;&#21644;&#22806;&#25512;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#25105;&#30417;&#30563;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#21305;&#37197;&#20219;&#24847;&#30340;&#12289;&#30456;&#23545;&#38750;&#32467;&#26500;&#21270;&#30340;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#30340;&#24773;&#20917;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20165;&#20381;&#36182;&#20110;&#28508;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#30340;&#37319;&#26679;&#25216;&#26415;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;VAE&#20013;&#36890;&#24120;&#35266;&#23519;&#21040;&#30340;&#37325;&#26500;&#36136;&#37327;&#21644;&#29983;&#25104;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23398;&#20064;&#20986;&#19968;&#31181;&#26080;&#38656;&#36807;&#24230;&#27491;&#21017;&#21270;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26500;&#35299;&#30721;&#22120;&#23398;&#20064;&#20102;&#19968;&#20010;&#23618;&#27425;&#30340;&#28508;&#21464;&#37327;&#65292;&#20174;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#25110;&#30417;&#30563;&#26469;&#23545;&#20449;&#24687;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#20986;&#25913;&#21892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#29983;&#25104;&#12289;&#35299;&#32544;&#21644;&#22806;&#25512;&#65292;&#20351;&#29992;&#20102;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2006.07796v4 Announce Type: replace  Abstract: We study the problem of self-supervised structured representation learning using autoencoders for downstream tasks such as generative modeling. Unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance typically observed in VAEs. We design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. Our structural decoders learn a hierarchy of latent variables, thereby ordering the information without any additional regularization or supervision. We demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challengi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;HawkEye&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#22312;&#22788;&#29702;&#31163;&#32676;&#20540;&#21644;&#22122;&#22768;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16785</link><description>&lt;p&gt;
&#36890;&#36807;HawkEye Loss&#22312;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#20013;&#25552;&#39640;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficiency and Robustness in Support Vector Regression with HawkEye Loss. (arXiv:2401.16785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16785
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;HawkEye&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#22312;&#22788;&#29702;&#31163;&#32676;&#20540;&#21644;&#22122;&#22768;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#22312;&#38754;&#23545;&#31163;&#32676;&#20540;&#21644;&#22122;&#22768;&#26102;&#65292;SVR&#36935;&#21040;&#20102;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20351;&#29992;&#20102;&#949;-insensitive&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#20855;&#26377;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;SVR&#24050;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#24179;&#28369;&#29305;&#24615;&#30340;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#65292;&#20419;&#36827;&#20102;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#30340;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#35843;&#30340;&#26159;&#65292;&#36825;&#20123;&#26377;&#30028;&#21644;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#20855;&#26377;&#19968;&#20010;&#19981;&#25935;&#24863;&#30340;&#21306;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;HawkEye&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#19978;&#36848;&#32422;&#26463;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;HawkEye&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;SVR&#20013;&#30340;&#31532;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#31361;&#20986;&#26174;&#31034;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector regression (SVR) has garnered significant popularity over the past two decades owing to its wide range of applications across various fields. Despite its versatility, SVR encounters challenges when confronted with outliers and noise, primarily due to the use of the $\varepsilon$-insensitive loss function. To address this limitation, SVR with bounded loss functions has emerged as an appealing alternative, offering enhanced generalization performance and robustness. Notably, recent developments focus on designing bounded loss functions with smooth characteristics, facilitating the adoption of gradient-based optimization algorithms. However, it's crucial to highlight that these bounded and smooth loss functions do not possess an insensitive zone. In this paper, we address the aforementioned constraints by introducing a novel symmetric loss function named the HawkEye loss function. It is worth noting that the HawkEye loss function stands out as the first loss function in SVR
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01218</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#36866;&#24212;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#39044;&#27979;&#30340;&#25463;&#24452;&#65292;&#23548;&#33268;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23481;&#26131;&#34920;&#29616;&#20986;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#21033;&#29992;&#20301;&#20110;&#24320;&#22836;&#25110;&#26411;&#23614;&#25110;&#36755;&#20837;&#20013;&#29305;&#23450;&#20301;&#32622;&#32447;&#32034;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#24037;&#20316;&#38656;&#35201;&#22806;&#37096;&#20559;&#24046;&#30693;&#35782;&#25110;&#24102;&#27880;&#37322;&#30340;&#38750;&#20559;&#20506;&#26679;&#26412;&#65292;&#22312;&#23454;&#38469;&#20013;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#65288;ZOE&#65289;&#26694;&#26550;&#23545;LLMs&#36827;&#34892;&#20301;&#32622;&#21435;&#20559;&#12290;ZOE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#25110;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#26080;&#30417;&#30563;&#21709;&#24212;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20174;&#23545;&#40784;&#65288;MSA&#65289;&#27169;&#22359;&#26469;&#20462;&#21098;&#36825;&#20123;&#21709;&#24212;&#12290;&#23545;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZOE&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20844;&#24179;GNN&#30340;&#20551;&#35774;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01591</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#32988;&#20110;&#36951;&#25022;&#65306;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Better Fair than Sorry: Adversarial Missing Data Imputation for Fair GNNs. (arXiv:2311.01591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20844;&#24179;GNN&#30340;&#20551;&#35774;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32570;&#22833;&#20445;&#25252;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#20915;&#31574;&#21487;&#33021;&#20250;&#23545;&#29305;&#23450;&#31038;&#21306;&#20135;&#29983;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#65292;&#32780;GNNs&#24050;&#32463;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;GNNs&#24037;&#20316;&#35201;&#20040;&#20551;&#35774;&#20445;&#25252;&#23646;&#24615;&#26159;&#23436;&#20840;&#34987;&#35266;&#23519;&#21040;&#30340;&#65292;&#35201;&#20040;&#20551;&#35774;&#32570;&#22833;&#25968;&#25454;&#30340;&#22635;&#20805;&#26159;&#20844;&#24179;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#22635;&#20805;&#20013;&#30340;&#20559;&#24046;&#20250;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#32467;&#26524;&#20013;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#39640;&#22320;&#20272;&#35745;&#20102;&#20854;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Better Fair than Sorry&#65288;BFtS&#65289;&#65292;&#20026;&#20844;&#24179;GNNs&#20351;&#29992;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;BFtS&#32972;&#21518;&#30340;&#20851;&#38190;&#35774;&#35745;&#21407;&#21017;&#26159;&#22635;&#20805;&#24212;&#35813;&#36817;&#20284;&#20110;&#20844;&#24179;GNN&#30340;&#26368;&#22256;&#38590;&#24773;&#20917;&#65292;&#21363;&#22312;&#26368;&#20248;&#21270;&#20844;&#24179;&#24615;&#26368;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19977;&#26041;&#23545;&#25239;&#26041;&#26696;&#26469;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#22312;&#36825;&#20010;&#26041;&#26696;&#20013;&#65292;&#20004;&#20010;&#23545;&#25163;&#20849;&#21516;&#23545;&#25239;&#20844;&#24179;GNN&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;BFtS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of learning fair Graph Neural Networks (GNNs) under missing protected attributes. GNNs have achieved state-of-the-art results in many relevant tasks where decisions might disproportionately impact specific communities. However, existing work on fair GNNs assumes that either protected attributes are fully-observed or that the missing data imputation is fair. In practice, biases in the imputation will be propagated to the model outcomes, leading them to overestimate the fairness of their predictions. We address this challenge by proposing Better Fair than Sorry (BFtS), a fair missing data imputation model for protected attributes used by fair GNNs. The key design principle behind BFtS is that imputations should approximate the worst-case scenario for the fair GNN -- i.e. when optimizing fairness is the hardest. We implement this idea using a 3-player adversarial scheme where two adversaries collaborate against the fair GNN. Experiments using synthetic and
&lt;/p&gt;</description></item><item><title>&#36125;&#21494;&#26031;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#26041;&#27861;&#65288;BayesMBAR&#65289;&#26159;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#65288;MBAR&#65289;&#26041;&#27861;&#30340;&#36125;&#21494;&#26031;&#25512;&#24191;&#12290;&#36890;&#36807;&#25972;&#21512;&#37319;&#26679;&#37197;&#32622;&#21644;&#20808;&#39564;&#20998;&#24067;&#65292;BayesMBAR&#35745;&#31639;&#20102;&#33258;&#30001;&#33021;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.20699</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Multistate Bennett Acceptance Ratio Methods. (arXiv:2310.20699v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20699
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#26041;&#27861;&#65288;BayesMBAR&#65289;&#26159;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#65288;MBAR&#65289;&#26041;&#27861;&#30340;&#36125;&#21494;&#26031;&#25512;&#24191;&#12290;&#36890;&#36807;&#25972;&#21512;&#37319;&#26679;&#37197;&#32622;&#21644;&#20808;&#39564;&#20998;&#24067;&#65292;BayesMBAR&#35745;&#31639;&#20102;&#33258;&#30001;&#33021;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#65288;MBAR&#65289;&#26041;&#27861;&#26159;&#35745;&#31639;&#28909;&#21147;&#23398;&#29366;&#24577;&#19979;&#33258;&#30001;&#33021;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BayesMBAR&#65292;&#21363;MBAR&#26041;&#27861;&#30340;&#36125;&#21494;&#26031;&#25512;&#24191;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;&#28909;&#21147;&#23398;&#29366;&#24577;&#30340;&#37319;&#26679;&#37197;&#32622;&#19982;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#25972;&#21512;&#65292;BayesMBAR&#35745;&#31639;&#20102;&#33258;&#30001;&#33021;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#21033;&#29992;&#21518;&#39564;&#20998;&#24067;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#33258;&#30001;&#33021;&#30340;&#20272;&#35745;&#20540;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#20351;&#29992;&#22343;&#21248;&#20808;&#39564;&#20998;&#24067;&#26102;&#65292;BayesMBAR&#21487;&#20197;&#24674;&#22797;MBAR&#30340;&#32467;&#26524;&#65292;&#20294;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#20851;&#33258;&#30001;&#33021;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#29992;&#26102;&#65292;BayesMBAR&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#38750;&#22343;&#21248;&#20808;&#39564;&#20998;&#24067;&#23558;&#36825;&#20123;&#20449;&#24687;&#32435;&#20837;&#20272;&#35745;&#36807;&#31243;&#20013;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24341;&#20837;&#20851;&#20110;&#33258;&#30001;&#33021;&#26354;&#38754;&#24179;&#28369;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;BayesMBAR&#27604;MBAR&#26041;&#27861;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach for computing free energies of thermodynamic states. In this work, we introduce BayesMBAR, a Bayesian generalization of the MBAR method. By integrating configurations sampled from thermodynamic states with a prior distribution, BayesMBAR computes a posterior distribution of free energies. Using the posterior distribution, we derive free energy estimations and compute their associated uncertainties. Notably, when a uniform prior distribution is used, BayesMBAR recovers the MBAR's result but provides more accurate uncertainty estimates. Additionally, when prior knowledge about free energies is available, BayesMBAR can incorporate this information into the estimation procedure by using non-uniform prior distributions. As an example, we show that, by incorporating the prior knowledge about the smoothness of free energy surfaces, BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's widespr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#31867;&#23545;&#25163;&#31034;&#20363;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#22823;&#35268;&#27169;&#23545;&#25239;&#31034;&#20363;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#23545;&#20110;&#20154;&#31867;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16955</link><description>&lt;p&gt;
&#25171;&#30772;&#12289;&#27169;&#20223;&#12289;&#20462;&#22797;&#65306;&#36890;&#36807;&#29983;&#25104;&#20154;&#31867;&#25915;&#20987;&#25552;&#39640;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks. (arXiv:2310.16955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#31867;&#23545;&#25163;&#31034;&#20363;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#22823;&#35268;&#27169;&#23545;&#25239;&#31034;&#20363;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#23545;&#20110;&#20154;&#31867;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#38656;&#35201;&#23545;&#25239;&#20154;&#31867;&#23545;&#25163;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#25163;&#30340;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#31181;&#26377;&#25928;&#20294;&#26114;&#36149;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35757;&#32451;&#38024;&#23545;&#23567;&#25200;&#21160;&#65288;&#22914;&#35789;&#26367;&#25442;&#65289;&#30340;&#21512;&#25104;&#25915;&#20987;&#23454;&#38469;&#19978;&#24182;&#19981;&#33021;&#25552;&#39640;&#23545;&#25239;&#20154;&#31867;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#31867;&#23545;&#25163;&#31034;&#20363;&#26469;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#22823;&#35268;&#27169;&#23545;&#25239;&#31034;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;ANLI&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36845;&#20195;&#30340;&#23545;&#25239;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#36807;&#31243;&#25910;&#38598;&#24471;&#21040;&#30340;&#12290;&#19982;&#20165;&#22312;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;&#25915;&#20987;&#19978;&#36827;&#34892;&#35757;&#32451;&#30456;&#27604;&#65292;&#20063;&#22312;&#25105;&#20204;&#30340;&#21512;&#25104;&#23545;&#25239;&#31034;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#26410;&#26469;&#22238;&#21512;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;ANLI&#19978;&#65292;&#25105;&#20204;&#30475;&#21040;&#20102;&#23545;&#24403;&#21069;&#25915;&#20987;&#38598;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65288;44.1% -&gt; 50.1%&#65289;&#65292;&#20197;&#21450;&#23545;&#20004;&#20010;&#26410;&#35265;&#36807;&#30340;&#20154;&#31867;&#29983;&#25104;&#25915;&#20987;&#22238;&#21512;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65288;32.5% -&gt; 43%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\,\to\,$43
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12595</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal Similarity-Based Hierarchical Bayesian Models. (arXiv:2310.12595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#23545;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#30001;&#30456;&#20851;&#20219;&#21153;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#33021;&#22312;&#22240;&#26524;&#26426;&#21046;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22797;&#26434;&#30142;&#30149;&#30340;&#35266;&#23519;&#24615;&#21307;&#23398;&#25968;&#25454;&#22312;&#19981;&#21516;&#24739;&#32773;&#38388;&#20855;&#26377;&#30142;&#30149;&#22240;&#26524;&#26426;&#21046;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#32473;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#26032;&#24739;&#32773;&#36827;&#34892;&#27867;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;&#22788;&#29702;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#21253;&#25324;&#20026;&#25972;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#65292;&#25110;&#32773;&#21033;&#29992;&#20998;&#23618;&#12289;&#20803;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20174;&#27719;&#38598;&#30340;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#31181;&#36890;&#29992;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The key challenge underlying machine learning is generalisation to new data. This work studies generalisation for datasets consisting of related tasks that may differ in causal mechanisms. For example, observational medical data for complex diseases suffers from heterogeneity in causal mechanisms of disease across patients, creating challenges for machine learning algorithms that need to generalise to new patients outside of the training dataset. Common approaches for learning supervised models with heterogeneous datasets include learning a global model for the entire dataset, learning local models for each tasks' data, or utilising hierarchical, meta-learning and multi-task learning approaches to learn how to generalise from data pooled across multiple tasks. In this paper we propose causal similarity-based hierarchical Bayesian models to improve generalisation to new tasks by learning how to pool data from training tasks with similar causal mechanisms. We apply this general modelling
&lt;/p&gt;</description></item><item><title>NeuroCUT&#26159;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#30340;&#22270;&#20998;&#21306;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65292;&#21363;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11787</link><description>&lt;p&gt;
NeuroCUT&#65306;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#22270;&#20998;&#21306;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroCUT: A Neural Approach for Robust Graph Partitioning. (arXiv:2310.11787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11787
&lt;/p&gt;
&lt;p&gt;
NeuroCUT&#26159;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#30340;&#22270;&#20998;&#21306;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65292;&#21363;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#21306;&#26088;&#22312;&#23558;&#22270;&#20998;&#21106;&#20026;k&#20010;&#19981;&#30456;&#20132;&#30340;&#23376;&#38598;&#65292;&#21516;&#26102;&#20248;&#21270;&#29305;&#23450;&#30340;&#20998;&#21306;&#30446;&#26631;&#12290;&#30001;&#20110;&#20854;&#32452;&#21512;&#24615;&#36136;&#65292;&#22823;&#37096;&#20998;&#19982;&#22270;&#20998;&#21306;&#30456;&#20851;&#30340;&#38382;&#39064;&#37117;&#21576;&#29616;&#20986;NP&#38590;&#24230;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#26102;&#24102;&#26377;&#36817;&#20284;&#20445;&#35777;&#65292;&#26377;&#26102;&#21017;&#27809;&#26377;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#26041;&#27861;&#38024;&#23545;&#29305;&#23450;&#30340;&#20998;&#21306;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#24050;&#30693;&#30340;&#25991;&#29486;&#20013;&#30340;&#20998;&#21306;&#30446;&#26631;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#31070;&#32463;&#26041;&#27861;&#24212;&#36816;&#32780;&#29983;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;NeuroCut&#25193;&#23637;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#12290;NeuroCut&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#36825;&#20123;&#20449;&#24687;&#22312;&#26597;&#35810;&#26102;&#25552;&#20379;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning base
&lt;/p&gt;</description></item><item><title>ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2310.09298</link><description>&lt;p&gt;
ByteStack-ID: &#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#21033;&#29992;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09298
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;"ByteStack-ID"&#65292;&#19968;&#31181;&#19987;&#20026;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;ByteStack-ID&#26680;&#24515;&#26159;&#21033;&#29992;&#20174;&#36127;&#36733;&#25968;&#25454;&#30340;&#39057;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#21253;&#32423;&#20449;&#24687;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#37327;&#25968;&#25454;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#22522;&#26412;&#22534;&#21472;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;ByteStack-ID&#19982;&#20256;&#32479;&#30340;&#22534;&#21472;&#26041;&#27861;&#19981;&#21516;&#12290;&#23427;&#23558;&#38468;&#21152;&#30340;&#20803;&#23398;&#20064;&#22120;&#23618;&#26080;&#32541;&#38598;&#25104;&#21040;&#36830;&#25509;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#20013;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Fourier&#31070;&#32463;&#25805;&#20316;&#31526;(FNO)&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;FNO&#29256;&#26412;&#30340;He&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#27169;&#24335;&#25130;&#26029;&#21644;&#23494;&#38598;&#36830;&#25509;&#32593;&#32476;&#30456;&#20284;&#30340;&#29305;&#28857;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#36127;&#21021;&#22987;&#21270;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06379</link><description>&lt;p&gt;
Fourier&#31070;&#32463;&#25805;&#20316;&#31526;&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#65306;&#37325;&#26032;&#23457;&#35270;&#28151;&#27788;&#36793;&#32536;
&lt;/p&gt;
&lt;p&gt;
Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos. (arXiv:2310.06379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Fourier&#31070;&#32463;&#25805;&#20316;&#31526;(FNO)&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;FNO&#29256;&#26412;&#30340;He&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#27169;&#24335;&#25130;&#26029;&#21644;&#23494;&#38598;&#36830;&#25509;&#32593;&#32476;&#30456;&#20284;&#30340;&#29305;&#28857;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#36127;&#21021;&#22987;&#21270;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Fourier&#31070;&#32463;&#25805;&#20316;&#31526;(FNO)&#30340;&#21021;&#22987;&#21270;&#20559;&#24046;&#12290;&#24314;&#31435;&#20102;&#19968;&#20010;&#38024;&#23545;FNO&#30340;&#24179;&#22343;&#22330;&#29702;&#35770;&#65292;&#20174;&#8220;&#28151;&#27788;&#36793;&#32536;&#8221;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#38543;&#26426;FNO&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#34892;&#20026;&#34920;&#29616;&#20986;&#19982;FNO&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#36825;&#26159;&#30001;&#27169;&#24335;&#25130;&#26029;&#24341;&#36215;&#30340;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#19982;&#23494;&#38598;&#36830;&#25509;&#32593;&#32476;&#30456;&#20284;&#30340;&#29305;&#28857;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;FNO&#29256;&#26412;&#30340;He&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#36127;&#21021;&#22987;&#21270;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#21021;&#22987;&#21270;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#24471;32&#23618;FNO&#30340;&#35757;&#32451;&#31283;&#23450;&#65292;&#26080;&#38656;&#39069;&#22806;&#25216;&#26415;&#25110;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the initialization bias of the Fourier neural operator (FNO). A mean-field theory for FNO is established, analyzing the behavior of the random FNO from an ``edge of chaos'' perspective. We uncover that the forward and backward propagation behaviors exhibit characteristics unique to FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Building upon this observation, we also propose a FNO version of the He initialization scheme to mitigate the negative initialization bias leading to training instability. Experimental results demonstrate the effectiveness of our initialization scheme, enabling stable training of a 32-layer FNO without the need for additional techniques or significant performance degradation.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26080;&#26631;&#35760;&#30340;&#22495;&#22806;&#25968;&#25454;&#32435;&#20837;&#21322;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#19982;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#20102;&#39640;&#25928;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#35813;&#26694;&#26550;&#22312;&#39640;&#26031;&#28151;&#21512;&#20998;&#31867;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.00027</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#30340;&#22495;&#22806;&#25968;&#25454;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlabeled Out-Of-Domain Data Improves Generalization. (arXiv:2310.00027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00027
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26080;&#26631;&#35760;&#30340;&#22495;&#22806;&#25968;&#25454;&#32435;&#20837;&#21322;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#19982;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#20102;&#39640;&#25928;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#35813;&#26694;&#26550;&#22312;&#39640;&#26031;&#28151;&#21512;&#20998;&#31867;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#21322;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#26368;&#23567;&#21270;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#25110;&#38750;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#26223;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20801;&#35768;&#26080;&#26631;&#35760;&#26679;&#26412;&#22312;&#24635;&#21464;&#24046;&#24847;&#20041;&#19978;&#30053;&#24494;&#20559;&#31163;&#22495;&#20869;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#19982;&#33258;&#30417;&#30563;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#20102;&#35757;&#32451;&#38454;&#27573;&#30340;&#39640;&#25928;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22312;$\mathbb{R}^d$&#20013;&#30340;&#20004;&#20010;&#39640;&#26031;&#28151;&#21512;&#20998;&#31867;&#38382;&#39064;&#65292;&#38500;&#20102;&#26469;&#33258;&#30495;&#23454;&#20998;&#24067;&#30340;$m$&#20010;&#29420;&#31435;&#26631;&#35760;&#26679;&#26412;&#20043;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#19968;&#32452;$n$&#20010;&#65288;&#36890;&#24120;$n\gg m$&#65289;&#22495;&#22806;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;&#24050;&#30693;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#65292;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;$\propto\left(d/m\right)$&#36827;&#34892;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training. As a result, we also leverage efficient polynomial-time algorithms for the training stage. From a theoretical standpoint, we apply our framework on the classification problem of a mixture of two Gaussians in $\mathbb{R}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\gg m$) out of domain and unlabeled samples are gievn as well. Using only the labeled data, it is known that the generalization error can be bounded by $\propto\left(d/m\right
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.14053</link><description>&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#25209;&#37327;&#35757;&#32451;&#27867;&#21270;&#24615;&#33021;&#30340;LARS&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20351;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#32553;&#25918;&#27604;(LARS)&#26469;&#25506;&#32034;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#20855;&#26377;&#28909;&#36523;&#38454;&#27573;&#30340;LARS&#31639;&#27861;&#30001;&#20110;&#20887;&#20313;&#30340;&#27604;&#20363;&#32553;&#25918;&#23548;&#33268;&#22312;&#26089;&#26399;&#38519;&#20837;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21518;&#26399;&#22266;&#23450;&#30340;&#38497;&#23789;&#19979;&#38477;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#36941;&#21382;&#26089;&#26399;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Time Varying LARS (TVLARS)&#65292;&#23427;&#29992;&#21487;&#37197;&#32622;&#30340;&#31867;&#20284;sigmoid&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#22312;&#21021;&#22987;&#38454;&#27573;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;TVLARS&#22312;&#26089;&#26399;&#20419;&#36827;&#20102;&#26799;&#24230;&#25506;&#32034;&#65292;&#36229;&#36234;&#20102;&#23574;&#38160;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#36880;&#28176;&#36807;&#28193;&#21040;LARS&#20197;&#23454;&#29616;&#21518;&#26399;&#30340;&#31283;&#20581;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#22987;&#32456;&#20248;&#20110;LARS&#21644;LAMB&#65292;&#20998;&#31867;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#36798;&#21040;2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26696;&#20363;&#20013;&#65292;TVLARS&#37117;&#32988;&#36807;&#20102;LARS&#21644;LAMB&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#21319;&#20102;
&lt;/p&gt;
&lt;p&gt;
This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#23545;&#35937;&#26816;&#27979;&#20013;&#20266;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#21644;&#37327;&#21270;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09599</link><description>&lt;p&gt;
MEDL-U&#65306;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;3D&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential Deep Learning. (arXiv:2309.09599v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#23545;&#35937;&#26816;&#27979;&#20013;&#20266;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#29983;&#25104;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#21644;&#37327;&#21270;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22522;&#20110;3D&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#35201;&#27714;&#24341;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#25361;&#25112;&#65292;&#36825;&#36890;&#24120;&#26159;&#32321;&#37325;&#19988;&#32791;&#26102;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#20986;&#29616;&#20102;&#20960;&#31181;&#24369;&#30417;&#30563;&#30340;3D&#23545;&#35937;&#26816;&#27979;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#21253;&#21547;&#22122;&#22768;&#65292;&#19981;&#22914;&#20154;&#24037;&#26631;&#27880;&#30340;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#35299;&#20915;&#20266;&#26631;&#31614;&#20013;&#22266;&#26377;&#27169;&#31946;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEDL-U&#65292;&#23427;&#26159;&#22522;&#20110;MTrans&#30340;EDL&#26694;&#26550;&#65292;&#19981;&#20165;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#36824;&#37327;&#21270;&#20102;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;EDL&#24212;&#29992;&#20110;3D&#23545;&#35937;&#26816;&#27979;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;(1)&#30456;&#23545;&#36739;&#20302;&#30340;&#20266;&#26631;&#31614;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three primary challenges: (1) relatively lower pseudolab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35777;&#26126;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#22478;&#24066;&#21464;&#21270;&#30417;&#27979;&#38382;&#39064;&#30340;&#21487;&#34892;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#21644;&#20809;&#23398;&#22810;&#20809;&#35889;&#35266;&#27979;&#25968;&#25454;&#65292;&#25104;&#21151;&#30417;&#27979;&#20102;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#22312;&#20420;&#20044;&#20914;&#31361;&#24320;&#22987;&#38454;&#27573;&#30340;&#30456;&#20851;&#22478;&#24066;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.08607</link><description>&lt;p&gt;
&#22312;2022/23&#24180;&#30417;&#27979;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#30340;&#22478;&#24066;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Monitoring Urban Changes in Mariupol/Ukraine in 2022/23. (arXiv:2309.08607v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35777;&#26126;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#22478;&#24066;&#21464;&#21270;&#30417;&#27979;&#38382;&#39064;&#30340;&#21487;&#34892;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#21644;&#20809;&#23398;&#22810;&#20809;&#35889;&#35266;&#27979;&#25968;&#25454;&#65292;&#25104;&#21151;&#30417;&#27979;&#20102;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#22312;&#20420;&#20044;&#20914;&#31361;&#24320;&#22987;&#38454;&#27573;&#30340;&#30456;&#20851;&#22478;&#24066;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#30417;&#27979;&#22478;&#24066;&#21464;&#21270;&#30340;&#33021;&#21147;&#20855;&#26377;&#24040;&#22823;&#30340;&#31038;&#20250;&#32463;&#27982;&#21033;&#30410;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#33021;&#23637;&#31034;&#22312;&#35757;&#32451;&#25110;&#36801;&#31227;&#39046;&#22495;&#20043;&#22806;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#26412;&#30740;&#31350;&#22312;&#29616;&#26377;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20173;&#28982;&#21487;&#20197;&#23545;&#20197;&#21518;&#30340;&#24180;&#20221;&#36827;&#34892;&#22478;&#24066;&#21464;&#21270;&#30417;&#27979;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#23545;&#20844;&#20849;&#21644;&#20813;&#36153;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#35775;&#38382;&#26377;&#38480;&#30340;&#24773;&#20917;&#26469;&#25351;&#23548;&#36801;&#31227;&#12290;&#20026;&#20102;&#25552;&#20379;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#65292;&#25105;&#20204;&#30340;&#30417;&#27979;&#26041;&#27861;&#30340;&#26680;&#24515;&#25968;&#25454;&#21253;&#25324;&#26469;&#33258;Sentinel 1&#65288;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65289;&#21644;Sentinel 2&#65288;&#20809;&#23398;&#22810;&#20809;&#35889;&#65289;&#30340;&#22810;&#27169;&#24577;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#21644;&#20809;&#23398;&#22810;&#20809;&#35889;&#35266;&#27979;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#23454;&#38469;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#30417;&#27979;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#19982;&#20420;&#20044;&#20914;&#31361;&#24320;&#22987;&#26102;&#30340;&#30456;&#20851;&#22478;&#24066;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to constantly monitor urban changes is of large socio-economic interest. Previous works have already shown approaches in this field with the use of Deep Neural Networks (DNNs) and transfer learning. However, they fell short in demonstrating temporal scale outside of either the training or transfer domain.  This work builds on existing research and proves that transfer learning with the use of historic data is a feasible solution, which still allows the urban change monitoring of later years. We considered a case with limited access to public and free Very High Resolution (VHR) imagery to guide the transfer. To provide a high temporal resolution, the core data of our monitoring method comprised multi-modal Synthetic Aperture Radar (SAR) and optical multispectral observations from Sentinel 1 and Sentinel 2, respectively.  We chose a practical application of our methods for monitoring urban-related changes in the city of Mariupol in Ukraine during the beginning of the Russo-Uk
&lt;/p&gt;</description></item><item><title>ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06651</link><description>&lt;p&gt;
ConR: &#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConR: Contrastive Regularizer for Deep Imbalanced Regression. (arXiv:2309.06651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06651
&lt;/p&gt;
&lt;p&gt;
ConR&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#20854;&#22810;&#25968;&#37051;&#23621;&#20013;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#24067;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#24456;&#24120;&#35265;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#20102;&#32422;&#26463;&#65292;&#20197;&#34920;&#31034;&#23569;&#25968;&#31867;&#21035;&#26631;&#31614;&#24182;&#36991;&#20813;&#23545;&#22810;&#25968;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;&#22823;&#37327;&#30340;&#19981;&#24179;&#34913;&#26041;&#27861;&#22788;&#29702;&#20102;&#20998;&#31867;&#26631;&#31614;&#31354;&#38388;&#65292;&#20294;&#22312;&#36830;&#32493;&#26631;&#31614;&#31354;&#38388;&#30340;&#22238;&#24402;&#38382;&#39064;&#19978;&#26410;&#33021;&#26377;&#25928;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#36830;&#32493;&#26631;&#31614;&#20043;&#38388;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20851;&#32852;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26377;&#25928;&#24314;&#27169;&#20851;&#31995;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConR&#65292;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#26631;&#31614;&#30456;&#20284;&#24615;&#65292;&#38450;&#27490;&#23569;&#25968;&#26679;&#26412;&#30340;&#29305;&#24449;&#34987;&#25240;&#21472;&#21040;&#23427;&#20204;&#30340;&#22810;&#25968;&#37051;&#23621;&#20013;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#30340;&#30456;&#20284;&#24615;&#20316;&#20026;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#25351;&#31034;&#22120;&#65292;ConR&#21306;&#20998;&#20102;&#26631;&#31614;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23545;&#36825;&#20123;&#19981;&#19968;&#33268;&#26045;&#21152;&#24809;&#32602;&#12290;ConR&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#31574;&#30053;&#20851;&#27880;&#26631;&#31614;&#31354;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#21442;&#25968;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#27169;&#22411;&#35757;&#32451;&#19981;&#21463;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39564;&#35777;&#21644;&#24212;&#29992;&#23454;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#26085;&#26412;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.03447</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#23485;&#24102;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;: &#24320;&#21457;&#19982;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation. (arXiv:2309.03447v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#21442;&#25968;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#27169;&#22411;&#35757;&#32451;&#19981;&#21463;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39564;&#35777;&#21644;&#24212;&#29992;&#23454;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#26085;&#26412;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#65288;GANO&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#24378;&#38663;&#21160;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26681;&#25454;&#30697;&#38663;&#32423;&#65288;M&#65289;&#12289;&#26029;&#35010;&#36317;&#31163;&#65288;R_{rup}&#65289;&#12289;&#39030;&#37096;30m&#22788;&#30340;&#26102;&#38388;&#24179;&#22343;&#21098;&#20999;&#27874;&#36895;&#24230;&#65288;V_{S30}&#65289;&#21644;&#26500;&#36896;&#29615;&#22659;&#25110;&#26029;&#23618;&#31867;&#22411;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#65292;&#36825;&#26159;&#19968;&#31181;&#20998;&#36776;&#29575;&#26080;&#20851;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#35757;&#32451;&#19982;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#26080;&#20851;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#31639;&#27861;&#65288;&#20197;&#19979;&#31616;&#31216;cGM-GANO&#65289;&#24182;&#35752;&#35770;&#20854;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#21335;&#21152;&#24030;&#22320;&#38663;&#20013;&#24515;&#65288;SCEC&#65289;&#23485;&#24102;&#24179;&#21488;&#65288;BBP&#65289;&#20135;&#29983;&#30340;&#27169;&#25311;&#22320;&#38663;&#21160;&#39564;&#35777;&#20102;cGM-GANO&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26085;&#26412;&#30340;KiK-net&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;cGM-GANO&#65292;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#37325;&#26032;&#29983;&#25104;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven model for ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) that combines recent advancements in machine learning and open access strong motion data sets to generate three-component acceleration time histories conditioned on moment magnitude ($M$), rupture distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and tectonic environment or style of faulting. We use Neural Operators, a resolution invariant architecture that guarantees that the model training is independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (referred to heretofore as cGM-GANO) and discuss its advantages compared to previous work. Next, we verify the cGM-GANO framework using simulated ground motions generated with the Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the framework can rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;Bandit&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#38544;&#31169;&#39044;&#31639;&#19979;&#30340;&#38590;&#24230;&#21306;&#22495;&#65292;&#24182;&#21457;&#29616;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#27604;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#26356;&#26377;&#25928;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00557</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#21644;&#38598;&#20013;&#24335;&#24046;&#20998;&#38544;&#31169;&#22312;Bandit&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interactive and Concentrated Differential Privacy for Bandits. (arXiv:2309.00557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;Bandit&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#38544;&#31169;&#39044;&#31639;&#19979;&#30340;&#38590;&#24230;&#21306;&#22495;&#65292;&#24182;&#21457;&#29616;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#27604;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#26356;&#26377;&#25928;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bandit&#38382;&#39064;&#22312;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#26696;&#21644;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#25935;&#24863;&#30340;&#29992;&#25143;&#25968;&#25454;&#65292;&#22240;&#27492;&#38544;&#31169;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20132;&#20114;&#24335;&#24046;&#20998;&#38544;&#31169;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22522;&#20110;&#21487;&#20449;&#38598;&#20013;&#24335;&#20915;&#31574;&#32773;&#30340;Bandit&#38382;&#39064;&#30340;&#38544;&#31169;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#32431;&#949;-&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#30340;Bandit&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#22312;&#29702;&#35299;&#38646;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;(zCDP)&#30340;Bandit&#38382;&#39064;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#38024;&#23545;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36951;&#25022;&#30340;&#26368;&#23567;&#26368;&#22823;&#21644;&#38382;&#39064;&#30456;&#20851;&#19979;&#30028;&#65292;&#20174;&#32780;&#37327;&#21270;&#20102;&#36825;&#20123;&#24773;&#20917;&#19979;&#961;-&#20840;&#23616;zCDP&#30340;&#20195;&#20215;&#12290;&#36825;&#20123;&#19979;&#30028;&#25581;&#31034;&#20102;&#22522;&#20110;&#38544;&#31169;&#39044;&#31639;&#961;&#30340;&#20004;&#20010;&#22256;&#38590;&#21306;&#22495;&#65292;&#24182;&#34920;&#26126;&#961;-&#20840;&#23616;zCDP&#27604;&#32431;&#949;-&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#20135;&#29983;&#30340;&#36951;&#25022;&#26356;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#30340;&#961;-&#20840;&#23616;zCDP&#31639;&#27861;&#65292;&#21363;AdaC-UCB&#21644;AdaC-GOPE&#12290;&#36825;&#20004;&#20010;&#31639;&#27861;&#37117;&#20351;&#29992;&#20102;&#39640;&#26031;&#26426;&#21046;&#30340;&#20849;&#21516;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDP incurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13498</link><description>&lt;p&gt;
&#36867;&#31163;&#26679;&#26412;&#38519;&#38449;&#65306;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;PaiDEs&#65289;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#21033;&#29992;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#37197;&#23545;&#36317;&#31163;&#26469;&#24314;&#31435;&#29109;&#30340;&#36793;&#30028;&#65292;&#24182;&#23558;&#36825;&#20123;&#36793;&#30028;&#20316;&#20026;&#22522;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#26368;&#36817;&#22522;&#20110;&#26679;&#26412;&#30340;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#29992;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;PaiDEs&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#65288;&#26368;&#22810;100&#20493;&#65289;&#19978;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#26368;&#22810;100&#20493;&#65289;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35780;&#20272;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#65306;&#19968;&#32500;&#27491;&#24358;&#25968;&#25454;&#65292;&#25670;&#21160;&#29289;&#20307;&#65288;Pendulum-v0&#65289;&#65292;&#36339;&#36291;&#26426;&#22120;&#20154;&#65288;Hopper-v2&#65289;&#65292;&#34434;&#34433;&#26426;&#22120;&#20154;&#65288;Ant-v2&#65289;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;Humanoid-v2&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26469;&#23637;&#31034;PaiDEs&#22312;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23618;&#24402;&#19968;&#21270;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#20559;&#31227;&#38382;&#39064;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#29305;&#24449;&#24402;&#19968;&#21270;&#65292;&#20351;&#24471;&#23545;&#20005;&#37325;&#20542;&#26012;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21152;&#36895;&#20840;&#23616;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#26497;&#31471;&#26631;&#31614;&#20559;&#31227;&#19979;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.09565</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#29702;&#35299;&#26497;&#31471;&#26631;&#31614;&#20559;&#31227;&#19979;&#30340;&#23618;&#24402;&#19968;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift. (arXiv:2308.09565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23618;&#24402;&#19968;&#21270;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#20559;&#31227;&#38382;&#39064;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#29992;&#29305;&#24449;&#24402;&#19968;&#21270;&#65292;&#20351;&#24471;&#23545;&#20005;&#37325;&#20542;&#26012;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21152;&#36895;&#20840;&#23616;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#26497;&#31471;&#26631;&#31614;&#20559;&#31227;&#19979;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#24402;&#19968;&#21270;&#65288;LN&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#22312;&#22522;&#30784;&#27169;&#22411;&#30340;&#26102;&#20195;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;LN&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#36215;&#20316;&#29992;&#20173;&#28982;&#26159;&#20010;&#35868;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23618;&#24402;&#19968;&#21270;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#20559;&#31227;&#38382;&#39064;&#20043;&#38388;&#30340;&#28145;&#21051;&#32852;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;FL&#20013;&#30340;&#23618;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35268;&#33539;&#21270;&#26041;&#27861;&#22312;FL&#20013;&#30340;&#20851;&#38190;&#36129;&#29486;&#26426;&#21046;&#65292;&#31216;&#20043;&#20026;&#29305;&#24449;&#24402;&#19968;&#21270;&#65288;FN&#65289;&#65292;&#23427;&#22312;&#20998;&#31867;&#22120;&#22836;&#20043;&#21069;&#23558;&#24402;&#19968;&#21270;&#24212;&#29992;&#20110;&#28508;&#22312;&#29305;&#24449;&#34920;&#31034;&#12290;&#34429;&#28982;LN&#21644;FN&#19981;&#20250;&#25552;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#25511;&#21046;&#29305;&#24449;&#23849;&#28291;&#21644;&#23616;&#37096;&#36807;&#25311;&#21512;&#65292;&#20351;&#24471;&#23545;&#20005;&#37325;&#20542;&#26012;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21152;&#36895;&#20840;&#23616;&#35757;&#32451;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#35268;&#33539;&#21270;&#22312;&#26497;&#31471;&#26631;&#31614;&#20559;&#31227;&#19979;&#21487;&#20197;&#24341;&#36215;&#26631;&#20934;&#22522;&#20934;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21106;&#38500;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to unde
&lt;/p&gt;</description></item><item><title>MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;</title><link>http://arxiv.org/abs/2308.06686</link><description>&lt;p&gt;
MDB&#65306;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MDB: Interactively Querying Datasets and Models. (arXiv:2308.06686v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06686
&lt;/p&gt;
&lt;p&gt;
MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#24320;&#21457;&#32773;&#38656;&#35201;&#33021;&#22815;&#31995;&#32479;&#22320;&#35843;&#35797;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MDB&#65292;&#19968;&#20010;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#35843;&#35797;&#26694;&#26550;&#12290;MDB&#36890;&#36807;&#23558;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#34920;&#36798;&#24615;&#26597;&#35810;&#30340;&#24037;&#20855;&#12290;&#26597;&#35810;&#21487;&#37325;&#29992;&#19988;&#26131;&#20110;&#20462;&#25913;&#65292;&#20351;&#24471;&#35843;&#35797;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#20197;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#26816;&#27979;&#12289;&#20559;&#24046;&#21457;&#29616;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#25968;&#25454;&#22635;&#20805;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;MDB&#22312;&#33258;&#21160;&#39550;&#39542;&#35270;&#39057;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#30103;&#35760;&#24405;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26368;&#39640;10&#20493;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;40%&#30340;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#21457;&#32773;&#33021;&#22815;&#25104;&#21151;&#26500;&#24314;&#22797;&#26434;&#26597;&#35810;&#26469;&#25551;&#36848;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20174;&#19968;&#20010;G-&#26368;&#20248;&#35774;&#35745;&#20013;&#38543;&#26426;&#36873;&#25321;&#33218;&#26469;&#23454;&#29616;&#26368;&#20339;&#33218;&#30340;&#40065;&#26834;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2307.15154</link><description>&lt;p&gt;
A/B&#27979;&#35797;&#21644;&#20855;&#26377;&#38750;&#31283;&#24577;&#40065;&#26834;&#24615;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity. (arXiv:2307.15154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20174;&#19968;&#20010;G-&#26368;&#20248;&#35774;&#35745;&#20013;&#38543;&#26426;&#36873;&#25321;&#33218;&#26469;&#23454;&#29616;&#26368;&#20339;&#33218;&#30340;&#40065;&#26834;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#33021;&#23384;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#32473;&#23450;&#26377;&#38480;&#33218;&#38598;&#21512;X&#65292;&#22266;&#23450;&#39044;&#31639;T&#20197;&#21450;&#19981;&#21487;&#39044;&#27979;&#30340;&#21442;&#25968;&#24207;&#21015;&#952;&#65292;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#20339;&#33218;x*&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#22312;&#31283;&#24577;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#38169;&#35823;&#27010;&#29575;&#38543;&#30528;&#39044;&#31639;&#30340;&#22686;&#21152;&#32780;&#25351;&#25968;&#19979;&#38477;&#12290;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;A/B/n&#22810;&#21464;&#37327;&#27979;&#35797;&#22330;&#26223;&#20013;&#65292;&#29615;&#22659;&#26159;&#38750;&#31283;&#24577;&#30340;&#65292;&#32780;&#19968;&#20010;&#26399;&#26395;&#31283;&#24577;&#30340;&#31639;&#27861;&#24456;&#23481;&#26131;&#22833;&#36133;&#12290;&#20026;&#20102;&#20855;&#26377;&#40065;&#26834;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20174;X&#30340;&#19968;&#20010;G-&#26368;&#20248;&#35774;&#35745;&#20013;&#20197;&#38543;&#26426;&#21644;&#38750;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#36873;&#25321;&#33218;&#65292;&#37027;&#20040;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#33218;&#30340;&#40065;&#26834;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the fixed-budget best-arm identification (BAI) problem for linear bandits in a potentially non-stationary environment. Given a finite arm set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an algorithm will aim to correctly identify the best arm $x^* := \arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as high as possible. Prior work has addressed the stationary setting where $\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability decreases as $\exp(-T /\rho^*)$ for a problem-dependent constant $\rho^*$. But in many real-world $A/B/n$ multivariate testing scenarios that motivate our work, the environment is non-stationary and an algorithm expecting a stationary setting can easily fail. For robust identification, it is well-known that if arms are chosen randomly and non-adaptively from a G-optimal design over $\mathcal{X}$ at each 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10187</link><description>&lt;p&gt;
&#38544;&#31169;&#25918;&#22823;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23376;&#37319;&#26679;&#20316;&#20026;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#24615;&#36136;&#12290;&#36825;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#36890;&#36807;&#23376;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#30340;&#32467;&#26524;&#21040;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#20026;&#20854;&#34987;&#36873;&#25321;&#27010;&#29575;&#30340;&#20498;&#25968;&#12290;&#27599;&#20010;&#28857;&#30340;&#36873;&#25321;&#27010;&#29575;&#30340;&#26435;&#37325;&#23545;&#38544;&#31169;&#30340;&#24433;&#21709;&#24182;&#19981;&#26126;&#26174;&#12290;&#19968;&#26041;&#38754;&#65292;&#36739;&#20302;&#30340;&#36873;&#25321;&#27010;&#29575;&#20250;&#23548;&#33268;&#26356;&#24378;&#30340;&#38544;&#31169;&#25918;&#22823;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26435;&#37325;&#36234;&#39640;&#65292;&#22312;&#28857;&#34987;&#36873;&#25321;&#26102;&#65292;&#28857;&#23545;&#26426;&#21046;&#36755;&#20986;&#30340;&#24433;&#21709;&#23601;&#36234;&#24378;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36825;&#20004;&#20010;&#24433;&#21709;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#21516;&#26102;&#27604;&#22343;&#21248;&#23376;&#37319;&#26679;&#20855;&#26377;&#26356;&#24378;&#30340;&#38544;&#31169;&#21644;&#26356;&#22909;&#30340;&#25928;&#29992;&#65292;&#24182;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#21644;&#35299;&#20915;&#20102;&#38544;&#31169;&#20248;&#21270;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;...
&lt;/p&gt;
&lt;p&gt;
We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2307.03577</link><description>&lt;p&gt;
&#21487;&#32534;&#31243;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#12289;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#25454;&#20849;&#20139;&#30340;&#38480;&#21046;&#65292;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#20173;&#28982;&#34987;&#20302;&#25928;&#21033;&#29992;&#12290;&#23613;&#31649;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#21407;&#22987;&#20998;&#24067;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#24212;&#29992;&#31243;&#24207;&#36824;&#38656;&#35201;&#39069;&#22806;&#30340;&#29983;&#25104;&#25968;&#25454;&#32422;&#26463;&#12290;&#29616;&#26377;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#22788;&#29702;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20363;&#22914;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25110;&#22686;&#21152;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#32570;&#20047;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#25509;&#21475;&#26469;&#22768;&#26126;&#19968;&#33324;&#35268;&#33539;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ProgSyn&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#24182;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#65292;ProgSyn&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#25552;&#20379;&#30340;&#35268;&#33539;&#19978;&#33258;&#21160;&#25512;&#23548;&#20986;&#21487;&#24494;&#20998;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#35268;&#33539;&#21487;&#20197;&#20351;&#29992;&#32479;&#35745;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09739</link><description>&lt;p&gt;
&#23398;&#20064;&#21463;&#38480;&#21160;&#21147;&#23398;&#30340;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stabilized Neural Differential Equations for Learning Constrained Dynamics. (arXiv:2306.09739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#31283;&#23450;&#39033;&#20351;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#20174;&#25968;&#25454;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#25512;&#26029;&#20986;&#30340;&#21160;&#24577;&#31995;&#32479;&#20445;&#30041;&#24050;&#30693;&#32422;&#26463;&#26465;&#20214;&#65288;&#20363;&#22914;&#23432;&#24658;&#23450;&#24459;&#25110;&#23545;&#20801;&#35768;&#30340;&#31995;&#32479;&#29366;&#24577;&#30340;&#38480;&#21046;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;SNDEs&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#24378;&#21046;&#20351;&#29992;&#20219;&#24847;&#27969;&#24418;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#31283;&#23450;&#39033;&#65292;&#24403;&#28155;&#21152;&#21040;&#21407;&#22987;&#21160;&#24577;&#31995;&#32479;&#20013;&#26102;&#65292;&#21487;&#20197;&#23558;&#32422;&#26463;&#27969;&#24418;&#25104;&#20026;&#28176;&#36827;&#31283;&#23450;&#30340;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25152;&#26377;&#24120;&#35265;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#27169;&#22411;&#20860;&#23481;&#24182;&#24191;&#27867;&#36866;&#29992;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;SNDE&#22312;&#25193;&#23637;&#21487;&#32435;&#20837;NODE&#35757;&#32451;&#30340;&#32422;&#26463;&#31867;&#22411;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.
&lt;/p&gt;</description></item><item><title>&#29992;&#22810;&#23610;&#24230;&#27969;&#36827;&#34892;&#20108;&#32500;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#24314;&#27169;&#65292;&#21487;&#35782;&#21035;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#24182;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04689</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27969;&#29992;&#20110;&#40065;&#26834;&#21644;&#26368;&#20248;&#23431;&#23449;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multiscale Flow for Robust and Optimal Cosmological Analysis. (arXiv:2306.04689v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04689
&lt;/p&gt;
&lt;p&gt;
&#29992;&#22810;&#23610;&#24230;&#27969;&#36827;&#34892;&#20108;&#32500;&#23431;&#23449;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#24314;&#27169;&#65292;&#21487;&#35782;&#21035;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#24182;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#27969;(Convolutional Normalizing Flow)&#29992;&#20110;&#29983;&#25104;&#20108;&#32500;&#23431;&#23449;&#23398;&#25968;&#25454;&#65292;&#24182;&#23545;&#20043;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23567;&#27874;&#22522;&#30784;&#20998;&#35299;&#23431;&#23449;&#23398;&#22330;&#65292;&#28982;&#21518;&#23558;&#19981;&#21516;&#32423;&#21035;&#30340;&#23567;&#27874;&#20998;&#37327;&#20998;&#21035;&#24314;&#27169;&#12290;&#36890;&#36807;&#36880;&#39033;&#27714;&#21644;&#24471;&#20986;&#21407;&#22987;&#23431;&#23449;&#23398;&#22330;&#30340;&#23545;&#25968;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#20998;&#31163;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#24182;&#35782;&#21035;&#20854;&#20013;&#26410;&#30693;&#30340;&#23610;&#24230;&#30456;&#20851;&#31995;&#32479;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Multiscale Flow, a generative Normalizing Flow that creates samples and models the field-level likelihood of two-dimensional cosmological data such as weak lensing. Multiscale Flow uses hierarchical decomposition of cosmological fields via a wavelet basis, and then models different wavelet components separately as Normalizing Flows. The log-likelihood of the original cosmological field can be recovered by summing over the log-likelihood of each wavelet term. This decomposition allows us to separate the information from different scales and identify distribution shifts in the data such as unknown scale-dependent systematics. The resulting likelihood analysis can not only identify these types of systematics, but can also be made optimal, in the sense that the Multiscale Flow can learn the full likelihood at the field without any dimensionality reduction. We apply Multiscale Flow to weak lensing mock datasets for cosmological inference, and show that it significantly outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#65292;&#25512;&#32763;&#20102;&#29616;&#26377;&#25216;&#26415;&#23545;&#36890;&#20449;&#22270;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;D-SGD&#22312;&#20984;&#35774;&#32622;&#20013;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#27867;&#21270;&#30028;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.02939</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#19982;&#27867;&#21270;&#20998;&#26512;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Stability and Generalization Analysis of the Decentralized SGD Algorithm. (arXiv:2306.02939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#25913;&#36827;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20998;&#26512;&#65292;&#25512;&#32763;&#20102;&#29616;&#26377;&#25216;&#26415;&#23545;&#36890;&#20449;&#22270;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;D-SGD&#22312;&#20984;&#35774;&#32622;&#20013;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#27867;&#21270;&#30028;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(D-SGD)&#31639;&#27861;&#30340;&#26032;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;&#26041;&#27861;&#12290;&#24471;&#21040;&#30340;&#32467;&#26524;&#22823;&#22823;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#25512;&#32763;&#20102;&#23427;&#20204;&#20851;&#20110;&#36890;&#20449;&#22270;&#23545;&#27867;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#20363;&#22914;&#65292;&#22312;&#20984;&#35774;&#32622;&#20013;&#65292;&#26080;&#35770;&#22270;&#30340;&#36873;&#25321;&#22914;&#20309;&#65292;D-SGD&#20855;&#26377;&#19982;&#32463;&#20856;SGD&#31639;&#27861;&#30456;&#21516;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21453;&#30452;&#35273;&#30340;&#32467;&#26524;&#26469;&#33258;&#20110;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36825;&#20250;&#38544;&#34255;&#19968;&#20010;&#19982;&#20998;&#24067;&#24335;&#22330;&#26223;&#19981;&#20860;&#23481;&#30340;&#26368;&#32456;&#20840;&#23616;&#24179;&#22343;&#21270;&#27493;&#39588;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#20513;&#23548;&#20998;&#26512;&#26412;&#22320;&#21442;&#25968;&#30340;&#19978;&#30830;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22270;&#30830;&#23454;&#23545;&#27867;&#21270;&#20135;&#29983;&#24433;&#21709;&#12290;&#19982;&#20043;&#21069;&#30340;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21363;&#20351;&#23545;&#20110;&#38750;&#36830;&#25509;&#22270;&#20063;&#33021;&#20135;&#29983;&#38750;&#24179;&#20961;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new generalization error analysis for the Decentralized Stochastic Gradient Descent (D-SGD) algorithm based on algorithmic stability. The obtained results largely improve upon state-of-the-art results, and even invalidate their claims that the communication graph has a detrimental effect on generalization. For instance, we show that in convex settings, D-SGD has the same generalization bounds as the classical SGD algorithm, no matter the choice of graph. We exhibit that this counter-intuitive result comes from considering the average of local parameters, which hides a final global averaging step incompatible with the decentralized scenario. In light of this observation, we advocate to analyze the supremum over local parameters and show that in this case, the graph does have an impact on the generalization. Unlike prior results, our analysis yields non-vacuous bounds even for non-connected graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#37051;&#22495;&#28151;&#28102;&#24230;&#37327;&#26469;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28151;&#28102;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#21306;&#20998;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#65292;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02285</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#28151;&#28102;&#33410;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Clarify Confused Nodes Through Separated Learning. (arXiv:2306.02285v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#37051;&#22495;&#28151;&#28102;&#24230;&#37327;&#26469;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28151;&#28102;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#21306;&#20998;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#65292;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#23548;&#21521;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#36136;&#33410;&#28857;&#65292;&#36825;&#25361;&#25112;&#20102;&#32463;&#20856;GNN&#30340;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#24182;&#38459;&#30861;&#20102;&#20854;&#24615;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#25968;&#20173;&#35774;&#35745;&#20102;&#20855;&#26377;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#38388;&#20849;&#20139;&#26435;&#37325;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20123;&#21162;&#21147;&#20013;&#21253;&#21547;&#20102;&#39640;&#38454;&#20449;&#24687;&#21644;&#22810;&#36890;&#36947;&#26550;&#26500;&#65292;&#20294;&#24448;&#24448;&#25928;&#26524;&#19981;&#20339;&#12290;&#23569;&#25968;&#30740;&#31350;&#23581;&#35797;&#35757;&#32451;&#19981;&#21516;&#33410;&#28857;&#32452;&#30340;&#20998;&#31163;&#23398;&#20064;&#65292;&#20294;&#21463;&#21040;&#20102;&#19981;&#21512;&#36866;&#30340;&#20998;&#31163;&#24230;&#37327;&#21644;&#20302;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#37051;&#22495;&#28151;&#28102;&#65288;NC&#65289;&#65292;&#20197;&#20415;&#26356;&#21487;&#38752;&#22320;&#20998;&#31163;&#33410;&#28857;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20855;&#26377;&#19981;&#21516;NC&#20540;&#30340;&#33410;&#28857;&#32452;&#22312;&#32452;&#20869;&#20934;&#30830;&#24230;&#21644;&#21487;&#35270;&#21270;&#23884;&#20837;&#19978;&#23384;&#22312;&#19968;&#23450;&#24046;&#24322;&#12290;&#36825;&#20026;&#22522;&#20110;&#37051;&#22495;&#28151;&#28102;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;NC-GCN&#65289;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved remarkable advances in graph-oriented tasks. However, real-world graphs invariably contain a certain proportion of heterophilous nodes, challenging the homophily assumption of classical GNNs and hindering their performance. Most existing studies continue to design generic models with shared weights between heterophilous and homophilous nodes. Despite the incorporation of high-order messages or multi-channel architectures, these efforts often fall short. A minority of studies attempt to train different node groups separately but suffer from inappropriate separation metrics and low efficiency. In this paper, we first propose a new metric, termed Neighborhood Confusion (NC), to facilitate a more reliable separation of nodes. We observe that node groups with different levels of NC values exhibit certain differences in intra-group accuracy and visualized embeddings. These pave the way for Neighborhood Confusion-guided Graph Convolutional Network (N
&lt;/p&gt;</description></item><item><title>vFedSec&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;Secure Layer&#65292;&#26088;&#22312;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#32852;&#21512;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#25928;&#26524;&#26174;&#33879;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16794</link><description>&lt;p&gt;
&#36890;&#36807;&#23433;&#20840;&#23618;&#23454;&#29616;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#39640;&#25928;&#23433;&#20840;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
vFedSec: Efficient Secure Aggregation for Vertical Federated Learning via Secure Layer. (arXiv:2305.16794v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16794
&lt;/p&gt;
&lt;p&gt;
vFedSec&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;Secure Layer&#65292;&#26088;&#22312;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#32852;&#21512;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#25928;&#26524;&#26174;&#33879;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20027;&#35201;&#20851;&#27880;&#27178;&#21521;&#21010;&#20998;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;&#35768;&#22810;&#26377;&#36259;&#30340;&#38382;&#39064;&#20013;&#65292;&#20010;&#20307;&#25968;&#25454;&#28857;&#20998;&#25955;&#22312;&#22402;&#30452;&#30340;&#23458;&#25143;&#31471;/&#32452;&#32455;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38656;&#35201;&#21442;&#19982;&#32773;&#20043;&#38388;&#20132;&#25442;&#20013;&#38388;&#36755;&#20986;&#21644;&#26799;&#24230;&#65292;&#33509;&#19981;&#32771;&#34385;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;vFedSec&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#30340;&#23433;&#20840;&#23618;&#35774;&#35745;&#21644;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#24433;&#21709;&#35757;&#32451;&#32489;&#25928;&#65292;&#21516;&#26102;&#26377;&#25928;&#20445;&#25252;&#31169;&#20154;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#30340;&#24212;&#29992;&#24615;&#21644;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most work in privacy-preserving federated learning (FL) has been focusing on horizontally partitioned datasets where clients share the same sets of features and can train complete models independently. However, in many interesting problems, individual data points are scattered across different clients/organizations in a vertical setting. Solutions for this type of FL require the exchange of intermediate outputs and gradients between participants, posing a potential risk of privacy leakage when privacy and security concerns are not considered. In this work, we present vFedSec - a novel design with an innovative Secure Layer for training vertical FL securely and efficiently using state-of-the-art security modules in secure aggregation. We theoretically demonstrate that our method does not impact the training performance while protecting private data effectively. Empirically results also show its applicability with extensive experiments that our design can achieve the protection with negl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$p$-NormSoftmax&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15508</link><description>&lt;p&gt;
&#36890;&#36807;&#20107;&#21518;&#23545;&#25968;&#24402;&#19968;&#21270;&#21644;&#28201;&#24230;&#32553;&#25918;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving selective classification performance of deep neural networks through post-hoc logit normalization and temperature scaling. (arXiv:2305.15508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;$p$-NormSoftmax&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#28508;&#22312;&#38169;&#35823;&#36890;&#36807;&#25918;&#24323;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#38024;&#23545;&#30340;&#26159;&#20248;&#21270;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#35823;&#20998;&#31867;&#26816;&#27979;&#24615;&#33021;&#65292;&#21363;&#36890;&#36807;&#23558;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#20540;&#20998;&#37197;&#32473;&#27491;&#30830;&#30340;&#39044;&#27979;&#26469;&#21306;&#20998;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;$p$-NormSoftmax&#65292;&#36890;&#36807;&#23545;&#25968;&#36827;&#34892;$p$-&#33539;&#25968;&#24402;&#19968;&#21270;&#21644;&#28201;&#24230;&#32553;&#25918;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of selective classification for deep neural networks, where a model is allowed to abstain from low-confidence predictions to avoid potential errors. Specifically, we tackle the problem of optimizing the confidence estimator of a fixed classifier, aiming to enhance its misclassification detection performance, i.e., its ability to discriminate between correct and incorrect predictions by assigning higher confidence values to the correct ones. Previous work has found that different classifiers exhibit varying levels of misclassification detection performance, particularly when using the maximum softmax probability (MSP) as a measure of confidence. However, we argue that these findings are mainly due to a sub-optimal confidence estimator being used for each model. To overcome this issue, we propose a simple and efficient post-hoc confidence estimator, named $p$-NormSoftmax, which consists of transforming the logits through $p$-norm normalization and tempera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCNN&#30340;&#27169;&#22359;&#21270;&#21644;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#21333;&#29420;&#23545;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#30340;&#27599;&#20010;&#25104;&#20998;&#36827;&#34892;&#24314;&#27169;&#12290;SCNN&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;MTS&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;MTS&#25968;&#25454;&#20998;&#35299;&#20026;&#32467;&#26500;&#21270;&#21644;&#24322;&#26500;&#25104;&#20998;&#65292;&#28982;&#21518;&#20998;&#21035;&#25512;&#26029;&#36825;&#20123;&#25104;&#20998;&#30340;&#28436;&#21270;&#65292;&#33021;&#22815;&#23454;&#29616;&#27604;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13036</link><description>&lt;p&gt;
&#23398;&#20064;&#32467;&#26500;&#21270;&#25104;&#20998;&#65306;&#36808;&#21521;&#27169;&#22359;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Structured Components: Towards Modular and Interpretable Multivariate Time Series Forecasting. (arXiv:2305.13036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SCNN&#30340;&#27169;&#22359;&#21270;&#21644;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#21333;&#29420;&#23545;&#31354;&#38388;-&#26102;&#38388;&#27169;&#24335;&#30340;&#27599;&#20010;&#25104;&#20998;&#36827;&#34892;&#24314;&#27169;&#12290;SCNN&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;MTS&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;MTS&#25968;&#25454;&#20998;&#35299;&#20026;&#32467;&#26500;&#21270;&#21644;&#24322;&#26500;&#25104;&#20998;&#65292;&#28982;&#21518;&#20998;&#21035;&#25512;&#26029;&#36825;&#20123;&#25104;&#20998;&#30340;&#28436;&#21270;&#65292;&#33021;&#22815;&#23454;&#29616;&#27604;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19968;&#20010;&#26497;&#20026;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#38382;&#39064;&#12290; MTS&#39044;&#27979;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#31354;&#38388; - &#26102;&#38388;&#27169;&#24335;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#21333;&#29420;&#23545;&#31354;&#38388; - &#26102;&#38388;&#27169;&#24335;&#30340;&#27599;&#20010;&#25104;&#20998;&#36827;&#34892;&#24314;&#27169;&#12290; &#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#21629;&#21517;&#20026;SCNN&#65292;&#32553;&#20889;&#20026;&#32467;&#26500;&#21270;&#22522;&#20110;&#25104;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time-series (MTS) forecasting is a paramount and fundamental problem in many real-world applications. The core issue in MTS forecasting is how to effectively model complex spatial-temporal patterns. In this paper, we develop a modular and interpretable forecasting framework, which seeks to individually model each component of the spatial-temporal patterns. We name this framework SCNN, short for Structured Component-based Neural Network. SCNN works with a pre-defined generative process of MTS, which arithmetically characterizes the latent structure of the spatial-temporal patterns. In line with its reverse process, SCNN decouples MTS data into structured and heterogeneous components and then respectively extrapolates the evolution of these components, the dynamics of which is more traceable and predictable than the original MTS. Extensive experiments are conducted to demonstrate that SCNN can achieve superior performance over state-of-the-art models on three real-world data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SAL&#21644;SCoreBO&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11005</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#33258;&#26657;&#27491;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Bayesian Optimization through Bayesian Active Learning. (arXiv:2304.11005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SAL&#21644;SCoreBO&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#24050;&#25104;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#39640;&#26031;&#36807;&#31243;&#30340;&#23436;&#20840;&#21457;&#25381;&#38656;&#35201;&#24039;&#22937;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#32780;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#26377;&#20851;&#20110;&#25214;&#21040;&#27491;&#30830;&#36229;&#21442;&#25968;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36873;&#25321;&#22909;&#30340;&#36229;&#21442;&#25968;&#23545;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26126;&#30830;&#20248;&#20808;&#32771;&#34385;&#27492;&#30446;&#26631;&#30340;&#25910;&#36141;&#20989;&#25968;&#12290;&#32479;&#35745;&#36317;&#31163;&#20027;&#21160;&#23398;&#20064;&#65288;SAL&#65289;&#32771;&#34385;&#21518;&#39564;&#26679;&#26412;&#30340;&#24179;&#22343;&#19981;&#19968;&#33268;&#24615;&#65292;&#30001;&#32479;&#35745;&#36317;&#31163;&#27979;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#27979;&#35797;&#20989;&#25968;&#19978;&#65292;&#23427;&#32988;&#36807;&#20102;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#26657;&#27491;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SCoreBO&#65289;&#65292;&#23427;&#23558;SAL&#25193;&#23637;&#21040;&#21516;&#26102;&#25191;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#36229;&#21442;&#25968;&#23398;&#20064;&#12290;&#30456;&#27604;&#20256;&#32479;BO&#65292;SCoreBO&#20197;&#25913;&#36827;&#30340;&#36895;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#22312;&#26368;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25628;&#32034;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are cemented as the model of choice in Bayesian optimization and active learning. Yet, they are severely dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding the right hyperparameters in the literature. We demonstrate the impact of selecting good hyperparameters for GPs and present two acquisition functions that explicitly prioritize this goal. Statistical distance-based Active Learning (SAL) considers the average disagreement among samples from the posterior, as measured by a statistical distance. It is shown to outperform the state-of-the-art in Bayesian active learning on a number of test functions. We then introduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to perform Bayesian optimization and active hyperparameter learning simultaneously. SCoreBO learns the model hyperparameters at improved rates compared to vanilla BO, while outperforming the latest Bayesian optimization met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36827;&#34892;&#30452;&#25509;&#24494;&#38663;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#32858;&#28966;&#30340;&#28304;&#22270;&#20687;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#30340;&#35760;&#24405;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#19988;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04315</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24494;&#38663;&#28304;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Microseismic source imaging using physics-informed neural networks with hard constraints. (arXiv:2304.04315v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36827;&#34892;&#30452;&#25509;&#24494;&#38663;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#32858;&#28966;&#30340;&#28304;&#22270;&#20687;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#30340;&#35760;&#24405;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#19988;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#38663;&#28304;&#25104;&#20687;&#22312;&#34987;&#21160;&#22320;&#38663;&#30417;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#30001;&#20110;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#23481;&#26131;&#20986;&#29616;&#28151;&#21472;&#38382;&#39064;&#65292;&#23548;&#33268;&#20854;&#24120;&#24120;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#30452;&#25509;&#24494;&#38663;&#25104;&#20687;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#32858;&#28966;&#30340;&#28304;&#22270;&#20687;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#30340;&#35760;&#24405;&#12290;&#25105;&#20204;&#20351;&#29992;PINNs&#34920;&#31034;&#22810;&#39057;&#27874;&#22330;&#65292;&#28982;&#21518;&#24212;&#29992;&#36870;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25552;&#21462;&#28304;&#22270;&#20687;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#30828;&#32422;&#26463;&#26469;&#20462;&#25913;&#39057;&#22495;&#27874;&#22330;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20174;&#32780;&#26412;&#36136;&#19978;&#28385;&#36275;&#36793;&#30028;&#26465;&#20214;&#65288;&#34920;&#23618;&#19978;&#30340;&#27979;&#37327;&#25968;&#25454;&#65289;&#65292;&#36991;&#20813;&#20102;&#22312;PINNs&#20013;&#24179;&#34913;&#25968;&#25454;&#21644;PDE&#25439;&#22833;&#30340;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#28145;&#24230;&#30340;&#22240;&#26524;&#24615;&#25439;&#22833;&#23454;&#29616;&#65292;&#20197;&#25552;&#39640;PINNs&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;Overthrust&#27169;&#22411;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#19988;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microseismic source imaging plays a significant role in passive seismic monitoring. However, such a process is prone to failure due to the aliasing problem when dealing with sparse measured data. Thus, we propose a direct microseismic imaging framework based on physics-informed neural networks (PINNs), which can generate focused source images, even with very sparse recordings. We use the PINNs to represent a multi-frequency wavefield and then apply the inverse Fourier transform to extract the source image. Specially, we modify the representation of the frequency-domain wavefield to inherently satisfy the boundary conditions (the measured data on the surface) by means of the hard constraint, which helps to avoid the difficulty in balancing the data and PDE losses in PINNs. Furthermore, we propose the causality loss implementation with respect to depth to enhance the convergence of PINNs. The numerical experiments on the Overthrust model show that the method can admit reliable and accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#21644;&#27880;&#20837;&#23545;&#25239;&#24615;&#21442;&#25968;&#22122;&#22768;&#65292;&#23558;SNN&#24212;&#29992;&#31243;&#24207;&#31163;&#32447;&#35757;&#32451;&#21644;&#37096;&#32626;&#21040;Dynap-SE2&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#20248;&#21270;&#21518;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#20110;&#30828;&#20214;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12167</link><description>&lt;p&gt;
&#20351;&#29992;Rockpool&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#37096;&#32626;&#21040;&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;Dynap-SE2&#19978;
&lt;/p&gt;
&lt;p&gt;
Training and Deploying Spiking NN Applications to the Mixed-Signal Neuromorphic Chip Dynap-SE2 with Rockpool. (arXiv:2303.12167v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#21644;&#27880;&#20837;&#23545;&#25239;&#24615;&#21442;&#25968;&#22122;&#22768;&#65292;&#23558;SNN&#24212;&#29992;&#31243;&#24207;&#31163;&#32447;&#35757;&#32451;&#21644;&#37096;&#32626;&#21040;Dynap-SE2&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#20248;&#21270;&#21518;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#20110;&#30828;&#20214;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20869;&#30340;&#31232;&#30095;&#24322;&#27493;&#35745;&#31639;&#25552;&#20379;&#26497;&#20302;&#21151;&#32791;&#30340;&#36793;&#32536;&#25512;&#29702;&#36127;&#36733;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#25311;&#30828;&#20214;&#21442;&#25968;&#30340;&#21463;&#38480;&#21487;&#25511;&#24615;&#20197;&#21450;&#30001;&#20110;&#21046;&#36896;&#38750;&#29702;&#24819;&#24615;&#25152;&#23548;&#33268;&#30340;&#27169;&#25311;&#30005;&#36335;&#30340;&#26080;&#24847;&#21442;&#25968;&#21644;&#21160;&#24577;&#21464;&#21270;&#65292;&#23558;&#31283;&#20581;&#30340;&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#21040;&#36825;&#20123;&#35774;&#22791;&#26159;&#22797;&#26434;&#30340;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23558;SNN&#24212;&#29992;&#31243;&#24207;&#31163;&#32447;&#35757;&#32451;&#21644;&#37096;&#32626;&#21040;&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;Dynap-SE2&#30340;&#26032;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#37325;&#37327;&#37327;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;&#32593;&#32476;&#30340;&#21442;&#25968;&#65292;&#24182;&#32467;&#21512;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27880;&#20837;&#23545;&#25239;&#24615;&#21442;&#25968;&#22122;&#22768;&#12290;&#20248;&#21270;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#25269;&#24481;&#37327;&#21270;&#21644;&#35774;&#22791;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#65292;&#20351;&#35813;&#26041;&#27861;&#25104;&#20026;&#20855;&#26377;&#30828;&#20214;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#30340;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24320;&#28304;&#35774;&#35745;&#24037;&#20855;Rockpool&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed-signal neuromorphic processors provide extremely low-power operation for edge inference workloads, taking advantage of sparse asynchronous computation within Spiking Neural Networks (SNNs). However, deploying robust applications to these devices is complicated by limited controllability over analog hardware parameters, unintended parameter and dynamics variations of analog circuits due to fabrication non-idealities. Here we demonstrate a novel methodology for offline training and deployment of spiking neural networks (SNNs) to the mixed-signal neuromorphic processor Dynap-SE2. The methodology utilizes an unsupervised weight quantization method to optimize the network's parameters, coupled with adversarial parameter noise injection during training. The optimized network is shown to be robust to the effects of quantization and device mismatch, making the method a promising candidate for real-world applications with hardware constraints. This work extends Rockpool, an open-source de
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#27169;&#22411;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#29992;&#25143;&#25968;&#37327;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14115</link><description>&lt;p&gt;
&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Inverse Solvability and Security with Applications to Federated Learning. (arXiv:2211.14115v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14115
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#27169;&#22411;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#29992;&#25143;&#25968;&#37327;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#32447;&#24615;&#21069;&#21521;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#31034;&#20363;&#65292;&#20854;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#22312;&#26412;&#25991;&#20013;&#24471;&#21040;&#23450;&#20041;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21442;&#19982;&#32473;&#23450;&#36845;&#20195;&#30340;&#22823;&#37327;&#29992;&#25143;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25152;&#25552;&#20986;&#27010;&#24565;&#30340;&#21487;&#33021;&#25193;&#23637;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concepts of inverse solvability and security for a generic linear forward model and demonstrate how they can be applied to models used in federated learning. We provide examples of such models which differ in the resulting inverse solvability and security as defined in this paper. We also show how the large number of users participating in a given iteration of federated learning can be leveraged to increase both solvability and security. Finally, we discuss possible extensions of the presented concepts including the nonlinear case.
&lt;/p&gt;</description></item></channel></rss>