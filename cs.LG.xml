<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#37326;&#22806;&#22270;&#20687;&#38598;&#36827;&#34892;&#32852;&#21512;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19968;&#33268;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.16201</link><description>&lt;p&gt;
ASIC: &#23545;&#37326;&#22806;&#31232;&#30095;&#22270;&#20687;&#38598;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ASIC: Aligning Sparse in-the-wild Image Collections. (arXiv:2303.16201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#37326;&#22806;&#22270;&#20687;&#38598;&#36827;&#34892;&#32852;&#21512;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19968;&#33268;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#37326;&#22806;&#22270;&#20687;&#38598;&#36827;&#34892;&#32852;&#21512;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#20316;&#21697;&#35201;&#20040;&#20551;&#23450;&#26377;ground-truth&#30340;&#20851;&#38190;&#28857;&#27880;&#37322;&#65292;&#35201;&#20040;&#20551;&#23450;&#26377;&#19968;&#20010;&#29289;&#20307;&#31867;&#21035;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20197;&#19978;&#20004;&#20010;&#20551;&#35774;&#37117;&#19981;&#36866;&#29992;&#20110;&#23384;&#22312;&#20110;&#19990;&#30028;&#19978;&#30340;&#29289;&#20307;&#30340;&#23614;&#37096;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25216;&#26415;&#65292;&#30452;&#25509;&#22312;&#29305;&#23450;&#29289;&#20307;/&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#22270;&#20687;&#38598;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#33719;&#24471;&#25972;&#20010;&#38598;&#21512;&#30340;&#19968;&#33268;&#19988;&#31264;&#23494;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViT&#65289;&#27169;&#22411;&#30340;&#28145;&#24230;&#29305;&#24449;&#20013;&#33719;&#24471;&#30340;&#25104;&#23545;&#26368;&#36817;&#37051;&#20316;&#20026;&#22122;&#22768;&#21644;&#31232;&#30095;&#20851;&#38190;&#28857;&#21305;&#37197;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#23427;&#20204;&#23494;&#38598;&#21644;&#31934;&#30830;&#21305;&#37197;&#65292;&#21516;&#26102;&#23558;&#22270;&#20687;&#38598;&#21512;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#35268;&#33539;&#32593;&#26684;&#20013;&#12290;&#22312;CUB&#21644;SPair-71k&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20840;&#23616;&#19968;&#33268;&#24615;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#26041;&#27861;&#65292;&#20174;&#21355;&#26143;&#35270;&#35282;&#39044;&#27979;&#27431;&#27954;&#22320;&#21306;&#26893;&#34987;&#23545;&#22825;&#27668;&#30340;&#21709;&#24212;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#30899;&#30417;&#27979;&#30340;&#24635;&#21021;&#32423;&#29983;&#20135;&#21147;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2303.16198</link><description>&lt;p&gt;
&#21033;&#29992;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#65292;&#20174;&#21355;&#26143;&#35270;&#35282;&#39044;&#27979;&#23616;&#37096;&#27668;&#35937;&#23545;&#26893;&#34987;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction. (arXiv:2303.16198v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#26041;&#27861;&#65292;&#20174;&#21355;&#26143;&#35270;&#35282;&#39044;&#27979;&#27431;&#27954;&#22320;&#21306;&#26893;&#34987;&#23545;&#22825;&#27668;&#30340;&#21709;&#24212;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#30899;&#30417;&#27979;&#30340;&#24635;&#21021;&#32423;&#29983;&#20135;&#21147;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Sentinel 2&#21355;&#26143;&#27979;&#37327;&#27431;&#27954;&#22320;&#21306;&#30340;&#22825;&#27668;&#26469;&#24314;&#31435;&#26893;&#34987;&#23545;&#22825;&#27668;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#26041;&#27861;&#37325;&#28857;&#20851;&#27880;&#22810;&#20809;&#35889;&#22270;&#20687;&#30340;&#30495;&#23454;&#24863;&#65292;&#32780;&#34893;&#29983;&#30340;&#26893;&#34987;&#21160;&#24577;&#23578;&#26410;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27668;&#35937;&#25351;&#23548;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#39044;&#27979;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23398;&#20064;&#30340;&#20113;&#23618;&#25513;&#27169;&#21644;&#36866;&#24403;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#25193;&#23637;&#20102;EarthNet2021&#25968;&#25454;&#38598;&#20197;&#36866;&#21512;&#26893;&#34987;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#39046;&#20808;&#30340;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#26041;&#27861;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#24314;&#31435;&#30340;&#26893;&#34987;&#21160;&#24577;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65306;&#29992;&#20110;&#30899;&#30417;&#27979;&#30340;&#24635;&#21021;&#32423;&#29983;&#20135;&#21147;&#30340;&#25512;&#26029;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#24314;&#31435;&#22522;&#20110;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for modeling vegetation response to weather in Europe as measured by the Sentinel 2 satellite. Existing satellite imagery forecasting approaches focus on photorealistic quality of the multispectral images, while derived vegetation dynamics have not yet received as much attention. We leverage both spatial and temporal context by extending state-of-the-art video prediction methods with weather guidance. We extend the EarthNet2021 dataset to be suitable for vegetation modeling by introducing a learned cloud mask and an appropriate evaluation scheme. Qualitative and quantitative experiments demonstrate superior performance of our approach over a wide variety of baseline methods, including leading approaches to satellite imagery forecasting. Additionally, we show how our modeled vegetation dynamics can be leveraged in a downstream task: inferring gross primary productivity for carbon monitoring. To the best of our knowledge, this work presents the first models fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;BC-IRL&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27867;&#21270;&#24615;&#26356;&#24378;&#30340;&#22870;&#21169;&#20989;&#25968;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#27867;&#21270;&#22330;&#26223;&#20013;&#21462;&#24471;&#20004;&#20493;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.16194</link><description>&lt;p&gt;
BC-IRL: &#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#21487;&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
BC-IRL: Learning Generalizable Reward Functions from Demonstrations. (arXiv:2303.16194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;BC-IRL&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27867;&#21270;&#24615;&#26356;&#24378;&#30340;&#22870;&#21169;&#20989;&#25968;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#27867;&#21270;&#22330;&#26223;&#20013;&#21462;&#24471;&#20004;&#20493;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#21527;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;IRL&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26368;&#22823;&#21270;&#26368;&#22823;&#29109;&#30446;&#26631;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#36807;&#24230;&#25311;&#21512;&#20110;&#31034;&#33539;&#12290;&#36825;&#26679;&#30340;&#22870;&#21169;&#20989;&#25968;&#38590;&#20197;&#20026;&#26410;&#34987;&#31034;&#33539;&#35206;&#30422;&#30340;&#29366;&#24577;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#22870;&#21169;&#65292;&#36825;&#22312;&#20351;&#29992;&#22870;&#21169;&#26469;&#23398;&#20064;&#26032;&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#26102;&#26159;&#19968;&#20010;&#37325;&#22823;&#21155;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;BC-IRL&#65292;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#22312;&#19982;&#26368;&#22823;&#29109;IRL&#26041;&#27861;&#30456;&#27604;&#26356;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#19982;MaxEnt&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#22870;&#21169;&#21442;&#25968;&#65292;&#20197;&#20415;&#25152;&#35757;&#32451;&#30340;&#31574;&#30053;&#26356;&#22909;&#22320;&#21305;&#37197;&#31034;&#33539;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BC-IRL&#22312;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#21644;&#20004;&#20010;&#36830;&#32493;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#23398;&#20064;&#21040;&#26356;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#22312;&#25361;&#25112;&#24615;&#30340;&#27867;&#21270;&#22330;&#26223;&#20013;&#25104;&#21151;&#29575;&#26159;&#22522;&#32447;&#30340;&#20004;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#26041;&#27861;&#30340;&#24207;&#21015;&#27169;&#22411;&#35268;&#21010;&#38598;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#22312;BabyAI&#21644;Atari&#31561;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16189</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#36827;&#34892;&#24207;&#21015;&#27169;&#22411;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning with Sequence Models through Iterative Energy Minimization. (arXiv:2303.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#26041;&#27861;&#30340;&#24207;&#21015;&#27169;&#22411;&#35268;&#21010;&#38598;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#22312;BabyAI&#21644;Atari&#31561;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24207;&#21015;&#24314;&#27169;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#30340;&#24207;&#21015;&#27169;&#22411;&#24212;&#29992;&#20110;&#35268;&#21010;&#65292;&#21363;&#24076;&#26395;&#33719;&#24471;&#19968;&#31995;&#21015;&#21160;&#20316;&#30340;&#36712;&#36857;&#20197;&#36798;&#21040;&#26576;&#20010;&#30446;&#26631;&#65292;&#24182;&#19981;&#37027;&#20040;&#30452;&#25509;&#12290;&#24207;&#21015;&#27169;&#22411;&#30340;&#20856;&#22411;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#25490;&#38500;&#20102;&#23545;&#36739;&#26089;&#27493;&#39588;&#30340;&#39034;&#24207;&#32454;&#21270;&#65292;&#36825;&#38480;&#21046;&#20102;&#39044;&#27979;&#35745;&#21010;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#23558;&#35268;&#21010;&#19982;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#24182;&#35828;&#26126;&#36825;&#31181;&#36807;&#31243;&#22914;&#20309;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#23548;&#33268;&#25913;&#36827;&#30340;RL&#24615;&#33021;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#25429;&#25417;&#20102;&#21160;&#20316;&#36712;&#36857;&#19978;&#30340;&#38544;&#24335;&#33021;&#37327;&#20989;&#25968;&#65292;&#24182;&#23558;&#35268;&#21010;&#24418;&#24335;&#21270;&#20026;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#33021;&#37327;&#30340;&#21160;&#20316;&#36712;&#36857;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#36825;&#20010;&#36807;&#31243;&#22914;&#20309;&#22312;BabyAI&#21644;Atari&#29615;&#22659;&#19979;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that sequence modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing sequence models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of sequence models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with sequence models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#37319;&#26679;&#36807;&#31243;&#65292;&#20351;&#29992;&#21487;&#35270;&#21270;&#24605;&#32500;&#20256;&#36882;&#27169;&#22411;&#26469;&#32553;&#23567;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#30456;&#23545;&#26631;&#20934;&#26080;&#26465;&#20214;&#29983;&#25104;&#65292;FID&#25552;&#39640;25-50%&#12290;</title><link>http://arxiv.org/abs/2303.16187</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#24605;&#32500;&#20256;&#36882;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Visual Chain-of-Thought Diffusion Models. (arXiv:2303.16187v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#37319;&#26679;&#36807;&#31243;&#65292;&#20351;&#29992;&#21487;&#35270;&#21270;&#24605;&#32500;&#20256;&#36882;&#27169;&#22411;&#26469;&#32553;&#23567;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#30456;&#23545;&#26631;&#20934;&#26080;&#26465;&#20214;&#29983;&#25104;&#65292;FID&#25552;&#39640;25-50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36817;&#26399;&#36827;&#23637;&#26159;&#24778;&#20154;&#30340;&#65292;&#36825;&#36866;&#29992;&#20110;&#20197;&#25991;&#26412;&#25551;&#36848;&#12289;&#22330;&#26223;&#24067;&#23616;&#25110;&#32032;&#25551;&#20026;&#26465;&#20214;&#30340;&#27169;&#22411;&#12290;&#26080;&#26465;&#20214;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20063;&#22312;&#25913;&#36827;&#65292;&#20294;&#33853;&#21518;&#20110;&#26465;&#20214;&#27169;&#22411;&#65292;&#20197;&#31867;&#26631;&#31614;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20134;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20004;&#38454;&#27573;&#37319;&#26679;&#36807;&#31243;&#65292;&#32553;&#23567;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#37319;&#26679;&#25551;&#36848;&#22270;&#20687;&#35821;&#20041;&#20869;&#23481;&#30340;&#23884;&#20837;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#23884;&#20837;&#30340;&#26465;&#20214;&#19979;&#37319;&#26679;&#22270;&#20687;&#65292;&#28982;&#21518;&#20002;&#24323;&#36825;&#20010;&#23884;&#20837;&#12290;&#36825;&#26679;&#20570;&#35753;&#25105;&#20204;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#26080;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#30456;&#23545;&#20110;&#26631;&#20934;&#26080;&#26465;&#20214;&#29983;&#25104;&#65292;FID&#65288;Frechet inception distance&#65289;&#26368;&#22810;&#25552;&#39640;&#20102;25-50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#32676;&#19981;&#21464;&#27969;&#24418;&#38382;&#39064;&#65292;&#36890;&#36807;&#31215;&#20998;&#22312;&#19981;&#21464;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;&#20986;K-&#19981;&#21464;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#21033;&#29992;K&#20013;&#30340;&#24186;&#27491;&#19981;&#21487;&#32422;&#34920;&#31034;&#30697;&#38453;&#23545;&#20854;&#36827;&#34892;&#23545;&#35282;&#21270;&#65292;&#24182;&#32473;&#20986;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#20844;&#24335;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;L_N&#25910;&#25947;&#20110;Laplace-Beltrami&#31639;&#23376;&#65292;&#25910;&#25947;&#36895;&#24230;&#38543;&#30528;&#23545;&#31216;&#32676;K&#30340;&#32500;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.16169</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#32676;&#19981;&#21464;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Maps for Group-Invariant Manifolds. (arXiv:2303.16169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#32676;&#19981;&#21464;&#27969;&#24418;&#38382;&#39064;&#65292;&#36890;&#36807;&#31215;&#20998;&#22312;&#19981;&#21464;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;&#20986;K-&#19981;&#21464;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#21033;&#29992;K&#20013;&#30340;&#24186;&#27491;&#19981;&#21487;&#32422;&#34920;&#31034;&#30697;&#38453;&#23545;&#20854;&#36827;&#34892;&#23545;&#35282;&#21270;&#65292;&#24182;&#32473;&#20986;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#35745;&#31639;&#20844;&#24335;&#12290;&#21516;&#26102;&#65292;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;L_N&#25910;&#25947;&#20110;Laplace-Beltrami&#31639;&#23376;&#65292;&#25910;&#25947;&#36895;&#24230;&#38543;&#30528;&#23545;&#31216;&#32676;K&#30340;&#32500;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24403;&#25968;&#25454;&#38598;&#23545;&#32039;Lie&#32676;K&#30340;&#20316;&#29992;&#20855;&#26377;&#19981;&#21464;&#24615;&#26102;&#65292;&#27969;&#24418;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;&#29616;&#26377;&#25968;&#25454;&#28857;K&#30340;&#36712;&#36947;&#19978;&#31215;&#20998;&#65292;&#23558;&#25968;&#25454;&#35825;&#23548;&#30340;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25193;&#23637;&#21040;K-&#19981;&#21464;&#31639;&#23376;L&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;K&#30340;&#24186;&#27491;&#19981;&#21487;&#32422;&#34920;&#31034;&#30697;&#38453;&#26469;&#23545;&#35282;&#21270;K-&#19981;&#21464;&#31639;&#23376;L&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;L&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#26174;&#24335;&#20844;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;L_N&#25910;&#25947;&#21040;&#25968;&#25454;&#27969;&#24418;&#30340;Laplace-Beltrami&#31639;&#23376;&#65292;&#25910;&#25947;&#36895;&#24230;&#24471;&#21040;&#25913;&#36827;&#65292;&#25913;&#36827;&#38543;&#30528;&#23545;&#31216;&#32676;K&#30340;&#32500;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#23558;Landa&#21644;Shkolnisky&#30340;&#21487;&#36716;&#21160;&#22270;&#25289;&#26222;&#25289;&#26031;&#26694;&#26550;&#20174;SO&#65288;2&#65289;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#20219;&#24847;&#32039;Lie&#32676;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we consider the manifold learning problem when the data set is invariant under the action of a compact Lie group $K$. Our approach consists in augmenting the data-induced graph Laplacian by integrating over orbits under the action of $K$ of the existing data points. We prove that this $K$-invariant Laplacian operator $L$ can be diagonalized by using the unitary irreducible representation matrices of $K$, and we provide an explicit formula for computing the eigenvalues and eigenvectors of $L$. Moreover, we show that the normalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operator of the data manifold with an improved convergence rate, where the improvement grows with the dimension of the symmetry group $K$. This work extends the steerable graph Laplacian framework of Landa and Shkolnisky from the case of $\operatorname{SO}(2)$ to arbitrary compact Lie groups.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20844;&#21496;&#30408;&#21033;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#22521;&#35757;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#21644;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#30340;&#20998;&#26512;&#24072;&#30456;&#27604;&#20250;&#20135;&#29983;&#36739;&#23569;&#30340;&#36807;&#24230;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2303.16158</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#36130;&#25253;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Behavioral Machine Learning? Computer Predictions of Corporate Earnings also Overreact. (arXiv:2303.16158v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20844;&#21496;&#30408;&#21033;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#22521;&#35757;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#21644;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#30340;&#20998;&#26512;&#24072;&#30456;&#27604;&#20250;&#20135;&#29983;&#36739;&#23569;&#30340;&#36807;&#24230;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#27604;&#20154;&#31867;&#26356;&#20026;&#20934;&#30830;&#12290;&#20294;&#26159;&#65292;&#25991;&#29486;&#24182;&#26410;&#27979;&#35797;&#31639;&#27861;&#39044;&#27979;&#26159;&#21542;&#26356;&#20026;&#29702;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;&#31639;&#27861;&#65288;&#21253;&#25324;&#32447;&#24615;&#22238;&#24402;&#21644;&#19968;&#31181;&#21517;&#20026;Gradient Boosted Regression Trees&#30340;&#27969;&#34892;&#31639;&#27861;&#65289;&#23545;&#20110;&#20844;&#21496;&#30408;&#21033;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;GBRT&#24179;&#22343;&#32988;&#36807;&#32447;&#24615;&#22238;&#24402;&#21644;&#20154;&#31867;&#32929;&#24066;&#20998;&#26512;&#24072;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#19988;&#26080;&#27861;&#28385;&#36275;&#29702;&#24615;&#39044;&#26399;&#26631;&#20934;&#12290;&#36890;&#36807;&#38477;&#20302;&#23398;&#20064;&#29575;&#65292;&#21487;&#26368;&#23567;&#31243;&#24230;&#19978;&#20943;&#23569;&#36807;&#24230;&#21453;&#24212;&#31243;&#24230;&#65292;&#20294;&#36825;&#20250;&#29306;&#29298;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#36807;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#27604;&#20256;&#32479;&#35757;&#32451;&#30340;&#20998;&#26512;&#24072;&#20135;&#29983;&#30340;&#36807;&#24230;&#21453;&#24212;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#32929;&#24066;&#20998;&#26512;&#24072;&#30340;&#39044;&#27979;&#21453;&#26144;&#20986;&#26426;&#22120;&#31639;&#27861;&#27809;&#26377;&#25429;&#25417;&#21040;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable evidence that machine learning algorithms have better predictive abilities than humans in various financial settings. But, the literature has not tested whether these algorithmic predictions are more rational than human predictions. We study the predictions of corporate earnings from several algorithms, notably linear regressions and a popular algorithm called Gradient Boosted Regression Trees (GBRT). On average, GBRT outperformed both linear regressions and human stock analysts, but it still overreacted to news and did not satisfy rational expectation as normally defined. By reducing the learning rate, the magnitude of overreaction can be minimized, but it comes with the cost of poorer out-of-sample prediction accuracy. Human stock analysts who have been trained in machine learning methods overreact less than traditionally trained analysts. Additionally, stock analyst predictions reflect information not otherwise available to machine algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24341;&#23548;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21021;&#22987;&#25506;&#32034;&#36807;&#31243;&#20013;&#23398;&#20064;&#27599;&#20010;&#32593;&#32476;&#21442;&#25968;&#30340;&#24341;&#23548;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#32593;&#32476;&#26102;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#32593;&#32476;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#35753;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#21407;&#26412;&#21482;&#26377;&#19968;&#20010;&#36739;&#22823;&#30340;&#32593;&#32476;&#25165;&#33021;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.16154</link><description>&lt;p&gt;
&#24341;&#23548;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Guided Transfer Learning. (arXiv:2303.16154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16154
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24341;&#23548;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21021;&#22987;&#25506;&#32034;&#36807;&#31243;&#20013;&#23398;&#20064;&#27599;&#20010;&#32593;&#32476;&#21442;&#25968;&#30340;&#24341;&#23548;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#32593;&#32476;&#26102;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#32593;&#32476;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#35753;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#21407;&#26412;&#21482;&#26377;&#19968;&#20010;&#36739;&#22823;&#30340;&#32593;&#32476;&#25165;&#33021;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#19982;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36824;&#38656;&#35201;&#30456;&#23545;&#24212;&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#30340;&#25216;&#26415;&#26159;&#26126;&#26234;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24341;&#23548;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#21644;&#20559;&#32622;&#37117;&#26377;&#33258;&#24049;&#30340;&#24341;&#23548;&#21442;&#25968;&#65292;&#25351;&#31034;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#35813;&#21442;&#25968;&#20801;&#35768;&#25913;&#21464;&#22810;&#23569;&#12290;&#24341;&#23548;&#21442;&#25968;&#26159;&#22312;&#21021;&#22987;&#25506;&#32034;&#36807;&#31243;&#20013;&#23398;&#20064;&#30340;&#12290;&#24341;&#23548;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#20943;&#23569;&#35757;&#32451;&#32593;&#32476;&#25152;&#38656;&#30340;&#36164;&#28304;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#24341;&#23548;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#20351;&#32593;&#32476;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#21407;&#26412;&#21482;&#26377;&#19968;&#20010;&#36739;&#22823;&#30340;&#32593;&#32476;&#25165;&#33021;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#24403;&#25968;&#25454;&#37327;&#12289;&#27169;&#22411;&#22823;&#23567;&#25110;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#36798;&#21040;&#26497;&#38480;&#26102;&#65292;&#24341;&#23548;&#36801;&#31227;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning requires exuberant amounts of data and computation. Also, models require equally excessive growth in the number of parameters. It is, therefore, sensible to look for technologies that reduce these demands on resources. Here, we propose an approach called guided transfer learning. Each weight and bias in the network has its own guiding parameter that indicates how much this parameter is allowed to change while learning a new task. Guiding parameters are learned during an initial scouting process. Guided transfer learning can result in a reduction in resources needed to train a network. In some applications, guided transfer learning enables the network to learn from a small amount of data. In other cases, a network with a smaller number of parameters can learn a task which otherwise only a larger network could learn. Guided transfer learning potentially has many applications when the amount of data, model size, or the availability of computational resources reach their l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22240;&#23376;&#27169;&#22411;&#21644;&#25910;&#32553;&#30340;&#26041;&#27861;&#39044;&#27979;&#22823;&#22411;&#23454;&#29616;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20998;&#35299;&#22238;&#25253;&#21327;&#26041;&#24046;&#30697;&#38453;&#24182;&#20351;&#29992;&#21521;&#37327;&#24322;&#36136;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#22522;&#20934;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#65292;&#24182;&#23548;&#33268;&#23545;&#26368;&#23567;&#26041;&#24046;&#32452;&#21512;&#30340;&#26356;&#22909;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.16151</link><description>&lt;p&gt;
&#39044;&#27979;&#22823;&#22411;&#23454;&#29616;&#21327;&#26041;&#24046;&#30697;&#38453;:&#22240;&#23376;&#27169;&#22411;&#21644;&#25910;&#32553;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage. (arXiv:2303.16151v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22240;&#23376;&#27169;&#22411;&#21644;&#25910;&#32553;&#30340;&#26041;&#27861;&#39044;&#27979;&#22823;&#22411;&#23454;&#29616;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20998;&#35299;&#22238;&#25253;&#21327;&#26041;&#24046;&#30697;&#38453;&#24182;&#20351;&#29992;&#21521;&#37327;&#24322;&#36136;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#22522;&#20934;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#65292;&#24182;&#23548;&#33268;&#23545;&#26368;&#23567;&#26041;&#24046;&#32452;&#21512;&#30340;&#26356;&#22909;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26469;&#39044;&#27979;&#25910;&#30410;&#30340;&#22823;&#22411;&#23454;&#29616;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#24182;&#23545;S&amp;P 500&#30340;&#25104;&#20998;&#32929;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#25968;&#28798;&#38590;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#20225;&#19994;&#32423;&#21035;&#22240;&#23376;&#65288;&#22914;&#22823;&#23567;&#12289;&#20215;&#20540;&#21644;&#30408;&#21033;&#33021;&#21147;&#65289;&#20998;&#35299;&#22238;&#25253;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#24182;&#22312;&#27531;&#24046;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#20351;&#29992;&#37096;&#38376;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#26368;&#23567;&#32477;&#23545;&#25910;&#32553;&#21644;&#36873;&#25321;&#36816;&#31639;&#31526;&#65288;LASSO&#65289;&#30340;&#21521;&#37327;&#24322;&#36136;&#33258;&#22238;&#24402;&#65288;VHAR&#65289;&#27169;&#22411;&#23545;&#35813;&#38480;&#21046;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#12290;&#30456;&#23545;&#20110;&#26631;&#20934;&#22522;&#20934;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#65292;&#24182;&#23548;&#33268;&#23545;&#26368;&#23567;&#26041;&#24046;&#32452;&#21512;&#30340;&#26356;&#22909;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model to forecast large realized covariance matrices of returns, applying it to the constituents of the S\&amp;P 500 daily. To address the curse of dimensionality, we decompose the return covariance matrix using standard firm-level factors (e.g., size, value, and profitability) and use sectoral restrictions in the residual covariance matrix. This restricted model is then estimated using vector heterogeneous autoregressive (VHAR) models with the least absolute shrinkage and selection operator (LASSO). Our methodology improves forecasting precision relative to standard benchmarks and leads to better estimates of minimum variance portfolios.
&lt;/p&gt;</description></item><item><title>VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.16150</link><description>&lt;p&gt;
VIDIMU: &#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35774;&#22791;&#35760;&#24405;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#21644;IMU&#36816;&#21160;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16150
&lt;/p&gt;
&lt;p&gt;
VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#21644;&#20020;&#24202;&#29983;&#29289;&#21147;&#23398;&#26159;&#29289;&#29702;&#36828;&#31243;&#24247;&#22797;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#21160;&#20316;&#25968;&#25454;&#38598;&#19981;&#33021;&#29992;&#20110;&#30740;&#31350;&#23454;&#39564;&#23460;&#22806;&#36816;&#21160;&#33719;&#21462;&#24773;&#20917;&#19979;&#30340;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;VIDIMU&#25968;&#25454;&#38598;&#30340;&#30446;&#30340;&#26159;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#20116;&#20010;&#24815;&#24615;&#20256;&#24863;&#22120;&#27880;&#20876;&#30340;13&#31181;&#27963;&#21160;&#12290;&#35760;&#24405;&#35270;&#39057;&#30340;54&#20010;&#21463;&#35797;&#32773;&#20013;&#65292;&#20854;&#20013;16&#20010;&#21463;&#35797;&#32773;&#21516;&#26102;&#36824;&#26377;&#24815;&#24615;&#20256;&#24863;&#22120;&#35760;&#24405;&#12290;VIDIMU&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65306;i&#65289;&#25152;&#36873;&#25321;&#30340;&#21160;&#20316;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;ii&#65289;&#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35270;&#39057;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450; iii&#65289;&#23454;&#29616;&#20102;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#24815;&#24615;&#25968;&#25454;&#20013;&#23545;&#19977;&#32500;&#36523;&#20307;&#23039;&#21183;&#36319;&#36394;&#21644;&#36816;&#21160;&#37325;&#24314;&#22312;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#26412;&#38754;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#21152;&#25343;&#22823;&#32654;&#20803;&#20817;&#32654;&#22269;&#32654;&#20803;&#27719;&#29575;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#21407;&#27833;&#20316;&#20026;&#21152;&#25343;&#22823;&#20027;&#35201;&#21830;&#21697;&#23545;&#27719;&#29575;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16149</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#23439;&#35266;&#32463;&#27982;&#22522;&#26412;&#38754;&#23545;&#27719;&#29575;&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Explaining Exchange Rate Forecasts with Macroeconomic Fundamentals Using Interpretive Machine Learning. (arXiv:2303.16149v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#26412;&#38754;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#21152;&#25343;&#22823;&#32654;&#20803;&#20817;&#32654;&#22269;&#32654;&#20803;&#27719;&#29575;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#21407;&#27833;&#20316;&#20026;&#21152;&#25343;&#22823;&#20027;&#35201;&#21830;&#21697;&#23545;&#27719;&#29575;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#21644;&#32463;&#27982;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#65292;&#20197;&#21450;&#32463;&#27982;&#29615;&#22659;&#30340;&#39057;&#32321;&#21464;&#21270;&#65292;&#20351;&#24471;&#38590;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#24471;&#21040;&#21463;&#29702;&#35770;&#25903;&#25345;&#30340;&#35299;&#37322;&#12290;&#35299;&#37322;&#29992;&#20110;&#39044;&#27979;&#37325;&#35201;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#30340;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#19981;&#21516;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12289;&#22686;&#24378;&#23545;&#39044;&#27979;&#27169;&#22411;&#30340;&#20449;&#20219;&#24230;&#20197;&#21450;&#20351;&#39044;&#27979;&#26356;&#20855;&#25805;&#20316;&#24615;&#20855;&#26377;&#39640;&#24230;&#30340;&#20215;&#20540;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#26412;&#38754;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#21152;&#25343;&#22823;&#32654;&#20803;&#20817;&#32654;&#22269;&#32654;&#20803;&#27719;&#29575;&#24182;&#22312;&#19968;&#20010;&#35299;&#37322;&#24615;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27719;&#29575;&#65292;&#24182;&#37319;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20934;&#30830;&#20998;&#26512;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#35299;&#37322;&#30340;&#36755;&#20986;&#23454;&#26045;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21407;&#27833;&#20316;&#20026;&#21152;&#25343;&#22823;&#30340;&#20027;&#35201;&#21830;&#21697;&#65292;&#23545;&#27719;&#29575;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complexity and ambiguity of financial and economic systems, along with frequent changes in the economic environment, have made it difficult to make precise predictions that are supported by theory-consistent explanations. Interpreting the prediction models used for forecasting important macroeconomic indicators is highly valuable for understanding relations among different factors, increasing trust towards the prediction models, and making predictions more actionable. In this study, we develop a fundamental-based model for the Canadian-U.S. dollar exchange rate within an interpretative framework. We propose a comprehensive approach using machine learning to predict the exchange rate and employ interpretability methods to accurately analyze the relationships among macroeconomic variables. Moreover, we implement an ablation study based on the output of the interpretations to improve the predictive accuracy of the models. Our empirical results show that crude oil, as Canada's main com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;&#65292;&#20174;&#22240;&#26524;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24433;&#21709;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#65292;&#21253;&#25324;&#20116;&#31181;&#20027;&#35201;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#12289;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#21644;&#31038;&#20132;&#23186;&#20307;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16148</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#22240;&#32032;&#30340;&#24314;&#27169;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modelling Determinants of Cryptocurrency Prices: A Bayesian Network Approach. (arXiv:2303.16148v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;&#65292;&#20174;&#22240;&#26524;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24433;&#21709;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#65292;&#21253;&#25324;&#20116;&#31181;&#20027;&#35201;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#12289;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#21644;&#31038;&#20132;&#23186;&#20307;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#24635;&#20540;&#21644;&#26367;&#20195;&#27604;&#29305;&#24065;&#30340;&#21152;&#23494;&#36135;&#24065;&#25968;&#37327;&#30340;&#22686;&#38271;&#25552;&#20379;&#20102;&#25237;&#36164;&#26426;&#20250;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#39044;&#27979;&#20854;&#20215;&#26684;&#27874;&#21160;&#30340;&#22797;&#26434;&#24230;&#12290;&#22312;&#36825;&#20010;&#27874;&#21160;&#24615;&#30456;&#23545;&#36739;&#24369;&#30340;&#24066;&#22330;&#20013;&#65292;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#38656;&#35201;&#30830;&#23450;&#24433;&#21709;&#20215;&#26684;&#30340;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20174;&#22240;&#26524;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#24433;&#21709;&#26367;&#20195;&#27604;&#29305;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#65292;&#29305;&#21035;&#22320;&#65292;&#30740;&#31350;&#20102;&#20116;&#20010;&#20027;&#35201;&#30340;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#65292;&#21253;&#25324;&#40644;&#37329;&#12289;&#30707;&#27833;&#21644;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#31561;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#20197;&#21450;&#31038;&#20132;&#23186;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#30001;&#20116;&#20010;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#30340;&#21382;&#21490;&#20215;&#26684;&#25968;&#25454;&#12289;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#25968;&#25454;&#26500;&#25104;&#30340;&#22240;&#26524;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#29992;&#20110;&#22240;&#26524;&#25512;&#29702;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of market capitalisation and the number of altcoins (cryptocurrencies other than Bitcoin) provide investment opportunities and complicate the prediction of their price movements. A significant challenge in this volatile and relatively immature market is the problem of predicting cryptocurrency prices which needs to identify the factors influencing these prices. The focus of this study is to investigate the factors influencing altcoin prices, and these factors have been investigated from a causal analysis perspective using Bayesian networks. In particular, studying the nature of interactions between five leading altcoins, traditional financial assets including gold, oil, and S\&amp;P 500, and social media is the research question. To provide an answer to the question, we create causal networks which are built from the historic price data of five traditional financial assets, social media data, and price data of altcoins. The ensuing networks are used for causal reasoning and diag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20116;&#31181;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;COVID-19&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;&#22312;&#20998;&#25955;&#24335;&#29615;&#22659;&#19979;&#65292;&#24490;&#29615;&#26435;&#37325;&#20256;&#36882;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24635;&#20307;&#24615;&#33021;&#65292;&#22312;&#21442;&#19982;&#21307;&#38498;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#32467;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.16141</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;COVID-19&#26816;&#27979;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Federated Learning Models for COVID-19 Detection. (arXiv:2303.16141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20116;&#31181;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;COVID-19&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;&#22312;&#20998;&#25955;&#24335;&#29615;&#22659;&#19979;&#65292;&#24490;&#29615;&#26435;&#37325;&#20256;&#36882;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24635;&#20307;&#24615;&#33021;&#65292;&#22312;&#21442;&#19982;&#21307;&#38498;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#32467;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35786;&#26029;COVID-19&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;&#30001;&#20110;&#25968;&#25454;&#21644;&#38544;&#31169;&#27861;&#35268;&#30340;&#38480;&#21046;&#65292;&#21307;&#38498;&#19968;&#33324;&#26080;&#27861;&#35775;&#38382;&#20854;&#20182;&#21307;&#38498;&#30340;&#25968;&#25454;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#21033;&#29992;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#22312;&#21307;&#38498;&#20013;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#20294;&#26159;&#65292;&#37096;&#32626;FL&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#32593;&#32476;&#36890;&#20449;&#36164;&#28304;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20116;&#31181;FL&#31639;&#27861;&#22312;COVID-19&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;&#25105;&#20204;&#25645;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;CNN&#32593;&#32476;&#30340;&#20998;&#25955;&#24335;&#29615;&#22659;&#65292;&#24182;&#23558;FL&#31639;&#27861;&#30340;&#24615;&#33021;&#19982;&#38598;&#20013;&#24335;&#29615;&#22659;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21442;&#19982;&#32773;&#25968;&#37327;&#12289;&#32852;&#37030;&#36718;&#25968;&#21644;&#36873;&#25321;&#31639;&#27861;&#31561;&#22240;&#32032;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24490;&#29615;&#26435;&#37325;&#20256;&#36882;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24635;&#20307;&#24615;&#33021;&#65292;&#22312;&#21442;&#19982;&#21307;&#38498;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#32467;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;FL&#31639;&#27861;&#22312;&#20998;&#25955;&#24335;&#29615;&#22659;&#20013;&#23545;COVID-19&#26816;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is effective in diagnosing COVID-19 and requires a large amount of data to be effectively trained. Due to data and privacy regulations, hospitals generally have no access to data from other hospitals. Federated learning (FL) has been used to solve this problem, where it utilizes a distributed setting to train models in hospitals in a privacy-preserving manner. Deploying FL is not always feasible as it requires high computation and network communication resources. This paper evaluates five FL algorithms' performance and resource efficiency for Covid-19 detection. A decentralized setting with CNN networks is set up, and the performance of FL algorithms is compared with a centralized environment. We examined the algorithms with varying numbers of participants, federated rounds, and selection algorithms. Our results show that cyclic weight transfer can have better overall performance, and results are better with fewer participating hospitals. Our results demonstrate good perf
&lt;/p&gt;</description></item><item><title>&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;RC&#26609;&#38750;&#32447;&#24615;&#24314;&#27169;&#21442;&#25968;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#21161;&#20110;&#30830;&#23450;&#20854;&#26368;&#21487;&#33021;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.16140</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#25913;&#36827;RC&#26609;&#38750;&#32447;&#24615;&#24314;&#27169;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Machine learning tools to improve nonlinear modeling parameters of RC columns. (arXiv:2303.16140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16140
&lt;/p&gt;
&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;RC&#26609;&#38750;&#32447;&#24615;&#24314;&#27169;&#21442;&#25968;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#21161;&#20110;&#30830;&#23450;&#20854;&#26368;&#21487;&#33021;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21442;&#25968;&#23545;&#20110;&#28151;&#20957;&#22303;&#32467;&#26500;&#30340;&#38750;&#32447;&#24615;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#25311;&#36275;&#20197;&#24341;&#36215;&#20498;&#22604;&#30340;&#22320;&#38663;&#20107;&#20214;&#26102;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#38598;&#25552;&#39640;&#22320;&#38663;&#35780;&#20272;&#26631;&#20934;&#20013;&#38750;&#32447;&#24615;&#24314;&#27169;&#35201;&#27714;&#30340;&#26368;&#22823;&#38556;&#30861;&#20043;&#19968;&#65306;&#30830;&#23450;&#32467;&#26500;&#32452;&#20214;&#26368;&#21487;&#33021;&#30340;&#22833;&#25928;&#27169;&#24335;&#65292;&#24182;&#23454;&#26045;&#33021;&#22815;&#35782;&#21035;&#36755;&#20837;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#21644;&#36755;&#20837;&#21442;&#25968;&#19982;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#25968;&#25454;&#25311;&#21512;&#25216;&#26415;&#12290;&#26412;&#25991;&#20351;&#29992;Scikit-learn&#21644;Pytorch&#24211;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#26657;&#20934;ASCE 41&#21644;ACI 369.1&#26631;&#20934;&#20013;&#24378;&#21270;&#28151;&#20957;&#22303;&#26609;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#21442;&#25968;MP a&#21644;b&#30340;&#26041;&#31243;&#21644;&#40657;&#30418;&#25968;&#20540;&#27169;&#22411;&#65292;&#24182;&#20272;&#35745;&#20854;&#26368;&#21487;&#33021;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;MP a&#21644;b&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#21161;&#20110;&#30830;&#23450;RC&#26609;&#30340;&#26368;&#21487;&#33021;&#22833;&#25928;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling parameters are essential to the fidelity of nonlinear models of concrete structures subjected to earthquake ground motions, especially when simulating seismic events strong enough to cause collapse. This paper addresses two of the most significant barriers to improving nonlinear modeling provisions in seismic evaluation standards using experimental data sets: identifying the most likely mode of failure of structural components, and implementing data fitting techniques capable of recognizing interdependencies between input parameters and nonlinear relationships between input parameters and model outputs. Machine learning tools in the Scikit-learn and Pytorch libraries were used to calibrate equations and black-box numerical models for nonlinear modeling parameters (MP) a and b of reinforced concrete columns defined in the ASCE 41 and ACI 369.1 standards, and to estimate their most likely mode of failure. It was found that machine learning regression models and machine learning 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16133</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#35299;&#20915;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#20219;&#21153;&#19981;&#19968;&#33268;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#25928;&#65292;&#20445;&#35777;&#23427;&#20204;&#22312;&#21508;&#33258;&#25903;&#25345;&#30340;&#20219;&#21153;&#20013;&#30340;&#19968;&#33268;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#35748;&#20026;&#19981;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#36755;&#20986;&#30340;&#22823;&#22411;&#31995;&#32479;&#26469;&#35828;&#26159;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#19968;&#33268;&#65292;&#22240;&#27492;&#65292;&#35780;&#20272;&#21487;&#33021;&#21253;&#25324;&#19981;&#21516;&#27169;&#24577;&#36755;&#20986;&#30340;&#38750;&#24120;&#24322;&#26500;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#23567;&#22411;&#20294;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#26469;&#21019;&#24314;&#23545;&#27604;&#38598;&#65292;&#20197;&#26356;&#25913;&#37329;&#26631;&#31614;&#65292;&#24182;&#27010;&#36848;&#20102;&#29992;&#20110;&#36890;&#36807;&#23545;&#27604;&#25509;&#36817;&#21407;&#22987;&#21644;&#20462;&#25913;&#21518;&#30340;&#23454;&#20363;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16132</link><description>&lt;p&gt;
Transformer&#21644;Snowball&#22270;&#21367;&#31215;&#23398;&#20064;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25110;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25551;&#36848;&#21644;&#24314;&#27169;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#36825;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;&#30340;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#37319;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21033;&#29992;&#32654;&#22269;&#24066;&#22330;&#20215;&#26684;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;Numerai-Signals&#30446;&#26631;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16117</link><description>&lt;p&gt;
&#38754;&#21521;&#37329;&#34701;&#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature Engineering Methods on Multivariate Time-Series Data for Financial Data Science Competitions. (arXiv:2303.16117v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16117
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#37319;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21033;&#29992;&#32654;&#22269;&#24066;&#22330;&#20215;&#26684;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;Numerai-Signals&#30446;&#26631;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#23545;&#32654;&#22269;&#24066;&#22330;&#20215;&#26684;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#27979;&#35797;&#27169;&#22411;&#22312;Numerai-Signals&#30446;&#26631;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply different feature engineering methods for time-series to US market price data. The predictive power of models are tested against Numerai-Signals targets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#20445;&#25345;&#22522;&#26412;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#27169;&#25311;&#20013;&#30340;&#36830;&#32493;&#19981;&#21464;&#37327;&#26469;&#35774;&#35745;&#26356;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#65292;&#20851;&#38190;&#22312;&#20110;&#36890;&#36807;&#32416;&#38169;&#20445;&#25345;&#19981;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16110</link><description>&lt;p&gt;
&#36890;&#36807;&#32416;&#38169;&#20445;&#25345;&#19981;&#21464;&#37327;&#22312;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Invariant preservation in machine learned PDE solvers via error correction. (arXiv:2303.16110v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#20445;&#25345;&#22522;&#26412;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#27169;&#25311;&#20013;&#30340;&#36830;&#32493;&#19981;&#21464;&#37327;&#26469;&#35774;&#35745;&#26356;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#65292;&#20851;&#38190;&#22312;&#20110;&#36890;&#36807;&#32416;&#38169;&#20445;&#25345;&#19981;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#22312;&#21487;&#33021;&#24102;&#26469;&#30340;&#20934;&#30830;&#24615;&#21644;/&#25110;&#36895;&#24230;&#19978;&#30340;&#28508;&#22312;&#25910;&#30410;&#25442;&#21462;&#26631;&#20934;&#25968;&#20540;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;&#20445;&#35777;&#27714;&#35299;&#22120;&#36755;&#20986;&#31934;&#30830;&#35299;&#30340;&#21807;&#19968;&#26041;&#27861;&#26159;&#22312;&#32593;&#26684;&#38388;&#36317;$\Delta x$&#21644;&#26102;&#38388;&#27493;&#38271;$\Delta t$&#36235;&#36817;&#20110;&#38646;&#30340;&#26497;&#38480;&#19979;&#20351;&#29992;&#25910;&#25947;&#26041;&#27861;&#12290;&#23398;&#20250;&#22312;&#22823;$\Delta x$&#21644;/&#25110;$\Delta t$&#19979;&#26356;&#26032;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#27714;&#35299;&#22120;&#27704;&#36828;&#26080;&#27861;&#20445;&#35777;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;&#19968;&#23450;&#31243;&#24230;&#30340;&#35823;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#22240;&#27492;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#22914;&#20309;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27714;&#35299;&#22120;&#20197;&#32473;&#20986;&#25105;&#20204;&#24895;&#24847;&#23481;&#24525;&#30340;&#38169;&#35823;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#25345;&#22522;&#26412;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#27169;&#25311;&#20013;&#30340;&#36830;&#32493;&#19981;&#21464;&#37327;&#26469;&#35774;&#35745;&#26356;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#12290;&#36825;&#31867;&#19981;&#21464;&#37327;&#30340;&#31034;&#20363;&#21253;&#25324;&#36136;&#37327;&#23432;&#24658;&#12289;&#33021;&#37327;&#23432;&#24658;&#12289;&#28909;&#21147;&#23398;&#31532;&#20108;&#23450;&#24459;&#21644;&#65288;&#25110;&#65289;&#38750;&#36127;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#24456;&#31616;&#21333;&#65306;&#20026;&#20102;&#20445;&#25345;&#19981;&#21464;&#37327;&#65292;&#25105;&#20204;&#24378;&#21046;&#27714;&#35299;&#22120;&#30340;&#35823;&#24046;&#19982;&#28385;&#36275;&#30456;&#20851;&#19981;&#21464;&#37327;&#30340;&#31163;&#25955;&#20989;&#25968;&#31354;&#38388;&#27491;&#20132;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#31034;&#20363;PDE&#19978;&#28436;&#31034;&#20102;&#36825;&#19968;&#24605;&#24819;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;Burgers&#26041;&#31243;&#12289;Navier-Stokes&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;Euler&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learned partial differential equation (PDE) solvers trade the reliability of standard numerical methods for potential gains in accuracy and/or speed. The only way for a solver to guarantee that it outputs the exact solution is to use a convergent method in the limit that the grid spacing $\Delta x$ and timestep $\Delta t$ approach zero. Machine learned solvers, which learn to update the solution at large $\Delta x$ and/or $\Delta t$, can never guarantee perfect accuracy. Some amount of error is inevitable, so the question becomes: how do we constrain machine learned solvers to give us the sorts of errors that we are willing to tolerate? In this paper, we design more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE. Examples of such invariants include conservation of mass, conservation of energy, the second law of thermodynamics, and/or non-negative density. Our key insight is simple: to preserve invariants,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#21450;&#20854;&#27010;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#20844;&#36335;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16109</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles Using Transformer Networks. (arXiv:2303.16109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16109
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#21450;&#20854;&#27010;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#20844;&#36335;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20854;&#20182;&#36947;&#36335;&#29992;&#25143;&#65288;&#21253;&#25324;&#36710;&#36742;&#65289;&#30340;&#34892;&#20026;&#65288;&#21363;&#25805;&#20316;/&#36712;&#36857;&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#25110;&#33258;&#21160;&#21270;&#39550;&#39542;&#31995;&#32479;&#65288;ADSs&#65289;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#36710;&#36742;&#26410;&#26469;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#34892;&#39542;&#22330;&#26223;&#65292;&#19968;&#20010;&#36710;&#36742;&#36890;&#24120;&#23384;&#22312;&#22810;&#20010;&#26410;&#26469;&#34892;&#20026;&#27169;&#24335;&#26159;&#21512;&#29702;&#30340;&#12290;&#22240;&#27492;&#65292;&#22810;&#27169;&#24577;&#39044;&#27979;&#21487;&#20197;&#25552;&#20379;&#27604;&#21333;&#27169;&#24577;&#39044;&#27979;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20351;AV&#33021;&#22815;&#26356;&#22909;&#22320;&#35780;&#20272;&#39118;&#38505;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#26694;&#26550;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#21450;&#20854;&#27010;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#23450;&#21046;&#30340;&#25805;&#20316;&#39044;&#27979;&#38382;&#39064;&#20844;&#24335;&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#31181;&#23450;&#21046;&#30340;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#20844;&#36335;&#39550;&#39542;&#25968;&#25454;&#38598;&#65288;&#21363;NGSIM&#21644;highD&#65289;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#27169;&#24577;&#25805;&#20316;&#21644;&#36712;&#36857;&#39044;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#24471;&#21040;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the behaviour (i.e. manoeuvre/trajectory) of other road users, including vehicles, is critical for the safe and efficient operation of autonomous vehicles (AVs), a.k.a. automated driving systems (ADSs). Due to the uncertain future behaviour of vehicles, multiple future behaviour modes are often plausible for a vehicle in a given driving scene. Therefore, multimodal prediction can provide richer information than single-mode prediction enabling AVs to perform a better risk assessment. To this end, we propose a novel multimodal prediction framework that can predict multiple plausible behaviour modes and their likelihoods. The proposed framework includes a bespoke problem formulation for manoeuvre prediction, a novel transformer-based prediction model, and a tailored training method for multimodal manoeuvre and trajectory prediction. The performance of the framework is evaluated using two public benchmark highway driving datasets, namely NGSIM and highD. The results show that th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#31639;&#27861;&#26469;&#25552;&#21462;&#31232;&#30095;&#24120;&#25968;&#30697;&#38453;&#21015;&#23545;&#20013;&#30340;&#20849;&#21516;&#23376;&#34920;&#36798;&#24335;&#65292;&#20351;&#29992;&#21152;&#27861;&#26641;&#21387;&#32553;&#36825;&#20123;&#34920;&#36798;&#24335;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;5&#20493;&#30340;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#22312;CSE&#25552;&#21462;&#21644;&#30697;&#38453;&#20056;&#27861;&#26041;&#38754;&#26174;&#33879;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2303.16106</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#21516;&#23376;&#34920;&#36798;&#24335;&#30340;&#31232;&#30095;&#24120;&#25968;&#30697;&#38453;&#21387;&#32553;&#21644;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
Common Subexpression-based Compression and Multiplication of Sparse Constant Matrices. (arXiv:2303.16106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#31639;&#27861;&#26469;&#25552;&#21462;&#31232;&#30095;&#24120;&#25968;&#30697;&#38453;&#21015;&#23545;&#20013;&#30340;&#20849;&#21516;&#23376;&#34920;&#36798;&#24335;&#65292;&#20351;&#29992;&#21152;&#27861;&#26641;&#21387;&#32553;&#36825;&#20123;&#34920;&#36798;&#24335;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;5&#20493;&#30340;&#21387;&#32553;&#29575;&#65292;&#21516;&#26102;&#22312;CSE&#25552;&#21462;&#21644;&#30697;&#38453;&#20056;&#27861;&#26041;&#38754;&#26174;&#33879;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#65292;&#27169;&#22411;&#21442;&#25968;&#34987;&#20462;&#21098;&#21644;&#37327;&#21270;&#20197;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#12290;&#21387;&#32553;&#26041;&#27861;&#21644;&#20849;&#21516;&#23376;&#34920;&#36798;&#24335;&#28040;&#38500;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#31232;&#30095;&#24120;&#25968;&#30697;&#38453;&#20197;&#22312;&#20302;&#25104;&#26412;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#20849;&#21516;&#23376;&#34920;&#36798;&#24335;&#28040;&#38500;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#22411;&#30697;&#38453;&#26102;&#32553;&#25918;&#24182;&#19981;&#33391;&#22909;&#12290;&#22312;200x200&#30697;&#38453;&#20013;&#25552;&#21462;CSE&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#65292;&#32780;&#23427;&#20204;&#30340;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#27604;&#20256;&#32479;&#30697;&#38453;&#20056;&#27861;&#26041;&#27861;&#25191;&#34892;&#26102;&#38388;&#26356;&#38271;&#12290;&#27492;&#22806;&#65292;&#19981;&#23384;&#22312;&#21033;&#29992;CSE&#30340;&#30697;&#38453;&#21387;&#32553;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#31639;&#27861;&#26469;&#25552;&#21462;&#24120;&#25968;&#30697;&#38453;&#21015;&#23545;&#20013;&#30340;CSE&#12290;&#23427;&#21487;&#22312;&#19968;&#20998;&#38047;&#20869;&#20026;1000x1000&#30697;&#38453;&#29983;&#25104;&#21152;&#27861;&#26641;&#12290;&#20026;&#20102;&#21387;&#32553;&#21152;&#27861;&#26641;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#21387;&#32553;&#31232;&#30095;&#34892;&#65288;CSR&#65289;&#20197;&#21253;&#25324;CSE&#30340;&#21387;&#32553;&#26684;&#24335;&#12290;&#34429;&#28982;&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;5&#20493;&#30340;&#21387;&#32553;&#29575;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;CSE&#25552;&#21462;&#21644;&#30697;&#38453;&#20056;&#27861;&#26041;&#38754;&#37117;&#20855;&#26377;&#26174;&#30528;&#30340;&#21152;&#36895;&#20316;&#29992;&#65292;&#20351;&#23427;&#20204;&#26356;&#36866;&#21512;&#22312;&#20302;&#25104;&#26412;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning inference, model parameters are pruned and quantized to reduce the model size. Compression methods and common subexpression (CSE) elimination algorithms are applied on sparse constant matrices to deploy the models on low-cost embedded devices. However, the state-of-the-art CSE elimination methods do not scale well for handling large matrices. They reach hours for extracting CSEs in a $200 \times 200$ matrix while their matrix multiplication algorithms execute longer than the conventional matrix multiplication methods. Besides, there exist no compression methods for matrices utilizing CSEs. As a remedy to this problem, a random search-based algorithm is proposed in this paper to extract CSEs in the column pairs of a constant matrix. It produces an adder tree for a $1000 \times 1000$ matrix in a minute. To compress the adder tree, this paper presents a compression format by extending the Compressed Sparse Row (CSR) to include CSEs. While compression rates of more than $5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16105</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21464;&#20998;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variational Distribution Learning for Unsupervised Text-to-Image Generation. (arXiv:2303.16105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#22270;&#20687;&#30340;&#25991;&#26412;&#35828;&#26126;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#26469;&#29983;&#25104;&#22270;&#20687;&#19982;&#23545;&#24212;&#25991;&#26412;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23558;&#20004;&#20010;&#22495;&#20013;&#30340;&#25968;&#25454;&#23545;&#40784;&#65292;&#24182;&#20197;&#25968;&#25454;&#23545;&#30340;&#22270;&#20687;-&#25991;&#26412;CLIP&#23884;&#20837;&#20026;&#26465;&#20214;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#26469;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;&#36825;&#20004;&#20010;&#22495;&#20013;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#22522;&#20110;&#21464;&#20998;&#25512;&#26029;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#32473;&#23450;&#22270;&#20687;&#21450;&#20854;CLIP&#29305;&#24449;&#30340;&#38544;&#34255;&#25991;&#26412;&#23884;&#20837;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#30528;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a text-to-image generation algorithm based on deep neural networks when text captions for images are unavailable during training. In this work, instead of simply generating pseudo-ground-truth sentences of training images using existing image captioning methods, we employ a pretrained CLIP model, which is capable of properly aligning embeddings of images and corresponding texts in a joint space and, consequently, works well on zero-shot recognition tasks. We optimize a text-to-image generation model by maximizing the data log-likelihood conditioned on pairs of image-text CLIP embeddings. To better align data in the two domains, we employ a principled way based on a variational inference, which efficiently estimates an approximate posterior of the hidden text embedding given an image and its CLIP feature. Experimental results validate that the proposed framework outperforms existing approaches by large margins under unsupervised and semi-supervised text-to-image generation se
&lt;/p&gt;</description></item><item><title>adapter-ALBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;NLP&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#29616;&#22810;&#20219;&#21153;&#25512;&#29702;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#23454;&#29616;&#25968;&#25454;&#37325;&#29992;&#65292;&#24182;&#25552;&#39640;&#23545;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16100</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#20869;&#23384;&#32467;&#26500;&#30340;&#36793;&#32536;NLP&#25512;&#29702;&#30340;&#33410;&#33021;&#20219;&#21153;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures. (arXiv:2303.16100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16100
&lt;/p&gt;
&lt;p&gt;
adapter-ALBERT&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;NLP&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23454;&#29616;&#22810;&#20219;&#21153;&#25512;&#29702;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#23454;&#29616;&#25968;&#25454;&#37325;&#29992;&#65292;&#24182;&#25552;&#39640;&#23545;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#20219;&#21153;&#38656;&#35201;&#20180;&#32454;&#30340;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;ALBERT&#21487;&#20197;&#29992;&#20110;&#22312;&#31227;&#21160;&#31995;&#32479;&#32423;&#33455;&#29255;&#19978;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25512;&#29702;&#65292;&#24182;&#37197;&#22791;&#33258;&#23450;&#20041;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#34429;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#32531;&#35299;&#36816;&#34892;&#21333;&#20010;NLP&#20219;&#21153;&#30340;&#24310;&#36831;&#12289;&#33021;&#37327;&#21644;&#38754;&#31215;&#25104;&#26412;&#65292;&#20294;&#23545;&#20110;&#23454;&#29616;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#38656;&#35201;&#22312;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#20219;&#21153;&#30340;&#22810;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#22522;&#30784;&#19978;&#25191;&#34892;&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#33455;&#29255;&#20869;&#23384;&#38656;&#27714;&#25110;&#25903;&#20184;&#33455;&#29255;&#22806;&#37096;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;adapter-ALBERT&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26368;&#22823;&#21270;&#25968;&#25454;&#37325;&#29992;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#23545;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Executing machine learning inference tasks on resource-constrained edge devices requires careful hardware-software co-design optimizations. Recent examples have shown how transformer-based deep neural network models such as ALBERT can be used to enable the execution of natural language processing (NLP) inference on mobile systems-on-chip housing custom hardware accelerators. However, while these existing solutions are effective in alleviating the latency, energy, and area costs of running single NLP tasks, achieving multi-task inference requires running computations over multiple variants of the model parameters, which are tailored to each of the targeted tasks. This approach leads to either prohibitive on-chip memory requirements or paying the cost of off-chip memory access. This paper proposes adapter-ALBERT, an efficient model optimization for maximal data reuse across different tasks. The proposed model's performance and robustness to data compression methods are evaluated across s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#31569;&#33021;&#32791;&#24322;&#24120;&#26816;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#35270;&#21270;&#26041;&#27861;&#36741;&#21161;&#29702;&#35299;&#27169;&#22411;&#25152;&#25429;&#25417;&#21040;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.16097</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#24314;&#31569;&#33021;&#32791;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attention Boosted Autoencoder for Building Energy Anomaly Detection. (arXiv:2303.16097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#31569;&#33021;&#32791;&#24322;&#24120;&#26816;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#35270;&#21270;&#26041;&#27861;&#36741;&#21161;&#29702;&#35299;&#27169;&#22411;&#25152;&#25429;&#25417;&#21040;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24314;&#31569;&#26234;&#33021;&#30005;&#34920;&#25910;&#38598;&#30340;&#25968;&#25454;&#21487;&#20197;&#24110;&#21161;&#21046;&#23450;&#33410;&#33021;&#25919;&#31574;&#12290;&#22914;&#26524;&#33021;&#22815;&#21450;&#26089;&#26816;&#27979;&#24314;&#31569;&#36816;&#34892;&#29366;&#20917;&#30340;&#20559;&#24046;&#65292;&#24182;&#37319;&#21462;&#36866;&#24403;&#30340;&#25514;&#26045;&#65292;&#23558;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#33410;&#33021;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21457;&#29616;&#25910;&#38598;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;&#30446;&#21069;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#26469;&#25429;&#25417;&#27491;&#24120;&#25110;&#21487;&#25509;&#21463;&#30340;&#36816;&#34892;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24314;&#27169;&#24314;&#31569;&#30340;&#33021;&#32791;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#26679;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#25429;&#25417;&#36825;&#20123;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21576;&#29616;&#20102;&#32467;&#26524;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#26041;&#27861;&#26469;&#29702;&#35299;&#27169;&#22411;&#25152;&#25429;&#25417;&#21040;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging data collected from smart meters in buildings can aid in developing policies towards energy conservation. Significant energy savings could be realised if deviations in the building operating conditions are detected early, and appropriate measures are taken. Towards this end, machine learning techniques can be used to automate the discovery of these abnormal patterns in the collected data. Current methods in anomaly detection rely on an underlying model to capture the usual or acceptable operating behaviour. In this paper, we propose a novel attention mechanism to model the consumption behaviour of a building and demonstrate the effectiveness of the model in capturing the relations using sample case studies. A real-world dataset is modelled using the proposed architecture, and the results are presented. A visualisation approach towards understanding the relations captured by the model is also presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21407;&#23376;&#23398;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#30456;&#31354;&#38388;&#31215;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26367;&#20195;&#20256;&#32479;&#25968;&#20540;&#31215;&#20998;&#35268;&#21017;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16088</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#30456;&#31354;&#38388;&#31215;&#20998;&#22312;&#21407;&#23376;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GNN-Assisted Phase Space Integration with Application to Atomistics. (arXiv:2303.16088v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21407;&#23376;&#23398;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#30456;&#31354;&#38388;&#31215;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26367;&#20195;&#20256;&#32479;&#25968;&#20540;&#31215;&#20998;&#35268;&#21017;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#27169;&#25311;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#36716;&#25442;&#20026;&#22522;&#20110;&#32479;&#35745;&#21147;&#23398;&#30340;&#30456;&#31354;&#38388;&#34920;&#31034;&#65292;&#21487;&#20197;&#20811;&#26381;&#21407;&#23376;&#23398;&#30340;&#26102;&#38388;&#23610;&#24230;&#38480;&#21046;&#12290;&#22312;&#30456;&#31354;&#38388;&#20013;&#65292;&#26368;&#22823;&#29109;&#25110;&#39640;&#26031;&#30456;&#21253;&#65288;GPP&#65289;&#31561;&#36817;&#20284;&#26041;&#27861;&#20197;&#26102;&#38388;&#32553;&#20943;&#30340;&#26041;&#24335;&#28436;&#21270;&#21407;&#23376;&#38598;&#21512;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#38656;&#35201;&#35745;&#31639;&#21407;&#23376;&#38598;&#21512;&#22312;&#25972;&#20010;&#30456;&#31354;&#38388;&#20013;&#26114;&#36149;&#30340;&#39640;&#32500;&#31215;&#20998;&#12290;&#24120;&#24120;&#21033;&#29992;&#20302;&#38454;&#25968;&#20540;&#31215;&#20998;&#26469;&#39640;&#25928;&#23436;&#25104;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27492;&#19978;&#19979;&#25991;&#20013;&#30340;&#25968;&#20540;&#31215;&#20998;&#23384;&#22312;&#22266;&#26377;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22788;&#29702;&#24102;&#26377;&#32570;&#38519;&#30340;&#26230;&#26684;&#26102;&#65292;&#20250;&#25439;&#22351;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Monte-Carlo&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20316;&#20026;&#24120;&#29992;&#25968;&#20540;&#31215;&#20998;&#35268;&#21017;&#30340;&#26367;&#20195;&#21697;&#65292;&#20811;&#26381;&#20102;&#23427;&#20204;&#30340;&#32570;&#38519;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#25311;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overcoming the time scale limitations of atomistics can be achieved by switching from the state-space representation of Molecular Dynamics (MD) to a statistical-mechanics-based representation in phase space, where approximations such as maximum-entropy or Gaussian phase packets (GPP) evolve the atomistic ensemble in a time-coarsened fashion. In practice, this requires the computation of expensive high-dimensional integrals over all of phase space of an atomistic ensemble. This, in turn, is commonly accomplished efficiently by low-order numerical quadrature. We show that numerical quadrature in this context, unfortunately, comes with a set of inherent problems, which corrupt the accuracy of simulations -- especially when dealing with crystal lattices with imperfections. As a remedy, we demonstrate that Graph Neural Networks, trained on Monte-Carlo data, can serve as a replacement for commonly used numerical quadrature rules, overcoming their deficiencies and significantly improving the 
&lt;/p&gt;</description></item><item><title>FedLEO&#26159;&#19968;&#31181;&#22312;&#36229;&#23494;&#38598;LEO&#21355;&#26143;&#26143;&#24231;&#19978;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;LEO&#19978;&#36827;&#34892;&#35270;&#39057;/&#25968;&#25454;&#20256;&#36755;&#21644;&#32858;&#31867;&#65292;&#32780;&#20302;&#26102;&#24310;&#30340;&#22320;&#38754;&#32593;&#20851;&#26381;&#21153;&#22120; (GS) &#20165;&#36127;&#36131;&#21021;&#22987;&#20449;&#21495;&#25511;&#21046;&#12290;&#38543;&#30528;LEO&#26381;&#21153;&#22120;&#30340;&#21464;&#21270;&#21644;&#37325;&#26032;&#32858;&#31867;&#65292;FedLEO&#33021;&#22815;&#36866;&#24212;&#21160;&#24577;LEO&#26143;&#24231;&#24182;&#22312;&#19981;&#21516;&#30340;LEO&#26143;&#24231;&#35774;&#32622;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16071</link><description>&lt;p&gt;
&#20809;&#23398;&#26143;&#38388;&#38142;&#36335;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#36793;&#32536;&#36873;&#25321;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Edge Selection and Clustering for Federated Learning in Optical Inter-LEO Satellite Constellation. (arXiv:2303.16071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16071
&lt;/p&gt;
&lt;p&gt;
FedLEO&#26159;&#19968;&#31181;&#22312;&#36229;&#23494;&#38598;LEO&#21355;&#26143;&#26143;&#24231;&#19978;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;LEO&#19978;&#36827;&#34892;&#35270;&#39057;/&#25968;&#25454;&#20256;&#36755;&#21644;&#32858;&#31867;&#65292;&#32780;&#20302;&#26102;&#24310;&#30340;&#22320;&#38754;&#32593;&#20851;&#26381;&#21153;&#22120; (GS) &#20165;&#36127;&#36131;&#21021;&#22987;&#20449;&#21495;&#25511;&#21046;&#12290;&#38543;&#30528;LEO&#26381;&#21153;&#22120;&#30340;&#21464;&#21270;&#21644;&#37325;&#26032;&#32858;&#31867;&#65292;FedLEO&#33021;&#22815;&#36866;&#24212;&#21160;&#24577;LEO&#26143;&#24231;&#24182;&#22312;&#19981;&#21516;&#30340;LEO&#26143;&#24231;&#35774;&#32622;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947; (LEO) &#21355;&#26143;&#21487;&#20197;&#25910;&#38598;&#22823;&#37327;&#30340;&#24433;&#20687;&#25110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22240;&#27492;&#24050;&#32463;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#65292;&#25968;&#25454;&#35757;&#32451;&#36807;&#31243;&#26159;&#22312;&#22320;&#38754;&#20113;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#24456;&#39640;&#30340;&#20256;&#36755;&#24320;&#38144;&#12290;&#26368;&#36817; LEO &#30340;&#21457;&#23637;&#26356;&#36843;&#20999;&#22320;&#38656;&#35201;&#25552;&#20379;&#20855;&#22791;&#22686;&#24378;&#30340;&#26426;&#36733;&#35745;&#31639;&#33021;&#21147;&#30340;&#36229;&#23494;&#38598; LEO &#21355;&#26143;&#26143;&#24231;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; LEO &#21355;&#26143;&#26143;&#24231;&#19978;&#36827;&#34892;&#21327;&#20316;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861; (FedLEO)&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#36807;&#31243;&#37117;&#20998;&#37197;&#22312;&#20855;&#22791;&#20302;&#36733;&#33655;&#26143;&#38388;&#20256;&#36755;&#30340; LEO &#19978;&#65292;&#32780;&#20302;&#26102;&#24310;&#30340;&#22320;&#38754;&#32593;&#20851;&#26381;&#21153;&#22120; (GS) &#20165;&#36127;&#36131;&#21021;&#22987;&#20449;&#21495;&#25511;&#21046;&#12290;GS &#39318;&#20808;&#36873;&#25321;&#19968;&#20010; LEO &#26381;&#21153;&#22120;&#65292;&#32780;&#20854; LEO &#23458;&#25143;&#31471;&#37117;&#36890;&#36807;&#32858;&#31867;&#26426;&#21046;&#21644;&#20809;&#23398;&#26143;&#38388;&#38142;&#36335;(ISLs)&#30340;&#36890;&#20449;&#33021;&#21147;&#20915;&#23450;&#12290;&#26356;&#25442; LEO &#26381;&#21153;&#22120;&#26102;&#30340;&#37325;&#26032;&#32858;&#31867;&#23558;&#35302;&#21457;&#26032;&#19968;&#36718;&#30340; LEO &#23458;&#25143;&#31471;&#32858;&#31867;&#65292;&#20174;&#32780;&#20351; FedLEO &#33021;&#22815;&#36866;&#24212;&#21160;&#24577; LEO &#26143;&#24231;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;FedLEO &#21487;&#20197;&#22312;&#19981;&#21516;&#30340; LEO &#26143;&#24231;&#35774;&#32622;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Earth orbit (LEO) satellites have been prosperously deployed for various Earth observation missions due to its capability of collecting a large amount of image or sensor data. However, traditionally, the data training process is performed in the terrestrial cloud server, which leads to a high transmission overhead. With the recent development of LEO, it is more imperative to provide ultra-dense LEO constellation with enhanced on-board computation capability. Benefited from it, we have proposed a collaborative federated learning over LEO satellite constellation (FedLEO). We allocate the entire process on LEOs with low payload inter-satellite transmissions, whilst the low-delay terrestrial gateway server (GS) only takes care for initial signal controlling. The GS initially selects an LEO server, whereas its LEO clients are all determined by clustering mechanism and communication capability through the optical inter-satellite links (ISLs). The re-clustering of changing LEO server will
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24816;&#24615;&#23398;&#20064;&#21482;&#23545;&#38169;&#35823;&#26679;&#26412;&#36827;&#34892;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#24555;&#36895;&#12289;&#33410;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#23618;MLP&#27169;&#22411;&#19978;&#36798;&#21040;&#20102;99.2&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#21305;&#37197;&#30340;&#21453;&#21521;&#20256;&#25773;&#32593;&#32476;&#24555;7.6&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.16067</link><description>&lt;p&gt;
&#24816;&#24615;&#23398;&#20064;&#65306;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#24555;&#36895;&#12289;&#33410;&#33021;&#31361;&#35302;&#21487;&#22609;&#24615;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity. (arXiv:2303.16067v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16067
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24816;&#24615;&#23398;&#20064;&#21482;&#23545;&#38169;&#35823;&#26679;&#26412;&#36827;&#34892;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#24555;&#36895;&#12289;&#33410;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#23618;MLP&#27169;&#22411;&#19978;&#36798;&#21040;&#20102;99.2&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#21305;&#37197;&#30340;&#21453;&#21521;&#20256;&#25773;&#32593;&#32476;&#24555;7.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#21363;&#20351;&#26679;&#26412;&#34987;&#27491;&#30830;&#20998;&#31867;&#65292;&#21442;&#25968;&#20063;&#20250;&#22312;&#27599;&#20010;&#35797;&#39564;&#20013;&#26356;&#26032;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#38598;&#20013;&#23398;&#20064;&#24046;&#38169;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24816;&#24615;&#23398;&#20064;&#65292;&#20165;&#23545;&#38169;&#35823;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#12290;&#24816;&#24615;&#23398;&#20064;&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#24816;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#25968;&#25454;&#38598;&#36739;&#22823;&#26102;&#29305;&#21035;&#36866;&#29992;&#12290;&#20363;&#22914;&#65292;&#22312;&#20351;&#29992;&#21333;&#23618;MLP&#23545;&#25193;&#23637;MNIST&#36827;&#34892;&#27979;&#35797;&#20934;&#30830;&#29575;&#36798;&#21040;99.2&#65285;&#65292;&#24182;&#19988;&#27604;&#21305;&#37197;&#21453;&#21521;&#20256;&#25773;&#32593;&#32476;&#24555;7.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training neural networks for classification tasks with backpropagation, parameters are updated on every trial, even if the sample is classified correctly. In contrast, humans concentrate their learning effort on errors. Inspired by human learning, we introduce lazy learning, which only learns on incorrect samples. Lazy learning can be implemented in a few lines of code and requires no hyperparameter tuning. Lazy learning achieves state-of-the-art performance and is particularly suited when datasets are large. For instance, it reaches 99.2% test accuracy on Extended MNIST using a single-layer MLP, and does so 7.6x faster than a matched backprop network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#34928;&#31469;&#21551;&#31034;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20248;&#21270;&#21521;&#20840;&#23616;&#20998;&#31867;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#20840;&#23616;&#35760;&#24518;&#21521;&#37327;&#26469;&#34917;&#25937;&#21442;&#25968;&#27874;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16066</link><description>&lt;p&gt;
&#21463;&#31070;&#32463;&#34928;&#31469;&#21551;&#31034;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse Inspired Federated Learning with Non-iid Data. (arXiv:2303.16066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#34928;&#31469;&#21551;&#31034;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20248;&#21270;&#21521;&#20840;&#23616;&#20998;&#31867;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#20840;&#23616;&#35760;&#24518;&#21521;&#37327;&#26469;&#34917;&#25937;&#21442;&#25968;&#27874;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#24322;&#26500;&#35774;&#22791;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;&#38750;iid&#65289;&#29305;&#24615;&#65292;&#23548;&#33268;&#26412;&#22320;&#26356;&#26032;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24433;&#21709;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#26412;&#22320;&#35757;&#32451;&#21644;&#32858;&#21512;&#36807;&#31243;&#20197;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#26410;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#21463;&#31070;&#32463;&#34928;&#31469;&#29616;&#35937;&#21551;&#21457;&#65292;&#25105;&#20204;&#24378;&#21046;&#27599;&#20010;&#23458;&#25143;&#31471;&#20248;&#21270;&#21521;&#20840;&#23616;&#20998;&#31867;&#30340;&#26368;&#20339;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20854;&#21021;&#22987;&#21270;&#20026;&#38543;&#26426;&#30340;&#31616;&#21333;&#20845;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;&#24182;&#22312;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#23558;&#20854;&#20316;&#20026;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#21333;&#20803;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#22266;&#23450;&#12290;&#22312;&#30830;&#20445;&#25152;&#26377;&#23458;&#25143;&#31471;&#23398;&#20064;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20026;&#27599;&#20010;&#31867;&#21035;&#28155;&#21152;&#20840;&#23616;&#35760;&#24518;&#21521;&#37327;&#65292;&#20197;&#34917;&#25937;&#30001;&#20110;&#31867;&#20869;&#26465;&#20214;&#20998;&#24067;&#20559;&#24046;&#24341;&#36215;&#30340;&#21442;&#25968;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in federated learning is the non-independent and identically distributed (non-iid) characteristics between heterogeneous devices, which cause significant differences in local updates and affect the performance of the central server. Although many studies have been proposed to address this challenge, they only focus on local training and aggregation processes to smooth the changes and fail to achieve high performance with deep learning models. Inspired by the phenomenon of neural collapse, we force each client to be optimized toward an optimal global structure for classification. Specifically, we initialize it as a random simplex Equiangular Tight Frame (ETF) and fix it as the unit optimization target of all clients during the local updating. After guaranteeing all clients are learning to converge to the global optimum, we propose to add a global memory vector for each category to remedy the parameter fluctuation caused by the bias of the intra-class condition dist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30693;&#35782;&#33976;&#39311;&#30340; GAN &#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#21464;&#20998;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19979;&#38480;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309; GAN &#25110;&#23494;&#38598;&#39044;&#27979;&#32593;&#32476;&#65292;&#24050;&#22312;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16050</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#33021;&#37327;&#27169;&#22411;&#30340;&#20449;&#24687;&#29702;&#35770; GAN &#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic GAN Compression with Variational Energy-based Model. (arXiv:2303.16050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30693;&#35782;&#33976;&#39311;&#30340; GAN &#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#21464;&#20998;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19979;&#38480;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309; GAN &#25110;&#23494;&#38598;&#39044;&#27979;&#32593;&#32476;&#65292;&#24050;&#22312;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30693;&#35782;&#33976;&#39311;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#21464;&#20998;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#30001;&#20110;&#22312;&#36830;&#32493;&#22495;&#20013;&#30452;&#25509;&#35745;&#31639;&#20114;&#20449;&#24687;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#21464;&#20998;&#19979;&#38480;&#20195;&#26367;&#20248;&#21270;&#23398;&#29983;&#32593;&#32476;&#12290;&#20026;&#20102;&#23454;&#29616;&#32039;&#23494;&#30340;&#19979;&#38480;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#22788;&#29702;&#39640;&#32500;&#22270;&#20687;&#21644;&#20687;&#32032;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#28789;&#27963;&#21464;&#20998;&#20998;&#24067;&#12290;&#30001;&#20110;&#36825;&#20010;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#26041;&#20415;&#22320;&#21152;&#20837;&#21040;&#20219;&#20309;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#65292;&#29978;&#33267;&#26159;&#23494;&#38598;&#39044;&#27979;&#32593;&#32476;&#65292;&#20363;&#22914;&#22270;&#20687;&#22686;&#24378;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an information-theoretic knowledge distillation approach for the compression of generative adversarial networks, which aims to maximize the mutual information between teacher and student networks via a variational optimization based on an energy-based model. Because the direct computation of the mutual information in continuous domains is intractable, our approach alternatively optimizes the student network by maximizing the variational lower bound of the mutual information. To achieve a tight lower bound, we introduce an energy-based model relying on a deep neural network to represent a flexible variational distribution that deals with high-dimensional images and consider spatial dependencies between pixels, effectively. Since the proposed method is a generic optimization algorithm, it can be conveniently incorporated into arbitrary generative adversarial networks and even dense prediction networks, e.g., image enhancement models. We demonstrate that the proposed algorithm 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16047</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25506;&#32034;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#25972;&#20010;&#20248;&#31168;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39046;&#22495;&#19987;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#29983;&#25104;&#21333;&#20010;&#27169;&#22411;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#19981;&#21033;&#20110;&#27492;&#31867;&#20132;&#20114;&#12290;&#36817;&#20284;&#21644;&#25506;&#32034;Rashomon&#38598;&#65292;&#21363;&#25152;&#26377;&#36817;&#20046;&#26368;&#20248;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#29992;&#25143;&#21487;&#25628;&#32034;&#30340;&#31354;&#38388;&#21253;&#21547;&#22810;&#26679;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#36817;&#20284;&#20855;&#26377;&#22266;&#23450;&#25903;&#25345;&#38598;&#30340;GAMs&#30340;Rashomon&#38598;&#30340;&#26925;&#29699;&#24418;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26925;&#29699;&#24418;&#36817;&#20284;&#20102;&#35768;&#22810;&#19981;&#21516;&#25903;&#25345;&#38598;&#30340;Rashomon&#38598;&#12290;&#36817;&#20284;&#30340;Rashomon&#38598;&#20026;&#35299;&#20915;&#23454;&#38469;&#25361;&#25112;&#65292;&#20363;&#22914;&#65288;1&#65289;&#30740;&#31350;&#27169;&#22411;&#31867;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65307;&#65288;2&#65289;&#22312;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;&#26597;&#25214;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#21644;&#24635;&#32467;&#65292;&#36825;&#26159;&#19968;&#31181;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.16004</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Malware Detection with Graph Representation Learning. (arXiv:2303.16004v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#21644;&#24635;&#32467;&#65292;&#36825;&#26159;&#19968;&#31181;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#31614;&#21517;&#21644;&#21551;&#21457;&#24335;&#30340;&#26816;&#27979;&#26041;&#27861;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#22312;&#26410;&#30693;&#25915;&#20987;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#28151;&#28102;&#25216;&#26415;&#36731;&#26494;&#35268;&#36991;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#24050;&#25104;&#20026;&#20256;&#32479;&#26041;&#27861;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#22312;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#25968;&#25454;&#19978;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;&#24694;&#24847;&#36719;&#20214;&#20013;&#23398;&#20064;&#26356;&#20581;&#22766;&#34920;&#31034;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#20851;&#20110;&#22522;&#20110;&#22270;&#24418;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#28145;&#20837;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#27010;&#25324;&#21644;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware detection has become a major concern due to the increasing number and complexity of malware. Traditional detection methods based on signatures and heuristics are used for malware detection, but unfortunately, they suffer from poor generalization to unknown attacks and can be easily circumvented using obfuscation techniques. In recent years, Machine Learning (ML) and notably Deep Learning (DL) achieved impressive results in malware detection by learning useful representations from data and have become a solution preferred over traditional methods. More recently, the application of such techniques on graph-structured data has achieved state-of-the-art performance in various domains and demonstrates promising results in learning more robust representations from malware. Yet, no literature review focusing on graph-based deep learning for malware detection exists. In this survey, we provide an in-depth literature review to summarize and unify existing works under the common approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#30452;&#25509;&#20174;&#22270;&#29255;&#20013;&#35745;&#31639;&#31616;&#32504;&#32433;&#32447;&#30340;&#32447;&#23494;&#24230;&#65292;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;&#65292;&#20026;&#33402;&#26415;&#21697;&#20445;&#25252;&#21644;&#20462;&#22797;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#32447;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15999</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#22238;&#24402;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31616;&#32504;&#32433;&#32447;&#25968;&#30446;&#35745;&#31639;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;
&lt;/p&gt;
&lt;p&gt;
Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models. (arXiv:2303.15999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#30452;&#25509;&#20174;&#22270;&#29255;&#20013;&#35745;&#31639;&#31616;&#32504;&#32433;&#32447;&#30340;&#32447;&#23494;&#24230;&#65292;&#20197;&#24212;&#29992;&#20110;&#21476;&#32769;&#27833;&#30011;&#65292;&#20026;&#33402;&#26415;&#21697;&#20445;&#25252;&#21644;&#20462;&#22797;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#32447;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#23545;&#31616;&#32504;&#32433;&#32447;&#23494;&#24230;&#30340;&#20272;&#35745;&#65292;&#20197;&#36827;&#34892;&#30011;&#24067;&#20998;&#26512;&#12290;&#35813;&#26032;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#20174;&#22270;&#20687;&#20013;&#35745;&#31639;&#32447;&#23494;&#24230;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#22312;&#33402;&#26415;&#20445;&#23384;&#21644;&#20462;&#22797;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work the authors develop regression approaches based on deep learning to perform thread density estimation for plain weave canvas analysis. Previous approaches were based on Fourier analysis, that are quite robust for some scenarios but fail in some other, in machine learning tools, that involve pre-labeling of the painting at hand, or the segmentation of thread crossing points, that provides good estimations in all scenarios with no need of pre-labeling. The segmentation approach is time-consuming as estimation of the densities is performed after locating the crossing points. In this novel proposal, we avoid this step by computing the density of threads directly from the image with a regression deep learning model. We also incorporate some improvements in the initial preprocessing of the input image with an impact on the final error. Several models are proposed and analyzed to retain the best one. Furthermore, we further reduce the density estimation error by introducing a sem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.15991</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#28145;&#65292;&#36825;&#38459;&#30861;&#20102;&#32852;&#21512;&#23398;&#20064;&#31561;&#38544;&#31169;&#22686;&#24378;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#24335;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#30340;&#27665;&#20027;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#23558;&#36793;&#32536;&#35745;&#31639;&#33539;&#24335;&#21644;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;PSL&#65289;&#30456;&#32467;&#21512;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#36890;&#36807;&#36880;&#23618;&#27169;&#22411;&#20998;&#35010;&#23558;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;PSL&#26041;&#26696;&#20250;&#20135;&#29983;&#36807;&#22810;&#30340;&#35757;&#32451;&#24310;&#36831;&#21644;&#22823;&#37327;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PSL&#26694;&#26550;&#8212;&#8212;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#65292;&#20197;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPSL&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#24182;&#34892;&#21270;&#65292;&#24182;&#36890;&#36807;&#26368;&#21518;&#19968;&#23618;&#26799;&#24230;&#32858;&#21512;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#35774;&#22791;&#30340;&#24322;&#26500;&#36890;&#36947;&#26465;&#20214;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPSL&#36890;&#36807;&#23558;&#36890;&#20449;&#25104;&#26412;&#21644;&#35757;&#32451;&#26102;&#38388;&#20998;&#21035;&#38477;&#20302;76&#65285;&#21644;63&#65285;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15975</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#38271;&#23398;&#20064;&#32773;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#36830;&#32493;&#22320;&#21457;&#29616;&#26032;&#27010;&#24565;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26399;&#26395;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#38750;&#24120;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#37096;&#20998;&#35299;&#20915;&#65292;&#20854;&#20013;&#35201;&#20040;&#20026;&#21457;&#29616;&#26032;&#27010;&#24565;&#25552;&#20379;&#26377;&#26631;&#21495;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914; NCD&#65289;&#65292;&#35201;&#20040;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22686;&#37327;&#27493;&#39588;&#20013;&#21457;&#29983;&#65288;&#20363;&#22914;&#31867; iNCD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026; MSc-iNCD&#65292;&#20854;&#20013;&#23398;&#20064;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#22312;&#36739;&#38271;&#30340;&#23398;&#20064;&#24773;&#22659;&#19979;&#20855;&#26377;&#24377;&#24615;&#65292;&#32780;&#19988;&#19982;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20934;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;CTM&#12289;ProdLDA&#21644;ETM&#65289;&#65292;&#21033;&#29992;&#22235;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#25506;&#35752;&#20102;&#20351;&#29992;dropout&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.15973</link><description>&lt;p&gt;
&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30495;&#30340;&#38656;&#35201;&#20351;&#29992;dropout&#21527;&#65311;&#20851;&#20110;dropout&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling. (arXiv:2303.15973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;CTM&#12289;ProdLDA&#21644;ETM&#65289;&#65292;&#21033;&#29992;&#22235;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#25506;&#35752;&#20102;&#20351;&#29992;dropout&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dropout&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#25216;&#24039;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#23613;&#31649;&#36825;&#31181;&#27491;&#21017;&#21270;&#25216;&#24039;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#26080;&#30417;&#30563;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;VAE&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65289;&#65292;&#32570;&#20047;&#23545;&#20854;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;&#21363;&#65292;&#24773;&#22659;&#20027;&#39064;&#27169;&#22411;&#65288;CTM&#65289;&#65292;ProdLDA&#21644;&#23884;&#20837;&#24335;&#20027;&#39064;&#27169;&#22411;&#65288;ETM&#65289;&#65289;&#20013;&#65292;&#21033;&#29992;&#22235;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;VAE&#26550;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;dropout&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#20174;&#29983;&#25104;&#30340;&#20027;&#39064;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#24615;&#33021;&#30340;&#35282;&#24230;&#65292;&#34920;&#24449;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;dropout&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dropout is a widely used regularization trick to resolve the overfitting issue in large feedforward neural networks trained on a small dataset, which performs poorly on the held-out test subset. Although the effectiveness of this regularization trick has been extensively studied for convolutional neural networks, there is a lack of analysis of it for unsupervised models and in particular, VAE-based neural topic models. In this paper, we have analyzed the consequences of dropout in the encoder as well as in the decoder of the VAE architecture in three widely used neural topic models, namely, contextualized topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly available datasets. We characterize the dropout effect on these models in terms of the quality and predictive performance of the generated topics.
&lt;/p&gt;</description></item><item><title>SFHarmony&#26159;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#19988;&#33021;&#26377;&#25928;&#21327;&#35843;&#36328;&#25195;&#25551;&#22120;&#21644;&#30740;&#31350;&#30340;&#25968;&#25454;&#20197;&#21450;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15965</link><description>&lt;p&gt;
SFHarmony&#65306;&#26080;&#38656;&#28304;&#25968;&#25454;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis. (arXiv:2303.15965v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15965
&lt;/p&gt;
&lt;p&gt;
SFHarmony&#26159;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#19988;&#33021;&#26377;&#25928;&#21327;&#35843;&#36328;&#25195;&#25551;&#22120;&#21644;&#30740;&#31350;&#30340;&#25968;&#25454;&#20197;&#21450;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20195;&#34920;&#20020;&#24202;&#31070;&#32463;&#24433;&#20687;&#20154;&#32676;&#30340;&#29983;&#29289;&#23398;&#21464;&#24322;&#65292;&#33021;&#22815;&#21512;&#24182;&#26469;&#33258;&#25195;&#25551;&#20202;&#21644;&#30740;&#31350;&#30340;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;MRI&#25195;&#25551;&#20202;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#22270;&#20687;&#65292;&#23548;&#33268;&#20102;&#25152;&#35859;&#30340;&#8220;&#21327;&#35843;&#38382;&#39064;&#8221;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#20010;&#20154;&#20010;&#24615;&#29305;&#24449;&#65292;&#22240;&#27492;&#22312;&#20849;&#20139;&#25968;&#25454;&#26102;&#28041;&#21450;&#21040;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26080;&#28304;&#39046;&#22495;&#36866;&#24212;&#65288;SFDA&#65289;&#26041;&#27861;SFHarmony&#12290;&#36890;&#36807;&#23558;&#25104;&#20687;&#29305;&#24449;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#26368;&#23567;&#21270;&#28304;&#29305;&#24449;&#21644;&#30446;&#26631;&#29305;&#24449;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;Bhattacharyya&#36317;&#31163;&#65292;&#25105;&#20204;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#28304;&#25968;&#25454;&#20197;&#36866;&#24212;&#25110;&#30446;&#26631;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#30446;&#26631;&#25968;&#25454;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20855;&#26377;&#36328;&#25968;&#25454;&#22495;&#30340;&#20849;&#20139;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#39046;&#22495;&#36716;&#31227;&#20013;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#21327;&#35843;&#36328;&#25195;&#25551;&#22120;&#21644;&#30740;&#31350;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
To represent the biological variability of clinical neuroimaging populations, it is vital to be able to combine data across scanners and studies. However, different MRI scanners produce images with different characteristics, resulting in a domain shift known as the `harmonisation problem'. Additionally, neuroimaging data is inherently personal in nature, leading to data privacy concerns when sharing the data. To overcome these barriers, we propose an Unsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Through modelling the imaging features as a Gaussian Mixture Model and minimising an adapted Bhattacharyya distance between the source and target features, we can create a model that performs well for the target data whilst having a shared feature representation across the data domains, without needing access to the source data for adaptation or target labels. We demonstrate the performance of our method on simulated and real domain shifts, showing that the approach is ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#25972;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15963</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#22270;&#20687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multimodal and multicontrast image fusion via deep generative models. (arXiv:2303.15963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15963
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#25972;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#36880;&#28176;&#24847;&#35782;&#21040;&#20256;&#32479;&#30340;&#35786;&#26029;&#26631;&#31614;&#26080;&#27861;&#21487;&#38752;&#22320;&#25551;&#36848;&#22810;&#31181;&#20020;&#24202;&#34920;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#23588;&#20854;&#26159;&#24191;&#27867;&#30340;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#65288;&#20363;&#22914;&#25233;&#37057;&#30151;&#12289;&#28966;&#34385;&#30151;&#12289;&#34892;&#20026;&#34920;&#22411;&#65289;&#12290;&#24739;&#32773;&#30340;&#24322;&#36136;&#24615;&#21487;&#20197;&#36890;&#36807;&#23558;&#20010;&#20307;&#26681;&#25454;&#32463;&#39564;&#24471;&#20986;&#30340;&#20132;&#32455;&#36830;&#32493;&#30340;&#26032;&#31867;&#21035;&#20998;&#32452;&#26469;&#26356;&#22909;&#22320;&#25551;&#36848;&#65292;&#36825;&#20123;&#36830;&#32493;&#36328;&#36234;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#31867;&#21035;&#36793;&#30028;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#25658;&#24102;&#30528;&#20851;&#20110;&#27599;&#20010;&#24739;&#32773;&#22823;&#33041;&#30340;&#23500;&#21547;&#26102;&#31354;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#32463;&#36807;&#19968;&#31995;&#21015;&#20107;&#20808;&#26410;&#23398;&#20064;&#20026;&#27169;&#22411;&#30340;&#35757;&#32451;&#37096;&#20998;&#30340;&#36807;&#31243;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#22240;&#27492;&#27809;&#26377;&#38024;&#23545;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#26159;&#22240;&#20026;&#27599;&#20010;&#20010;&#20307;&#36890;&#24120;&#37117;&#26377;&#22810;&#20010;&#20840;&#33041;&#19977;&#32500;&#25104;&#20687;&#27169;&#24577;&#65292;&#24120;&#20276;&#38543;&#30528;&#28145;&#23618;&#30340;&#22522;&#22240;&#22411;&#21644;&#34920;&#22411;&#29305;&#24449;&#25551;&#36848;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#32452;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#23398;&#20064;&#19981;&#21516;&#25104;&#20687;&#27169;&#24577;&#30340;&#20849;&#21516;&#34920;&#31034;&#31354;&#38388;&#65292;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#27169;&#24577;&#29305;&#23450;&#21644;&#23545;&#27604;&#24230;&#29305;&#23450;&#30340;&#32534;&#30721;-&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;OPENNeuro&#25968;&#25454;&#24211;&#30340;174&#21517;MDD&#24739;&#32773;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#32467;&#26500;T1&#21152;&#26435;MRI&#12289;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#21644;&#38745;&#24687;&#21151;&#33021;MRI&#65288;rsfMRI&#65289;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22810;&#27169;&#24577;&#21644;&#22810;&#23545;&#27604;&#24230;&#30340;&#25104;&#20687;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20998;&#31867;&#34920;&#29616;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has become progressively more evident that classic diagnostic labels are unable to reliably describe the complexity and variability of several clinical phenotypes. This is particularly true for a broad range of neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral phenotypes). Patient heterogeneity can be better described by grouping individuals into novel categories based on empirically derived sections of intersecting continua that span across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient's brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is because every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic char
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20462;&#25913;&#29256;&#30340;Edge-Popup&#21644;Biprop&#31639;&#27861;&#65292;&#21517;&#20026;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;&#65292;&#35782;&#21035;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#26435;&#37325;&#23376;&#38598;&#65292;&#20197;&#36827;&#34892;&#23618;&#20869;&#37325;&#29992;&#65292;&#24182;&#25214;&#21040;&#39640;&#31934;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#31232;&#30095;&#24230;&#65292;&#24182;&#29992;&#30456;&#21453;&#30340;&#21457;&#29616;&#26469;&#34917;&#20805;&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2303.15953</link><description>&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#23376;&#32593;&#32476;&#19982;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Randomly Initialized Subnetworks with Iterative Weight Recycling. (arXiv:2303.15953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20462;&#25913;&#29256;&#30340;Edge-Popup&#21644;Biprop&#31639;&#27861;&#65292;&#21517;&#20026;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;&#65292;&#35782;&#21035;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#26435;&#37325;&#23376;&#38598;&#65292;&#20197;&#36827;&#34892;&#23618;&#20869;&#37325;&#29992;&#65292;&#24182;&#25214;&#21040;&#39640;&#31934;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#31232;&#30095;&#24230;&#65292;&#24182;&#29992;&#30456;&#21453;&#30340;&#21457;&#29616;&#26469;&#34917;&#20805;&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#35748;&#20026;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21253;&#21547;&#20960;&#20010;&#23376;&#32593;&#32476;&#65292;&#36825;&#20123;&#23376;&#32593;&#32476;&#30340;&#31934;&#24230;&#19982;&#21516;&#19968;&#26550;&#26500;&#30340;&#24050;&#32463;&#23436;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#27714;&#32593;&#32476;&#20855;&#26377;&#36275;&#22815;&#30340;&#36807;&#21442;&#25968;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65288;Edge-Popup&#21644;Biprop&#65289;&#30340;&#20462;&#25913;&#29256;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#39069;&#22806;&#23384;&#20648;&#25104;&#26412;&#25110;&#32553;&#25918;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#31934;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#31639;&#27861;&#65292;&#36845;&#20195;&#24335;&#26435;&#37325;&#22238;&#25910;&#65292;&#35782;&#21035;&#38543;&#24847;&#21021;&#22987;&#21270;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#26435;&#37325;&#23376;&#38598;&#20197;&#36827;&#34892;&#23618;&#20869;&#37325;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#26356;&#39640;&#30340;&#20462;&#21098;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#8220;&#22238;&#25910;&#8221;&#29616;&#26377;&#26435;&#37325;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#31232;&#30095;&#24230;&#12290;&#38500;&#20102;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;&#65292;&#25105;&#20204;&#36824;&#29992;&#30456;&#21453;&#30340;&#21457;&#29616;&#26469;&#34917;&#20805;&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#65306;&#39640;&#31934;&#24230;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#23376;&#32593;&#32476;&#20135;&#29983;&#19981;&#21516;&#30340;&#25513;&#30721;&#65292;&#23613;&#31649;&#23427;&#20204;&#20849;&#20139;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture. However, current methods require that the network is sufficiently overparameterized. In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse. Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the "recycling" of existing weights. In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy, randomly initialized subnetwork's produce diverse masks, despite bei
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#24102;&#29699;&#35856;&#29305;&#24449;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#36830;&#32493;&#28145;&#24230;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#20351;&#29992;&#65292;&#20854;&#20013;&#28145;&#24230;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#35777;&#25454;&#19979;&#30028;&#26469;&#20272;&#35745;&#20026;&#26680;&#36229;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#21464;&#20998;&#23398;&#20064;&#29699;&#35856;&#30456;&#20301;&#24341;&#20837;&#20102;&#26412;&#24449;&#22522;&#30784;&#30340;&#31232;&#30095;&#24615;&#65292;&#20351;&#24471;&#21487;&#20197;&#22788;&#29702;&#27604;&#20197;&#21069;&#26356;&#22823;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#24182;&#20801;&#35768;&#23398;&#20064;&#39640;&#39057;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.15948</link><description>&lt;p&gt;
&#24102;&#29699;&#35856;&#29305;&#24449;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Sparse Gaussian Processes with Spherical Harmonic Features Revisited. (arXiv:2303.15948v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#24102;&#29699;&#35856;&#29305;&#24449;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#36830;&#32493;&#28145;&#24230;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#20351;&#29992;&#65292;&#20854;&#20013;&#28145;&#24230;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#35777;&#25454;&#19979;&#30028;&#26469;&#20272;&#35745;&#20026;&#26680;&#36229;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#21464;&#20998;&#23398;&#20064;&#29699;&#35856;&#30456;&#20301;&#24341;&#20837;&#20102;&#26412;&#24449;&#22522;&#30784;&#30340;&#31232;&#30095;&#24615;&#65292;&#20351;&#24471;&#21487;&#20197;&#22788;&#29702;&#27604;&#20197;&#21069;&#26356;&#22823;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#24182;&#20801;&#35768;&#23398;&#20064;&#39640;&#39057;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24102;&#26377;&#29699;&#35856;&#29305;&#24449;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#19982;&#20854;&#30456;&#20851;&#30340;RKHS&#12289;&#20854;&#29305;&#24449;&#20540;&#32467;&#26500;&#20197;&#21450;&#28145;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#26032;&#30340;&#26680;&#20989;&#25968;&#65292;&#23427;&#23545;&#24212;&#20110;&#20855;&#26377;&#36830;&#32493;&#28145;&#24230;&#30340;&#28145;&#24230;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#20844;&#24335;&#20013;&#65292;&#28145;&#24230;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#35777;&#25454;&#19979;&#30028;&#26469;&#20272;&#35745;&#20026;&#26680;&#36229;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#23398;&#20064;&#29699;&#35856;&#30456;&#20301;&#24341;&#20837;&#20102;&#26412;&#24449;&#22522;&#30784;&#30340;&#31232;&#30095;&#24615;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#22788;&#29702;&#27604;&#20197;&#21069;&#26356;&#22823;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#24182;&#19988;&#20801;&#35768;&#23398;&#20064;&#39640;&#39057;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the Gaussian process model with spherical harmonic features and study connections between the associated RKHS, its eigenstructure and deep models. Based on this, we introduce a new class of kernels which correspond to deep models of continuous depth. In our formulation, depth can be estimated as a kernel hyper-parameter by optimizing the evidence lower bound. Further, we introduce sparseness in the eigenbasis by variational learning of the spherical harmonic phases. This enables scaling to larger input dimensions than previously, while also allowing for learning of high frequency variations. We validate our approach on machine learning benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep Selection&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#22810;&#30446;&#26631;&#25163;&#26415;&#24405;&#20687;&#20013;&#36873;&#25321;&#26368;&#20339;&#35270;&#35282;&#30340;&#25668;&#20687;&#26426;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#19987;&#23478;&#27880;&#37322;&#65292;&#39044;&#27979;&#25668;&#20687;&#26426;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#20174;&#32780;&#20248;&#21270;&#25163;&#26415;&#35760;&#24405;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15947</link><description>&lt;p&gt;
&#28145;&#24230;&#36873;&#25321;&#65306;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#24405;&#20687;&#30340;&#20840;&#30417;&#30563;&#25668;&#20687;&#22836;&#36873;&#25321;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings. (arXiv:2303.15947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15947
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep Selection&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#22810;&#30446;&#26631;&#25163;&#26415;&#24405;&#20687;&#20013;&#36873;&#25321;&#26368;&#20339;&#35270;&#35282;&#30340;&#25668;&#20687;&#26426;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#19987;&#23478;&#27880;&#37322;&#65292;&#39044;&#27979;&#25668;&#20687;&#26426;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#20174;&#32780;&#20248;&#21270;&#25163;&#26415;&#35760;&#24405;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25163;&#26415;&#23460;&#20013;&#35760;&#24405;&#25163;&#26415;&#26159;&#25945;&#32946;&#21644;&#35780;&#20272;&#21307;&#30103;&#27835;&#30103;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25163;&#26415;&#36807;&#31243;&#20013;&#30446;&#26631;&#21463;&#21040;&#20005;&#37325;&#30340;&#36974;&#25377;&#65292;&#22914;&#25163;&#26415;&#22330;&#22320;&#12289;&#25163;&#26415;&#24037;&#20855;&#25110;&#21307;&#29983;&#30340;&#25163;&#65292;&#25152;&#20197;&#35760;&#24405;&#36825;&#20123;&#30446;&#26631;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#35760;&#24405;&#31995;&#32479;&#65292;&#22312;&#25163;&#26415;&#28783;&#20013;&#23884;&#20837;&#22810;&#20010;&#25668;&#20687;&#22836;&#65292;&#24182;&#20551;&#23450;&#20219;&#20309;&#26102;&#20505;&#33267;&#23569;&#26377;&#19968;&#21488;&#25668;&#20687;&#26426;&#22312;&#35760;&#24405;&#27809;&#26377;&#36974;&#25377;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#23884;&#20837;&#30340;&#25668;&#20687;&#26426;&#33719;&#24471;&#20102;&#22810;&#20010;&#35270;&#39057;&#24207;&#21015;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#22810;&#20010;&#35270;&#39057;&#24207;&#21015;&#20013;&#36873;&#25321;&#26368;&#20339;&#25163;&#26415;&#35270;&#22270;&#30340;&#20219;&#21153;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25163;&#26415;&#22330;&#22320;&#30340;&#38754;&#31215;&#22823;&#23567;&#36873;&#25321;&#25668;&#20687;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23398;&#20064;&#19987;&#23478;&#27880;&#37322;&#30340;&#30417;&#30563;&#26469;&#39044;&#27979;&#25668;&#20687;&#26426;&#36873;&#25321;&#27010;&#29575;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35760;&#24405;&#20102;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25972;&#23481;&#25163;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#25668;&#20687;&#26426;&#20999;&#25442;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recording surgery in operating rooms is an essential task for education and evaluation of medical treatment. However, recording the desired targets, such as the surgery field, surgical tools, or doctor's hands, is difficult because the targets are heavily occluded during surgery. We use a recording system in which multiple cameras are embedded in the surgical lamp, and we assume that at least one camera is recording the target without occlusion at any given time. As the embedded cameras obtain multiple video sequences, we address the task of selecting the camera with the best view of the surgery. Unlike the conventional method, which selects the camera based on the area size of the surgery field, we propose a deep neural network that predicts the camera selection probability from multiple video sequences by learning the supervision of the expert annotation. We created a dataset in which six different types of plastic surgery are recorded, and we provided the annotation of camera switch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#21697;&#22270;&#21367;&#31215;&#30340;&#24402;&#32435;&#24335;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25237;&#24433;&#26500;&#24314;&#29289;&#21697;-&#29289;&#21697;&#22270;&#65292;&#24182;&#37319;&#29992;&#21367;&#31215;&#23558;&#39640;&#38454;&#20851;&#32852;&#27880;&#20837;&#29289;&#21697;&#23884;&#20837;&#65292;&#21516;&#26102;&#23558;&#29992;&#25143;&#34920;&#31034;&#24418;&#25104;&#21152;&#26435;&#30340;&#21152;&#26435;&#21644;&#12290;</title><link>http://arxiv.org/abs/2303.15946</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#21697;&#22270;&#21367;&#31215;&#30340;&#24402;&#32435;&#24335;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Item Graph Convolution Collaborative Filtering for Inductive Recommendations. (arXiv:2303.15946v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#21697;&#22270;&#21367;&#31215;&#30340;&#24402;&#32435;&#24335;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25237;&#24433;&#26500;&#24314;&#29289;&#21697;-&#29289;&#21697;&#22270;&#65292;&#24182;&#37319;&#29992;&#21367;&#31215;&#23558;&#39640;&#38454;&#20851;&#32852;&#27880;&#20837;&#29289;&#21697;&#23884;&#20837;&#65292;&#21516;&#26102;&#23558;&#29992;&#25143;&#34920;&#31034;&#24418;&#25104;&#21152;&#26435;&#30340;&#21152;&#26435;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;GCN&#34987;&#29992;&#20316;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23558;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20316;&#20026;&#20108;&#20998;&#22270;&#30340;&#36793;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#32570;&#20047;&#38468;&#21152;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#37319;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#29992;&#25143;&#23884;&#20837;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#36825;&#20123;&#31639;&#27861;&#26412;&#36136;&#19978;&#26159;&#36716;&#25442;&#22411;&#30340;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#20026;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#29992;&#25143;&#29983;&#25104;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#31639;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#26159;&#24402;&#32435;&#24335;&#30340;&#65292;&#21516;&#26102;&#20165;&#20381;&#36182;&#20110;&#38544;&#24335;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20108;&#20998;&#22270;&#20132;&#20114;&#32593;&#32476;&#30340;&#21152;&#26435;&#25237;&#24433;&#26500;&#24314;&#29289;&#21697;-&#29289;&#21697;&#22270;&#24182;&#37319;&#29992;&#21367;&#31215;&#23558;&#39640;&#38454;&#20851;&#32852;&#27880;&#20837;&#29289;&#21697;&#23884;&#20837;&#65292;&#21516;&#26102;&#23558;&#29992;&#25143;&#34920;&#31034;&#24418;&#25104;&#21152;&#26435;&#30340;&#21152;&#26435;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCN) have been recently employed as core component in the construction of recommender system algorithms, interpreting user-item interactions as the edges of a bipartite graph. However, in the absence of side information, the majority of existing models adopt an approach of randomly initialising the user embeddings and optimising them throughout the training process. This strategy makes these algorithms inherently transductive, curtailing their ability to generate predictions for users that were unseen at training time. To address this issue, we propose a convolution-based algorithm, which is inductive from the user perspective, while at the same time, depending only on implicit user-item interaction data. We propose the construction of an item-item graph through a weighted projection of the bipartite interaction network and to employ convolution to inject higher order associations into item embeddings, while constructing user representations as weighted su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#32676;&#38598;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#21644;&#32858;&#31867;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#26631;&#35760;&#30340;&#28304;&#22495;&#25968;&#25454;&#26469;&#35757;&#32451;&#35828;&#35805;&#20154;&#23884;&#20837;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#19987;&#29992;&#30340;&#23545;&#27604;&#20013;&#24515;&#25439;&#22833;&#35757;&#32451;&#35813;&#32593;&#32476;&#26469;&#25552;&#39640;&#32676;&#38598;&#36136;&#37327;&#12290;&#22312;&#19981;&#20351;&#29992;&#30446;&#26631;&#22495;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;CN-Celeb1&#35780;&#20272;&#38598;&#19978;&#23454;&#29616;&#20102;8.10&#65285;&#30340;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#12290;</title><link>http://arxiv.org/abs/2303.15944</link><description>&lt;p&gt;
&#32676;&#38598;&#24341;&#23548;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#28145;&#24230;&#35828;&#35805;&#20154;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Cluster-Guided Unsupervised Domain Adaptation for Deep Speaker Embedding. (arXiv:2303.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15944
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#32676;&#38598;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#21644;&#32858;&#31867;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#26631;&#35760;&#30340;&#28304;&#22495;&#25968;&#25454;&#26469;&#35757;&#32451;&#35828;&#35805;&#20154;&#23884;&#20837;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#19987;&#29992;&#30340;&#23545;&#27604;&#20013;&#24515;&#25439;&#22833;&#35757;&#32451;&#35813;&#32593;&#32476;&#26469;&#25552;&#39640;&#32676;&#38598;&#36136;&#37327;&#12290;&#22312;&#19981;&#20351;&#29992;&#30446;&#26631;&#22495;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;CN-Celeb1&#35780;&#20272;&#38598;&#19978;&#23454;&#29616;&#20102;8.10&#65285;&#30340;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20266;&#26631;&#31614;&#21487;&#20197;&#20026;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#20570;&#20986;&#36129;&#29486;&#12290;&#21463;&#21040;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#22120;&#26631;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#20197;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#30340;&#33258;&#25105;&#35757;&#32451;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#38598;&#24341;&#23548;&#30340;UDA&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#32858;&#31867;&#23545;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#23558;&#26631;&#35760;&#30340;&#28304;&#22495;&#25968;&#25454;&#21644;&#20266;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#35757;&#32451;&#35828;&#35805;&#20154;&#23884;&#20837;&#32593;&#32476;&#12290;&#20026;&#20102;&#25552;&#39640;&#32676;&#38598;&#36136;&#37327;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#23545;&#32858;&#31867;&#19987;&#29992;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#32593;&#32476;&#30340;&#23545;&#27604;&#20013;&#24515;&#25439;&#22833;&#26469;&#35757;&#32451;&#35813;&#32593;&#32476;&#12290;&#30446;&#26631;&#26159;&#38477;&#20302;&#23884;&#20837;&#19982;&#20854;&#20998;&#37197;&#30340;&#32676;&#38598;&#20013;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#22686;&#22823;&#23884;&#20837;&#19982;&#20854;&#20182;&#32676;&#38598;&#20013;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#20351;&#29992;VoxCeleb2&#20316;&#20026;&#28304;&#22495;&#65292;CN-Celeb1&#20316;&#20026;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#30446;&#26631;&#22495;&#30340;&#20219;&#20309;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;CN-Celeb1&#35780;&#20272;&#38598;&#19978;&#23454;&#29616;8.10&#65285;&#30340;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that pseudo labels can contribute to unsupervised domain adaptation (UDA) for speaker verification. Inspired by the self-training strategies that use an existing classifier to label the unlabeled data for retraining, we propose a cluster-guided UDA framework that labels the target domain data by clustering and combines the labeled source domain data and pseudo-labeled target domain data to train a speaker embedding network. To improve the cluster quality, we train a speaker embedding network dedicated for clustering by minimizing the contrastive center loss. The goal is to reduce the distance between an embedding and its assigned cluster center while enlarging the distance between the embedding and the other cluster centers. Using VoxCeleb2 as the source domain and CN-Celeb1 as the target domain, we demonstrate that the proposed method can achieve an equal error rate (EER) of 8.10% on the CN-Celeb1 evaluation set without using any labels from the target domain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292; &#20197;&#35757;&#32451;&#26356;&#31934;&#30830;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.15939</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20154;&#36896;&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Physics-guided adversarial networks for artificial digital image correlation data generation. (arXiv:2303.15939v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292; &#20197;&#35757;&#32451;&#26356;&#31934;&#30830;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#65288;DIC&#65289;&#24050;&#25104;&#20026;&#35780;&#20272;&#21147;&#23398;&#23454;&#39564;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#23454;&#39564;&#12290;&#35780;&#20272;&#38656;&#35201;&#20934;&#30830;&#30340;&#35010;&#32441;&#36335;&#24452;&#21644;&#35010;&#32441;&#23574;&#31471;&#20301;&#32622;&#20449;&#24687;&#65292;&#30001;&#20110;&#22266;&#26377;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#21407;&#22240;&#65292;&#36825;&#24456;&#38590;&#33719;&#24471;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#32473;&#23450;&#26631;&#35760;&#30340;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#27492;&#30456;&#20851;&#20449;&#24687;&#38750;&#24120;&#25104;&#21151;&#12290;&#20026;&#20102;&#35757;&#32451;&#20855;&#26377;&#24191;&#27867;&#27867;&#21270;&#33021;&#21147;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#38656;&#35201;&#22823;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#25968;&#25454;&#36890;&#24120;&#24456;&#23569;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20915;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#30495;&#23454;&#36824;&#26159;&#20551;&#30340;&#65292;&#35813;&#37492;&#21035;&#22120;&#21478;&#22806;&#25509;&#25910;&#23548;&#20986;&#30340;von Mises&#31561;&#25928;&#24212;&#21464;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#36825;&#31181;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;GAN&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20135;&#29983;&#22823;&#37327;&#30340;&#20154;&#36896;DIC&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital image correlation (DIC) has become a valuable tool in the evaluation of mechanical experiments, particularly fatigue crack growth experiments. The evaluation requires accurate information of the crack path and crack tip position, which is difficult to obtain due to inherent noise and artefacts. Machine learning models have been extremely successful in recognizing this relevant information given labelled DIC displacement data. For the training of robust models, which generalize well, big data is needed. However, data is typically scarce in the field of material science and engineering because experiments are expensive and time-consuming. We present a method to generate synthetic DIC displacement data using generative adversarial networks with a physics-guided discriminator. To decide whether data samples are real or fake, this discriminator additionally receives the derived von Mises equivalent strain. We show that this physics-guided approach leads to improved results in terms 
&lt;/p&gt;</description></item><item><title>HERMES Pathfinder&#26159;&#19968;&#20010;&#22312;&#36712;&#25506;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#21019;&#26032;&#30340;&#25506;&#27979;&#22120;&#30417;&#27979;&#39640;&#33021;&#23431;&#23449;&#30636;&#21464;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#20449;&#21495;&#21040;&#36798;&#19981;&#21516;&#25506;&#27979;&#22120;&#30340;&#24310;&#36831;&#26102;&#38388;&#33719;&#24471;&#31934;&#30830;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#33322;&#22825;&#39640;&#33021;&#25506;&#27979;&#22120;&#32972;&#26223;&#35745;&#25968;&#29575;&#30340;&#26032;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.15936</link><description>&lt;p&gt;
&#23547;&#25214;&#38271;&#26102;&#38388;&#24494;&#24369;&#30340;&#22825;&#25991;&#39640;&#33021;&#30636;&#21464;&#29616;&#35937;&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching for long faint astronomical high energy transients: a data driven approach. (arXiv:2303.15936v1 [astro-ph.HE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15936
&lt;/p&gt;
&lt;p&gt;
HERMES Pathfinder&#26159;&#19968;&#20010;&#22312;&#36712;&#25506;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#21019;&#26032;&#30340;&#25506;&#27979;&#22120;&#30417;&#27979;&#39640;&#33021;&#23431;&#23449;&#30636;&#21464;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#20449;&#21495;&#21040;&#36798;&#19981;&#21516;&#25506;&#27979;&#22120;&#30340;&#24310;&#36831;&#26102;&#38388;&#33719;&#24471;&#31934;&#30830;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#33322;&#22825;&#39640;&#33021;&#25506;&#27979;&#22120;&#32972;&#26223;&#35745;&#25968;&#29575;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
HERMES&#65288;High Energy Rapid Modular Ensemble of Satellites&#65289;&#26159;&#19968;&#20010;&#22312;&#36712;&#25506;&#27979;&#31995;&#32479;&#30340;&#21069;&#23548;&#37096;&#32626;&#65292;&#30001;&#20845;&#20010;3U&#32435;&#31859;&#21355;&#26143;&#32452;&#25104;&#65292;&#25176;&#31649;&#30528;&#29992;&#20110;&#30417;&#27979;&#23431;&#23449;&#39640;&#33021;&#30636;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#20294;&#21019;&#26032;&#30340;&#25506;&#27979;&#22120;&#12290;HERMES Pathfinder&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35777;&#26126;&#20351;&#29992;&#24494;&#22411;&#30828;&#20214;&#21487;&#20197;&#33719;&#24471;&#39640;&#33021;&#23431;&#23449;&#30636;&#21464;&#29616;&#35937;&#30340;&#31934;&#30830;&#20301;&#32622;&#20449;&#24687;&#12290;&#36890;&#36807;&#30740;&#31350;&#20449;&#21495;&#21040;&#36798;&#19981;&#21516;&#25506;&#27979;&#22120;&#30340;&#24310;&#36831;&#26102;&#38388;&#65292;&#21487;&#20197;&#33719;&#21462;&#30636;&#21464;&#29616;&#35937;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#24037;&#20855;&#26469;&#20805;&#20998;&#21033;&#29992;HERMES Pathfinder&#26410;&#26469;&#30340;&#31185;&#23398;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#33322;&#22825;&#39640;&#33021;&#25506;&#27979;&#22120;&#32972;&#26223;&#35745;&#25968;&#29575;&#30340;&#26032;&#26694;&#26550;&#65307;&#36825;&#26159;&#37492;&#21035;&#24494;&#24369;&#30340;&#22825;&#20307;&#29289;&#29702;&#30636;&#21464;&#29616;&#35937;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#20272;&#35745;&#25506;&#27979;&#22120;&#30340;&#32972;&#26223;&#35745;&#25968;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
HERMES (High Energy Rapid Modular Ensemble of Satellites) pathfinder is an in-orbit demonstration consisting of a constellation of six 3U nano-satellites hosting simple but innovative detectors for the monitoring of cosmic high-energy transients. The main objective of HERMES Pathfinder is to prove that accurate position of high-energy cosmic transients can be obtained using miniaturized hardware. The transient position is obtained by studying the delay time of arrival of the signal to different detectors hosted by nano-satellites on low Earth orbits. To this purpose, the goal is to achive an overall accuracy of a fraction of a micro-second. In this context, we need to develop novel tools to fully exploit the future scientific data output of HERMES Pathfinder. In this paper, we introduce a new framework to assess the background count rate of a space-born, high energy detector; a key step towards the identification of faint astrophysical transients. We employ a Neural Network (NN) to est
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.15916</link><description>&lt;p&gt;
&#20174;&#31169;&#26377;&#21040;&#20844;&#26377;&#65306;&#22312;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#19979;&#23545;GAN&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification. (arXiv:2303.15916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#37117;&#24456;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#31169;&#20154;&#25968;&#25454;&#26102;&#65292;&#20960;&#20010;&#38480;&#21046;&#20351;&#24471;&#38590;&#20197;&#22312;&#36825;&#20123;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#31169;&#23494;&#22320;&#29983;&#25104;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#22312;&#20998;&#31867;&#22120;&#20043;&#19978;&#30452;&#25509;&#24212;&#29992;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#20197;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#20174;&#31169;&#26377;&#25968;&#25454;&#21019;&#24314;&#20844;&#20849;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38024;&#23545;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#38750;&#24120;&#31361;&#20986;&#30340;&#22522;&#20110;GAN&#30340;&#26550;&#26500;&#12290;&#19982;&#20808;&#21069;&#20027;&#35201;&#23616;&#38480;&#20110;&#22270;&#20687;&#39046;&#22495;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#33539;&#22260;&#26159;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23588;&#20854;&#26159;GSWGAN&#22312;&#22810;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;DPWGAN&#12290;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;GSWGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#24773;&#22659;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has proven to be successful in various domains and for different tasks. However, when it comes to private data several restrictions are making it difficult to use deep learning approaches in these application fields. Recent approaches try to generate data privately instead of applying a privacy-preserving mechanism directly, on top of the classifier. The solution is to create public data from private data in a manner that preserves the privacy of the data. In this work, two very prominent GAN-based architectures were evaluated in the context of private time series classification. In contrast to previous work, mostly limited to the image domain, the scope of this benchmark was the time series domain. The experiments show that especially GSWGAN performs well across a variety of public datasets outperforming the competitor DPWGAN. An analysis of the generated datasets further validates the superiority of GSWGAN in the context of time series generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38450;&#24481;&#33976;&#39311;&#26426;&#21046;&#19982;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#26469;&#38477;&#20302;&#27169;&#22411;&#23545;&#26377;&#27602;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.15901</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#38450;&#24481;&#33976;&#39311;&#20316;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm. (arXiv:2303.15901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38450;&#24481;&#33976;&#39311;&#26426;&#21046;&#19982;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#26469;&#38477;&#20302;&#27169;&#22411;&#23545;&#26377;&#27602;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#40065;&#26834;&#24615;&#36896;&#25104;&#20102;&#26497;&#22823;&#23041;&#32961;&#12290;&#23613;&#31649;&#20351;&#29992;&#20102;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#32773;&#31713;&#25913;&#30340;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#38450;&#24481;&#36825;&#31181;&#23545;&#25239;&#25915;&#20987;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#38450;&#24481;&#33976;&#39311;&#26426;&#21046;&#19982;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;(DAE)&#30456;&#32467;&#21512;&#12290;&#35813;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#37325;&#26500;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#23475;&#30340;&#23545;&#25239;&#36755;&#20837;&#26469;&#38477;&#20302;&#33976;&#39311;&#27169;&#22411;&#23545;&#26377;&#27602;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#20102;&#31934;&#24515;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#35782;&#21035;&#21644;&#37325;&#26500;&#20102;&#26377;&#27602;&#30340;&#36755;&#20837;&#65292;&#21516;&#26102;&#20063;&#32771;&#34385;&#20102;&#22686;&#24378;DNN&#30340;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#22312;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;DNN&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks significantly threaten the robustness of deep neural networks (DNNs). Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work proposes a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). This technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data to assess the proposed method's performance. Our experimental findings demonstrate that our method successfully identified and reconstructed the poisonous inputs while also considering enhancing the DNN's resilience. The proposed approach provides a potent and robust defense mechanism for DNNs in various applications where data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39044;&#20248;&#21270;&#31574;&#30053;&#35299;&#20915;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#22312;&#39640;&#32500;&#24230;&#38382;&#39064;&#19978;&#30340;&#20302;&#25928;&#24615;&#65292;&#29992;&#20110;&#35774;&#35745;&#24314;&#31569;&#24213;&#26495;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#20248;&#21270;&#38382;&#39064;&#19978;&#36827;&#34892;&#39044;&#20248;&#21270;&#65292;&#28982;&#21518;&#23558;&#35299;&#20915;&#26041;&#26696;&#26144;&#23556;&#21040;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#35757;&#32451;&#20986;&#26356;&#20934;&#30830;&#30340;&#24314;&#31569;&#35774;&#35745;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15896</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#32500;&#39044;&#20248;&#21270;&#23454;&#29616;&#19977;&#32500;&#24314;&#31569;&#30340;&#39640;&#25928;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Quality Diversity Optimization of 3D Buildings through 2D Pre-optimization. (arXiv:2303.15896v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39044;&#20248;&#21270;&#31574;&#30053;&#35299;&#20915;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#22312;&#39640;&#32500;&#24230;&#38382;&#39064;&#19978;&#30340;&#20302;&#25928;&#24615;&#65292;&#29992;&#20110;&#35774;&#35745;&#24314;&#31569;&#24213;&#26495;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#20248;&#21270;&#38382;&#39064;&#19978;&#36827;&#34892;&#39044;&#20248;&#21270;&#65292;&#28982;&#21518;&#23558;&#35299;&#20915;&#26041;&#26696;&#26144;&#23556;&#21040;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#35757;&#32451;&#20986;&#26356;&#20934;&#30830;&#30340;&#24314;&#31569;&#35774;&#35745;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#21487;&#29992;&#20110;&#26377;&#25928;&#22320;&#21019;&#24314;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24110;&#21161;&#24037;&#31243;&#24072;&#30340;&#30452;&#35273;&#21028;&#26029;&#12290;&#20294;&#22312;&#38750;&#24120;&#26114;&#36149;&#30340;&#38382;&#39064;&#20013;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#19981;&#22815;&#39640;&#25928;&#65292;&#38656;&#35201;&#36827;&#34892;&#25104;&#21315;&#19978;&#19975;&#27425;&#35780;&#20272;&#12290;&#21363;&#20351;&#36741;&#20197;&#26367;&#20195;&#27169;&#22411;&#30340;&#24110;&#21161;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#20063;&#38656;&#35201;&#20960;&#30334;&#29978;&#33267;&#20960;&#21315;&#27425;&#35780;&#20272;&#65292;&#36825;&#20351;&#24471;&#23427;&#24456;&#38590;&#25104;&#20026;&#21487;&#34892;&#30340;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#22312;&#36739;&#20302;&#32500;&#20248;&#21270;&#38382;&#39064;&#19978;&#20351;&#29992;&#39044;&#20248;&#21270;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35299;&#20915;&#26041;&#26696;&#26144;&#23556;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20197;&#35774;&#35745;&#26368;&#23567;&#21270;&#39118;&#25200;&#24314;&#31569;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#20108;&#32500;&#24314;&#31569;&#24213;&#26495;&#30340;&#27969;&#29305;&#24449;&#39044;&#27979;&#19977;&#32500;&#24314;&#31569;&#21608;&#22260;&#30340;&#27969;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#23545;2D&#24314;&#31569;&#24213;&#26495;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#27604;&#20351;&#29992;&#31867;&#20284;Sobol&#24207;&#21015;&#36825;&#26679;&#30340;&#22635;&#20805;&#31639;&#27861;&#25152;&#36873;&#20986;&#24213;&#26495;&#30340;&#39044;&#27979;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#22810;&#26679;&#30340;&#24314;&#31569;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality diversity algorithms can be used to efficiently create a diverse set of solutions to inform engineers' intuition. But quality diversity is not efficient in very expensive problems, needing 100.000s of evaluations. Even with the assistance of surrogate models, quality diversity needs 100s or even 1000s of evaluations, which can make it use infeasible. In this study we try to tackle this problem by using a pre-optimization strategy on a lower-dimensional optimization problem and then map the solutions to a higher-dimensional case. For a use case to design buildings that minimize wind nuisance, we show that we can predict flow features around 3D buildings from 2D flow features around building footprints. For a diverse set of building designs, by sampling the space of 2D footprints with a quality diversity algorithm, a predictive model can be trained that is more accurate than when trained on a set of footprints that were selected with a space-filling algorithm like the Sobol seque
&lt;/p&gt;</description></item><item><title>VIVE3D &#20351;&#29992; 3D GAN &#25216;&#26415;&#23454;&#29616;&#20102;&#29420;&#31435;&#35270;&#35282;&#35270;&#39057;&#32534;&#36753;&#65292;&#33021;&#22815;&#20445;&#30041;&#36523;&#20221;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#26032;&#30340; GAN &#21453;&#28436;&#25216;&#26415;&#21644;&#22836;&#37096;&#26032;&#35270;&#35282;&#30340;&#32534;&#36753;&#31561;&#21019;&#26032;&#65292;&#29983;&#25104;&#39640;&#20445;&#30495;&#30340;&#20154;&#33080;&#32534;&#36753;&#65292;&#19982;&#21407;&#22987;&#35270;&#39057;&#20197;&#19968;&#33268;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#24335;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2303.15893</link><description>&lt;p&gt;
VIVE3D: &#20351;&#29992;&#19977;&#32500;&#24863;&#30693; GAN &#23454;&#29616;&#29420;&#31435;&#35270;&#35282;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs. (arXiv:2303.15893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15893
&lt;/p&gt;
&lt;p&gt;
VIVE3D &#20351;&#29992; 3D GAN &#25216;&#26415;&#23454;&#29616;&#20102;&#29420;&#31435;&#35270;&#35282;&#35270;&#39057;&#32534;&#36753;&#65292;&#33021;&#22815;&#20445;&#30041;&#36523;&#20221;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#26032;&#30340; GAN &#21453;&#28436;&#25216;&#26415;&#21644;&#22836;&#37096;&#26032;&#35270;&#35282;&#30340;&#32534;&#36753;&#31561;&#21019;&#26032;&#65292;&#29983;&#25104;&#39640;&#20445;&#30495;&#30340;&#20154;&#33080;&#32534;&#36753;&#65292;&#19982;&#21407;&#22987;&#35270;&#39057;&#20197;&#19968;&#33268;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#24335;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102; VIVE3D&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340; 3D GAN &#30340;&#33021;&#21147;&#25193;&#23637;&#21040;&#35270;&#39057;&#32534;&#36753;&#65292;&#24182;&#33021;&#20197;&#20445;&#30041;&#36523;&#20221;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#26041;&#24335;&#21576;&#29616;&#36755;&#20837;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26500;&#24314;&#22359;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545; 3D GAN &#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; GAN &#21453;&#28436;&#25216;&#26415;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#22810;&#24103;&#24182;&#20248;&#21270;&#30456;&#26426;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#35821;&#20041;&#20154;&#33080;&#32534;&#36753;&#65288;&#20363;&#22914;&#24180;&#40836;&#21644;&#34920;&#24773;&#65289;&#65292;&#25105;&#20204;&#39318;&#27425;&#28436;&#31034;&#20102;&#22522;&#20110; 3D GAN &#30340;&#20869;&#22312;&#23646;&#24615;&#21644;&#25105;&#20204;&#30340;&#20809;&#27969;&#24341;&#23548;&#30340;&#21512;&#25104;&#25216;&#26415;&#25152;&#21551;&#29992;&#30340;&#26174;&#31034;&#22836;&#37096;&#26032;&#35270;&#35282;&#30340;&#32534;&#36753;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;VIVE3D &#20197;&#19968;&#33268;&#30340;&#36136;&#37327;&#20174;&#19968;&#31995;&#21015;&#25668;&#20687;&#26426;&#35270;&#35282;&#29983;&#25104;&#39640;&#20445;&#30495;&#20154;&#33080;&#32534;&#36753;&#65292;&#19982;&#21407;&#22987;&#35270;&#39057;&#20197;&#19968;&#33268;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#24335;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VIVE3D, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way. We propose two new building blocks. First, we introduce a novel GAN inversion technique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters. Second, besides traditional semantic face edits (e.g. for age and expression), we are the first to demonstrate edits that show novel views of the head enabled by the inherent properties of 3D GANs and our optical flow-guided compositing technique to combine the head with the background video. Our experiments demonstrate that VIVE3D generates high-fidelity face edits at consistent quality from a range of camera viewpoints which are composited with the original video in a temporally and spatially consistent manner.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;Data-Agnostic Consolidation&#65288;DAC&#65289;&#65292;&#22312;&#19981;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;&#25439;&#22833;&#65292;&#22312;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;SCD&#20043;&#38388;&#30340;&#21069;&#21521;&#36716;&#31227;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15888</link><description>&lt;p&gt;
&#26080;&#38656;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning. (arXiv:2303.15888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;Data-Agnostic Consolidation&#65288;DAC&#65289;&#65292;&#22312;&#19981;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;&#25439;&#22833;&#65292;&#22312;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;SCD&#20043;&#38388;&#30340;&#21069;&#21521;&#36716;&#31227;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20998;&#24067;&#24335;&#23398;&#20064;&#36890;&#24120;&#30001;&#33258;&#25105;&#20013;&#24515;&#30340;&#35774;&#22791;&#65288;SCD&#65289;&#32452;&#25104;&#65292;&#23427;&#20204;&#29420;&#31435;&#23398;&#20064;&#26412;&#22320;&#20219;&#21153;&#24182;&#19981;&#24895;&#24847;&#20026;&#20854;&#20182;SCD&#30340;&#24615;&#33021;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#22914;&#20309;&#20197;&#38646;&#25104;&#26412;&#23454;&#29616;&#21333;&#20010;SCD&#30340;&#21069;&#21521;&#36716;&#31227;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;SCD&#36866;&#24212;&#26412;&#22320;&#20219;&#21153;&#65292;CL&#27169;&#22411;&#23558;&#30001;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#30693;&#35782;&#21512;&#24182;&#32780;&#26080;&#38656;&#26597;&#30475;SCD&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#30340;CL&#26041;&#27861;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;Data-Agnostic Consolidation&#65288;DAC&#65289;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#21363;&#21487;&#21512;&#24182;SC&#27169;&#22411;&#30340;&#27969;&#12290;DAC&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;&#25439;&#22833;&#22312;&#28508;&#31354;&#38388;&#20013;&#25191;&#34892;&#33976;&#39311;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DAC&#20351;SCD&#20043;&#38388;&#30340;&#21069;&#21521;&#36716;&#31227;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;Split CIFAR100&#65292;CORe50&#21644;Split TinyImageNet&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#26080;&#38656;&#25490;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning on the edge often comprises self-centered devices (SCD) which learn local tasks independently and are unwilling to contribute to the performance of other SDCs. How do we achieve forward transfer at zero cost for the single SCDs? We formalize this problem as a Distributed Continual Learning scenario, where SCD adapt to local tasks and a CL model consolidates the knowledge from the resulting stream of models without looking at the SCD's private data. Unfortunately, current CL methods are not directly applicable to this scenario. We propose Data-Agnostic Consolidation (DAC), a novel double knowledge distillation method that consolidates the stream of SC models without using the original data. DAC performs distillation in the latent space via a novel Projected Latent Distillation loss. Experimental results show that DAC enables forward transfer between SCDs and reaches state-of-the-art accuracy on Split CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;Wyner&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;&#65292;&#21253;&#25324;&#21464;&#20998;&#24418;&#24335;&#21644;&#34920;&#29616;&#24418;&#24335;&#65292;&#36890;&#36807;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#27492;&#36896;&#25104;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15866</link><description>&lt;p&gt;
Wyner&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Alternating Minimization Solvers for Wyner Multi-View Unsupervised Learning. (arXiv:2303.15866v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;Wyner&#22810;&#35270;&#22270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;&#65292;&#21253;&#25324;&#21464;&#20998;&#24418;&#24335;&#21644;&#34920;&#29616;&#24418;&#24335;&#65292;&#36890;&#36807;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#27492;&#36896;&#25104;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;Wyner&#36890;&#29992;&#20449;&#24687;&#26694;&#26550;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#20132;&#26367;&#26368;&#23567;&#21270;&#21407;&#21017;&#24320;&#21457;&#35745;&#31639;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;&#31532;&#19968;&#31181;&#20844;&#24335;&#34987;&#31216;&#20026;&#8220;&#21464;&#20998;&#24418;&#24335;&#8221;&#65292;&#20854;&#22797;&#26434;&#24230;&#38543;&#30528;&#35270;&#22270;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#32447;&#24615;&#22686;&#38271;&#65292;&#24182;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#32039;&#26463;&#32538;&#21644;&#25289;&#26684;&#26391;&#26085;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#31532;&#20108;&#31181;&#20844;&#24335;&#65292;&#21363;&#8220;&#34920;&#29616;&#24418;&#24335;&#8221;&#65292;&#34987;&#35777;&#26126;&#21253;&#25324;&#24050;&#30693;&#32467;&#26524;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#29256;&#26412;&#30340;&#22810;&#37325;&#20056;&#25968;&#20132;&#26367;&#26041;&#21521;&#31639;&#27861;&#65288;ADMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#25910;&#25947;&#24615;&#22312;&#26576;&#20123;&#30456;&#20851;&#33539;&#22260;&#20869;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we adopt Wyner common information framework for unsupervised multi-view representation learning. Within this framework, we propose two novel formulations that enable the development of computational efficient solvers based on the alternating minimization principle. The first formulation, referred to as the {\em variational form}, enjoys a linearly growing complexity with the number of views and is based on a variational-inference tight surrogate bound coupled with a Lagrangian optimization objective function. The second formulation, i.e., the {\em representational form}, is shown to include known results as special cases. Here, we develop a tailored version from the alternating direction method of multipliers (ADMM) algorithm for solving the resulting non-convex optimization problem. In the two cases, the convergence of the proposed solvers is established in certain relevant regimes. Furthermore, our empirical results demonstrate the effectiveness of the proposed methods 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#23618;&#25351;&#32441;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#35774;&#22791;&#20449;&#24687;&#65292;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15860</link><description>&lt;p&gt;
Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26080;&#30417;&#30563;&#22810;&#23618;&#26080;&#32447;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless Fingerprinting. (arXiv:2303.15860v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#23618;&#25351;&#32441;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#35774;&#22791;&#20449;&#24687;&#65292;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#25351;&#32441;&#35782;&#21035;&#26159;&#19968;&#31181;&#21033;&#29992;&#30828;&#20214;&#19981;&#23436;&#21892;&#21644;&#26080;&#32447;&#20449;&#36947;&#21464;&#21270;&#20316;&#20026;&#26631;&#35782;&#30340;&#35774;&#22791;&#35782;&#21035;&#26041;&#27861;&#12290;&#38500;&#20102;&#29289;&#29702;&#23618;&#29305;&#24449;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#65288;&#20363;&#22914;&#21253;&#38271;&#24230;&#65289;&#35782;&#21035;&#29992;&#25143;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#35299;&#23494;&#26377;&#25928;&#36733;&#33655;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#25351;&#32441;&#35782;&#21035;&#26694;&#26550;&#65292;&#32852;&#21512;&#32771;&#34385;&#22810;&#23618;&#29305;&#24449;&#20197;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#22810;&#35270;&#22270;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#21363;&#22810;&#24418;&#24335;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#32858;&#21512;&#22810;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#35774;&#22791;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#26377;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#25512;&#23548;&#33719;&#24471;&#31616;&#21333;&#30340;&#23548;&#20986;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#25152;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#33719;&#24471;&#32039;&#23494;&#30340;&#20195;&#29702;&#30028;&#38480;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#22312;&#25552;&#21462;&#29305;&#24449;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#20811;&#26381;&#22810;&#35270;&#22270;&#25968;&#25454;&#20998;&#24067;&#38388;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless fingerprinting refers to a device identification method leveraging hardware imperfections and wireless channel variations as signatures. Beyond physical layer characteristics, recent studies demonstrated that user behaviours could be identified through network traffic, e.g., packet length, without decryption of the payload. Inspired by these results, we propose a multi-layer fingerprinting framework that jointly considers the multi-layer signatures for improved identification performance. In contrast to previous works, by leveraging the recent multi-view machine learning paradigm, i.e., data with multiple forms, our method can cluster the device information shared among the multi-layer features without supervision. Our information-theoretic approach can be extended to supervised and semi-supervised settings with straightforward derivations. In solving the formulated problem, we obtain a tight surrogate bound using variational inference for efficient optimization. In extracting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;SAR&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12289;&#39044;&#27979;&#26102;&#38388;&#21644;&#36755;&#20837;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15852</link><description>&lt;p&gt;
&#25506;&#32034;SAR&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;&#36890;&#36807;&#21464;&#21387;&#22120;&#23454;&#29616;&#19979;&#19968;&#20195;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Exploring Deep Learning Methods for Classification of SAR Images: Towards NextGen Convolutions via Transformers. (arXiv:2303.15852v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;SAR&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12289;&#39044;&#27979;&#26102;&#38388;&#21644;&#36755;&#20837;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;SAR&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#24694;&#21155;&#30340;&#20809;&#32447;&#21644;&#22825;&#27668;&#26465;&#20214;&#19979;&#24037;&#20316;&#24471;&#26356;&#22909;&#12290;&#20891;&#20107;&#31995;&#32479;&#26159;&#20854;&#20013;&#20043;&#19968;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#20837;&#30340;&#26368;&#26032;&#20808;&#36827;&#27169;&#22411;&#23545;SAR&#30446;&#26631;&#20998;&#31867;&#65288;MSTAR&#65289;&#30340;&#36866;&#29992;&#24615;&#12290;&#30001;&#20110;&#20219;&#20309;&#20026;&#20891;&#20107;&#31995;&#32479;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#37117;&#23558;&#26159;&#25112;&#30053;&#24615;&#21644;&#23454;&#26102;&#24615;&#30340;&#65292;&#22240;&#27492;&#20934;&#30830;&#24615;&#36890;&#24120;&#19981;&#26159;&#34913;&#37327;&#20854;&#24615;&#33021;&#30340;&#21807;&#19968;&#26631;&#20934;&#65292;&#39044;&#27979;&#26102;&#38388;&#21644;&#36755;&#20837;&#24674;&#22797;&#33021;&#21147;&#31561;&#20854;&#20182;&#37325;&#35201;&#21442;&#25968;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#23601;&#36825;&#20123;&#38382;&#39064;&#22312;SAR&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;SAR&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#36798;&#21040;&#26399;&#26395;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Images generated by high-resolution SAR have vast areas of application as they can work better in adverse light and weather conditions. One such area of application is in the military systems. This study is an attempt to explore the suitability of current state-of-the-art models introduced in the domain of computer vision for SAR target classification (MSTAR). Since the application of any solution produced for military systems would be strategic and real-time, accuracy is often not the only criterion to measure its performance. Other important parameters like prediction time and input resiliency are equally important. The paper deals with these issues in the context of SAR images. Experimental results show that deep learning models can be suitably applied in the domain of SAR image classification with the desired performance levels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#19981;&#30830;&#23450;&#22270;&#20687;&#20998;&#21106;&#20013;&#26631;&#31614;&#39118;&#26684;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#39118;&#26684;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20462;&#25913;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#26631;&#31614;&#39118;&#26684;&#20559;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15850</link><description>&lt;p&gt;
&#26631;&#31614;&#39118;&#26684;&#38382;&#39064;&#65306;&#22788;&#29702;&#19981;&#30830;&#23450;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#26631;&#31614;&#39118;&#26684;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
That Label's Got Style: Handling Label Style Bias for Uncertain Image Segmentation. (arXiv:2303.15850v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#19981;&#30830;&#23450;&#22270;&#20687;&#20998;&#21106;&#20013;&#26631;&#31614;&#39118;&#26684;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#39118;&#26684;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20462;&#25913;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#26631;&#31614;&#39118;&#26684;&#20559;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#32473;&#23450;&#36755;&#20837;&#30340;&#21487;&#33021;&#20998;&#21106;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20174;&#35757;&#32451;&#38598;&#20013;&#30340;&#27880;&#37322;&#32773;&#21464;&#24322;&#24615;&#20013;&#23398;&#20064;&#32780;&#26469;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#27880;&#37322;&#21487;&#33021;&#20250;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#27880;&#24037;&#20855;&#32780;&#26377;&#31995;&#32479;&#24046;&#24322;&#12290;&#36825;&#23548;&#33268;&#25968;&#25454;&#38598;&#26082;&#21253;&#21547;&#25968;&#25454;&#21464;&#24322;&#24615;&#65292;&#21448;&#21253;&#21547;&#19981;&#21516;&#30340;&#26631;&#31614;&#39118;&#26684;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#20250;&#23548;&#33268;&#27169;&#22411;&#20559;&#24046;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#26631;&#31614;&#39118;&#26684;&#25152;&#36896;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26356;&#26032;&#30340;&#24314;&#27169;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#26631;&#31614;&#39118;&#26684;&#30340;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20462;&#25913;&#20102;&#20004;&#20010;&#29992;&#20110;&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20943;&#23569;&#20102;&#26631;&#31614;&#39118;&#26684;&#20559;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#20998;&#21106;&#24615;&#33021;&#65292;&#22686;&#21152;&#20102;&#20998;&#21106;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#31934;&#36873;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26082;&#21253;&#21547;&#25968;&#25454;&#21464;&#24322;&#24615;&#65292;&#21448;&#21253;&#21547;&#19981;&#21516;&#30340;&#26631;&#31614;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation uncertainty models predict a distribution over plausible segmentations for a given input, which they learn from the annotator variation in the training set. However, in practice these annotations can differ systematically in the way they are generated, for example through the use of different labeling tools. This results in datasets that contain both data variability and differing label styles. In this paper, we demonstrate that applying state-of-the-art segmentation uncertainty models on such datasets can lead to model bias caused by the different label styles. We present an updated modelling objective conditioning on labeling style for aleatoric uncertainty estimation, and modify two state-of-the-art-architectures for segmentation uncertainty accordingly. We show with extensive experiments that this method reduces label style bias, while improving segmentation performance, increasing the applicability of segmentation uncertainty models in the wild. We curate two datasets
&lt;/p&gt;</description></item><item><title>GAS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;PINNs&#30340;&#25910;&#25947;&#36807;&#31243;&#24182;&#25552;&#39640;&#31934;&#24230;&#65292;&#24050;&#22312;2D&#21040;10D&#38382;&#39064;&#30340;&#25968;&#20540;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#20110;&#28145;&#23618;&#27714;&#35299;&#22120;&#12289;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15849</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#29992;&#20110;PINNs
&lt;/p&gt;
&lt;p&gt;
GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs. (arXiv:2303.15849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15849
&lt;/p&gt;
&lt;p&gt;
GAS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;PINNs&#30340;&#25910;&#25947;&#36807;&#31243;&#24182;&#25552;&#39640;&#31934;&#24230;&#65292;&#24050;&#22312;2D&#21040;10D&#38382;&#39064;&#30340;&#25968;&#20540;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#20110;&#28145;&#23618;&#27714;&#35299;&#22120;&#12289;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;PINNs&#26041;&#27861;&#22240;&#20854;&#39640;&#32500;&#38382;&#39064;&#22788;&#29702;&#30340;&#39640;&#25928;&#24615;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#30456;&#23545;&#36739;&#20302;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#21463;&#33258;&#36866;&#24212;&#26377;&#38480;&#20803;&#26041;&#27861;&#21644;&#22686;&#37327;&#23398;&#20064;&#24605;&#24819;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAS&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;PINNs&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;GAS&#21033;&#29992;&#24403;&#21069;&#30340;&#27531;&#24046;&#20449;&#24687;&#29983;&#25104;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20197;&#37319;&#26679;&#20854;&#20182;&#28857;&#65292;&#36825;&#20123;&#25968;&#25454;&#23558;&#19982;&#21382;&#21490;&#25968;&#25454;&#19968;&#36215;&#35757;&#32451;&#65292;&#21152;&#24555;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;2D&#21040;10D&#38382;&#39064;&#30340;&#25968;&#20540;&#27169;&#25311;&#20013;&#65292;GAS&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#28145;&#23618;&#27714;&#35299;&#22120;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent study of the deep learning in scientific computation, the PINNs method has drawn widespread attention for solving PDEs. Compared with traditional methods, PINNs can efficiently handle high-dimensional problems, while the accuracy is relatively low, especially for highly irregular problems. Inspired by the idea of adaptive finite element methods and incremental learning, we propose GAS, a Gaussian mixture distribution-based adaptive sampling method for PINNs. During the training procedure, GAS uses the current residual information to generate a Gaussian mixture distribution for the sampling of additional points, which are then trained together with history data to speed up the convergence of loss and achieve a higher accuracy. Several numerical simulations on 2d to 10d problems show that GAS is a promising method which achieves the state-of-the-art accuracy among deep solvers, while being comparable with traditional numerical solvers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#36827;&#34892;&#32954;&#30284;&#26089;&#26399;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.15846</link><description>&lt;p&gt;
&#21033;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#33655;&#20848;&#21307;&#30103;&#31508;&#35760;&#39044;&#27979;&#32954;&#30284;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes. (arXiv:2303.15846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#36827;&#34892;&#32954;&#30284;&#26089;&#26399;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#33655;&#20848;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#26089;&#26399;&#39044;&#27979;&#32954;&#30284;&#30340;&#38382;&#39064;&#12290;&#22240;&#20026;&#32954;&#30284;&#22312;&#21021;&#32423;&#20445;&#20581;&#20013;&#30340;&#24739;&#30149;&#29575;&#36739;&#20302;&#65292;&#25152;&#20197;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#19979;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#24182;&#30740;&#31350;&#65306;1&#65289;&#22914;&#20309;&#23558;\textit {&#36719;&#25552;&#31034;&#35843;&#25972;} - &#19968;&#31181;&#20351;&#29992;&#23567;&#37327;&#35757;&#32451;&#25968;&#25454;&#35843;&#25972;PLMs&#30340;NLP&#25216;&#26415; - &#19982;&#26631;&#20934;&#27169;&#22411;&#24494;&#35843;&#36827;&#34892;&#27604;&#36739;&#65307; 2&#65289;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#35774;&#32622;&#20013;&#65292;&#26159;&#21542;&#31616;&#21333;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#65288;WEMs&#65289;&#21487;&#20197;&#27604;PLMs&#26356;&#20581;&#22766;&#65307;&#20197;&#21450;3&#65289;&#24403;&#35757;&#32451;&#31508;&#35760;&#26469;&#33258;&#23569;&#37327;&#24739;&#32773;&#26102;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;1&#65289;&#36719;&#25552;&#31034;&#35843;&#25972;&#26159;&#26631;&#20934;&#27169;&#22411;&#24494;&#35843;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#65307; 2&#65289;PLMs&#27604;&#36739;&#31616;&#21333;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#20294;&#26356;&#24046;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate different natural language processing (NLP) approaches based on contextualised word representations for the problem of early prediction of lung cancer using free-text patient medical notes of Dutch primary care physicians. Because lung cancer has a low prevalence in primary care, we also address the problem of classification under highly imbalanced classes. Specifically, we use large Transformer-based pretrained language models (PLMs) and investigate: 1) how \textit{soft prompt-tuning} -- an NLP technique used to adapt PLMs using small amounts of training data -- compares to standard model fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more robust compared to PLMs in highly imbalanced settings; and 3) how models fare when trained on notes from a small number of patients. We find that 1) soft-prompt tuning is an efficient alternative to standard model fine-tuning; 2) PLMs show better discrimination but worse calibration compared to simpler stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#23545;&#21333;&#20010;&#35266;&#27979;&#32467;&#26524;&#26377;&#20581;&#22766;&#24615;</title><link>http://arxiv.org/abs/2303.15845</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21487;&#35777;&#26126;&#20855;&#26377;&#20581;&#22766;&#24615;:&#38134;&#28246;&#21453;&#38382;&#39064;&#30340;&#36880;&#28857;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems. (arXiv:2303.15845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#23545;&#21333;&#20010;&#35266;&#27979;&#32467;&#26524;&#26377;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#25104;&#20026;&#37319;&#26679;&#38134;&#28246;&#21453;&#38382;&#39064;&#21518;&#39564;&#27010;&#29575;&#30340;&#24378;&#22823;&#24037;&#20855;. &#32463;&#20856;&#30340;&#36125;&#21494;&#26031;&#25991;&#29486;&#24050;&#32463;&#30693;&#36947;&#21518;&#39564;&#27979;&#24230;&#23545;&#20808;&#21069;&#27979;&#24230;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;(&#21253;&#25324;&#35266;&#23519;&#30340;&#25200;&#21160;)&#38750;&#24120; robust. &#20294;&#26159;, &#23601;&#25105;&#20204;&#25152;&#30693;, &#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#36824;&#27809;&#34987;&#30740;&#31350;&#36807;. &#22312;&#26412;&#25991;&#20013;, &#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36866;&#24403;&#23398;&#20064;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20010;&#35266;&#27979;&#20540;&#26041;&#38754;&#25552;&#20379;&#20102;&#20581;&#22766;&#30340;&#32467;&#26524;.
&lt;/p&gt;
&lt;p&gt;
Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#24182;&#36991;&#20813;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#38656;&#27714;&#20197;&#25552;&#39640;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15844</link><description>&lt;p&gt;
&#29983;&#25104;&#21512;&#29702;&#30340;&#23545;&#31574;&#24207;&#21015;&#29992;&#20110;&#39044;&#27979;&#24615;&#27969;&#31243;&#20998;&#26512;&#65306;CREATED
&lt;/p&gt;
&lt;p&gt;
CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics. (arXiv:2303.15844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#24182;&#36991;&#20813;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#38656;&#27714;&#20197;&#25552;&#39640;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#27969;&#31243;&#20998;&#26512;&#20851;&#27880;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#20363;&#22914;&#36816;&#34892;&#27969;&#31243;&#23454;&#20363;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;LSTM&#65289;&#36827;&#34892;&#36825;&#26679;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#22797;&#26434;&#32780;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#21453;&#20107;&#23454;&#33021;&#22238;&#31572;&#8220;&#22914;&#26524;&#8230;&#8230;&#20250;&#24590;&#26679;&#8221;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#26377;&#21161;&#20110;&#29702;&#35299;&#39044;&#27979;&#32972;&#21518;&#30340;&#25512;&#29702;&#12290;&#24403;&#21069;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#30340;&#26041;&#27861;&#19981;&#32771;&#34385;&#27969;&#31243;&#34892;&#20026;&#65292;&#20174;&#32780;&#23548;&#33268;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#27969;&#31243;&#23454;&#20363;&#65292;&#25110;&#20005;&#37325;&#20381;&#36182;&#20110;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#35758;&#35757;&#32451;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#35745;&#31639;&#21453;&#20107;&#23454;&#24207;&#21015;&#30340;&#27010;&#29575;&#65292;&#36825;&#23558;&#22312;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#26102;&#20445;&#35777;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive process analytics focuses on predicting future states, such as the outcome of running process instances. These techniques often use machine learning models or deep learning models (such as LSTM) to make such predictions. However, these deep models are complex and difficult for users to understand. Counterfactuals answer ``what-if'' questions, which are used to understand the reasoning behind the predictions. For example, what if instead of emailing customers, customers are being called? Would this alternative lead to a different outcome? Current methods to generate counterfactual sequences either do not take the process behavior into account, leading to generating invalid or infeasible counterfactual process instances, or heavily rely on domain knowledge. In this work, we propose a general framework that uses evolutionary methods to generate counterfactual sequences. Our framework does not require domain knowledge. Instead, we propose to train a Markov model to compute the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36328;&#32452;&#32455;&#30340;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#27861;&#24459;&#23454;&#20307;&#20043;&#38388;&#26102;&#65292;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#21644;&#38656;&#35201;&#20132;&#25442;&#22823;&#37327;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#26174;&#31034;&#20102;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15834</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#26426;&#22120;&#23398;&#20064;&#22312;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#36328;&#32452;&#32455;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning. (arXiv:2303.15834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36328;&#32452;&#32455;&#30340;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#27861;&#24459;&#23454;&#20307;&#20043;&#38388;&#26102;&#65292;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#21644;&#38656;&#35201;&#20132;&#25442;&#22823;&#37327;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#26174;&#31034;&#20102;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20381;&#36182;&#20110;&#36830;&#25509;&#21508;&#31181;&#25968;&#25454;&#28304;&#12290;&#34429;&#28982;&#22312;&#32452;&#32455;&#20869;&#37096;&#29983;&#25104;&#26356;&#22823;&#30340;&#25968;&#25454;&#27744;&#36890;&#24120;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#22312;&#65288;&#36328;&#32452;&#32455;&#30340;&#65289;&#20225;&#19994;&#32593;&#32476;&#20013;&#24212;&#29992;&#20998;&#26512;&#20173;&#28982;&#21463;&#21040;&#20005;&#37325;&#30340;&#21046;&#32422;&#12290;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#27861;&#24459;&#23454;&#20307;&#20043;&#38388;&#65292;&#29978;&#33267;&#21487;&#33021;&#36328;&#36234;&#22810;&#20010;&#22269;&#23478;&#65292;&#22240;&#27492;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#30340;&#25285;&#24551;&#20197;&#21450;&#38656;&#35201;&#20132;&#25442;&#30340;&#25968;&#25454;&#37327;&#26159;&#21019;&#24314;&#26377;&#25928;&#30340;&#31995;&#32479;&#33539;&#22260;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#38556;&#30861;&#65292;&#21516;&#26102;&#20173;&#28982;&#23454;&#29616;&#21331;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#20174;&#32780;&#22312;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#37319;&#29992;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#24037;&#19994;&#29992;&#20363;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25191;&#34892;&#32593;&#32476;&#24191;&#27867;&#20998;&#26512;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful analytics solutions that provide valuable insights often hinge on the connection of various data sources. While it is often feasible to generate larger data pools within organizations, the application of analytics within (inter-organizational) business networks is still severely constrained. As data is distributed across several legal units, potentially even across countries, the fear of disclosing sensitive information as well as the sheer volume of the data that would need to be exchanged are key inhibitors for the creation of effective system-wide solutions -- all while still reaching superior prediction performance. In this work, we propose a meta machine learning method that deals with these obstacles to enable comprehensive analyses within a business network. We follow a design science research approach and evaluate our method with respect to feasibility and performance in an industrial use case. First, we show that it is feasible to perform network-wide analyses that 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;PDExplain&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#65292;&#26497;&#22823;&#22320;&#21327;&#21161;&#20102;&#24314;&#31435;&#29289;&#29702;&#31185;&#23398;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#29616;&#35937;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.15827</link><description>&lt;p&gt;
PDExplain&#65306;PDEs &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24773;&#22659;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PDExplain: Contextual Modeling of PDEs in the Wild. (arXiv:2303.15827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15827
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PDExplain&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#65292;&#26497;&#22823;&#22320;&#21327;&#21161;&#20102;&#24314;&#31435;&#29289;&#29702;&#31185;&#23398;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#29616;&#35937;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;PDExplain&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#25805;&#20316;&#21592;&#23450;&#20041;&#30340;PDE&#23478;&#26063;&#30340;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#23478;&#26063;&#30340;&#19968;&#33324;&#24418;&#24335;&#36827;&#34892;&#39304;&#36865;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#29616;&#35937;&#20013;&#25910;&#38598;&#21040;&#30340;&#26368;&#23567;&#26679;&#26412;&#65292;&#20854;&#20013;&#26679;&#26412;&#19982; PDE &#23478;&#26063;&#30456;&#20851;&#65292;&#20294;&#19981;&#19968;&#23450;&#23646;&#20110;&#35757;&#32451;&#38454;&#27573;&#30475;&#21040;&#30340;&#20855;&#20307; PDE &#38598;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31639;&#27861;&#22914;&#20309;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;PDE&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;PDE&#30340;&#21487;&#35299;&#37322;&#24418;&#24335;&#65292;&#36825;&#31181;&#29305;&#24449;&#21487;&#20197;&#21327;&#21161;&#36890;&#36807;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#26469;&#23545;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32771;&#23519;&#20102;&#20854;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an explainable method for solving Partial Differential Equations by using a contextual scheme called PDExplain. During the training phase, our method is fed with data collected from an operator-defined family of PDEs accompanied by the general form of this family. In the inference phase, a minimal sample collected from a phenomenon is provided, where the sample is related to the PDE family but not necessarily to the set of specific PDEs seen in the training phase. We show how our algorithm can predict the PDE solution for future timesteps. Moreover, our method provides an explainable form of the PDE, a trait that can assist in modelling phenomena based on data in physical sciences. To verify our method, we conduct extensive experimentation, examining its quality both in terms of prediction error and explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#26144;&#23556;&#28608;&#20809;&#31561;&#31163;&#23376;&#20307;&#21152;&#36895;&#22120;&#30340;&#35299;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20123;&#22312;&#31867;&#20284;&#28608;&#20809;&#26463;&#25928;&#29575;&#30340;&#21516;&#26102;&#24179;&#34913;&#26463;&#33021;&#19982;&#30005;&#33655;&#30340;Pareto&#26368;&#20248;&#35299;&#65292;&#20294;&#24403;&#24212;&#29992;&#38656;&#35201;&#29305;&#23450;&#30446;&#26631;&#33021;&#37327;&#30340;&#31890;&#23376;&#26463;&#26102;&#38656;&#35201;&#26435;&#34913;&#33021;&#37327;&#23637;&#23485;&#19982;&#21152;&#36895;&#22120;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.15825</link><description>&lt;p&gt;
&#19968;&#31181;&#28608;&#20809;&#31561;&#31163;&#23376;&#20307;&#21152;&#36895;&#22120;&#30340;Pareto&#20248;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Pareto Optimization of a Laser Wakefield Accelerator. (arXiv:2303.15825v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#26144;&#23556;&#28608;&#20809;&#31561;&#31163;&#23376;&#20307;&#21152;&#36895;&#22120;&#30340;&#35299;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20123;&#22312;&#31867;&#20284;&#28608;&#20809;&#26463;&#25928;&#29575;&#30340;&#21516;&#26102;&#24179;&#34913;&#26463;&#33021;&#19982;&#30005;&#33655;&#30340;Pareto&#26368;&#20248;&#35299;&#65292;&#20294;&#24403;&#24212;&#29992;&#38656;&#35201;&#29305;&#23450;&#30446;&#26631;&#33021;&#37327;&#30340;&#31890;&#23376;&#26463;&#26102;&#38656;&#35201;&#26435;&#34913;&#33021;&#37327;&#23637;&#23485;&#19982;&#21152;&#36895;&#22120;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#36895;&#22120;&#24615;&#33021;&#21442;&#25968;&#30340;&#20248;&#21270;&#21463;&#21040;&#20247;&#22810;&#26435;&#34913;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#25214;&#21040;&#26410;&#30693;&#31995;&#32479;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#36866;&#24403;&#24179;&#34913;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#33021;&#22815;&#20197;&#38750;&#24120;&#39640;&#25928;&#30340;&#26041;&#24335;&#26144;&#23556;&#28608;&#20809;&#31561;&#31163;&#23376;&#20307;&#21152;&#36895;&#22120;&#35299;&#31354;&#38388;&#12290;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;&#19982;&#26576;&#20010;&#33021;&#37327;&#30340;&#30005;&#23376;&#26463;&#30456;&#20851;&#30340;&#36129;&#29486;&#65292;&#24182;&#35266;&#23519;&#21040;&#23384;&#22312;&#24191;&#27867;&#30340;Pareto&#26368;&#20248;&#35299;&#33539;&#22260;&#65292;&#20854;&#22312;&#31867;&#20284;&#28608;&#20809;&#26463;&#25928;&#29575;&#30340;&#21516;&#26102;&#24179;&#34913;&#20102;&#26463;&#33021;&#19982;&#30005;&#33655;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24212;&#29992;&#65288;&#22914;&#20809;&#28304;&#65289;&#38656;&#35201;&#29305;&#23450;&#30446;&#26631;&#33021;&#37327;&#30340;&#31890;&#23376;&#26463;&#12290;&#19968;&#26086;&#24341;&#20837;&#36825;&#26679;&#30340;&#32422;&#26463;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#33021;&#37327;&#23637;&#23485;&#19982;&#21152;&#36895;&#22120;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#30452;&#25509;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;\emph{a posteriori}&#26631;&#37327;&#21270;&#30340;&#30446;&#26631;&#26377;&#25928;&#22320;&#20998;&#35299;&#25506;&#26597;&#19982;&#21033;&#29992;&#20855;&#20307;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of accelerator performance parameters is limited by numerous trade-offs and finding the appropriate balance between optimization goals for an unknown system is challenging to achieve. Here we show that multi-objective Bayesian optimization can map the solution space of a laser wakefield accelerator in a very sample-efficient way. Using a Gaussian mixture model, we isolate contributions related to an electron bunch at a certain energy and we observe that there exists a wide range of Pareto-optimal solutions that trade beam energy versus charge at similar laser-to-beam efficiency. However, many applications such as light sources require particle beams at a certain target energy. Once such a constraint is introduced we observe a direct trade-off between energy spread and accelerator efficiency. We furthermore demonstrate how specific solutions can be exploited using \emph{a posteriori} scalarization of the objectives, thereby efficiently splitting the exploration and exploita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SDES&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#20248;&#21270;&#12289;&#24674;&#22797;&#21644;&#35780;&#20272;&#20197;&#21450;&#25913;&#36827;&#31561;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#31354;&#38388;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15821</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#31163;&#25955;&#21270;&#28436;&#21270;&#25628;&#32034;&#30340;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Multi-Objective Security Games Provably via Space Discretization Based Evolutionary Search. (arXiv:2303.15821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SDES&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#20248;&#21270;&#12289;&#24674;&#22797;&#21644;&#35780;&#20272;&#20197;&#21450;&#25913;&#36827;&#31561;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#31354;&#38388;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#39046;&#22495;&#65292;&#22810;&#30446;&#26631;&#23433;&#20840;&#21338;&#24328;(MOSGs)&#20801;&#35768;&#38450;&#24481;&#32773;&#21516;&#26102;&#20445;&#25252;&#22810;&#20010;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#12290;MOSGs&#26088;&#22312;&#21516;&#26102;&#26368;&#22823;&#21270;&#25152;&#26377;&#24322;&#36136;&#24615;&#22238;&#25253;&#65292;&#20363;&#22914;&#29983;&#21629;&#12289;&#37329;&#38065;&#21644;&#29359;&#32618;&#29575;&#65292;&#32780;&#19981;&#21512;&#24182;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#20445;&#25252;&#30340;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#33021;&#36229;&#20986;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;MOSGs&#21463;&#21040;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#31216;&#20026;SDES&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#25628;&#32034;&#26469;&#25193;&#23637;MOSGs&#30340;&#22823;&#35268;&#27169;&#30446;&#26631;&#21644;&#24322;&#36136;&#24615;&#25915;&#20987;&#32773;&#12290;SDES&#30001;&#22235;&#20010;&#36830;&#32493;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;&#31163;&#25955;&#21270;&#12289;&#20248;&#21270;&#12289;&#24674;&#22797;&#21644;&#35780;&#20272;&#20197;&#21450;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SDES&#20351;&#29992;&#21338;&#24328;&#29702;&#35770;&#20013;&#30340;&#26368;&#22823;&#19981;&#24179;&#31561;&#24615;&#21407;&#29702;&#23558;&#21407;&#22987;&#30340;&#39640;&#32500;&#36830;&#32493;&#35299;&#31354;&#38388;&#31163;&#25955;&#21270;&#20026;&#20302;&#32500;&#31163;&#25955;&#35299;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of security, multi-objective security games (MOSGs) allow defenders to simultaneously protect targets from multiple heterogeneous attackers. MOSGs aim to simultaneously maximize all the heterogeneous payoffs, e.g., life, money, and crime rate, without merging heterogeneous attackers. In real-world scenarios, the number of heterogeneous attackers and targets to be protected may exceed the capability of most existing state-of-the-art methods, i.e., MOSGs are limited by the issue of scalability. To this end, this paper proposes a general framework called SDES based on many-objective evolutionary search to scale up MOSGs to large-scale targets and heterogeneous attackers. SDES consists of four consecutive key components, i.e., discretization, optimization, restoration and evaluation, and refinement. Specifically, SDES first discretizes the originally high-dimensional continuous solution space to the low-dimensional discrete one by the maximal indifference property in game theo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;IVR-Q&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#36890;&#36807;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#36991;&#20813;&#20102;OOD&#21160;&#20316;&#24102;&#26469;&#30340;&#20215;&#20540;&#20989;&#25968;&#20559;&#31227;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;IVR loss&#26469;&#25913;&#21892;&#31574;&#30053;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15810</link><description>&lt;p&gt;
&#27809;&#26377;OOD&#21160;&#20316;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#30340;&#26679;&#26412;&#20869;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization. (arXiv:2303.15810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;IVR-Q&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#36890;&#36807;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#36991;&#20813;&#20102;OOD&#21160;&#20316;&#24102;&#26469;&#30340;&#20215;&#20540;&#20989;&#25968;&#20559;&#31227;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;IVR loss&#26469;&#25913;&#21892;&#31574;&#30053;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#38754;&#20020;&#19968;&#20010;&#25240;&#34935;&#38382;&#39064;&#65306;&#25913;&#21892;&#31574;&#30053;&#20197;&#36229;&#36234;&#34892;&#20026;&#31574;&#30053;&#19982;&#38480;&#21046;&#31574;&#30053;&#20197;&#38480;&#21046;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20559;&#24046;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30001;&#20110;&#20351;&#29992;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260; (OOD) &#30340;&#21160;&#20316;&#35745;&#31639; Q &#20540;&#20250;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#26679;&#26412;&#20869;&#23398;&#20064;&#33539;&#24335;&#65288;IQL&#65289;&#36890;&#36807;&#23545;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#20998;&#20301;&#22238;&#24402;&#26469;&#25913;&#21892;&#31574;&#30053;&#65292;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#32780;&#19981;&#26597;&#35810;&#20219;&#20309;&#26410;&#35265;&#21160;&#20316;&#30340;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#22788;&#29702;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#26679;&#26412;&#20869;&#23398;&#20064;&#33539;&#20363;&#22312;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270; (IVR) &#26694;&#26550;&#19979;&#24471;&#20197;&#20135;&#29983;&#12290;&#36825;&#32473;&#20102;&#25105;&#20204;&#26356;&#28145;&#21051;&#30340;&#29702;&#35299;&#20026;&#20160;&#20040;&#26679;&#26412;&#20869;&#23398;&#20064;&#33539;&#20363;&#26377;&#25928;&#65292;&#21363;&#23427;&#23558;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#65292;IVR-Q&#65292;&#23427;&#36890;&#36807;&#35268;&#33539;&#21270;&#31574;&#30053;&#30340;&#20215;&#20540;&#20989;&#25968;&#20197;&#36991;&#20813;OOD&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;IVR loss&#26469;&#25913;&#21892;&#31574;&#30053;&#12290;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IVR-Q&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#32447;RL&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recently proposed \textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#35823;&#24046;&#26377;&#19968;&#20010;&#19979;&#30028;&#65292;&#22312;&#36739;&#22823;&#33539;&#22260;&#30340;&#26680;&#20989;&#25968;&#20013;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#20351;&#24471;&#36807;&#25311;&#21512;&#30340;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.15809</link><description>&lt;p&gt;
&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24369;
&lt;/p&gt;
&lt;p&gt;
Kernel interpolation generalizes poorly. (arXiv:2303.15809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#35823;&#24046;&#26377;&#19968;&#20010;&#19979;&#30028;&#65292;&#22312;&#36739;&#22823;&#33539;&#22260;&#30340;&#26680;&#20989;&#25968;&#20013;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#20351;&#24471;&#36807;&#25311;&#21512;&#30340;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#22238;&#24402;&#30740;&#31350;&#30340;&#22797;&#20852;&#20013;&#65292;&#19968;&#20010;&#26368;&#26377;&#36259;&#30340;&#38382;&#39064;&#21487;&#33021;&#26159;&#26680;&#25554;&#20540;&#26159;&#21542;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#28145;&#24230;&#32593;&#32476;&#39046;&#22495;&#20013;&#30340;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;$\varepsilon&gt;0$&#65292;&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#35823;&#24046;&#37117;&#26377;&#19968;&#20010;&#19979;&#30028;$\Omega(n^{-\varepsilon})$&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#26680;&#25554;&#20540;&#22312;&#36739;&#22823;&#33539;&#22260;&#30340;&#26680;&#20989;&#25968;&#20013;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20316;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#25512;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#22312;&#29699;&#19978;&#23450;&#20041;&#30340;&#36807;&#25311;&#21512;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most interesting problems in the recent renaissance of the studies in kernel regression might be whether the kernel interpolation can generalize well, since it may help us understand the `benign overfitting henomenon' reported in the literature on deep networks. In this paper, under mild conditions, we show that for any $\varepsilon&gt;0$, the generalization error of kernel interpolation is lower bounded by $\Omega(n^{-\varepsilon})$. In other words, the kernel interpolation generalizes poorly for a large class of kernels. As a direct corollary, we can show that overfitted wide neural networks defined on sphere generalize poorly.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15799</link><description>&lt;p&gt;
&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#21450;&#21442;&#19982;&#32773;&#20043;&#38388;&#39057;&#32321;&#30340;&#36890;&#20449;&#23558;&#20943;&#32531;&#25910;&#25947;&#36895;&#29575;&#24182;&#22686;&#21152;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24120;&#35268;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#32858;&#21512;&#26799;&#24230;&#26469;&#25913;&#21892;&#26412;&#22320;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36827;&#19968;&#27493;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#21644;&#20840;&#23616;&#21442;&#25968;&#30340;&#20559;&#24046;&#12290;&#20197;&#19978;&#31574;&#30053;&#35201;&#27714;&#22312;&#27599;&#20010;&#26412;&#22320;&#36845;&#20195;&#20013;&#25910;&#38598;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#21442;&#25968;&#21644;&#26799;&#24230;&#65292;&#30001;&#20110;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#27809;&#26377;&#36890;&#20449;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#24341;&#20837;&#31216;&#20026;&#20840;&#23616;&#22343;&#20540;&#22330;&#21644;&#26412;&#22320;&#22343;&#20540;&#22330;&#30340;&#20004;&#20010;&#22343;&#20540;&#22330;&#26415;&#35821;&#26469;&#23436;&#25104;&#32858;&#21512;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;&#29983;&#24577;&#22270;&#34920;&#12301;&#30340;&#25991;&#26723;&#26694;&#26550;&#65292;&#21487;&#20197;&#36879;&#26126;&#22320;&#38598;&#20013;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#20854;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.15772</link><description>&lt;p&gt;
&#29983;&#24577;&#22270;&#34920;&#65306;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Ecosystem Graphs: The Social Footprint of Foundation Models. (arXiv:2303.15772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15772
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;&#29983;&#24577;&#22270;&#34920;&#12301;&#30340;&#25991;&#26723;&#26694;&#26550;&#65292;&#21487;&#20197;&#36879;&#26126;&#22320;&#38598;&#20013;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#20854;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914; ChatGPT&#12289;StableDiffusion&#65289;&#24191;&#27867;&#24433;&#21709;&#31038;&#20250;&#65292;&#22240;&#27492;&#38656;&#35201;&#31038;&#20250;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#26412;&#36523;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20294;&#20026;&#20102;&#20934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;&#29983;&#24577;&#22270;&#34920;&#12301;&#30340;&#25991;&#26723;&#26694;&#26550;&#65292;&#20197;&#36879;&#26126;&#22320;&#38598;&#20013;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#30340;&#30693;&#35782;&#12290;&#29983;&#24577;&#22270;&#34920;&#30001;&#36164;&#20135;&#65288;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#12289;&#24212;&#29992;&#31243;&#24207;&#65289;&#32452;&#25104;&#65292;&#36825;&#20123;&#36164;&#20135;&#36890;&#36807;&#20381;&#36182;&#20851;&#31995;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#36825;&#20123;&#20851;&#31995;&#25351;&#31034;&#20102;&#25216;&#26415;&#65288;&#20363;&#22914; Bing &#22914;&#20309;&#20381;&#36182; GPT-4&#65289;&#21644;&#31038;&#20132;&#65288;&#20363;&#22914; Microsoft &#22914;&#20309;&#20381;&#36182; OpenAI&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#34917;&#20805;&#22270;&#24418;&#32467;&#26500;&#65292;&#27599;&#20010;&#36164;&#20135;&#37117;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#32454;&#31890;&#24230;&#30340;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#35768;&#21487;&#25110;&#22521;&#35757;&#25490;&#25918;&#65289;&#12290;&#25105;&#20204;&#22312; https://crfm.stanford.edu/ecosystem-graphs/ &#19978;&#24191;&#27867;&#35760;&#24405;&#29983;&#24577;&#31995;&#32479;&#12290;&#25130;&#33267; 2023 &#24180; 3 &#26376; 16 &#26085;&#65292;&#25105;&#20204;&#27880;&#37322;&#20102;&#26469;&#33258; 63 &#20010;&#32452;&#32455;&#30340; 262 &#20010;&#36164;&#20135;&#65288;64 &#20010;&#25968;&#25454;&#38598;&#65292;128 &#20010;&#27169;&#22411;&#65292;70 &#20010;&#24212;&#29992;&#31243;&#24207;&#65289;&#65292;&#23427;&#20204;&#30001; 356 &#31181;&#20381;&#36182;&#20851;&#31995;&#38142;&#25509;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#24577;&#22270;&#34920;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#30784;&#27169;&#22411;&#21450;&#20854;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#24230;&#65292;&#24182;&#21628;&#21505;&#37319;&#29992;&#23427;&#20316;&#20026;&#19968;&#31181;&#24120;&#35265;&#30340;&#25991;&#26723;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#21453;&#39304;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20989;&#25968;qEUBO&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#37319;&#38598;&#20989;&#25968;&#12290;&#22312;&#20805;&#20998;&#30340;&#26465;&#20214;&#19979;&#65292;qEUBO&#30340;&#36951;&#25022;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#29616;&#26377;&#37319;&#38598;&#20989;&#25968;qEI&#12290;</title><link>http://arxiv.org/abs/2303.15746</link><description>&lt;p&gt;
qEUBO: &#22522;&#20110;&#20915;&#31574;&#29702;&#35770;&#30340;&#12289;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#21453;&#39304;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization. (arXiv:2303.15746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#21453;&#39304;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20989;&#25968;qEUBO&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#37319;&#38598;&#20989;&#25968;&#12290;&#22312;&#20805;&#20998;&#30340;&#26465;&#20214;&#19979;&#65292;qEUBO&#30340;&#36951;&#25022;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#29616;&#26377;&#37319;&#38598;&#20989;&#25968;qEI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#36125;&#21494;&#26031;&#20248;&#21270;(PBO)&#26159;&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;&#20559;&#22909;&#21453;&#39304;&#20248;&#21270;&#20915;&#31574;&#21046;&#23450;&#32773;&#28508;&#22312;&#25928;&#29992;&#20989;&#25968;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#23558;&#26368;&#22909;&#36873;&#39033;&#30340;&#39044;&#26399;&#25928;&#29992;(qEUBO)&#24341;&#20837;PBO&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#37319;&#38598;&#20989;&#25968;&#12290;&#24403;&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#21709;&#24212;&#26080;&#22122;&#22768;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;qEUBO&#26159;&#19968;&#27493;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#19982;&#27969;&#34892;&#30340;&#30693;&#35782;&#26799;&#24230;&#37319;&#38598;&#20989;&#25968;&#31561;&#25928;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#65292;&#24403;&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#21709;&#24212;&#21463;&#21040;&#22122;&#22768;&#27745;&#26579;&#26102;&#65292;qEUBO&#22312;&#19968;&#27493;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#19978;&#20139;&#26377;&#38468;&#21152;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;qEUBO&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#65292;&#23427;&#20248;&#20110;PBO&#30340;&#26368;&#20808;&#36827;&#37319;&#38598;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#20805;&#20998;&#30340;&#27491;&#21017;&#21270;&#26465;&#20214;&#19979;&#65292;qEUBO&#30340;&#36125;&#21494;&#26031;&#31616;&#21333;&#36951;&#25022;&#23558;&#20197;$O(1/n)$&#30340;&#36895;&#24230;&#36235;&#36817;&#20110;&#38646;&#65292;&#20854;&#20013;$n$&#26159;&#26597;&#35810;&#25968;&#37327;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#27969;&#34892;&#30340;PBO&#37319;&#38598;&#20989;&#25968;qEI&#65292;&#31616;&#21333;&#36951;&#25022;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#20026;$O(1/\sqrt{n})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Preferential Bayesian optimization (PBO) is a framework for optimizing a decision maker's latent utility function using preference feedback. This work introduces the expected utility of the best option (qEUBO) as a novel acquisition function for PBO. When the decision maker's responses are noise-free, we show that qEUBO is one-step Bayes optimal and thus equivalent to the popular knowledge gradient acquisition function. We also show that qEUBO enjoys an additive constant approximation guarantee to the one-step Bayes-optimal policy when the decision maker's responses are corrupted by noise. We provide an extensive evaluation of qEUBO and demonstrate that it outperforms the state-of-the-art acquisition functions for PBO across many settings. Finally, we show that, under sufficient regularity conditions, qEUBO's Bayesian simple regret converges to zero at a rate $o(1/n)$ as the number of queries, $n$, goes to infinity. In contrast, we show that simple regret under qEI, a popular acquisiti
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#20854;&#22312;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#29305;&#24449;&#26102;MSE&#26354;&#32447;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#21452;&#23792;&#19979;&#38477;&#8221;&#29616;&#35937;&#30456;&#20284;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.15745</link><description>&lt;p&gt;
&#20851;&#20110;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;&#30340;&#29305;&#24449;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
On Feature Scaling of Recursive Feature Machines. (arXiv:2303.15745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#20854;&#22312;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#29305;&#24449;&#26102;MSE&#26354;&#32447;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#21452;&#23792;&#19979;&#38477;&#8221;&#29616;&#35937;&#30456;&#20284;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#36890;&#36807;&#19968;&#31995;&#21015;&#22312;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;(RFMs)&#30340;&#34892;&#20026;&#65292;RFMs&#26159;&#19968;&#31181;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#26469;&#36882;&#24402;&#22320;&#23398;&#20064;&#29305;&#24449;&#30340;&#26032;&#22411;&#26680;&#26426;&#22120;&#12290;&#24403;&#22312;&#25968;&#25454;&#38598;&#20013;&#36845;&#20195;&#22320;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#29305;&#24449;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22343;&#26041;&#35823;&#24046;(MSE)&#26354;&#32447;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#26377;&#36259;&#27169;&#24335;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#22122;&#22768;&#21442;&#25968;&#21644;&#30446;&#26631;&#20989;&#25968;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35266;&#23519;&#21040;&#30340;MSE&#26354;&#32447;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#8220;&#21452;&#23792;&#19979;&#38477;&#8221;&#29616;&#35937;&#30456;&#20284;&#65292;&#26263;&#31034;RFMs&#21644;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#26032;&#30340;&#32852;&#31995;&#12290;&#36825;&#20221;&#25253;&#21578;&#20026;&#26410;&#26469;&#30740;&#31350;&#36825;&#31181;&#22855;&#22937;&#34892;&#20026;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical report, we explore the behavior of Recursive Feature Machines (RFMs), a type of novel kernel machine that recursively learns features via the average gradient outer product, through a series of experiments on regression datasets. When successively adding random noise features to a dataset, we observe intriguing patterns in the Mean Squared Error (MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern. This behavior is consistent across different dataset sizes, noise parameters, and target functions. Interestingly, the observed MSE curves show similarities to the "double descent" phenomenon observed in deep neural networks, hinting at new connection between RFMs and neural network behavior. This report lays the groundwork for future research into this peculiar behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21512;&#21516;&#31639;&#23376;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#26377;&#30028;&#20056;&#27861;&#22122;&#22768;&#21644;&#21152;&#24615;&#27425;&#39640;&#26031;&#22122;&#22768;&#35774;&#32622;&#19979;&#30340;&#27987;&#24230;&#34892;&#20026;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25910;&#25947;&#35823;&#24046;&#30340;&#26497;&#22823;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#35823;&#24046;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#24052;&#25110;&#32773;&#36229;&#22810;&#39033;&#24335;&#23614;&#24052;&#12290;&#21516;&#26102;&#36824;&#21457;&#29616;&#65292;&#20056;&#27861;&#22122;&#22768;&#24773;&#20917;&#19979;&#19968;&#33324;&#19981;&#21487;&#33021;&#23454;&#29616;&#20122;&#25351;&#25968;&#23614;&#24052;&#12290;</title><link>http://arxiv.org/abs/2303.15740</link><description>&lt;p&gt;
&#21512;&#21516;&#25193;&#24352;&#38543;&#26426;&#36817;&#20284;&#30340;&#27987;&#24230;&#65306;&#21152;&#27861;&#21644;&#20056;&#27861;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Concentration of Contractive Stochastic Approximation: Additive and Multiplicative Noise. (arXiv:2303.15740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21512;&#21516;&#31639;&#23376;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#26377;&#30028;&#20056;&#27861;&#22122;&#22768;&#21644;&#21152;&#24615;&#27425;&#39640;&#26031;&#22122;&#22768;&#35774;&#32622;&#19979;&#30340;&#27987;&#24230;&#34892;&#20026;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25910;&#25947;&#35823;&#24046;&#30340;&#26497;&#22823;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#35823;&#24046;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#24052;&#25110;&#32773;&#36229;&#22810;&#39033;&#24335;&#23614;&#24052;&#12290;&#21516;&#26102;&#36824;&#21457;&#29616;&#65292;&#20056;&#27861;&#22122;&#22768;&#24773;&#20917;&#19979;&#19968;&#33324;&#19981;&#21487;&#33021;&#23454;&#29616;&#20122;&#25351;&#25968;&#23614;&#24052;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#20309;&#33539;&#25968;&#19979;&#65292;&#20855;&#26377;&#21512;&#21516;&#31639;&#23376;&#30340;&#38543;&#26426;&#36924;&#36817;(SA)&#31639;&#27861;&#30340;&#27987;&#24230;&#34892;&#20026;&#12290; &#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#24773;&#20917;&#65292;&#20854;&#20013;&#36845;&#20195;&#21487;&#33021;&#26080;&#30028;&#65306;&#65288;1&#65289;&#26377;&#30028;&#20056;&#27861;&#22122;&#22768;&#65292;&#65288;2&#65289;&#21152;&#24615;&#27425;&#39640;&#26031;&#22122;&#22768;&#12290; &#25105;&#20204;&#24471;&#21040;&#20102;&#20851;&#20110;&#25910;&#25947;&#35823;&#24046;&#30340;&#26497;&#22823;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#35823;&#24046;&#22312;&#21152;&#24615;&#22122;&#22768;&#35774;&#32622;&#19979;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#24052;&#65292;&#22312;&#20056;&#27861;&#22122;&#22768;&#35774;&#32622;&#19979;&#20855;&#26377;&#36229;&#22810;&#39033;&#24335;&#23614;&#24052;&#65288;&#24555;&#20110;&#22810;&#39033;&#24335;&#34928;&#20943;&#65289;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#26174;&#31034;&#36890;&#24120;&#26080;&#27861;&#36890;&#36807;&#20056;&#27861;&#22122;&#22768;&#30340;SA&#23454;&#29616;&#20122;&#25351;&#25968;&#23614;&#24052;&#12290; &#20026;&#20102;&#30830;&#31435;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20030;&#35770;&#35777;&#65292;&#20854;&#20013;&#28041;&#21450;&#36793;&#30028;&#35823;&#24046;&#30340;&#24191;&#20041;Moreau&#21253;&#32476;&#30340;&#30697;&#29983;&#25104;&#20989;&#25968;&#21644;&#25351;&#25968;&#36229;&#39532;&#23572;&#21487;&#22827;&#26500;&#36896;&#65292;&#20197;&#21551;&#29992;&#20351;&#29992;Ville&#30340;&#26497;&#22823;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the concentration behavior of a stochastic approximation (SA) algorithm under a contractive operator with respect to an arbitrary norm. We consider two settings where the iterates are potentially unbounded: (1) bounded multiplicative noise, and (2) additive sub-Gaussian noise. We obtain maximal concentration inequalities on the convergence errors, and show that these errors have sub-Gaussian tails in the additive noise setting, and super-polynomial tails (faster than polynomial decay) in the multiplicative noise setting. In addition, we provide an impossibility result showing that it is in general not possible to achieve sub-exponential tails for SA with multiplicative noise. To establish these results, we develop a novel bootstrapping argument that involves bounding the moment generating function of the generalized Moreau envelope of the error and the construction of an exponential supermartingale to enable using Ville's maximal inequality.  To demonstrate the a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#35828;&#26126;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.15739</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#36125;&#21494;&#26031;&#33258;&#30001;&#33021;
&lt;/p&gt;
&lt;p&gt;
Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases. (arXiv:2303.15739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#35828;&#26126;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21487;&#29992;&#20110;&#20272;&#35745;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#26410;&#30693;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#23578;&#26410;&#20174;&#29702;&#35770;&#35282;&#24230;&#23436;&#20840;&#28548;&#28165;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#19981;&#21487;&#35782;&#21035;&#30340;&#21644;&#22855;&#24322;&#30340;&#23398;&#20064;&#26426;&#22120;&#12290;&#27492;&#22806;&#65292;ReLU&#20989;&#25968;&#19981;&#21487;&#24494;&#65292;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20195;&#25968;&#25110;&#35299;&#26512;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#23427;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20102;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#21363;&#20351;&#23618;&#25968;&#27604;&#20272;&#35745;&#26410;&#30693;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#25152;&#24517;&#38656;&#30340;&#23618;&#25968;&#26356;&#22810;&#12290;&#30001;&#20110;Bayesian&#24191;&#20041;&#35823;&#24046;&#31561;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#33258;&#30001;&#33021;&#22686;&#21152;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#34920;&#26126;&#65292;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many research fields in artificial intelligence, it has been shown that deep neural networks are useful to estimate unknown functions on high dimensional input spaces. However, their generalization performance is not yet completely clarified from the theoretical point of view because they are nonidentifiable and singular learning machines. Moreover, a ReLU function is not differentiable, to which algebraic or analytic methods in singular learning theory cannot be applied. In this paper, we study a deep ReLU neural network in overparametrized cases and prove that the Bayesian free energy, which is equal to the minus log marginal likelihoodor the Bayesian stochastic complexity, is bounded even if the number of layers are larger than necessary to estimate an unknown data-generating function. Since the Bayesian generalization error is equal to the increase of the free energy as a function of a sample size, our result also shows that the Bayesian generalization error does not increase ev
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#30740;&#31350;&#21644;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#65292;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#28789;&#24863;&#65292;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#22312;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#20869;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.15725</link><description>&lt;p&gt;
&#27714;&#35299;&#27491;&#21017;&#21270;&#30340;exp&#12289;cosh&#21644;sinh&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Regularized Exp, Cosh and Sinh Regression Problems. (arXiv:2303.15725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15725
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#30740;&#31350;&#21644;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#65292;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#28789;&#24863;&#65292;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#22312;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#20869;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27880;&#24847;&#21147;&#35745;&#31639;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Transformer&#12289;GPT-4&#21644;ChatGPT&#65289;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#35813;&#25991;&#30740;&#31350;&#20102;&#21463;softmax/exp&#21333;&#20803;&#21551;&#21457;&#30340;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#12290;&#26631;&#20934;&#25351;&#25968;&#22238;&#24402;&#26159;&#38750;&#20984;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#20984;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#20197;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#35299;&#20915;&#38382;&#39064;&#12290;&#24418;&#24335;&#19978;&#65292;&#32473;&#23450;&#30697;&#38453;$A\in \mathbb{R}^{n\times d}$&#65292;$b\in \mathbb{R}^n$&#65292;$w\in\mathbb{R}^n$&#21644;&#20219;&#20309;&#20989;&#25968;$\exp,\cosh$&#21644;$\sinh$&#65292;&#35760;&#20316;$f$&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#20248;$x$&#65292;&#20351;&#24471;$0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$&#26368;&#23567;&#21270;&#12290;&#30740;&#31350;&#24182;&#35299;&#20915;&#20102;&#27491;&#21017;&#21270;&#25351;&#25968;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#20989;&#25968;&#20026;$\exp, \cosh$&#21644;$\sinh$&#65292;&#24182;&#20351;&#29992;&#36817;&#20284;&#29275;&#39039;&#27861;&#22312;&#36755;&#20837;&#31232;&#30095;&#26102;&#38388;&#20869;&#27714;&#35299;&#65292;&#28789;&#24863;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern machine learning, attention computation is a fundamental task for training large language models such as Transformer, GPT-4 and ChatGPT. In this work, we study exponential regression problem which is inspired by the softmax/exp unit in the attention mechanism in large language models. The standard exponential regression is non-convex. We study the regularization version of exponential regression problem which is a convex problem. We use approximate newton method to solve in input sparsity time.  Formally, in this problem, one is given matrix $A \in \mathbb{R}^{n \times d}$, $b \in \mathbb{R}^n$, $w \in \mathbb{R}^n$ and any of functions $\exp, \cosh$ and $\sinh$ denoted as $f$. The goal is to find the optimal $x$ that minimize $ 0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$. The straightforward method is to use the naive Newton's method. Let $\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let $\omega$ denote the exponent of matrix multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DistGER&#30340;&#20998;&#24067;&#24335;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#21033;&#29992;&#22810;&#31181;&#20248;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21313;&#20159;&#32423;&#21035;&#22270;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2303.15702</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#30340;&#20998;&#24067;&#24335;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Graph Embedding with Information-Oriented Random Walks. (arXiv:2303.15702v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DistGER&#30340;&#20998;&#24067;&#24335;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#21033;&#29992;&#22810;&#31181;&#20248;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21313;&#20159;&#32423;&#21035;&#22270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23884;&#20837;&#23558;&#22270;&#33410;&#28857;&#26144;&#23556;&#21040;&#20302;&#32500;&#21521;&#37327;&#20013;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#12289;&#22522;&#20110;&#20449;&#24687;&#23548;&#21521;&#38543;&#26426;&#28216;&#36208;&#30340;&#36890;&#29992;&#22270;&#23884;&#20837;&#26694;&#26550;DistGER&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#21313;&#20159;&#32423;&#21035;&#30340;&#22270;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#37327;&#24335;&#22320;&#35745;&#31639;&#20449;&#24687;&#23548;&#21521;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#24182;&#21033;&#29992;&#20102;&#22810;&#31181;&#37051;&#36817;&#24863;&#30693;&#12289;&#27969;&#24335;&#12289;&#24182;&#34892;&#30340;&#22270;&#21010;&#20998;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#26426;&#22120;&#19978;&#26082;&#39640;&#36136;&#37327;&#30340;&#26412;&#22320;&#21010;&#20998;&#65292;&#21448;&#20986;&#33394;&#30340;&#36127;&#36733;&#22343;&#34913;&#12290;&#21516;&#26102;DistGER&#25913;&#36827;&#20102;&#20998;&#24067;&#24335;Skip-Gram&#23398;&#20064;&#27169;&#22411;&#20197;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#20248;&#21270;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30340;&#35775;&#38382;&#23616;&#37096;&#24615;&#12289;CPU&#21534;&#21520;&#37327;&#21644;&#21516;&#27493;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks. The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a general-purpose, distributed, information-centric random walk-based graph embedding framework, DistGER, which can scale to embed billion-edge graphs. DistGER incrementally computes information-centric random walks. It further leverages a multi-proximity-aware, streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. DistGER also improves the distributed Skip-Gram learning model to generate node embeddings by optimizing the access locality, CPU throughput, and synchronizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#30149;&#29702;&#22270;&#20687;&#39044;&#35757;&#32451;&#23545;&#23567;&#35268;&#27169;&#30149;&#29702;&#22522;&#20934;&#24494;&#35843;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;PTCGA200&#20026;&#35757;&#32451;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#24494;&#35843;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;imagenet2012&#39044;&#35757;&#32451;&#12290;MoCov2&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;PCam200&#21644;segPANDA200&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.15693</link><description>&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#29992;&#20110;&#23567;&#35268;&#27169;&#30149;&#29702;&#22522;&#20934;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks. (arXiv:2303.15693v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#30149;&#29702;&#22270;&#20687;&#39044;&#35757;&#32451;&#23545;&#23567;&#35268;&#27169;&#30149;&#29702;&#22522;&#20934;&#24494;&#35843;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;PTCGA200&#20026;&#35757;&#32451;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#24494;&#35843;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;imagenet2012&#39044;&#35757;&#32451;&#12290;MoCov2&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;PCam200&#21644;segPANDA200&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#23567;&#22411;&#30446;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#26631;&#20934;&#27493;&#39588;&#12290;&#22823;&#22411;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#36890;&#29992;&#22270;&#20687;&#65288;&#20363;&#22914;imagenet2012&#65289;&#65292;&#32780;&#23567;&#22411;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#20855;&#26377;&#19982;&#22823;&#22411;&#25968;&#25454;&#38598;&#19981;&#21516;&#20998;&#24067;&#30340;&#19987;&#19994;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#24403;&#22823;&#22411;&#25968;&#25454;&#38598;&#26159;&#19987;&#19994;&#21270;&#30340;&#19988;&#20855;&#26377;&#19982;&#23567;&#22411;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#20998;&#24067;&#26102;&#65292;&#36825;&#31181;&#8220;&#22823;&#21040;&#23567;&#8221;&#30340;&#31574;&#30053;&#24182;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#26032;&#32534;&#35793;&#20102;&#19977;&#20010;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;PTCGA200&#65289;&#21644;&#20004;&#20010;&#25918;&#22823;&#35843;&#25972;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#65288;PCam200&#21644;segPANDA200&#65289;&#12290;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#20102;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32959;&#30244;&#20998;&#31867;&#21644;&#32452;&#32455;&#20998;&#21106;&#22522;&#20934;&#30340;&#24494;&#35843;&#12290;&#22312;PTCGA200&#19978;&#20197;MoCov2&#65292;SimCLR&#21644;BYOL&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#24494;&#35843;&#26102;&#27604;imagenet2012&#39044;&#35757;&#32451;&#26356;&#22909;&#65288;&#31934;&#24230;&#20998;&#21035;&#20026;83.94&#65285;&#65292;86.41&#65285;&#65292;84.91&#65285;&#21644;82.72&#65285;&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;MoCov2&#39044;&#35757;&#32451;&#30340;ResNet50&#22312;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;PCam200&#21644;segPANDA200&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#32780;&#27604;imagenet2012&#39044;&#35757;&#32451;&#30340;ResNet50&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining a deep learning model on large image datasets is a standard step before fine-tuning the model on small targeted datasets. The large dataset is usually general images (e.g. imagenet2012) while the small dataset can be specialized datasets that have different distributions from the large dataset. However, this 'large-to-small' strategy is not well-validated when the large dataset is specialized and has a similar distribution to small datasets. We newly compiled three hematoxylin and eosin-stained image datasets, one large (PTCGA200) and two magnification-adjusted small datasets (PCam200 and segPANDA200). Major deep learning models were trained with supervised and self-supervised learning methods and fine-tuned on the small datasets for tumor classification and tissue segmentation benchmarks. ResNet50 pretrained with MoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretraining when fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%, respect
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#37096;&#20998;&#26679;&#26412;&#30340;&#20132;&#21449;&#35270;&#22270;&#32858;&#21512;&#26426;&#21046;&#21644;&#20004;&#20010;&#21407;&#22411;&#23545;&#40784;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15689</link><description>&lt;p&gt;
&#24102;&#26377;&#20132;&#21449;&#35270;&#22270;&#37096;&#20998;&#26679;&#26412;&#21644;&#21407;&#22411;&#23545;&#40784;&#30340;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment. (arXiv:2303.15689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15689
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#37096;&#20998;&#26679;&#26412;&#30340;&#20132;&#21449;&#35270;&#22270;&#32858;&#21512;&#26426;&#21046;&#21644;&#20004;&#20010;&#21407;&#22411;&#23545;&#40784;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#25104;&#21151;&#30340;&#21069;&#25552;&#26159;&#26679;&#26412;&#22312;&#22810;&#35270;&#22270;&#20013;&#20445;&#25345;&#23436;&#25972;&#24615;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22810;&#35270;&#22270;&#26679;&#26412;&#30001;&#20110;&#25968;&#25454;&#25439;&#22351;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#32780;&#37096;&#20998;&#22320;&#21487;&#29992;&#65292;&#23548;&#33268;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#30740;&#31350;&#38519;&#20837;&#22256;&#22659;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#26469;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#20294;&#23384;&#22312;&#20197;&#19979;&#32570;&#28857;&#65306;&#65288;i&#65289;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#20132;&#21449;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#65292;&#24378;&#21046;&#27599;&#20010;&#26679;&#26412;&#22312;&#22810;&#20010;&#35270;&#22270;&#20013;&#30340;&#34920;&#31034;&#26159;&#23436;&#20840;&#30456;&#21516;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#24573;&#30053;&#35270;&#22270;&#24046;&#24322;&#21644;&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#22312;&#22810;&#20010;&#35270;&#22270;&#20013;&#19981;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#65292;&#25152;&#24471;&#21040;&#30340;&#32858;&#31867;&#21407;&#22411;&#21487;&#33021;&#19981;&#23545;&#40784;&#21644;&#20559;&#26012;&#65292;&#23548;&#33268;&#32858;&#21512;&#19981;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35270;&#22270;&#37096;&#20998;&#26679;&#26412;&#21644;&#21407;&#22411;&#23545;&#40784;&#32593;&#32476;(CPSPAN)&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#12290;&#39318;&#20808;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#37096;&#20998;&#26679;&#26412;&#30340;&#20132;&#21449;&#35270;&#22270;&#32858;&#21512;&#26426;&#21046;&#26469;&#25429;&#25417;&#35270;&#22270;&#20043;&#38388;&#30340;&#24046;&#24322;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#21407;&#22411;&#23545;&#40784;&#31574;&#30053;&#26469;&#35299;&#20915;&#21407;&#22411;&#19981;&#23545;&#40784;&#21644;&#32858;&#21512;&#19981;&#27491;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of existing multi-view clustering relies on the assumption of sample integrity across multiple views. However, in real-world scenarios, samples of multi-view are partially available due to data corruption or sensor failure, which leads to incomplete multi-view clustering study (IMVC). Although several attempts have been proposed to address IMVC, they suffer from the following drawbacks: i) Existing methods mainly adopt cross-view contrastive learning forcing the representations of each sample across views to be exactly the same, which might ignore view discrepancy and flexibility in representations; ii) Due to the absence of non-observed samples across multiple views, the obtained prototypes of clusters might be unaligned and biased, leading to incorrect fusion. To address the above issues, we propose a Cross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep Incomplete Multi-view Clustering. Firstly, unlike existing contrastive-based methods, we adopt pa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#24402;&#32435;&#24335;&#34920;&#31034;&#27169;&#22411;&#65288;iHT&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;iHT&#34920;&#31034;&#21487;&#36716;&#31227;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15682</link><description>&lt;p&gt;
&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;Transformer&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Transformers for Knowledge Graph Completion. (arXiv:2303.15682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#24402;&#32435;&#24335;&#34920;&#31034;&#27169;&#22411;&#65288;iHT&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;iHT&#34920;&#31034;&#21487;&#36716;&#31227;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#32467;&#26500;&#30340;&#24322;&#26500;&#24615;&#21644;&#22810;&#20851;&#31995;&#24615;&#65292;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;Transformer&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#25991;&#26412;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38754;&#21521;KG&#34917;&#20840;&#30340;&#24402;&#32435;&#24335;&#34920;&#31034;&#27169;&#22411;&#65288;iHT&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;iHT&#30001;&#23454;&#20307;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;BERT&#65289;&#21644;&#37051;&#23621;&#24863;&#30693;&#30340;&#20851;&#31995;&#35780;&#20998;&#20989;&#25968;&#32452;&#25104;&#65292;&#20004;&#32773;&#37117;&#30001;Transformer&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#22823;&#22411;KG&#25968;&#25454;&#38598;Wikidata5M&#19978;&#39044;&#35757;&#32451;iHT&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21305;&#37197;&#35780;&#20272;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;SOTA&#27169;&#22411;&#65292;&#24179;&#22343;&#20498;&#25968;&#25490;&#21517;&#25552;&#39640;&#20102;25&#65285;&#20197;&#19978;&#12290;&#24403;&#22312;&#20855;&#26377;&#23454;&#20307;&#21644;&#20851;&#31995;&#31227;&#20301;&#30340;&#36739;&#23567;KG&#19978;&#36827;&#19968;&#27493;&#24494;&#35843;&#26102;&#65292;&#39044;&#35757;&#32451;&#30340;iHT&#34920;&#31034;&#34987;&#35777;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;FB15K-237&#21644;WN18RR&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (e.g., BERT) and a neighbor-aware relational scoring function both parameterized by Transformers. We first pre-train iHT on a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art results on matched evaluations, with a relative improvement of more than 25% in mean reciprocal rank over previous SOTA models. When further fine-tuned on smaller KGs with either entity and relational shifts, pre-trained iHT representations are shown to be transferable, significantly improving the performance on FB15K-237 and WN18RR.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;GNN&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;&#36793;&#22686;&#24378;GNN&#21644;&#22810;GNN&#26469;&#35299;&#20915;&#26102;&#38388;&#26080;&#20851;PDE&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#26126;&#26174;&#27604;&#22522;&#32447;&#27169;&#22411;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15681</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#27714;&#35299;&#26102;&#38388;&#26080;&#20851;PDE&#30340;&#29289;&#29702;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
GNN-based physics solver for time-independent PDEs. (arXiv:2303.15681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;GNN&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#8212;&#8212;&#36793;&#22686;&#24378;GNN&#21644;&#22810;GNN&#26469;&#35299;&#20915;&#26102;&#38388;&#26080;&#20851;PDE&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#26126;&#26174;&#27604;&#22522;&#32447;&#27169;&#22411;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#22522;&#30784;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24050;&#34987;&#35777;&#26126;&#22312;&#20934;&#30830;&#24314;&#27169;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#24577;&#36807;&#31243;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#28982;&#32780;&#65292;&#26102;&#38388;&#26080;&#20851;&#38382;&#39064;&#38656;&#35201;&#36328;&#36234;&#35745;&#31639;&#22495;&#36827;&#34892;&#38271;&#36317;&#31163;&#30340;&#20449;&#24687;&#20132;&#25442;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;(GNN)&#26469;&#35828;&#38656;&#35201;&#26356;&#28145;&#30340;&#32593;&#32476;&#65292;&#36825;&#21453;&#36807;&#26469;&#21448;&#21487;&#33021;&#20250;&#25439;&#23475;&#25110;&#20943;&#32531;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;GNN&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#8212;&#8212;&#36793;&#22686;&#24378;GNN&#21644;&#22810;GNN&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#24212;&#29992;&#20110;&#26102;&#38388;&#26080;&#20851;&#30340;&#22266;&#20307;&#21147;&#23398;&#38382;&#39064;&#26102;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#26126;&#26174;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#65288;&#25552;&#39640;&#20102;1.5&#21040;2&#20493;&#65289;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#20307;&#31995;&#32467;&#26500;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#22495;&#12289;&#36793;&#30028;&#26465;&#20214;&#21644;&#26448;&#26009;&#12290;&#36825;&#37324;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22352;&#26631;&#36716;&#25442;&#26469;&#22788;&#29702;&#21464;&#37327;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-based deep learning frameworks have shown to be effective in accurately modeling the dynamics of complex physical systems with generalization capability across problem inputs. However, time-independent problems pose the challenge of requiring long-range exchange of information across the computational domain for obtaining accurate predictions. In the context of graph neural networks (GNNs), this calls for deeper networks, which, in turn, may compromise or slow down the training process. In this work, we present two GNN architectures to overcome this challenge - the Edge Augmented GNN and the Multi-GNN. We show that both these networks perform significantly better (by a factor of 1.5 to 2) than baseline methods when applied to time-independent solid mechanics problems. Furthermore, the proposed architectures generalize well to unseen domains, boundary conditions, and materials. Here, the treatment of variable domains is facilitated by a novel coordinate transformation that enabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#26469;&#25628;&#32034;&#26368;&#36866;&#21512;&#32473;&#23450;&#25945;&#24072;&#30340;&#26368;&#20339;&#23398;&#29983;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#25928;&#26524;&#12290;&#20854;&#36890;&#36807;&#24230;&#37327;&#35821;&#20041;&#28608;&#27963;&#26144;&#23556;&#26465;&#20214;&#19979;&#30340;&#30456;&#20284;&#24615;&#30697;&#38453;&#26469;&#36873;&#25321;&#26368;&#20339;&#23398;&#29983;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15678</link><description>&lt;p&gt;
DisWOT: &#26080;&#38656;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#23398;&#29983;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
DisWOT: Student Architecture Search for Distillation WithOut Training. (arXiv:2303.15678v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#26469;&#25628;&#32034;&#26368;&#36866;&#21512;&#32473;&#23450;&#25945;&#24072;&#30340;&#26368;&#20339;&#23398;&#29983;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#25928;&#26524;&#12290;&#20854;&#36890;&#36807;&#24230;&#37327;&#35821;&#20041;&#28608;&#27963;&#26144;&#23556;&#26465;&#20214;&#19979;&#30340;&#30456;&#20284;&#24615;&#30697;&#38453;&#26469;&#36873;&#25321;&#26368;&#20339;&#23398;&#29983;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;(KD)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#31528;&#37325;&#30340;&#25945;&#24072;&#30340;&#25351;&#23548;&#19979;&#25552;&#39640;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#22823;&#22411;&#26550;&#26500;&#24046;&#24322;&#38480;&#21046;&#20102;&#33976;&#39311;&#25928;&#26524;&#12290;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#33258;&#36866;&#24212;&#33976;&#39311;&#26041;&#27861;&#26469;&#20943;&#23569;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#25628;&#32034;&#32473;&#23450;&#25945;&#24072;&#30340;&#26368;&#20339;&#23398;&#29983;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is an effective training strategy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large architecture difference across the teacher-student pairs limits the distillation gains. In contrast to previous adaptive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work first empirically show that the optimal model under vanilla training cannot be the winner in distillation. Secondly, we find that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with final distillation performances. Thus, we efficiently measure similarity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithm without any training. In this way, our student architecture search for Distillation Wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38543;&#26426;&#35268;&#21010;&#21644;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#20013;&#20984;&#22810;&#32423;&#38543;&#26426;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#35268;&#21010;&#21644;&#21106;&#24179;&#38754;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15672</link><description>&lt;p&gt;
&#20984;&#22810;&#32423;&#38543;&#26426;&#20248;&#21270;&#30340;&#25968;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Numerical Methods for Convex Multistage Stochastic Optimization. (arXiv:2303.15672v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38543;&#26426;&#35268;&#21010;&#21644;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#20013;&#20984;&#22810;&#32423;&#38543;&#26426;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#35268;&#21010;&#21644;&#21106;&#24179;&#38754;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29615;&#22659;&#19979;&#28041;&#21450;&#39034;&#24207;&#20915;&#31574;&#30340;&#20248;&#21270;&#38382;&#39064;&#24050;&#22312;&#38543;&#26426;&#35268;&#21010;(SP)&#12289;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;(SOC)&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#20027;&#35201;&#38598;&#20013;&#35752;&#35770;SP&#21644;SOC&#24314;&#27169;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#26694;&#26550;&#20013;&#65292;&#23384;&#22312;&#33258;&#28982;&#24773;&#20917;&#19979;&#25152;&#32771;&#34385;&#30340;&#38382;&#39064;&#26159;&#20984;&#38382;&#39064;&#12290;&#39034;&#24207;&#20248;&#21270;&#30340;&#32463;&#20856;&#26041;&#27861;&#26159;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#65292;&#20294;&#23427;&#23384;&#22312;&#25152;&#35859;&#30340;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#38382;&#39064;&#65292;&#21363;&#38543;&#30528;&#29366;&#24577;&#21464;&#37327;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36817;&#24180;&#26469;&#65292;&#35299;&#20915;&#20984;&#22810;&#32423;&#38543;&#26426;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26159;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#30340;&#25104;&#26412;&#20989;&#25968;&#36880;&#27493;&#36924;&#36817;&#30340;&#21106;&#24179;&#38754;&#36817;&#20284;&#12290;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21106;&#24179;&#38754;&#31867;&#22411;&#31639;&#27861;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#35752;&#35770;&#20869;&#23481;&#20043;&#19968;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24212;&#29992;&#20110;&#22810;&#32423;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31867;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving sequential decisions in a stochastic environment were studied in Stochastic Programming (SP), Stochastic Optimal Control (SOC) and Markov Decision Processes (MDP). In this paper we mainly concentrate on SP and SOC modelling approaches. In these frameworks there are natural situations when the considered problems are convex. Classical approach to sequential optimization is based on dynamic programming. It has the problem of the so-called ``Curse of Dimensionality", in that its computational complexity increases exponentially with increase of dimension of state variables. Recent progress in solving convex multistage stochastic problems is based on cutting planes approximations of the cost-to-go (value) functions of dynamic programming equations. Cutting planes type algorithms in dynamical settings is one of the main topics of this paper. We also discuss Stochastic Approximation type methods applied to multistage stochastic optimization problems. From the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#21305;&#37197;&#36716;&#24405;&#25968;&#25454;&#37327;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15669</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages. (arXiv:2303.15669v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#21305;&#37197;&#36716;&#24405;&#25968;&#25454;&#37327;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#37327;&#36716;&#24405;&#38899;&#39057;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#33258;&#28982;&#30340;&#20154;&#31867;&#35821;&#38899;&#12290;&#20294;&#26159;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#36716;&#24405;&#25968;&#25454;&#24456;&#26114;&#36149;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;TTS&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#21305;&#37197;&#36716;&#24405;&#25968;&#25454;&#37327;&#65292;&#29992;&#20110;&#30446;&#26631;&#19979;&#28216;TTS&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#20174;&#25197;&#26354;&#30340;mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#20986;&#21435;&#25197;&#26354;&#30340;mel&#39057;&#35889;&#22270;&#65292;&#36825;&#21487;&#33021;&#20351;&#27169;&#22411;&#23398;&#20250;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#36866;&#24403;&#26102;&#38388;&#20998;&#37197;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24494;&#35843;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;&#20195;&#30721;&#21644;&#38899;&#39057;&#26679;&#26412;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural text-to-speech (TTS) models can synthesize natural human speech when trained on large amounts of transcribed speech. However, collecting such large-scale transcribed data is expensive. This paper proposes an unsupervised pre-training method for a sequence-to-sequence TTS model by leveraging large untranscribed speech data. With our pre-training, we can remarkably reduce the amount of paired transcribed data required to train the model for the target downstream TTS task. The main idea is to pre-train the model to reconstruct de-warped mel-spectrograms from warped ones, which may allow the model to learn proper temporal assignment relation between input and output sequences. In addition, we propose a data augmentation method that further improves the data efficiency in fine-tuning. We empirically demonstrate the effectiveness of our proposed method in low-resource language scenarios, achieving outstanding performance compared to competing methods. The code and audio samples are av
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;&#38027;&#38227;&#30898;&#65288;Bi2Te3&#65289;&#22312;&#28608;&#20809;&#31881;&#24202;&#29076;&#21270;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#21151;&#29575;&#22240;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#26448;&#26009;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15663</link><description>&lt;p&gt;
&#38027;&#38227;&#30898;&#28909;&#30005;&#26448;&#26009;&#22312;&#28608;&#20809;&#31881;&#24202;&#29076;&#21270;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#21151;&#29575;&#22240;&#23376;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predicting Thermoelectric Power Factor of Bismuth Telluride During Laser Powder Bed Fusion Additive Manufacturing. (arXiv:2303.15663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;&#38027;&#38227;&#30898;&#65288;Bi2Te3&#65289;&#22312;&#28608;&#20809;&#31881;&#24202;&#29076;&#21270;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#21151;&#29575;&#22240;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#26448;&#26009;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#24037;&#24207;&#65288;&#22914;&#28608;&#20809;&#31881;&#24202;&#29076;&#21270;&#65289;&#21487;&#20197;&#36890;&#36807;&#36880;&#23618;&#38138;&#25955;&#12289;&#29076;&#21270;&#31881;&#26411;&#30452;&#21040;&#21019;&#24314;&#33258;&#30001;&#24418;&#24577;&#30340;&#37096;&#20214;&#26469;&#21046;&#36896;&#29289;&#20307;&#12290;&#20026;&#20102;&#25913;&#21892;&#22686;&#26448;&#21046;&#36896;&#24037;&#24207;&#20013;&#25152;&#28041;&#21450;&#30340;&#26448;&#26009;&#29305;&#24615;&#65292;&#39044;&#27979;&#26448;&#26009;&#24615;&#36136;&#19982;&#21152;&#24037;&#26465;&#20214;&#30340;&#20989;&#25968;&#20851;&#31995;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#28909;&#30005;&#26448;&#26009;&#20013;&#65292;&#21151;&#29575;&#22240;&#23376;&#26159;&#34913;&#37327;&#26448;&#26009;&#23558;&#28909;&#33021;&#36716;&#25442;&#20026;&#30005;&#33021;&#25928;&#29575;&#30340;&#19968;&#31181;&#25351;&#26631;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#39044;&#27979;&#20102;&#19981;&#21516;&#28909;&#30005;&#26448;&#26009;&#30340;&#26448;&#26009;&#29305;&#24615;&#65292;&#20294;&#36824;&#27809;&#26377;&#25506;&#32034;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#38027;&#38227;&#30898; (Bi2Te3)&#22312;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#21151;&#29575;&#22240;&#23376;&#12290;&#36825;&#19968;&#28857;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;Bi2Te3&#26159;&#20302;&#28201;&#24212;&#29992;&#30340;&#26631;&#20934;&#26448;&#26009;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#28041;&#21450;&#21040;&#30340;&#21046;&#36896;&#21152;&#24037;&#21442;&#25968;&#25968;&#25454;&#21644;&#22312;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#25910;&#38598;&#30340;&#21407;&#20301;&#20256;&#24863;&#22120;&#30417;&#27979;&#25968;&#25454;&#26469;&#39044;&#27979;Bi2Te3&#30340;&#21151;&#29575;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
An additive manufacturing (AM) process, like laser powder bed fusion, allows for the fabrication of objects by spreading and melting powder in layers until a freeform part shape is created. In order to improve the properties of the material involved in the AM process, it is important to predict the material characterization property as a function of the processing conditions. In thermoelectric materials, the power factor is a measure of how efficiently the material can convert heat to electricity. While earlier works have predicted the material characterization properties of different thermoelectric materials using various techniques, implementation of machine learning models to predict the power factor of bismuth telluride (Bi2Te3) during the AM process has not been explored. This is important as Bi2Te3 is a standard material for low temperature applications. Thus, we used data about manufacturing processing parameters involved and in-situ sensor monitoring data collected during AM of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;DeepONet&#23454;&#29616;&#36793;&#30028;-&#35299;&#26144;&#23556;&#65292;&#20197;&#27714;&#35299;Toth&#30406;&#22320;&#22320;&#19979;&#27700;&#27969;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#22495;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#36793;&#30028;&#26465;&#20214;&#20316;&#20026;&#36755;&#20837;&#65292;&#36755;&#20986;&#22320;&#19979;&#27700;&#27969;&#26041;&#31243;&#30340;&#31283;&#24577;&#35299;&#12290;&#30740;&#31350;&#21033;&#29992;&#20613;&#37324;&#21494;&#32423;&#25968;&#25110;&#20998;&#27573;&#32447;&#24615;&#34920;&#31034;&#26469;&#36817;&#20284;&#39030;&#37096;&#21644;&#24213;&#37096;&#36793;&#30028;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#36793;&#30028;&#26465;&#20214;&#23454;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;DeepONet&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.15659</link><description>&lt;p&gt;
&#22303;&#22756;&#22320;&#19979;&#27700;&#27969;&#36793;&#30028;-&#35299;&#26144;&#23556;&#22312;Toth&#30406;&#22320;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Boundary-to-Solution Mapping for Groundwater Flows in a Toth Basin. (arXiv:2303.15659v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;DeepONet&#23454;&#29616;&#36793;&#30028;-&#35299;&#26144;&#23556;&#65292;&#20197;&#27714;&#35299;Toth&#30406;&#22320;&#22320;&#19979;&#27700;&#27969;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#22495;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#36793;&#30028;&#26465;&#20214;&#20316;&#20026;&#36755;&#20837;&#65292;&#36755;&#20986;&#22320;&#19979;&#27700;&#27969;&#26041;&#31243;&#30340;&#31283;&#24577;&#35299;&#12290;&#30740;&#31350;&#21033;&#29992;&#20613;&#37324;&#21494;&#32423;&#25968;&#25110;&#20998;&#27573;&#32447;&#24615;&#34920;&#31034;&#26469;&#36817;&#20284;&#39030;&#37096;&#21644;&#24213;&#37096;&#36793;&#30028;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#36793;&#30028;&#26465;&#20214;&#23454;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;DeepONet&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20219;&#24847;&#22320;&#24418;Toth&#30406;&#22320;&#22320;&#19979;&#27700;&#27969;&#26041;&#31243;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#36793;&#30028;-&#35299;&#26144;&#23556;&#12290;&#20316;&#32773;&#20351;&#29992;DeepONet&#29983;&#25104;&#36793;&#30028;-&#35299;&#26144;&#23556;&#65292;&#20197;&#27492;&#20195;&#26367;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#35813;&#26144;&#23556;&#23558;&#29289;&#29702;&#22495;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#36793;&#30028;&#26465;&#20214;&#20316;&#20026;&#36755;&#20837;&#65292;&#36755;&#20986;&#31283;&#24577;&#22320;&#19979;&#27700;&#27969;&#26041;&#31243;&#30340;&#35299;&#12290;&#20316;&#32773;&#36890;&#36807;&#25130;&#26029;&#20613;&#37324;&#21494;&#32423;&#25968;&#25110;&#20998;&#27573;&#32447;&#24615;&#34920;&#31034;&#26469;&#36817;&#20284;&#39030;&#37096;&#21644;&#24213;&#37096;&#36793;&#30028;&#65292;&#24182;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;DeepONet&#23454;&#29616;&#65306;&#19968;&#20010;&#23558;Toth&#30406;&#22320;&#23884;&#20837;&#30697;&#24418;&#35745;&#31639;&#22495;&#65292;&#21478;&#19968;&#20010;&#21017;&#36890;&#36807;&#38750;&#32447;&#24615;&#21464;&#25442;&#23558;&#20855;&#26377;&#20219;&#24847;&#39030;&#37096;&#21644;&#24213;&#37096;&#36793;&#30028;&#30340;Toth&#30406;&#22320;&#26144;&#23556;&#21040;&#30697;&#24418;&#35745;&#31639;&#22495;&#20013;&#12290;&#20316;&#32773;&#38024;&#23545;&#39030;&#37096;&#30340;Dirichlet&#21644;Robin&#36793;&#30028;&#26465;&#20214;&#20197;&#21450;&#24213;&#37096;&#30340;Neumann&#36793;&#30028;&#26465;&#20214;&#20998;&#21035;&#23454;&#29616;&#20102;DeepONet&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the authors propose a new approach to solving the groundwater flow equation in the Toth basin of arbitrary top and bottom topographies using deep learning. Instead of using traditional numerical solvers, they use a DeepONet to produce the boundary-to-solution mapping. This mapping takes the geometry of the physical domain along with the boundary conditions as inputs to output the steady state solution of the groundwater flow equation. To implement the DeepONet, the authors approximate the top and bottom boundaries using truncated Fourier series or piecewise linear representations. They present two different implementations of the DeepONet: one where the Toth basin is embedded in a rectangular computational domain, and another where the Toth basin with arbitrary top and bottom boundaries is mapped into a rectangular computational domain via a nonlinear transformation. They implement the DeepONet with respect to the Dirichlet and Robin boundary condition at the top and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26089;&#20135;&#20799;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.15656</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#39044;&#27979;&#26089;&#20135;&#20799;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;
&lt;/p&gt;
&lt;p&gt;
Predicting Adverse Neonatal Outcomes for Preterm Neonates with Multi-Task Learning. (arXiv:2303.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26089;&#20135;&#20799;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#26089;&#20135;&#20799;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#30340;&#35786;&#26029;&#23545;&#26089;&#20135;&#20799;&#30340;&#29983;&#23384;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20351;&#21307;&#29983;&#33021;&#22815;&#21450;&#26102;&#25552;&#20379;&#27835;&#30103;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#27979;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#20165;&#38598;&#20013;&#22312;&#39044;&#27979;&#21333;&#20010;&#32467;&#26524;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#32467;&#26524;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#32467;&#26524;&#27425;&#20248;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#19977;&#31181;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#23558;&#22810;&#20010;&#26032;&#29983;&#20799;&#32467;&#23616;&#30340;&#35786;&#26029;&#24418;&#24335;&#21270;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;MTL&#26694;&#26550;&#26469;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#19981;&#33391;&#26032;&#29983;&#20799;&#32467;&#23616;&#12290;&#29305;&#21035;&#26159;&#65292;MTL&#26694;&#26550;&#21253;&#21547;&#20849;&#20139;&#38544;&#34255;&#23618;&#21644;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#20998;&#25903;&#12290;&#25105;&#20204;&#20351;&#29992;121&#21517;&#26089;&#20135;&#20799;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;MTL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis of adverse neonatal outcomes is crucial for preterm survival since it enables doctors to provide timely treatment. Machine learning (ML) algorithms have been demonstrated to be effective in predicting adverse neonatal outcomes. However, most previous ML-based methods have only focused on predicting a single outcome, ignoring the potential correlations between different outcomes, and potentially leading to suboptimal results and overfitting issues. In this work, we first analyze the correlations between three adverse neonatal outcomes and then formulate the diagnosis of multiple neonatal outcomes as a multi-task learning (MTL) problem. We then propose an MTL framework to jointly predict multiple adverse neonatal outcomes. In particular, the MTL framework contains shared hidden layers and multiple task-specific branches. Extensive experiments have been conducted using Electronic Health Records (EHRs) from 121 preterm neonates. Empirical results demonstrate the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24335;&#32437;&#21521;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#20840;&#23616;&#25910;&#32553;&#32467;&#26500;&#21644;PSGD&#26041;&#27861;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#36951;&#25022;&#20316;&#20026;&#26102;&#38388;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15652</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#21160;&#24577;&#23450;&#20215;&#65306;&#20840;&#23616;&#25910;&#32553;&#27169;&#22411;&#19979;&#30340;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model. (arXiv:2303.15652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24335;&#32437;&#21521;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#20840;&#23616;&#25910;&#32553;&#32467;&#26500;&#21644;PSGD&#26041;&#27861;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#36951;&#25022;&#20316;&#20026;&#26102;&#38388;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#27969;&#24335;&#32437;&#21521;&#25968;&#25454;&#35774;&#32622;&#20013;&#30340;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#30446;&#30340;&#26159;&#26368;&#22823;&#21270;&#22312;&#22823;&#37327;&#23458;&#25143;&#32454;&#20998;&#20013;&#30340;&#32047;&#35745;&#21033;&#28070;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#21160;&#24577;&#27010;&#29575;&#27169;&#22411;&#65292;&#20854;&#20013;&#28040;&#36153;&#32773;&#30340;&#20559;&#22909;&#21644;&#20215;&#26684;&#25935;&#24863;&#24230;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#20247;&#25152;&#21608;&#30693;&#30340;&#21457;&#29616;&#65292;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#28040;&#36153;&#32773;&#20250;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20840;&#23616;&#25910;&#32553;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#20551;&#23450;&#19981;&#21516;&#32454;&#20998;&#20013;&#30340;&#28040;&#36153;&#32773;&#20559;&#22909;&#21487;&#20197;&#29992;&#31354;&#38388;&#33258;&#22238;&#24402;&#27169;&#22411;&#24456;&#22909;&#22320;&#36817;&#20284;&#12290;&#22312;&#36825;&#26679;&#30340;&#27969;&#24335;&#32437;&#21521;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36951;&#25022;&#26469;&#34913;&#37327;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#36951;&#25022;&#26159;&#19982;&#39044;&#20808;&#30693;&#36947;&#27169;&#22411;&#21442;&#25968;&#24207;&#21015;&#30340;&#21315;&#37324;&#30524;&#30456;&#27604;&#39044;&#26399;&#30340;&#25910;&#20837;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;PSGD&#65289;&#30340;&#23450;&#20215;&#31574;&#30053;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#20854;&#36951;&#25022;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#12289;&#27169;&#22411;&#21442;&#25968;&#30340;&#26102;&#38388;&#21464;&#21270;&#24615;&#21644;&#28040;&#36153;&#32773;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider dynamic pricing strategies in a streamed longitudinal data set-up where the objective is to maximize, over time, the cumulative profit across a large number of customer segments. We consider a dynamic probit model with the consumers' preferences as well as price sensitivity varying over time. Building on the well-known finding that consumers sharing similar characteristics act in similar ways, we consider a global shrinkage structure, which assumes that the consumers' preferences across the different segments can be well approximated by a spatial autoregressive (SAR) model. In such a streamed longitudinal set-up, we measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on penalized stochastic gradient descent (PSGD) and explicitly characterize its regret as functions of time, the temporal variability in the model pa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#20197;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;SGD&#22312;&#32447;&#23398;&#20064;&#30340;&#21518;&#24724;&#65292;&#33021;&#22815;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#36890;&#24120;&#20250;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#65292;&#33021;&#22815;&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.15634</link><description>&lt;p&gt;
&#23398;&#20064;&#36895;&#29575;&#34920;&#22312;&#20998;&#24067;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Rate Schedules in the Presence of Distribution Shift. (arXiv:2303.15634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15634
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#20197;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;SGD&#22312;&#32447;&#23398;&#20064;&#30340;&#21518;&#24724;&#65292;&#33021;&#22815;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#36890;&#24120;&#20250;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#65292;&#33021;&#22815;&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#20197;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;SGD&#22312;&#32447;&#23398;&#20064;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#39062;&#20998;&#26512;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#23398;&#20064;&#36895;&#29575;&#34920;&#65292;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21482;&#26377;&#24120;&#25968;&#24046;&#24322;&#30340;&#21518;&#24724;&#19978;&#19979;&#30028;&#12290;&#23545;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#22522;&#20110;&#20272;&#35745;&#27169;&#22411;&#30340;&#26799;&#24230;&#33539;&#25968;&#23450;&#20041;&#20102;&#19968;&#31181;&#21518;&#24724;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26102;&#38388;&#34920;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#39044;&#26399;&#21518;&#24724;&#30340;&#19978;&#38480;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#39044;&#35745;&#25439;&#22833;&#39046;&#22495;&#30340;&#21464;&#21270;&#38656;&#35201;&#26356;&#22810;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#26368;&#20248;&#23398;&#20064;&#36895;&#29575;&#34920;&#36890;&#24120;&#20250;&#22312;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#65292;&#20197;&#35828;&#26126;&#36825;&#20123;&#23398;&#20064;&#36895;&#29575;&#34920;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift, and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedule
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;SINDy&#21644;Peridynamic&#24046;&#20998;&#31639;&#23376;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#31227;&#21160;&#36793;&#30028;&#29616;&#35937;&#30340;&#28508;&#22312;&#29289;&#29702;&#23398;&#12290;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24179;&#28369;&#31227;&#21160;&#36793;&#30028;&#21069;&#31471;&#19988;&#27809;&#26377;&#31896;&#28082;&#21306;&#22495;&#30340;&#31227;&#21160;&#36793;&#30028;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15631</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;SINDy&#21644;Peridynamic&#24046;&#20998;&#31639;&#23376;&#30340;&#31227;&#21160;&#36793;&#30028;&#22810;&#29289;&#29702;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multiphysics discovery with moving boundaries using Ensemble SINDy and Peridynamic Differential Operator. (arXiv:2303.15631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;SINDy&#21644;Peridynamic&#24046;&#20998;&#31639;&#23376;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#31227;&#21160;&#36793;&#30028;&#29616;&#35937;&#30340;&#28508;&#22312;&#29289;&#29702;&#23398;&#12290;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24179;&#28369;&#31227;&#21160;&#36793;&#30028;&#21069;&#31471;&#19988;&#27809;&#26377;&#31896;&#28082;&#21306;&#22495;&#30340;&#31227;&#21160;&#36793;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#31227;&#21160;&#36793;&#30028;&#29616;&#35937;&#30340;&#28508;&#22312;&#29289;&#29702;&#23398;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#38598;&#21512;SINDy&#21644;Peridynamic&#24046;&#20998;&#31639;&#23376;&#65288;PDDO&#65289;&#30456;&#32467;&#21512;&#65292;&#24182;&#20551;&#35774;&#31227;&#21160;&#36793;&#30028;&#29289;&#29702;&#23398;&#22312;&#20854;&#33258;&#36523;&#30340;&#26059;&#36716;&#22352;&#26631;&#31995;&#20013;&#28436;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;2D Fisher-Stefan&#27169;&#22411;&#32771;&#34385;&#27979;&#37327;&#25968;&#25454;&#20013;&#30340;&#21508;&#20010;&#22122;&#22768;&#27700;&#24179;&#26469;&#28436;&#31034;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#21015;&#20986;&#20102;&#24674;&#22797;&#31995;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24674;&#22797;&#31995;&#25968;&#33719;&#24471;&#35299;&#26469;&#25551;&#32472;&#31227;&#21160;&#36793;&#30028;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23613;&#31649;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;Fisher-Stefan&#27169;&#22411;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20855;&#26377;&#24179;&#28369;&#31227;&#21160;&#36793;&#30028;&#21069;&#31471;&#19988;&#27809;&#26377;&#31896;&#28082;&#21306;&#22495;&#30340;&#31227;&#21160;&#36793;&#30028;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65306;https://github.com/alicanbekar/MB_PDDO-SINDy&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel framework for learning the underlying physics of phenomena with moving boundaries. The proposed approach combines Ensemble SINDy and Peridynamic Differential Operator (PDDO) and imposes an inductive bias assuming the moving boundary physics evolve in its own corotational coordinate system. The robustness of the approach is demonstrated by considering various levels of noise in the measured data using the 2D Fisher-Stefan model. The confidence intervals of recovered coefficients are listed, and the uncertainties of the moving boundary positions are depicted by obtaining the solutions with the recovered coefficients. Although the main focus of this study is the Fisher-Stefan model, the proposed approach is applicable to any type of moving boundary problem with a smooth moving boundary front without a mushy region. The code and data for this framework is available at: https://github.com/alicanbekar/MB_PDDO-SINDy.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#39318;&#20010;&#37327;&#21270;&#30340;&#37327;&#23376;&#19982;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#29992;&#31454;&#36895;&#27604;&#36739;&#65288;PQA&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#37327;&#23376;&#27169;&#22411;&#19982;&#26368;&#20248;&#32463;&#20856;&#31639;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.15626</link><description>&lt;p&gt;
&#23637;&#31034;&#37327;&#23376;&#20248;&#21183;&#30340;&#23454;&#29992;&#26694;&#26550;&#65306;&#37327;&#23376;&#27169;&#22411;&#19982;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#30340;&#31454;&#36895;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Framework for Demonstrating Practical Quantum Advantage: Racing Quantum against Classical Generative Models. (arXiv:2303.15626v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15626
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#39318;&#20010;&#37327;&#21270;&#30340;&#37327;&#23376;&#19982;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#29992;&#31454;&#36895;&#27604;&#36739;&#65288;PQA&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#37327;&#23376;&#27169;&#22411;&#19982;&#26368;&#20248;&#32463;&#20856;&#31639;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#32463;&#20856;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#19988;&#20195;&#34920;&#20102;&#22312;&#36817;&#26399;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#39033;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26694;&#26550;&#19978;&#65292;&#24182;&#24314;&#31435;&#20102;&#39318;&#20010;&#37327;&#21270;&#30340;&#37327;&#23376;&#19982;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#29992;&#31454;&#36895;&#27604;&#36739;&#65288;PQA&#65289;&#65292;&#21253;&#25324;&#37327;&#23376;&#30005;&#36335;&#20986;&#29983;&#26426;&#22120;&#65288;QCBMs&#65289;&#12289;&#21464;&#21387;&#22120;&#65288;TFs&#65289;&#12289;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;WGANs&#65289;&#12290;&#22312;&#23450;&#20041;&#20102;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;PQA&#22330;&#26223;&#20043;&#21518;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#27604;&#36739;&#37327;&#23376;&#27169;&#22411;&#19982;&#26368;&#20248;&#32463;&#20856;&#31639;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35753;&#36825;&#20123;&#27169;&#22411;&#22312;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#21644;&#19982;&#24212;&#29992;&#30456;&#20851;&#30340;&#27604;&#36187;&#29615;&#22659;&#20013;&#36827;&#34892;&#31454;&#36895;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20102;20&#20010;&#21464;&#37327;&#65288;&#37327;&#23376;&#27604;&#29305;&#65289;&#26469;&#23637;&#31034;&#21644;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has seen a rising interest in both classical and quantum machine learning, and it represents a promising candidate to obtain a practical quantum advantage in the near term. In this study, we build over a proposed framework for evaluating the generalization performance of generative models, and we establish the first quantitative comparative race towards practical quantum advantage (PQA) between classical and quantum generative models, namely Quantum Circuit Born Machines (QCBMs), Transformers (TFs), Recurrent Neural Networks (RNNs), Variational Autoencoders (VAEs), and Wasserstein Generative Adversarial Networks (WGANs). After defining four types of PQAs scenarios, we focus on what we refer to as potential PQA, aiming to compare quantum models with the best-known classical algorithms for the task at hand. We let the models race on a well-defined and application-relevant competition setting, where we illustrate and demonstrate our framework on 20 variables (qubits) g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#31649;&#29702;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#36164;&#28304;&#20013;&#28040;&#36153;&#32773;&#22522;&#20934;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#36827;&#34892;&#20272;&#35745;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#26696;&#65292;&#20877;&#36890;&#36807;&#24341;&#20837;&#28608;&#21169;&#20215;&#26684;&#19978;&#30340;&#25200;&#21160;&#23454;&#29616;&#21208;&#25506;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.15617</link><description>&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#28608;&#21169;&#30340;&#38656;&#27714;&#21709;&#24212;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning for Incentive-Based Demand Response. (arXiv:2303.15617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#31649;&#29702;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#36164;&#28304;&#20013;&#28040;&#36153;&#32773;&#22522;&#20934;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#36827;&#34892;&#20272;&#35745;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#26696;&#65292;&#20877;&#36890;&#36807;&#24341;&#20837;&#28608;&#21169;&#20215;&#26684;&#19978;&#30340;&#25200;&#21160;&#23454;&#29616;&#21208;&#25506;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#31649;&#29702;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;&#20856;&#22411;DR&#26426;&#21046;&#35201;&#27714;DR&#32463;&#29702;&#20026;&#21442;&#19982;&#30340;&#28040;&#36153;&#32773;&#20998;&#37197;&#19968;&#20010;&#22522;&#20934;&#65292;&#20854;&#20013;&#22522;&#20934;&#26159;&#28040;&#36153;&#32773;&#35745;&#25968;&#20107;&#23454;&#28040;&#32791;&#30340;&#20272;&#35745;&#65292;&#22914;&#26524;&#19981;&#21483;&#28040;&#36153;&#32773;&#25552;&#20379;DR&#26381;&#21153;&#65292;&#37027;&#20040;&#22522;&#20934;&#23601;&#26159;&#35745;&#25968;&#30340;&#29702;&#35770;&#28040;&#32791;&#12290;&#20272;&#31639;&#22522;&#20934;&#30340;&#25361;&#25112;&#22312;&#20110;&#28040;&#36153;&#32773;&#26377;&#40723;&#21169;&#33192;&#32960;&#22522;&#20934;&#20272;&#35745;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#22312;&#32447;&#20272;&#31639;&#22522;&#32447;&#21644;&#22312;&#36825;&#26679;&#30340;&#28608;&#21169;&#19979;&#20248;&#21270;&#19968;&#27573;&#26102;&#38388;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#26696;&#65292;&#23427;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#36827;&#34892;&#20272;&#35745;&#65292;&#21516;&#26102;&#22312;DR&#26381;&#21153;&#25110;&#36127;&#33655;&#35009;&#21098;&#30340;&#28608;&#21169;&#20215;&#26684;&#19978;&#24341;&#20837;&#25200;&#21160;&#65292;&#26088;&#22312;&#24179;&#34913;&#22312;&#32447;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#21208;&#25506;&#21644;&#24320;&#21457;&#30340;&#25240;&#34935;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#26696;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20248;&#25805;&#20316;&#30456;&#27604;&#38750;&#24120;&#20302;&#30340;&#36951;&#25022;&#24230;&#65288;$ \mathcal {O} \left((\log {T})^2\right)$&#65289;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning online to manage Demand Response (DR) resources. A typical DR mechanism requires the DR manager to assign a baseline to the participating consumer, where the baseline is an estimate of the counterfactual consumption of the consumer had it not been called to provide the DR service. A challenge in estimating baseline is the incentive the consumer has to inflate the baseline estimate. We consider the problem of learning online to estimate the baseline and to optimize the operating costs over a period of time under such incentives. We propose an online learning scheme that employs least-squares for estimation with a perturbation to the reward price (for the DR services or load curtailment) that is designed to balance the exploration and exploitation trade-off that arises with online learning. We show that, our proposed scheme is able to achieve a very low regret of $\mathcal{O}\left((\log{T})^2\right)$ with respect to the optimal operating
&lt;/p&gt;</description></item><item><title>HD-Bind&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#20998;&#23376;&#32467;&#26500;&#30340;&#36229;&#39640;&#32500;&#24230;&#20108;&#36827;&#21046;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#39044;&#27979;&#34507;&#30333;&#36136;&#20013;&#23567;&#20998;&#23376;&#30340;&#20998;&#23376;&#38388;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#23384;&#20648;&#21644;&#35745;&#31639;&#25152;&#38656;&#30340;&#20301;&#25968;&#65292;&#24182;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15604</link><description>&lt;p&gt;
HD-Bind&#65306;&#20351;&#29992;&#20302;&#31934;&#24230;&#12289;&#36229;&#39640;&#32500;&#24230;&#20108;&#36827;&#21046;&#32534;&#30721;&#20998;&#23376;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
HD-Bind: Encoding of Molecular Structure with Low Precision, Hyperdimensional Binary Representations. (arXiv:2303.15604v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15604
&lt;/p&gt;
&lt;p&gt;
HD-Bind&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#20998;&#23376;&#32467;&#26500;&#30340;&#36229;&#39640;&#32500;&#24230;&#20108;&#36827;&#21046;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#39044;&#27979;&#34507;&#30333;&#36136;&#20013;&#23567;&#20998;&#23376;&#30340;&#20998;&#23376;&#38388;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#23384;&#20648;&#21644;&#35745;&#31639;&#25152;&#38656;&#30340;&#20301;&#25968;&#65292;&#24182;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#21270;&#23398;&#21512;&#25104;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#31867;&#20284;&#33647;&#29289;&#30340;&#20998;&#23376;&#38598;&#21512;&#24050;&#32463;&#22686;&#38271;&#21040;&#25968;&#21313;&#20159;&#20010;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20174;&#22823;&#37327;&#33647;&#29289;&#20505;&#36873;&#20013;&#30830;&#23450;&#8220;&#21629;&#20013;&#8221;&#20998;&#23376;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#29983;&#29289;&#29289;&#29702;&#29702;&#35770;&#26469;&#35745;&#31639;&#33647;&#29289;&#19982;&#20854;&#34507;&#30333;&#36136;&#38774;&#26631;&#20043;&#38388;&#32467;&#21512;&#30456;&#20114;&#20316;&#29992;&#30340;&#21513;&#24067;&#26031;&#33258;&#30001;&#33021;&#30340;&#36817;&#20284;&#20540;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#21363;&#20351;&#23545;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#20998;&#23376;&#38598;&#21512;&#65292;&#20063;&#38656;&#35201;&#20986;&#33394;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;Hyperdimensional Computing&#65288;HDC&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#20302;&#31934;&#24230;&#20108;&#36827;&#21046;&#21521;&#37327;&#31639;&#26415;&#26469;&#26500;&#24314;&#25968;&#25454;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#28176;&#36827;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#20248;&#21270;&#26041;&#27861;&#22312;&#35768;&#22810;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20013;&#26159;&#24517;&#38656;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HD-Bind&#65292;&#19968;&#31181;&#23558;&#20998;&#23376;&#32467;&#26500;&#34920;&#31034;&#20026;&#36229;&#39640;&#32500;&#24230;&#20108;&#36827;&#21046;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#20013;&#23567;&#20998;&#23376;&#30340;&#20998;&#23376;&#38388;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;HD-Bind&#26500;&#24314;&#20102;&#36229;&#32500;&#20108;&#36827;&#21046;&#21521;&#37327;&#65292;&#34920;&#31034;&#20998;&#23376;&#32467;&#26500;&#65292;&#24182;&#33021;&#22815;&#25429;&#33719;&#19982;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30456;&#20851;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#23384;&#20648;&#21644;&#35745;&#31639;&#25152;&#38656;&#30340;&#20301;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;HD-Bind&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#22312;&#20854;&#20182;&#33647;&#29289;&#21457;&#29616;&#24212;&#29992;&#20013;&#20351;&#29992;HDC&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available collections of drug-like molecules have grown to comprise 10s of billions of possibilities in recent history due to advances in chemical synthesis. Traditional methods for identifying ``hit'' molecules from a large collection of potential drug-like candidates have relied on biophysical theory to compute approximations to the Gibbs free energy of the binding interaction between the drug to its protein target. A major drawback of the approaches is that they require exceptional computing capabilities to consider for even relatively small collections of molecules.  Hyperdimensional Computing (HDC) is a recently proposed learning paradigm that is able to leverage low-precision binary vector arithmetic to build efficient representations of the data that can be obtained without the need for gradient-based optimization approaches that are required in many conventional machine learning and deep learning approaches. This algorithmic simplicity allows for acceleration in hardwa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#20010;&#20154;&#20449;&#24687;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#23454;&#35777;&#21644;&#20998;&#26512;&#30740;&#31350;&#30340;&#24037;&#20316;&#65292;&#30740;&#31350;&#21253;&#25324;&#21407;&#22987;&#25968;&#25454;&#21644;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#21608;&#26399;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#20986;&#20854;&#20013;&#30340;&#23454;&#36341;&#21644;&#36947;&#24503;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.15592</link><description>&lt;p&gt;
&#25581;&#31034;&#20010;&#20154;&#20449;&#24687;&#23398;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Uncovering Bias in Personal Informatics. (arXiv:2303.15592v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#20010;&#20154;&#20449;&#24687;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#23454;&#35777;&#21644;&#20998;&#26512;&#30740;&#31350;&#30340;&#24037;&#20316;&#65292;&#30740;&#31350;&#21253;&#25324;&#21407;&#22987;&#25968;&#25454;&#21644;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#21608;&#26399;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#20986;&#20854;&#20013;&#30340;&#23454;&#36341;&#21644;&#36947;&#24503;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#26234;&#33021;&#25163;&#26426;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#39537;&#21160;&#30340;&#20010;&#20154;&#20449;&#24687;&#23398;&#65288;PI&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#25191;&#34892;&#30340;&#35265;&#35299;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#19982;&#20854;&#20581;&#24247;&#20449;&#24687;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#20351;&#20154;&#20204;&#33021;&#22815;&#36807;&#19978;&#26356;&#20581;&#24247;&#30340;&#29983;&#27963;&#12290;&#20170;&#22825;&#65292;&#25968;&#21313;&#20159;&#29992;&#25143;&#20351;&#29992;&#27492;&#31867;&#31995;&#32479;&#26469;&#30417;&#27979;&#19981;&#20165;&#26159;&#36523;&#20307;&#27963;&#21160;&#21644;&#30561;&#30496;&#65292;&#36824;&#26377;&#29983;&#21629;&#20307;&#24449;&#12289;&#22899;&#24615;&#20581;&#24247;&#21644;&#24515;&#33039;&#20581;&#24247;&#31561;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#19988;&#22788;&#29702;&#25935;&#24863;&#30340;PI&#25968;&#25454;&#65292;&#20294;&#20559;&#35265;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30340;&#35843;&#26597;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#23545;&#21253;&#25324;&#21407;&#22987;&#25968;&#25454;&#21644;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#21644;&#20998;&#26512;&#30740;&#31350;&#30340;&#24037;&#20316;&#65292;&#24182;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#26368;&#35814;&#32454;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information. Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others. %Despite their widespread usage, the processing of particularly sensitive personal data, and their proximity to domains known to be susceptible to bias, such as healthcare, bias in PI has not been investigated systematically. Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications. In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle. We use the most detailed framework to date for exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;IMWUT&#26399;&#21002;&#19978;&#36807;&#21435;&#20116;&#24180;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#65292;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#24046;&#23548;&#33268;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#38656;&#35201;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.15585</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#22238;&#39038;&#65306;&#36229;&#36234;&#20934;&#30830;&#24615;&#22312;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. (arXiv:2303.15585v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;IMWUT&#26399;&#21002;&#19978;&#36807;&#21435;&#20116;&#24180;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#65292;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#24046;&#23548;&#33268;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#38656;&#35201;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#12289;&#21487;&#31359;&#25140;&#21644;&#26222;&#21450;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#38761;&#21629;&#24615;&#25972;&#21512;&#12290;&#35774;&#22791;&#29616;&#22312;&#21487;&#20197;&#35786;&#26029;&#30142;&#30149;&#12289;&#39044;&#27979;&#24515;&#33039;&#19981;&#35268;&#21017;&#21160;&#65292;&#21457;&#25496;&#20154;&#31867;&#35748;&#30693;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#31639;&#27861;&#22312;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#31561;&#65289;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#27495;&#35270;&#24615;&#32467;&#26524;&#12290;&#36817;&#26399;&#65292;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#23398;&#65288;AI-Ethics&#65289;&#30740;&#31350;&#31038;&#21306;&#24320;&#22987;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#25581;&#31034;&#24182;&#26368;&#32456;&#23545;&#25239;&#36825;&#20123;&#20559;&#24046;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#36825;&#20123;&#25253;&#21578;&#26041;&#38754;UbiComp&#31038;&#21306;&#25152;&#37319;&#32435;&#30340;&#31243;&#24230;&#65292;&#24182;&#24378;&#35843;&#28508;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#36807;&#21435;&#20116;&#24180;&#65288;2018-2022&#65289;&#22312;ACM&#20132;&#20114;&#12289;&#31227;&#21160;&#12289;&#21487;&#31359;&#25140;&#21644;&#26222;&#36866;&#25216;&#26415;&#65288;IMWUT&#65289;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of mobile, wearable, and ubiquitous computing (UbiComp) is undergoing a revolutionary integration of machine learning. Devices can now diagnose diseases, predict heart irregularities, and unlock the full potential of human cognition. However, the underlying algorithms are not immune to biases with respect to sensitive attributes (e.g., gender, race), leading to discriminatory outcomes. The research communities of HCI and AI-Ethics have recently started to explore ways of reporting information about datasets to surface and, eventually, counter those biases. The goal of this work is to explore the extent to which the UbiComp community has adopted such ways of reporting and highlight potential shortcomings. Through a systematic review of papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022), we found that progress on algorithmic fairness within the UbiComp community lags behind. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15579</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#8212;&#8212;&#22522;&#20110;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#65288;WDRO&#65289;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#36825;&#31181;&#36716;&#25442;&#23558;&#25552;&#39640;WDRO&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#22240;&#20026;&#35843;&#25972;&#21518;&#30340;WDRO&#20272;&#35745;&#22120;&#28176;&#36827;&#26080;&#20559;&#24182;&#19988;&#22343;&#26041;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;&#35843;&#25972;&#21518;&#30340;WDRO&#19981;&#20250;&#21066;&#24369;WDRO&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#23384;&#22312;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#22914;&#20309;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#24320;&#21457;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#21518;&#30340;&#20272;&#35745;&#22120;&#27604;&#32463;&#20856;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24037;&#19994;&#24178;&#29157;&#36807;&#31243;&#20013;&#20197;&#38750;&#30772;&#22351;&#24615;&#21644;&#22312;&#32447;&#26041;&#24335;&#20272;&#31639;&#22823;&#37327;&#28388;&#20171;&#36136;&#20135;&#21697;&#30340;&#28287;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.15570</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#32447;&#38750;&#30772;&#22351;&#24615;&#24178;&#29157;&#36807;&#31243;&#20013;&#20272;&#31639;&#36807;&#28388;&#20171;&#36136;&#30340;&#27700;&#20998;&#21547;&#37327;
&lt;/p&gt;
&lt;p&gt;
Online Non-Destructive Moisture Content Estimation of Filter Media During Drying Using Artificial Neural Networks. (arXiv:2303.15570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24037;&#19994;&#24178;&#29157;&#36807;&#31243;&#20013;&#20197;&#38750;&#30772;&#22351;&#24615;&#21644;&#22312;&#32447;&#26041;&#24335;&#20272;&#31639;&#22823;&#37327;&#28388;&#20171;&#36136;&#20135;&#21697;&#30340;&#28287;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28287;&#24230;&#27979;&#37327;&#26159;&#24178;&#29157;&#36807;&#28388;&#20171;&#36136;&#20135;&#21697;&#21046;&#36896;&#36807;&#31243;&#20013;&#20248;&#21270;&#24178;&#29157;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;161&#20010;&#24178;&#29157;&#23454;&#39564;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24037;&#19994;&#24178;&#29157;&#36807;&#31243;&#20013;&#20197;&#38750;&#30772;&#22351;&#24615;&#21644;&#22312;&#32447;&#26041;&#24335;&#20272;&#31639;&#28287;&#24230;&#30340;&#26041;&#27861;&#12290;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20808;&#36827;&#28287;&#24230;&#20272;&#31639;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#27604;&#36739;&#12290;&#27169;&#22411;&#25311;&#21512;&#21644;&#22521;&#35757;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#23618;&#24863;&#30693;&#22120;&#21487;&#36798;&#21040;&#26368;&#20302;&#35823;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#28900;&#31665;&#35774;&#32622;&#25968;&#25454;&#65292;&#24178;&#29157;&#26102;&#38388;&#21644;&#20135;&#21697;&#28201;&#24230;&#65292;ANNs&#21487;&#29992;&#20110;&#21487;&#38752;&#22320;&#20272;&#31639;&#22823;&#37327;&#28388;&#20171;&#36136;&#20135;&#21697;&#30340;&#28287;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moisture content (MC) estimation is important in the manufacturing process of drying bulky filter media products as it is the prerequisite for drying optimization. In this study, a dataset collected by performing 161 drying industrial experiments is described and a methodology for MC estimation in an non-destructive and online manner during industrial drying is presented. An artificial neural network (ANN) based method is compared to state-of-the-art MC estimation methods reported in the literature. Results of model fitting and training show that a three-layer Perceptron achieves the lowest error. Experimental results show that ANNs combined with oven settings data, drying time and product temperature can be used to reliably estimate the MC of bulky filter media products.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;CP-ViT&#65292;&#23427;&#21033;&#29992;&#26680;&#24515;-&#22806;&#22260;&#21407;&#21017;&#37325;&#26500;&#20102;&#21464;&#24418;&#37329;&#21018;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#19977;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.15569</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#24515;-&#22806;&#22260;&#21407;&#21017;&#30340;&#21464;&#24418;&#37329;&#21018;&#33258;&#27880;&#24847;&#21147;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Core-Periphery Principle Guided Redesign of Self-Attention in Transformers. (arXiv:2303.15569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;CP-ViT&#65292;&#23427;&#21033;&#29992;&#26680;&#24515;-&#22806;&#22260;&#21407;&#21017;&#37325;&#26500;&#20102;&#21464;&#24418;&#37329;&#21018;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#19977;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26356;&#39640;&#25928;&#12289;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23545;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20107;&#21518;&#20998;&#26512;&#21457;&#29616;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24778;&#20154;&#22320;&#31867;&#20284;&#20110;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#34920;&#26126;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20849;&#20139;&#26576;&#20123;&#36890;&#29992;&#21407;&#21017;&#65292;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#25110;&#35748;&#30693;/&#34892;&#20026;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#31215;&#26497;&#27880;&#20837;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#32455;&#21407;&#21017;&#26469;&#25351;&#23548;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#26500;&#12290;&#25105;&#20204;&#21033;&#29992;&#22312;&#20154;&#33041;&#32593;&#32476;&#20013;&#24191;&#27867;&#23384;&#22312;&#30340;&#26680;&#24515;-&#22806;&#22260;&#65288;CP&#65289;&#32467;&#26500;&#26469;&#24341;&#23548;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViT&#65289;&#20013;&#33258;&#25105;&#27880;&#24847;&#30340;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#24182;&#23558;&#36825;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;CP-ViT&#12290;&#22312;CP-ViT&#20013;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#25805;&#20316;&#30001;&#19968;&#20010;&#20855;&#26377;&#26680;&#24515;-&#22806;&#22260;&#32467;&#26500;&#65288;CP&#22270;&#65289;&#30340;&#31232;&#30095;&#22270;&#23450;&#20041;&#65292;&#20854;&#20013;&#26680;&#24515;&#33410;&#28857;&#34987;&#37325;&#26032;&#35774;&#35745;&#21644;&#37325;&#26032;&#32452;&#32455;&#20197;&#22312;&#20449;&#24687;&#22788;&#29702;&#20013;&#21457;&#25381;&#32508;&#21512;&#21644;&#26680;&#24515;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#35270;&#35273;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#39564;&#35777;&#20102;CP-ViT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#19982;&#21407;&#22987;ViT&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#27169;&#22411;&#25928;&#29575;&#12289;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;CP-ViT&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing more efficient, reliable, and explainable neural network architectures is critical to studies that are based on artificial intelligence (AI) techniques. Previous studies, by post-hoc analysis, have found that the best-performing ANNs surprisingly resemble biological neural networks (BNN), which indicates that ANNs and BNNs may share some common principles to achieve optimal performance in either machine learning or cognitive/behavior tasks. Inspired by this phenomenon, we proactively instill organizational principles of BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP) organization, which is widely found in human brain networks, to guide the information communication mechanism in the self-attention of vision transformer (ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention operation between nodes is defined by a sparse graph with a Core-Periphery structure (CP graph), where the core nodes are redesigned and reorganized to play an integr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#30450;&#30446;&#38450;&#24481;&#26694;&#26550;&#65288;BDMAE&#65289;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38450;&#24481;&#30450;&#30446;&#21518;&#38376;&#25915;&#20987;&#65292;&#19981;&#38656;&#35201;&#39564;&#35777;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#36890;&#36807;&#27979;&#35797;&#22270;&#20687;&#21644; MAE &#36824;&#21407;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#26631;&#31614;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.15564</link><description>&lt;p&gt;
&#25513;&#30721;&#36824;&#21407;&#25216;&#26415;&#65306;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#22312;&#27979;&#35797;&#26102;&#38450;&#24481;&#30450;&#30446;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder. (arXiv:2303.15564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#30450;&#30446;&#38450;&#24481;&#26694;&#26550;&#65288;BDMAE&#65289;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38450;&#24481;&#30450;&#30446;&#21518;&#38376;&#25915;&#20987;&#65292;&#19981;&#38656;&#35201;&#39564;&#35777;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#36890;&#36807;&#27979;&#35797;&#22270;&#20687;&#21644; MAE &#36824;&#21407;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#26631;&#31614;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#20250;&#36890;&#36807;&#22312;&#22270;&#20687;&#19978;&#21472;&#21152;&#29305;&#27530;&#30340;&#35302;&#21457;&#22120;&#26469;&#24694;&#24847;&#25805;&#32437;&#27169;&#22411;&#34892;&#20026;&#65292;&#36825;&#31216;&#20026;&#21518;&#38376;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#19968;&#20123;&#39564;&#35777;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#20363;&#22914;&#24403;&#27169;&#22411;&#20316;&#20026;&#20113;&#26381;&#21153;&#25552;&#20379;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#27979;&#35797;&#26102;&#30340;&#30450;&#30446;&#21518;&#38376;&#38450;&#24481;&#23454;&#36341;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#40657;&#30418;&#27169;&#22411;&#12290;&#27599;&#20010;&#27979;&#35797;&#22270;&#20687;&#30340;&#30495;&#23454;&#26631;&#31614;&#38656;&#35201;&#20174;&#21487;&#30097;&#27169;&#22411;&#30340;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#24674;&#22797;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#21551;&#21457;&#24335;&#35302;&#21457;&#22120;&#25628;&#32034;&#19981;&#36866;&#29992;&#20110;&#22797;&#26434;&#35302;&#21457;&#22120;&#25110;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#29255;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#36890;&#29992;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#30450;&#30446;&#38450;&#24481;&#26694;&#26550;&#65288;BDMAE&#65289;&#65292;&#36890;&#36807;&#27979;&#35797;&#22270;&#20687;&#21644; MAE &#36824;&#21407;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#26631;&#31614;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to backdoor attacks, where an adversary maliciously manipulates the model behavior through overlaying images with special triggers. Existing backdoor defense methods often require accessing a few validation data and model parameters, which are impractical in many real-world applications, e.g., when the model is provided as a cloud service. In this paper, we address the practical task of blind backdoor defense at test time, in particular for black-box models. The true label of every test image needs to be recovered on the fly from the hard label predictions of a suspicious model. The heuristic trigger search in image space, however, is not scalable to complex triggers or high image resolution. We circumvent such barrier by leveraging generic image generation models, and propose a framework of Blind Defense with Masked AutoEncoder (BDMAE). It uses the image structural similarity and label consistency between the test image and MAE restorations to detec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20027;&#35201;&#20851;&#27880;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21644;&#25512;&#29702;&#20316;&#20026;&#26381;&#21153;&#65292;&#35782;&#21035;&#25361;&#25112;&#24182;&#35752;&#35770;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#65292;&#20197;&#26399;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#24320;&#21457;&#31169;&#23494;&#39640;&#25928;&#30340;ML&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15563</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65306;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning for healthcare: open challenges and future perspectives. (arXiv:2303.15563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20027;&#35201;&#20851;&#27880;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21644;&#25512;&#29702;&#20316;&#20026;&#26381;&#21153;&#65292;&#35782;&#21035;&#25361;&#25112;&#24182;&#35752;&#35770;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#65292;&#20197;&#26399;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#24320;&#21457;&#31169;&#23494;&#39640;&#25928;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36817;&#26469;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#29616;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#65292;&#28041;&#21450;&#30142;&#30149;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#24739;&#32773;&#27835;&#30103;&#31561;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#12290;&#30001;&#20110;&#21307;&#30103;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#65292;&#38544;&#31169;&#24517;&#39035;&#22312;&#25972;&#20010;ML&#27969;&#31243;&#20013;&#24471;&#21040;&#32771;&#34385;&#65292;&#21253;&#25324;&#20174;&#27169;&#22411;&#35757;&#32451;&#21040;&#25512;&#29702;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21644;&#25512;&#29702;&#20316;&#20026;&#26381;&#21153;&#65292;&#24182;&#23545;&#29616;&#26377;&#36235;&#21183;&#36827;&#34892;&#20840;&#38754;&#22238;&#39038;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#26426;&#20250;&#12290;&#26412;&#32508;&#36848;&#30340;&#30446;&#30340;&#26159;&#24341;&#23548;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24320;&#21457;&#31169;&#23494;&#39640;&#25928;&#30340;ML&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#26399;&#23558;&#30740;&#31350;&#25104;&#26524;&#36716;&#21270;&#20026;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has recently shown tremendous success in modeling various healthcare prediction tasks, ranging from disease diagnosis and prognosis to patient treatment. Due to the sensitive nature of medical data, privacy must be considered along the entire ML pipeline, from model training to inference. In this paper, we conduct a review of recent literature concerning Privacy-Preserving Machine Learning (PPML) for healthcare. We primarily focus on privacy-preserving training and inference-as-a-service, and perform a comprehensive review of existing trends, identify challenges, and discuss opportunities for future research directions. The aim of this review is to guide the development of private and efficient ML models in healthcare, with the prospects of translating research efforts into real-world settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;DIAMOND&#31639;&#27861;&#29992;&#20110;&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#22810;&#27969;&#20256;&#36755;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#25910;&#25947;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#38598;&#20013;&#24335;&#35745;&#31639;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#65292;&#20063;&#21487;&#20197;&#20998;&#24067;&#24335;&#23454;&#29616;&#25968;&#25454;&#21253;&#30340;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;15-20&#65285;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15544</link><description>&lt;p&gt;
&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#22810;&#27969;&#20256;&#36755;&#65306;&#22522;&#20110;&#25910;&#25947;&#22270;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach. (arXiv:2303.15544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;DIAMOND&#31639;&#27861;&#29992;&#20110;&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#22810;&#27969;&#20256;&#36755;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#25910;&#25947;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#38598;&#20013;&#24335;&#35745;&#31639;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#65292;&#20063;&#21487;&#20197;&#20998;&#24067;&#24335;&#23454;&#29616;&#25968;&#25454;&#21253;&#30340;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;15-20&#65285;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26080;&#32447;&#32593;&#32476;&#20013;&#22810;&#27969;&#20256;&#36755;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#19981;&#21516;&#27969;&#38388;&#30340;&#25968;&#25454;&#20449;&#21495;&#21487;&#33021;&#30001;&#20110;&#23427;&#20204;&#36335;&#24452;&#19978;&#30340;&#38142;&#36335;&#38388;&#30456;&#20114;&#24178;&#25200;&#32780;&#20351;&#38142;&#36335;&#23481;&#37327;&#20943;&#23567;&#12290;&#20854;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#31181;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#65292;&#36890;&#36807;&#36335;&#30001;&#22312;&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#20256;&#36755;&#27969;&#20197;&#26368;&#22823;&#21270;&#32593;&#32476;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#22823;&#37327;&#29366;&#24577;&#21644;&#25805;&#20316;&#31354;&#38388;&#65292;&#33719;&#24471;&#26368;&#20248;&#35299;&#30340;&#35745;&#31639;&#20195;&#20215;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND)&#12290;DIAMOND&#31639;&#27861;&#30340;&#35774;&#35745;&#20801;&#35768;&#37319;&#29992;&#28151;&#21512;&#24335;&#30340;&#38598;&#20013;&#24335;-&#20998;&#24067;&#24335;&#23454;&#29616;&#65292;&#36825;&#26159;5G&#21450;&#26356;&#39640;&#25216;&#26415;&#20013;&#20855;&#26377;&#20013;&#22830;&#21333;&#20803;&#37096;&#32626;&#29305;&#24449;&#30340;&#19968;&#31181;&#12290;&#38598;&#20013;&#24335;&#38454;&#27573;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24378;&#21270;&#23398;&#20064; (RL) &#36335;&#30001;&#20195;&#29702;&#35745;&#31639;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#12290;GNN-RL&#20195;&#29702;&#24050;&#32463;&#35757;&#32451;&#22909;&#20102;&#20197;&#23398;&#20064;&#32593;&#32476;&#29366;&#24577;&#24182;&#35774;&#35745;&#19968;&#20010;&#32771;&#34385;&#24178;&#25200;&#30340;&#36335;&#30001;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#31574;&#30053;&#34987;&#29992;&#20110;&#25351;&#23548;&#27839;&#26080;&#24178;&#25200;&#36335;&#24452;&#20256;&#36755;&#27969;&#65292;&#38543;&#21518;&#20998;&#24067;&#24335;&#38454;&#27573;&#21017;&#21033;&#29992;&#36825;&#20010;&#31574;&#30053;&#26469;&#20256;&#36755;&#25968;&#25454;&#21253;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;DIAMOND&#31639;&#27861;&#22312;&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#39640;&#20102;15-20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of of multi-flow transmission in wireless networks, where data signals from different flows can interfere with each other due to mutual interference between links along their routes, resulting in reduced link capacities. The objective is to develop a multi-flow transmission strategy that routes flows across the wireless interference network to maximize the network utility. However, obtaining an optimal solution is computationally expensive due to the large state and action spaces involved. To tackle this challenge, we introduce a novel algorithm called Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND). The design of DIAMOND allows for a hybrid centralized-distributed implementation, which is a characteristic of 5G and beyond technologies with centralized unit deployments. A centralized stage computes the multi-flow transmission strategy using a novel design of graph neural network (GNN) reinforcement learning (RL) routing ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39034;&#24207;&#35757;&#32451;GAN&#23545;&#25239;GAN&#20998;&#31867;&#22120;&#21487;&#20197;&#25581;&#31034;&#29420;&#31435;&#35757;&#32451;&#30340;GAN&#23454;&#20363;&#20043;&#38388;&#23384;&#22312;&#30340;&#30456;&#20851;&#8220;&#30693;&#35782;&#30450;&#21306;&#8221;&#65292;&#26377;&#26426;&#20250;&#25913;&#21892;GAN&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.15533</link><description>&lt;p&gt;
&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#39034;&#24207;&#35757;&#32451;&#19982;GAN&#20998;&#31867;&#22120;&#25581;&#31034;&#20102;&#29420;&#31435;&#35757;&#32451;&#30340;GAN&#23454;&#20363;&#20043;&#38388;&#23384;&#22312;&#30340;&#30456;&#20851;&#8220;&#30693;&#35782;&#30450;&#21306;&#8221;
&lt;/p&gt;
&lt;p&gt;
Sequential training of GANs against GAN-classifiers reveals correlated "knowledge gaps" present among independently trained GAN instances. (arXiv:2303.15533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#39034;&#24207;&#35757;&#32451;GAN&#23545;&#25239;GAN&#20998;&#31867;&#22120;&#21487;&#20197;&#25581;&#31034;&#29420;&#31435;&#35757;&#32451;&#30340;GAN&#23454;&#20363;&#20043;&#38388;&#23384;&#22312;&#30340;&#30456;&#20851;&#8220;&#30693;&#35782;&#30450;&#21306;&#8221;&#65292;&#26377;&#26426;&#20250;&#25913;&#21892;GAN&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#35777;&#26126;&#65292;&#21487;&#20197;&#35757;&#32451;&#19982;&#21327;&#21516;&#35757;&#32451;&#30340;&#37492;&#21035;&#22120;&#19981;&#21516;&#65292;&#23545;&#20174;&#20923;&#32467;GAN&#20013;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#8220;GAN&#20998;&#31867;&#22120;&#8221;&#12290;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#24037;&#20316;&#34920;&#26126;GAN&#35757;&#32451;&#20013;&#23384;&#22312;&#8220;&#30693;&#35782;&#30450;&#21306;&#8221;&#65288;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#22806;&#20266;&#29305;&#24449;&#65289;&#12290;&#26412;&#25991;&#36845;&#20195;&#22320;&#35757;&#32451;GAN&#20998;&#31867;&#22120;&#21644;&#35757;&#32451;GAN&#65292;&#20197;&#8220;&#27450;&#39575;&#8221;&#20998;&#31867;&#22120;&#65288;&#23581;&#35797;&#22635;&#34917;&#8220;&#30693;&#35782;&#30450;&#21306;&#8221;&#65289;&#65292;&#24182;&#30740;&#31350;&#20854;&#23545;GAN&#35757;&#32451;&#21160;&#24577;&#12289;&#36755;&#20986;&#36136;&#37327;&#21644;GAN&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#65292;&#19968;&#20010;&#26159;&#23567;&#22411;&#30340;&#20302;&#32500;&#22270;&#20687;&#65288;MNIST&#65289;&#30340;DCGAN&#26550;&#26500;&#65292;&#21478;&#19968;&#20010;&#26159;StyleGAN2&#65292;&#19968;&#31181;&#22312;&#39640;&#32500;&#22270;&#20687;&#65288;FFHQ&#65289;&#19978;&#35757;&#32451;&#30340;SOTA GAN&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;DCGAN&#26080;&#27861;&#26377;&#25928;&#22320;&#27450;&#39575;&#19968;&#20010;&#20998;&#24320;&#30340;GAN&#20998;&#31867;&#22120;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;StyleGAN2&#21487;&#20197;&#27450;&#39575;&#20998;&#24320;&#30340;&#20998;&#31867;&#22120;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#65292;&#34920;&#26126;&#23427;&#24050;&#32463;&#23398;&#20250;&#20102;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#40065;&#26834;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#39034;&#24207;&#35757;&#32451;GAN&#23545;&#25239;GAN&#20998;&#31867;&#22120;&#21487;&#20197;&#25581;&#31034;&#29420;&#31435;&#35757;&#32451;&#30340;GAN&#23454;&#20363;&#20043;&#38388;&#23384;&#22312;&#30340;&#30456;&#20851;&#8220;&#30693;&#35782;&#30450;&#21306;&#8221;&#65292;&#24182;&#26377;&#26426;&#20250;&#25913;&#21892;GAN&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Generative Adversarial Networks (GANs) generate realistic images remarkably well. Previous work has demonstrated the feasibility of "GAN-classifiers" that are distinct from the co-trained discriminator, and operate on images generated from a frozen GAN. That such classifiers work at all affirms the existence of "knowledge gaps" (out-of-distribution artifacts across samples) present in GAN training. We iteratively train GAN-classifiers and train GANs that "fool" the classifiers (in an attempt to fill the knowledge gaps), and examine the effect on GAN training dynamics, output quality, and GAN-classifier generalization. We investigate two settings, a small DCGAN architecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA GAN architecture trained on high dimensional images (FFHQ). We find that the DCGAN is unable to effectively fool a held-out GAN-classifier without compromising the output quality. However, StyleGAN2 can fool held-out classifiers with no change in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#27969;&#24418;&#30340;&#20998;&#23376;&#35856;&#27874;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23376;&#34920;&#38754;&#30340;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#29305;&#24449;&#20989;&#25968;&#26469;&#34920;&#31034;&#20998;&#23376;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#20960;&#20309;&#21644;&#21270;&#23398;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.15520</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#23398;&#20064;&#20998;&#23376;&#35856;&#27874;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Harmonic Molecular Representations on Riemannian Manifold. (arXiv:2303.15520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#27969;&#24418;&#30340;&#20998;&#23376;&#35856;&#27874;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23376;&#34920;&#38754;&#30340;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#29305;&#24449;&#20989;&#25968;&#26469;&#34920;&#31034;&#20998;&#23376;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#20960;&#20309;&#21644;&#21270;&#23398;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#30740;&#31350;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#23545;&#19977;&#32500;&#20998;&#23376;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#24050;&#25104;&#20026;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#21644;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#20250;&#38480;&#21046;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35856;&#27874;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#65288;HMR&#65289;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20998;&#23376;&#34920;&#38754;&#30340;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#29305;&#24449;&#20989;&#25968;&#26469;&#34920;&#31034;&#20998;&#23376;&#12290;HMR&#22312;2D&#40654;&#26364;&#27969;&#24418;&#19978;&#25552;&#20379;&#20102;&#20998;&#23376;&#20960;&#20309;&#21644;&#21270;&#23398;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#35856;&#27874;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#22312;&#34920;&#38754;&#27969;&#24418;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#35889;&#28040;&#24687;&#20256;&#36882;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#23376;&#32534;&#30721;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#19982;&#24403;&#21069;&#27169;&#22411;&#22312;&#23567;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#37197;&#20307;&#32467;&#21512;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular representation learning plays a crucial role in AI-assisted drug discovery research. Encoding 3D molecular structures through Euclidean neural networks has become the prevailing method in the geometric deep learning community. However, the equivariance constraints and message passing in Euclidean space may limit the network expressive power. In this work, we propose a Harmonic Molecular Representation learning (HMR) framework, which represents a molecule using the Laplace-Beltrami eigenfunctions of its molecular surface. HMR offers a multi-resolution representation of molecular geometric and chemical features on 2D Riemannian manifold. We also introduce a harmonic message passing method to realize efficient spectral message passing over the surface manifold for better molecular encoding. Our proposed method shows comparable predictive power to current models in small molecule property prediction, and outperforms the state-of-the-art deep learning models for ligand-binding pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#27599;&#20010;&#20132;&#36890;&#28857;&#30340;&#26102;&#38388;&#65292;&#35299;&#20915;&#20844;&#20132;&#36816;&#36755;&#20013;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19981;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15495</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#25353;&#38656;&#20844;&#20849;&#20132;&#36890;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit. (arXiv:2303.15495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#20844;&#20132;&#36710;&#21040;&#36798;&#27599;&#20010;&#20132;&#36890;&#28857;&#30340;&#26102;&#38388;&#65292;&#35299;&#20915;&#20844;&#20132;&#36816;&#36755;&#20013;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19981;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#30340;&#20027;&#35201;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#65292;&#20844;&#20132;&#36816;&#36755;&#23384;&#22312;&#30528;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20110;&#20056;&#23458;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#30340;&#20272;&#35745;&#26356;&#21152;&#20934;&#30830;&#21644;&#21487;&#38752;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#24310;&#35823;&#21644;&#20943;&#23569;&#20056;&#23458;&#20154;&#25968;&#65292;&#23588;&#20854;&#26159;&#22312;&#20381;&#38752;&#20844;&#20849;&#20132;&#36890;&#30340;&#22478;&#24066;&#20013;&#26356;&#21152;&#20005;&#37325;&#12290;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#19982;&#26102;&#38388;&#34920;&#19981;&#21305;&#37197;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#22266;&#23450;&#26102;&#21051;&#34920;&#30340;&#24310;&#36831;&#12290;&#26681;&#25454;&#26412;&#25991;&#22312;&#32445;&#32422;&#24066;&#20844;&#20132;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#30740;&#31350;&#65292;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#21644;&#23454;&#38469;&#35745;&#21010;&#26102;&#38388;&#20043;&#38388;&#23384;&#22312;&#24179;&#22343;&#32422;&#20843;&#20998;&#38047;&#25110;491&#31186;&#30340;&#24310;&#36831;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27599;&#20010;&#20132;&#36890;&#28857;&#65288;&#31449;&#65289;&#20844;&#20132;&#36710;&#30340;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#22823;&#37117;&#24066;&#21306;&#22495;&#20013;&#36328;&#25152;&#26377;&#20844;&#20132;&#32447;&#36335;&#38598;&#20307;&#39044;&#27979;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20026;&#20272;&#31639;&#20844;&#20132;&#36710;&#21040;&#36798;&#26102;&#38388;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the major public transportation systems in cities, bus transit has its problems, including more accuracy and reliability when estimating the bus arrival time for riders. This can lead to delays and decreased ridership, especially in cities where public transportation is heavily relied upon. A common issue is that the arrival times of buses do not match the schedules, resulting in latency for fixed schedules. According to the study in this paper on New York City bus data, there is an average delay of around eight minutes or 491 seconds mismatch between the bus arrivals and the actual scheduled time. This research paper presents a novel AI-based data-driven approach for estimating the arrival times of buses at each transit point (station). Our approach is based on a fully connected neural network and can predict the arrival time collectively across all bus lines in large metropolitan areas. Our neural-net data-driven approach provides a new way to estimate the arrival time of the b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38024;&#23545;&#38081;&#36335;&#32593;&#32476;&#19978;&#30340;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#36827;&#34892;&#30740;&#31350;&#12290;&#20351;&#29992;HetGNN&#21644;GraphSAGE&#30340;&#32452;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAGE-Het&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#21487;&#20197;&#22522;&#20110;&#19981;&#21516;&#30340;&#36793;&#25429;&#25417;&#21015;&#36710;&#12289;&#21015;&#36710;&#12289;&#31449;&#28857;&#20197;&#21450;&#31449;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15489</link><description>&lt;p&gt;
&#38081;&#36335;&#32593;&#32476;&#24310;&#35823;&#28436;&#21270;&#65306;&#19968;&#31181;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach. (arXiv:2303.15489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38024;&#23545;&#38081;&#36335;&#32593;&#32476;&#19978;&#30340;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#36827;&#34892;&#30740;&#31350;&#12290;&#20351;&#29992;HetGNN&#21644;GraphSAGE&#30340;&#32452;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAGE-Het&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#21487;&#20197;&#22522;&#20110;&#19981;&#21516;&#30340;&#36793;&#25429;&#25417;&#21015;&#36710;&#12289;&#21015;&#36710;&#12289;&#31449;&#28857;&#20197;&#21450;&#31449;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38081;&#36335;&#36816;&#33829;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#65288;&#31449;&#28857;&#65292;&#21015;&#36710;&#31561;&#65289;&#65292;&#29616;&#26377;&#30340;&#21516;&#36136;&#33410;&#28857;&#65288;&#21363;&#30456;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65289;&#30340;&#22270;/&#32593;&#32476;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HetGNN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65288;&#21363;&#24322;&#26500;&#33410;&#28857;&#65289;&#65292;&#20197;&#30740;&#31350;&#38081;&#36335;&#32593;&#32476;&#19978;&#30340;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;HetGNN&#27169;&#22411;&#21644;GraphSAGE&#21516;&#36136;GNN&#65288;HomoGNN&#65289;&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#31216;&#20026;SAGE-Het&#65292;&#26088;&#22312;&#22522;&#20110;&#19981;&#21516;&#30340;&#36793;&#25429;&#25417;&#21015;&#36710;&#65292;&#21015;&#36710;&#12289;&#31449;&#28857;&#20197;&#21450;&#31449;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#35201;&#27714;&#36755;&#20837;&#20855;&#26377;&#24658;&#23450;&#30340;&#32500;&#24230;&#65288;&#20363;&#22914;&#22312;&#30697;&#24418;&#25110;&#31867;&#20284;&#32593;&#26684;&#30340;&#25968;&#32452;&#20013;&#65289;&#25110;&#20165;&#20801;&#35768;&#22312;&#22270;&#20013;&#20351;&#29992;&#21516;&#36136;&#33410;&#28857;&#30456;&#27604;&#65292;SAGE-Het&#20801;&#35768;&#28789;&#27963;&#30340;&#36755;&#20837;&#21644;&#24322;&#26500;&#33410;&#28857;&#12290;&#25910;&#38598;&#20013;&#22269;&#21271;&#20140;&#24191;&#24030;&#32447;&#19978;&#20004;&#20010;&#31449;&#28857;&#30340;&#25968;&#25454;&#20197;&#39564;&#35777;HetGNN&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SAGE-Het&#27169;&#22411;&#22312;&#39044;&#27979;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#20363;&#22914;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;HomoGNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Railway operations involve different types of entities (stations, trains, etc.), making the existing graph/network models with homogenous nodes (i.e., the same kind of nodes) incapable of capturing the interactions between the entities. This paper aims to develop a heterogeneous graph neural network (HetGNN) model, which can address different types of nodes (i.e., heterogeneous nodes), to investigate the train delay evolution on railway networks. To this end, a graph architecture combining the HetGNN model and the GraphSAGE homogeneous GNN (HomoGNN), called SAGE-Het, is proposed. The aim is to capture the interactions between trains, trains and stations, and stations and other stations on delay evolution based on different edges. In contrast to the traditional methods that require the inputs to have constant dimensions (e.g., in rectangular or grid-like arrays) or only allow homogeneous nodes in the graph, SAGE-Het allows for flexible inputs and heterogeneous nodes. The data from two s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29305;&#24449;&#21487;&#20998;&#24615;&#23545;&#20110;&#27169;&#22411;&#22312;&#20998;&#24067;&#31227;&#20301;&#19979;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31163;&#25955;&#24230;&#30340;&#20998;&#25968;&#29992;&#20110;&#20272;&#35745;&#27979;&#35797;&#20934;&#30830;&#24230;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15488</link><description>&lt;p&gt;
&#35770;&#29305;&#24449;&#21487;&#20998;&#24615;&#22312;&#39044;&#27979;&#20998;&#24067;&#22806;&#35823;&#24046;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Feature Separability in Predicting Out-Of-Distribution Error. (arXiv:2303.15488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29305;&#24449;&#21487;&#20998;&#24615;&#23545;&#20110;&#27169;&#22411;&#22312;&#20998;&#24067;&#31227;&#20301;&#19979;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31163;&#25955;&#24230;&#30340;&#20998;&#25968;&#29992;&#20110;&#20272;&#35745;&#27979;&#35797;&#20934;&#30830;&#24230;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#26631;&#31614;&#30340;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#20272;&#35745;&#23454;&#38469;&#19978;&#24456;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#26041;&#27861;&#24378;&#35843;&#20998;&#24067;&#24046;&#24322;&#19982;&#20998;&#24067;&#22806;&#31934;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22823;&#30340;&#22495;&#38388;&#24046;&#24322;&#24182;&#19981;&#19968;&#23450;&#23548;&#33268;&#20302;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#26412;&#25991;&#20174;&#29305;&#24449;&#21487;&#20998;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31163;&#25955;&#24230;&#30340;&#25968;&#25454;&#38598;&#32423;&#21035;&#30340;&#20998;&#25968;&#65292;&#20197;&#20272;&#35745;&#22312;&#20998;&#24067;&#31227;&#20301;&#19979;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21463;&#34920;&#24449;&#23398;&#20064;&#20013;&#29305;&#24449;&#33391;&#22909;&#23646;&#24615;&#30340;&#21551;&#31034;&#65306;&#39640;&#20869;&#31867;&#31163;&#25955;&#24230;&#21644;&#39640;&#20869;&#31867;&#32039;&#33268;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20869;&#31867;&#31163;&#25955;&#24230;&#19982;&#27169;&#22411;&#20934;&#30830;&#24230;&#24378;&#30456;&#20851;&#65292;&#32780;&#20869;&#31867;&#32039;&#33268;&#24230;&#19981;&#21453;&#26144;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability, and propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.
&lt;/p&gt;</description></item><item><title>KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.15487</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15487
&lt;/p&gt;
&lt;p&gt;
KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#25110;&#35821;&#20041;&#32593;&#12290;&#23613;&#31649;&#23500;&#21547;&#20449;&#24687;&#65292;&#20294;&#22270;&#24418;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#22270;&#34917;&#20840;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#24050;&#32463;&#21463;&#21040;&#20851;&#27880;&#12290;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#26041;&#27861;&#65288;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22122;&#22768;&#22270;&#30340;&#31283;&#20581;&#24037;&#20855;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21495;&#26041;&#27861;&#21487;&#20197;&#23545;&#22270;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KeGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#33539;&#20363;&#65292;&#24182;&#20801;&#35768;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20174;&#26412;&#36136;&#19978;&#35762;&#65292;KeGNN&#30001;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#22522;&#20110;&#30446;&#26631;&#23558;&#30693;&#35782;&#22686;&#24378;&#23618;&#22534;&#21472;&#22312;&#20854;&#19978;&#65292;&#20197;&#20351;&#38024;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#39044;&#27979;&#24471;&#21040;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;KeGNN&#19982;&#20004;&#20010;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19968;&#36215;&#23454;&#20363;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27169;&#24577;&#35757;&#32451;-&#22810;&#27169;&#24577;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;HA-Fedformer&#65292;&#22312;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#23454;&#29616;&#22810;&#27169;&#24577;&#27979;&#35797;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15486</link><description>&lt;p&gt;
&#21333;&#27169;&#24577;&#35757;&#32451;-&#22810;&#27169;&#24577;&#39044;&#27979;&#65306;&#20855;&#26377;&#20998;&#23618;&#32858;&#21512;&#30340;&#36328;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation. (arXiv:2303.15486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27169;&#24577;&#35757;&#32451;-&#22810;&#27169;&#24577;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;HA-Fedformer&#65292;&#22312;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#23454;&#29616;&#22810;&#27169;&#24577;&#27979;&#35797;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#20174;&#22810;&#31181;&#27169;&#24577;&#20013;&#25366;&#25496;&#25968;&#25454;&#29305;&#24449;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#20102;&#25968;&#25454;&#20849;&#20139;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#23453;&#36149;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#34701;&#21512;&#65292;&#21363;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23616;&#38480;&#24615;&#22312;&#20110;&#20027;&#27969;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#27599;&#20010;&#26412;&#22320;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;&#25152;&#26377;&#27169;&#24577;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#21333;&#27169;&#24577;&#35757;&#32451;-&#22810;&#27169;&#24577;&#39044;&#27979;&#65288;UTMP&#65289;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;HA-Fedformer&#65292;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#20026;&#23458;&#25143;&#31471;&#25552;&#20379;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#21333;&#27169;&#24577;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#23454;&#29616;&#22810;&#27169;&#24577;&#27979;&#35797;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20854;&#20851;&#38190;&#20248;&#28857;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#20026;&#20943;&#36731;&#25968;&#25454;&#38750;II&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning has seen great success mining data features from multiple modalities with remarkable model performance improvement. Meanwhile, federated learning (FL) addresses the data sharing problem, enabling privacy-preserved collaborative training to provide sufficient precious data. Great potential, therefore, arises with the confluence of them, known as multimodal federated learning. However, limitation lies in the predominant approaches as they often assume that each local dataset records samples from all modalities. In this paper, we aim to bridge this gap by proposing an Unimodal Training - Multimodal Prediction (UTMP) framework under the context of multimodal federated learning. We design HA-Fedformer, a novel transformer-based model that empowers unimodal training with only a unimodal dataset at the client and multimodal testing by aggregating multiple clients' knowledge for better accuracy. The key advantages are twofold. Firstly, to alleviate the impact of data non-II
&lt;/p&gt;</description></item><item><title>TOFA&#20351;&#29992;&#26435;&#37325;&#20849;&#20139;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#20197;&#20248;&#21270;&#36229;&#32593;&#20197;&#36866;&#24212;&#21508;&#31181;&#35774;&#22791;&#30340;&#21508;&#31181;&#37096;&#32626;&#24773;&#20917;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;TOFA&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35745;&#31639;&#35757;&#32451;&#25104;&#26412;&#19982;&#37096;&#32626;&#26041;&#26696;&#25968;&#37327;&#26080;&#20851;&#12290;TOFA&#20351;&#29992;&#32479;&#19968;&#30340;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.15485</link><description>&lt;p&gt;
TOFA&#65306;&#19968;&#27425;&#36716;&#31227;&#20840;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
TOFA: Transfer-Once-for-All. (arXiv:2303.15485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15485
&lt;/p&gt;
&lt;p&gt;
TOFA&#20351;&#29992;&#26435;&#37325;&#20849;&#20139;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#20197;&#20248;&#21270;&#36229;&#32593;&#20197;&#36866;&#24212;&#21508;&#31181;&#35774;&#22791;&#30340;&#21508;&#31181;&#37096;&#32626;&#24773;&#20917;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;TOFA&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35745;&#31639;&#35757;&#32451;&#25104;&#26412;&#19982;&#37096;&#32626;&#26041;&#26696;&#25968;&#37327;&#26080;&#20851;&#12290;TOFA&#20351;&#29992;&#32479;&#19968;&#30340;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#20849;&#20139;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26088;&#22312;&#20026;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35768;&#22810;&#35774;&#22791;&#20248;&#21270;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;&#36229;&#32593;&#65289;&#20197;&#28385;&#36275;&#21508;&#31181;&#37096;&#32626;&#22330;&#26223;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#20174;&#22312;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#36229;&#32593;&#20013;&#25552;&#21462;&#22810;&#20010;&#27169;&#22411;&#65292;&#28982;&#21518;&#23545;&#24863;&#20852;&#36259;&#30340;&#36890;&#24120;&#24456;&#23567;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#19981;&#21516;&#27169;&#22411;&#37096;&#32626;&#26041;&#26696;&#30340;&#25968;&#37327;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Transfer-Once-For-All&#65288;TOFA&#65289;&#65292;&#29992;&#20110;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36229;&#32593;&#39118;&#26684;&#30340;&#35757;&#32451;&#65292;&#22312;&#20219;&#24847;&#25968;&#37327;&#30340;&#36793;&#32536;&#37096;&#32626;&#26041;&#26696;&#19978;&#20855;&#26377;&#24658;&#23450;&#30340;&#35745;&#31639;&#35757;&#32451;&#25104;&#26412;&#12290;&#32473;&#23450;&#20219;&#21153;&#65292;TOFA&#33719;&#24471;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20248;&#21270;&#20219;&#24847;&#25968;&#37327;&#30340;&#36793;&#32536;&#37096;&#32626;&#26041;&#26696;&#30340;&#25299;&#25169;&#21644;&#26435;&#37325;&#12290;&#20026;&#20102;&#20811;&#26381;&#23567;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;TOFA&#21033;&#29992;&#32479;&#19968;&#30340;&#21322;&#30417;&#30563;&#35757;&#32451;&#25439;&#22833;&#21516;&#26102;&#35757;&#32451;&#36229;&#32593;&#20869;&#30340;&#25152;&#26377;&#23376;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight-sharing neural architecture search aims to optimize a configurable neural network model (supernet) for a variety of deployment scenarios across many devices with different resource constraints. Existing approaches use evolutionary search to extract a number of models from a supernet trained on a very large data set, and then fine-tune the extracted models on the typically small, real-world data set of interest. The computational cost of training thus grows linearly with the number of different model deployment scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style training on small data sets with constant computational training cost over any number of edge deployment scenarios. Given a task, TOFA obtains custom neural networks, both the topology and the weights, optimized for any number of edge deployment scenarios. To overcome the challenges arising from small data, TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;INRR&#65292;&#21487;&#20197;&#25552;&#39640;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;INR&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#20449;&#21495;&#30340;&#33258;&#30456;&#20284;&#24615;&#19982;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24179;&#28369;&#24230;&#23436;&#32654;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;INRR&#30340;&#19968;&#31995;&#21015;&#20849;&#24615;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.15484</link><description>&lt;p&gt;
&#33258;&#36523;&#27491;&#21017;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Regularize implicit neural representation by itself. (arXiv:2303.15484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;INRR&#65292;&#21487;&#20197;&#25552;&#39640;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;INR&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#20449;&#21495;&#30340;&#33258;&#30456;&#20284;&#24615;&#19982;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24179;&#28369;&#24230;&#23436;&#32654;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20102;INRR&#30340;&#19968;&#31995;&#21015;&#20849;&#24615;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#27491;&#21017;&#21270;&#22120;&#65288;INRR&#65289;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;INR&#26159;&#19968;&#31181;&#23436;&#20840;&#36830;&#25509;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#34920;&#31034;&#20449;&#21495;&#30340;&#32454;&#33410;&#65292;&#24182;&#19981;&#21463;&#32593;&#26684;&#20998;&#36776;&#29575;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#22343;&#21248;&#37319;&#26679;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#25152;&#25552;&#20986;&#30340;INRR&#22522;&#20110;&#25152;&#23398;&#24471;&#30340;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#65288;DE&#65289;&#65292;&#20197;&#27979;&#37327;&#30697;&#38453;&#30340;&#34892;/&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#23558;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24179;&#28369;&#24615;&#36827;&#19968;&#27493;&#38598;&#25104;&#21040;&#23558;DE&#21442;&#25968;&#21270;&#20026;&#24494;&#23567;INR&#20013;&#12290;INRR&#36890;&#36807;&#23558;&#20449;&#21495;&#30340;&#33258;&#30456;&#20284;&#24615;&#19982;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24179;&#28369;&#24230;&#23436;&#32654;&#38598;&#25104;&#65292;&#25552;&#39640;&#20102;INR&#22312;&#20449;&#21495;&#34920;&#31034;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#26412;&#25991;&#36824;&#25581;&#31034;&#20102;&#19968;&#31995;&#21015;&#20174;INRR&#20013;&#24471;&#20986;&#30340;&#20849;&#24615;&#65292;&#21253;&#25324;&#25910;&#25947;&#36712;&#36857;&#21644;&#22810;&#23610;&#24230;&#30456;&#20284;&#24615;&#31561;&#21160;&#37327;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20063;&#21487;&#29992;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a regularizer called Implicit Neural Representation Regularizer (INRR) to improve the generalization ability of the Implicit Neural Representation (INR). The INR is a fully connected network that can represent signals with details not restricted by grid resolution. However, its generalization ability could be improved, especially with non-uniformly sampled data. The proposed INRR is based on learned Dirichlet Energy (DE) that measures similarities between rows/columns of the matrix. The smoothness of the Laplacian matrix is further integrated by parameterizing DE with a tiny INR. INRR improves the generalization of INR in signal representation by perfectly integrating the signal's self-similarity with the smoothness of the Laplacian matrix. Through well-designed numerical experiments, the paper also reveals a series of properties derived from INRR, including momentum methods like convergence trajectory and multi-scale similarity. Moreover, the proposed method could 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;L1&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;Fisher&#21098;&#26525;&#21644;&#38543;&#26426;&#21098;&#26525;&#22312;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#21644;&#21098;&#26525;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26377;&#25928;&#35745;&#31639;Fisher&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25209;&#22788;&#29702;Fisher&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2303.15479</link><description>&lt;p&gt;
&#25506;&#31350;&#31070;&#32463;&#32593;&#32476;&#20013;&#21098;&#26525;&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;&#20851;&#20110;&#8220;&#20013;&#22870;&#21048;&#20551;&#35828;&#8221;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Performance of Pruning Methods in Neural Networks: An Empirical Study of the Lottery Ticket Hypothesis. (arXiv:2303.15479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;L1&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;Fisher&#21098;&#26525;&#21644;&#38543;&#26426;&#21098;&#26525;&#22312;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#21644;&#21098;&#26525;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26377;&#25928;&#35745;&#31639;Fisher&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25209;&#22788;&#29702;Fisher&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#22312;&#8220;&#20013;&#22870;&#21048;&#20551;&#35828;&#8221;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;L1&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;Fisher&#21098;&#26525;&#21644;&#38543;&#26426;&#21098;&#26525;&#22312;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#21644;&#21098;&#26525;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#21253;&#25324;&#19968;&#27425;&#24615;&#21644;&#36845;&#20195;&#21098;&#26525;&#30340;&#35780;&#20272;&#65292;&#23545;&#32593;&#32476;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#30340;&#26435;&#37325;&#31227;&#21160;&#30340;&#32771;&#23519;&#65292;&#27604;&#36739;&#19981;&#21516;&#23485;&#24230;&#32593;&#32476;&#19978;&#21098;&#26525;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#21464;&#24471;&#38750;&#24120;&#31232;&#30095;&#26102;&#26041;&#27861;&#30340;&#24615;&#33021;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26377;&#25928;&#35745;&#31639;Fisher&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25209;&#22788;&#29702;Fisher&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the performance of different pruning methods in the context of the lottery ticket hypothesis. We compare the performance of L1 unstructured pruning, Fisher pruning, and random pruning on different network architectures and pruning scenarios. The experiments include an evaluation of one-shot and iterative pruning, an examination of weight movement in the network during pruning, a comparison of the pruning methods on networks of varying widths, and an analysis of the performance of the methods when the network becomes very sparse. Additionally, we propose and evaluate a new method for efficient computation of Fisher pruning, known as batched Fisher pruning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15477</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#19978;&#30340;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20869;&#22312;&#33021;&#22815;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#30456;&#20851;&#24615;&#65292;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#21453;&#26144;SPD&#27969;&#24418;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22266;&#23450;&#24230;&#37327;&#24352;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;SPD&#30697;&#38453;&#23398;&#20064;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;SPD&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#21033;&#29992;&#25289;&#22238;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;SPD&#27969;&#24418;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#24230;&#37327;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#12290;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;SPD&#32593;&#32476;&#21487;&#20197;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#39640;&#31934;&#24230;&#39640;&#25928;&#29575;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#19987;&#29992;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#24322;&#26500;&#24182;&#34892;&#32467;&#26500;&#65292;&#20854;&#20013;&#19981;&#20381;&#36182;&#20911;&#183;&#35834;&#20381;&#26364;&#32467;&#26500;&#30340;ASIC&#29992;&#20110;&#35780;&#20272;&#35745;&#31639;&#24320;&#38144;&#26368;&#22823;&#30340;&#21407;&#23376;&#21147;&#12290;&#19982;&#22522;&#20110;GPU&#30340;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#26356;&#20302;&#30340;&#21046;&#36896;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.15474</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#24322;&#26500;&#24182;&#34892;&#38750;&#20911;&#183;&#35834;&#20381;&#26364;&#32467;&#26500;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Heterogeneous Parallel Non-von Neumann Architecture System for Accurate and Efficient Machine Learning Molecular Dynamics. (arXiv:2303.15474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#39640;&#31934;&#24230;&#39640;&#25928;&#29575;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21160;&#21147;&#23398;&#35745;&#31639;&#30340;&#19987;&#29992;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#24322;&#26500;&#24182;&#34892;&#32467;&#26500;&#65292;&#20854;&#20013;&#19981;&#20381;&#36182;&#20911;&#183;&#35834;&#20381;&#26364;&#32467;&#26500;&#30340;ASIC&#29992;&#20110;&#35780;&#20272;&#35745;&#31639;&#24320;&#38144;&#26368;&#22823;&#30340;&#21407;&#23376;&#21147;&#12290;&#19982;&#22522;&#20110;GPU&#30340;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#26356;&#20302;&#30340;&#21046;&#36896;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#29992;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#29575;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#35745;&#31639;&#12290;&#35813;&#31995;&#32479;&#30001;&#24322;&#26500;&#24182;&#34892;&#30340;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#21644;&#24212;&#29992;&#29305;&#23450;&#38598;&#25104;&#30005;&#36335;&#65288;ASIC&#65289;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#19981;&#20381;&#36182;&#20911;&#183;&#35834;&#20381;&#26364;&#65288;NvN&#65289;&#20307;&#31995;&#32467;&#26500;&#65288;SilTerra 180 nm&#24037;&#33402;&#65289;&#30340;&#21435;&#20056;&#27861;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#35780;&#20272;&#21407;&#23376;&#21147;&#65292;&#36825;&#26159;MD&#30340;&#35745;&#31639;&#24320;&#38144;&#26368;&#22823;&#30340;&#37096;&#20998;&#12290;MD&#30340;&#25152;&#26377;&#20854;&#20182;&#35745;&#31639;&#37117;&#20351;&#29992;FPGA&#65288;Xilinx XC7Z100&#65289;&#23436;&#25104;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20026;&#20102;&#23454;&#29616;&#31867;&#20284;&#31934;&#24230;&#65292;&#22522;&#20110;&#20302;&#31471;&#21046;&#36896;&#24037;&#33402;&#65288;180 nm&#65289;&#30340;NvN&#31995;&#32479;&#27604;&#22522;&#20110;&#26174;&#21345;&#22788;&#29702;&#22120;&#65288;GPU&#65289;&#30340;&#26368;&#20808;&#36827;vN MLMD&#24555;1.6&#20493;&#65292;&#24182;&#19988;&#33021;&#28304;&#25928;&#29575;&#39640;&#20986;10^2-10^3&#20493;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;NvN&#24322;&#26500;&#24182;&#34892;&#26550;&#26500;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a special-purpose system to achieve high-accuracy and high-efficiency machine learning (ML) molecular dynamics (MD) calculations. The system consists of field programmable gate array (FPGA) and application specific integrated circuit (ASIC) working in heterogeneous parallelization. To be specific, a multiplication-less neural network (NN) is deployed on the non-von Neumann (NvN)-based ASIC (SilTerra 180 nm process) to evaluate atomic forces, which is the most computationally expensive part of MD. All other calculations of MD are done using FPGA (Xilinx XC7Z100). It is shown that, to achieve similar-level accuracy, the proposed NvN-based system based on low-end fabrication technologies (180 nm) is 1.6x faster and 10^2-10^3x more energy efficiency than state-of-the-art vN based MLMD using graphics processing units (GPUs) based on much more advanced technologies (12 nm), indicating superiority of the proposed NvN-based heterogeneous parallel architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#25688;&#35201;&#65292;&#23558;&#20256;&#32479;&#32858;&#21512;&#30340;&#20248;&#28857;&#19982;&#36890;&#36807;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#21512;&#24182;&#65292;&#26082;&#33021;&#20445;&#25345;&#31934;&#24230;&#21448;&#33021;&#20943;&#23569;&#25968;&#25454;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.15465</link><description>&lt;p&gt;
&#31934;&#30830;&#21487;&#21512;&#24182;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Exactly mergeable summaries. (arXiv:2303.15465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#25688;&#35201;&#65292;&#23558;&#20256;&#32479;&#32858;&#21512;&#30340;&#20248;&#28857;&#19982;&#36890;&#36807;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#21512;&#24182;&#65292;&#26082;&#33021;&#20445;&#25345;&#31934;&#24230;&#21448;&#33021;&#20943;&#23569;&#25968;&#25454;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#20013;&#65292;&#32858;&#21512;&#26159;&#20943;&#23569;&#25968;&#25454;&#22823;&#23567;&#65288;&#22797;&#26434;&#24615;&#65289;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#25968;&#25454;&#20998;&#26512;&#31243;&#24207;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#32858;&#21512;&#20989;&#25968;&#12290;&#20256;&#32479;&#32858;&#21512;&#30340;&#38382;&#39064;&#22312;&#20110;&#24448;&#24448;&#20250;&#20002;&#22833;&#22826;&#22810;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#32467;&#26524;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#25688;&#35201;&#65292;&#23427;&#23558;&#20256;&#32479;&#32858;&#21512;&#30340;&#20248;&#28857;&#19982;&#36890;&#36807;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#22797;&#26434;&#25688;&#35201;&#36827;&#34892;&#31934;&#30830;&#21512;&#24182;&#65292;&#24182;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#22797;&#26434;&#32858;&#21512;&#25688;&#35201;&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#24320;&#21457;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the analysis of large/big data sets, aggregation (replacing values of a variable over a group by a single value) is a standard way of reducing the size (complexity) of the data. Data analysis programs provide different aggregation functions.  Recently some books dealing with the theoretical and algorithmic background of traditional aggregation functions were published. A problem with traditional aggregation is that often too much information is discarded thus reducing the precision of the obtained results. A much better, preserving more information, summarization of original data can be achieved by representing aggregated data using selected types of complex data.  In complex data analysis the measured values over a selected group $A$ are aggregated into a complex object $\Sigma(A)$ and not into a single value. Most of the aggregation functions theory does not apply directly. In our contribution, we present an attempt to start building a theoretical background of complex aggregation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#28041;&#21450;&#22521;&#35757;&#12289;&#25512;&#29702;&#12289;&#19968;&#33324;&#21270;&#36793;&#30028;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#32452;&#25968;&#23398;&#25361;&#25112;&#65292;&#20026;&#25968;&#23398;&#23478;&#12289;&#32479;&#35745;&#23398;&#23478;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20132;&#27969;&#30340;&#24418;&#24335;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.15464</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#23398;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Mathematical Challenges in Deep Learning. (arXiv:2303.15464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#28041;&#21450;&#22521;&#35757;&#12289;&#25512;&#29702;&#12289;&#19968;&#33324;&#21270;&#36793;&#30028;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#32452;&#25968;&#23398;&#25361;&#25112;&#65292;&#20026;&#25968;&#23398;&#23478;&#12289;&#32479;&#35745;&#23398;&#23478;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20132;&#27969;&#30340;&#24418;&#24335;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;2012&#24180;&#30340;ImageNet&#25361;&#25112;&#20197;&#26469;&#65292;&#28145;&#24230;&#27169;&#22411;&#24050;&#32463;&#20027;&#23472;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#28145;&#24230;&#27169;&#22411;&#30340;&#22823;&#23567;&#20174;&#37027;&#26102;&#36215;&#19968;&#30452;&#22312;&#22686;&#21152;&#65292;&#36825;&#32473;&#22312;&#25163;&#26426;&#12289;&#20010;&#20154;&#30005;&#33041;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#26080;&#32447;&#22522;&#31449;&#31561;&#39046;&#22495;&#24212;&#29992;&#30340;&#36825;&#19968;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21015;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#65292;&#28085;&#30422;&#22521;&#35757;&#12289;&#25512;&#29702;&#12289;&#19968;&#33324;&#21270;&#36793;&#30028;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#29992;&#19968;&#20123;&#24418;&#24335;&#21270;&#35821;&#35328;&#26469;&#19982;&#25968;&#23398;&#23478;&#12289;&#32479;&#35745;&#23398;&#23478;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#20132;&#27969;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#26159;&#23545;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#38382;&#39064;&#30340;&#20027;&#35266;&#30475;&#27861;&#65292;&#23427;&#26377;&#30410;&#20110;&#38271;&#26399;&#30340;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models are dominating the artificial intelligence (AI) industry since the ImageNet challenge in 2012. The size of deep models is increasing ever since, which brings new challenges to this field with applications in cell phones, personal computers, autonomous cars, and wireless base stations. Here we list a set of problems, ranging from training, inference, generalization bound, and optimization with some formalism to communicate these challenges with mathematicians, statisticians, and theoretical computer scientists. This is a subjective view of the research questions in deep learning that benefits the tech industry in long run.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20195;&#29702;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;-&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#20013;&#20027;&#21160;&#35831;&#27714;&#24110;&#21161;&#65292;&#24182;&#20351;&#29992;&#21453;&#39304;&#25351;&#31034;&#30446;&#26631;&#29289;&#20307;&#22312;&#20854;&#35270;&#37326;&#20013;&#30340;&#20301;&#32622;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#21253;&#25324;&#26377;&#21644;&#27809;&#26377;&#21453;&#39304;&#30340;&#28151;&#21512;&#21095;&#38598;&#65292;&#20351;&#24471;&#20195;&#29702;&#26356;&#21152;&#31283;&#20581;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15453</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#39304;&#36827;&#34892;&#20855;&#36523;&#35270;&#35273;&#23548;&#33322;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Utilizing Feedback in Embodied Visual Navigation. (arXiv:2303.15453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20195;&#29702;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;-&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#20013;&#20027;&#21160;&#35831;&#27714;&#24110;&#21161;&#65292;&#24182;&#20351;&#29992;&#21453;&#39304;&#25351;&#31034;&#30446;&#26631;&#29289;&#20307;&#22312;&#20854;&#35270;&#37326;&#20013;&#30340;&#20301;&#32622;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#21253;&#25324;&#26377;&#21644;&#27809;&#26377;&#21453;&#39304;&#30340;&#28151;&#21512;&#21095;&#38598;&#65292;&#20351;&#24471;&#20195;&#29702;&#26356;&#21152;&#31283;&#20581;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;&#30446;&#26631;-&#29289;&#20307;&#23548;&#33322;&#20219;&#21153;&#20013;&#20027;&#21160;&#35831;&#27714;&#24110;&#21161;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#21453;&#39304;&#25351;&#31034;&#30446;&#26631;&#29289;&#20307;&#22312;&#20854;&#35270;&#37326;&#20013;&#30340;&#20301;&#32622;&#12290;&#20026;&#20102;&#20351;&#20195;&#29702;&#26356;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#21363;&#22312;&#32769;&#24072;&#21487;&#33021;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#35838;&#31243;&#21253;&#25324;&#26377;&#21644;&#27809;&#26377;&#21453;&#39304;&#30340;&#28151;&#21512;&#21095;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework for training an agent to actively request help in object-goal navigation tasks, with feedback indicating the location of the target object in its field of view. To make the agent more robust in scenarios where a teacher may not always be available, the proposed training curriculum includes a mix of episodes with and without feedback. The results show that this approach improves the agent's performance, even in the absence of feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#19977;&#35282;&#26041;&#31243;&#32452;&#19978;&#20351;&#29992;&#36890;&#29992;&#30340;&#8220;backslash&#8221;&#25110;&#39640;&#26031;&#28040;&#20803;&#26469;&#35745;&#31639;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#30340;&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#20248;&#21270;&#21644;&#23454;&#29616;&#20415;&#21033;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15449</link><description>&lt;p&gt;
&#36890;&#36807;BACKslash&#23454;&#29616;BACK substitution&#30340;BACKpropagation&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
BACKpropagation through BACK substitution with a BACKslash. (arXiv:2303.15449v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#19977;&#35282;&#26041;&#31243;&#32452;&#19978;&#20351;&#29992;&#36890;&#29992;&#30340;&#8220;backslash&#8221;&#25110;&#39640;&#26031;&#28040;&#20803;&#26469;&#35745;&#31639;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#30340;&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#20248;&#21270;&#21644;&#23454;&#29616;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#19977;&#35282;&#26041;&#31243;&#32452;&#19978;&#20351;&#29992;&#36890;&#29992;&#30340;&#8220;backslash&#8221;&#25110;&#39640;&#26031;&#28040;&#20803;&#26469;&#35745;&#31639;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#30340;&#32447;&#24615;&#20195;&#25968;&#20844;&#24335;&#12290;&#36890;&#24120;&#65292;&#30697;&#38453;&#20803;&#32032;&#26159;&#31639;&#23376;&#12290;&#36825;&#31687;&#35770;&#25991;&#26377;&#19977;&#20010;&#36129;&#29486;&#65306;1.&#29992;&#24038;&#20316;&#29992;&#30340;&#31639;&#23376;&#29702;&#35770;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#21462;&#20195;&#20256;&#32479;&#30340;&#33258;&#21160;&#24494;&#20998;&#22788;&#29702;&#20855;&#26377;&#30693;&#35782;&#20215;&#20540;&#12290;2.&#21487;&#20197;&#23558;&#31639;&#23376;&#25918;&#32622;&#22312;&#30697;&#38453;&#20013;&#20316;&#20026;&#23454;&#29616;&#36873;&#39033;&#30340;&#36719;&#20214;&#20013;&#65292;&#22914;Julia&#12290;3.&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#65292;&#8220;transpose dot&#8221;&#25805;&#20316;&#31526;&#8220;$\{\}^{T_\bullet}$&#8221;&#65292;&#20801;&#35768;&#21453;&#36716;&#31639;&#23376;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31639;&#23376;&#26041;&#27861;&#22312;&#36866;&#21512;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#65288;&#22914;Julia&#65289;&#30340;&#20248;&#38597;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#25277;&#35937;&#21487;&#20197;&#22312;&#20195;&#30721;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#26045;&#23637;&#31034;&#20102;&#36890;&#29992;&#32447;&#24615;&#20195;&#25968;&#22914;&#20309;&#23454;&#29616;&#20256;&#32479;&#29305;&#27530;&#24418;&#24335;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;backpropagation&#65292;&#20197;&#30697;&#38453;&#25805;&#20316;&#30340;&#24418;&#24335;&#20070;&#20889;&#65292;&#36825;&#25171;&#24320;&#20102;&#20248;&#21270;&#21644;&#23454;&#26045;&#20415;&#21033;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a linear algebra formulation of backpropagation which allows the calculation of gradients by using a generically written ``backslash'' or Gaussian elimination on triangular systems of equations. Generally the matrix elements are operators. This paper has three contributions:  1. It is of intellectual value to replace traditional treatments of automatic differentiation with a (left acting) operator theoretic, graph-based approach.  2. Operators can be readily placed in matrices in software in programming languages such as Ju lia as an implementation option.  3. We introduce a novel notation, ``transpose dot'' operator ``$\{\}^{T_\bullet}$'' that allows the reversal of operators.  We demonstrate the elegance of the operators approach in a suitable programming language consisting of generic linear algebra operators such as Julia \cite{bezanson2017julia}, and that it is possible to realize this abstraction in code. Our implementation shows how generic linear algebra can allow op
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30830;&#23450;&#24615;&#8221;&#21644;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#24471;&#20998;&#26041;&#27861;&#26469;&#37327;&#21270;&#20998;&#31867;&#20915;&#31574;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14568</link><description>&lt;p&gt;
&#37327;&#21270;&#20998;&#31867;&#20915;&#31574;&#30340;&#30830;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Classification Decision Certainty and Doubt. (arXiv:2303.14568v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30830;&#23450;&#24615;&#8221;&#21644;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#24471;&#20998;&#26041;&#27861;&#26469;&#37327;&#21270;&#20998;&#31867;&#20915;&#31574;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#23450;&#37327;&#34920;&#24449;&#21644;&#20272;&#35745;&#22312;&#20248;&#21270;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#35266;&#30340;&#24471;&#20998;&#65292;&#31216;&#20026;&#8220;&#30830;&#23450;&#24615;&#8221;&#21644;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#65292;&#21487;&#22312;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#20027;&#20041;&#26694;&#26550;&#19979;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#65288;&#22810;&#65289;&#20998;&#31867;&#20915;&#31574;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#39044;&#27979;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitative characterizations and estimations of uncertainty are of fundamental importance in optimization and decision-making processes. Herein, we propose intuitive scores, which we call \textit{certainty} and \textit{doubt}, that can be used in both a Bayesian and frequentist framework to assess and compare the quality and uncertainty of predictions in (multi-)classification decision machine learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;Green-Kubo&#26041;&#27861;&#24212;&#29992;&#20110;&#21322;&#23616;&#22495;&#26426;&#22120;&#23398;&#20064;&#21183;&#65292;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#25512;&#23548;&#20986;&#36866;&#24212;&#30340;&#28909;&#36890;&#37327;&#20844;&#24335;&#65292;&#25104;&#21151;&#35745;&#31639;&#20102;&#20108;&#27687;&#21270;&#38150;&#30340;&#28909;&#23548;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.14434</link><description>&lt;p&gt;
&#21322;&#23616;&#22495;&#26426;&#22120;&#23398;&#20064;&#21183;&#30340;&#28909;&#36890;&#37327;
&lt;/p&gt;
&lt;p&gt;
Heat flux for semi-local machine-learning potentials. (arXiv:2303.14434v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;Green-Kubo&#26041;&#27861;&#24212;&#29992;&#20110;&#21322;&#23616;&#22495;&#26426;&#22120;&#23398;&#20064;&#21183;&#65292;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#25512;&#23548;&#20986;&#36866;&#24212;&#30340;&#28909;&#36890;&#37327;&#20844;&#24335;&#65292;&#25104;&#21151;&#35745;&#31639;&#20102;&#20108;&#27687;&#21270;&#38150;&#30340;&#28909;&#23548;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Green-Kubo (GK)&#26041;&#27861;&#26159;&#26448;&#26009;&#28909;&#20256;&#36755;&#27169;&#25311;&#30340;&#20005;&#26684;&#26694;&#26550;&#65292;&#20294;&#23427;&#38656;&#35201;&#20934;&#30830;&#25551;&#36848;&#21183;&#33021;&#34920;&#38754;&#19988;&#25910;&#25947;&#32479;&#35745;&#37327;&#12290;&#26426;&#22120;&#23398;&#20064;&#21183;&#21487;&#20197;&#36798;&#21040;&#31532;&#19968;&#24615;&#21407;&#29702;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#19968;&#23567;&#37096;&#20998;&#25104;&#26412;&#20869;&#36229;&#36234;&#20854;&#27169;&#25311;&#26102;&#38388;&#21644;&#38271;&#24230;&#23610;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;GK&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#36817;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#22120;&#23398;&#20064;&#21183;&#31867;&#65292;&#35813;&#26041;&#27861;&#36845;&#20195;&#22320;&#32771;&#34385;&#21021;&#22987;&#20132;&#20114;&#25130;&#26029;&#20197;&#22806;&#30340;&#21322;&#23616;&#22495;&#20132;&#20114;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#36866;&#24212;&#30340;&#28909;&#36890;&#37327;&#20844;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#20108;&#27687;&#21270;&#38150;&#30340;&#28909;&#23548;&#29575;&#39564;&#35777;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Green-Kubo (GK) method is a rigorous framework for heat transport simulations in materials. However, it requires an accurate description of the potential-energy surface and carefully converged statistics. Machine-learning potentials can achieve the accuracy of first-principles simulations while allowing to reach well beyond their simulation time and length scales at a fraction of the cost. In this paper, we explain how to apply the GK approach to the recent class of message-passing machine-learning potentials, which iteratively consider semi-local interactions beyond the initial interaction cutoff. We derive an adapted heat flux formulation that can be implemented using automatic differentiation without compromising computational efficiency. The approach is demonstrated and validated by calculating the thermal conductivity of zirconium dioxide across temperatures.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.13489</link><description>&lt;p&gt;
&#20351;&#29992;&#31034;&#33539;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#19982;&#35268;&#21010;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13489
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#35797;&#38169;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20351;&#29992;&#31034;&#33539;&#21487;&#20197;&#35753;&#26234;&#33021;&#20307;&#21463;&#30410;&#20110;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#25506;&#32034;&#26368;&#20339;&#34892;&#21160;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#20351;&#29992;&#31034;&#33539;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20915;&#31574;&#21046;&#23450;&#33539;&#24335;&#65288;&#20363;&#22914;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#22312;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#22914;&#20309;&#24212;&#29992;&#31034;&#33539;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#25910;&#38598;&#31034;&#33539;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20030;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#20363;&#23376;&#65292;&#24182;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.12888</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20998;&#25552;&#21069;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;
&lt;/p&gt;
&lt;p&gt;
A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#32908;&#26775;&#27515;&#21644;&#24515;&#21147;&#34928;&#31469;&#26159;&#20027;&#35201;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#32654;&#22269;&#25968;&#30334;&#19975;&#20154;&#30340;&#20581;&#24247;&#12290;&#21457;&#23637;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#24739;&#32773;&#20013;&#65292;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26368;&#39640;&#12290;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#21450;&#26102;&#23454;&#26045;&#27835;&#30103;&#25514;&#26045;&#21487;&#20197;&#38450;&#27490;&#32570;&#34880;&#12289;&#20302;&#34880;&#21387;&#20197;&#21450;&#30001;&#20110;&#24515;&#28304;&#24615;&#20241;&#20811;&#23548;&#33268;&#24515;&#36755;&#20986;&#37327;&#38477;&#20302;&#30340;&#26377;&#23475;&#24490;&#29615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#20013;&#28023;&#37327;&#25968;&#25454;&#30340;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#19982;&#32570;&#20047;&#26377;&#25928;&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#23545;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#19968;&#30452;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#31216;&#20026;CShock&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#20837;&#20303;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#30340;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#30340;&#24515;&#28304;&#24615;&#20241;&#20811;&#21457;&#20316;&#12290;&#20026;&#20102;&#24320;&#21457;&#21644;&#39564;&#35777;CShock&#65292;&#25105;&#20204;&#20351;&#29992;&#30001;&#21307;&#24072;&#35009;&#23450;&#30340;&#32467;&#26524;&#27880;&#37322;&#20102;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;WL&#27979;&#35797;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#19977;&#27425;&#36845;&#20195;&#30340;$(d-1)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#19988;&#21482;&#38656;&#35201;&#19968;&#27425;&#36845;&#20195;&#30340;$d$-WL&#27979;&#35797;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12853</link><description>&lt;p&gt;
&#19977;&#27425;&#36845;&#20195;&#30340;$(1-d)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#28857;&#20113;&#30340;&#38750;&#31561;&#36317;&#21464;&#25442;. (arXiv:2303.12853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Three iterations of $(1-d)$-WL test distinguish non isometric clouds of $d$-dimensional points. (arXiv:2303.12853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;WL&#27979;&#35797;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#19977;&#27425;&#36845;&#20195;&#30340;$(d-1)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#19988;&#21482;&#38656;&#35201;&#19968;&#27425;&#36845;&#20195;&#30340;$d$-WL&#27979;&#35797;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Lehman (WL)&#27979;&#35797;&#26159;&#19968;&#20010;&#26816;&#26597;&#22270;&#21516;&#26500;&#30340;&#22522;&#26412;&#36845;&#20195;&#31639;&#27861;&#12290;&#23427;&#34987;&#35266;&#23519;&#21040;&#26159;&#20960;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#21487;&#20197;&#29992;&#36825;&#20010;&#27979;&#35797;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#29702;&#35299;&#12290;&#21463;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#28041;&#21450;&#19977;&#32500;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;WL&#27979;&#35797;&#23545;&#23436;&#25972;&#30340;&#36317;&#31163;&#22270;&#34920;&#31034;&#30340;&#27431;&#20960;&#37324;&#24471;&#28857;&#20113;&#26159;&#8220;&#23436;&#25972;&#30340;&#8221;&#26102;&#65292;&#23427;&#20309;&#26102;&#33021;&#22815;&#35782;&#21035;&#20986;&#20219;&#24847;&#19968;&#20010;&#20219;&#24847;&#28857;&#20113;.&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;$(d-1)$-&#32500;WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#20219;&#20309;$d\ge 2$&#37117;&#21487;&#20197;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#19977;&#27425;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;$d=2,3$&#26159;&#32039;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;$d$&#32500;WL&#27979;&#35797;&#21482;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#36845;&#20195;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for checking isomorphism of graphs. It has also been observed that it underlies the design of several graph neural network architectures, whose capabilities and performance can be understood in terms of the expressive power of this test. Motivated by recent developments in machine learning applications to datasets involving three-dimensional objects, we study when the WL test is {\em complete} for clouds of euclidean points represented by complete distance graphs, i.e., when it can distinguish, up to isometry, any arbitrary such cloud.  Our main result states that the $(d-1)$-dimensional WL test is complete for point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that only three iterations of the test suffice. Our result is tight for $d = 2, 3$. We also observe that the $d$-dimensional WL test only requires one iteration to achieve completeness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12748</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models. (arXiv:2303.12748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26657;&#20934;&#23545;&#20110;&#20445;&#35777;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#20351;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#27492;&#22312;&#30417;&#30563;&#20998;&#31867;&#27169;&#22411;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#38477;&#20302;&#35823;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#29702;&#26102;&#30340;&#26657;&#20934;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;CLIP&#12290;&#26412;&#30740;&#31350;&#34913;&#37327;&#20102;&#36328;&#30456;&#20851;&#21464;&#37327;&#65288;&#22914;&#25552;&#31034;&#65292;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#65289;&#30340;&#26657;&#20934;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;CLIP&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#23384;&#22312;&#35823;&#26657;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#19982;CLIP&#20316;&#20026;&#38646;&#26679;&#26412;&#25512;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#29992;&#20363;&#30456;&#19968;&#33268;&#65292;&#24182;&#23637;&#31034;&#20986;&#21333;&#20010;&#23398;&#20064;&#30340;&#28201;&#24230;&#20540;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#65288;&#30001;&#36873;&#23450;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#23450;&#20041;&#65289;&#65292;&#36328;&#19981;&#21516;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#25552;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration of deep learning models is crucial to their trustworthiness and safe usage, and as such, has been extensively studied in supervised classification models, with methods crafted to decrease miscalibration. However, there has yet to be a comprehensive study of the calibration of vision-language models that are used for zero-shot inference, like CLIP. We measure calibration across relevant variables like prompt, dataset, and architecture, and find that zero-shot inference with CLIP is miscalibrated. Furthermore, we propose a modified version of temperature scaling that is aligned with the common use cases of CLIP as a zero-shot inference model, and show that a single learned temperature generalizes for each specific CLIP model (defined by a chosen pre-training dataset and architecture) across inference dataset and prompt choice.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24066;&#22330;&#20013;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#24179;&#34913;&#23450;&#20215;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36873;&#25321;&#24615;&#35854;&#35328;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11522</link><description>&lt;p&gt;
&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24066;&#22330;&#20013;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#24179;&#34913;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Online Learning for Equilibrium Pricing in Markets under Incomplete Information. (arXiv:2303.11522v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24066;&#22330;&#20013;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#24179;&#34913;&#23450;&#20215;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36873;&#25321;&#24615;&#35854;&#35328;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#24179;&#34913;&#30340;&#30740;&#31350;&#26159;&#32463;&#27982;&#29702;&#35770;&#30340;&#26680;&#24515;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#25928;&#37197;&#32622;&#31232;&#32570;&#36164;&#28304;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23450;&#20215;&#22343;&#34913;&#30340;&#35745;&#31639;&#36890;&#24120;&#20381;&#36182;&#20110;&#23436;&#25972;&#30340;&#20010;&#20307;&#23646;&#24615;&#20449;&#24687;&#65292;&#22914;&#20379;&#24212;&#21830;&#30340;&#25104;&#26412;&#20989;&#25968;&#31561;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#19981;&#21487;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#23450;&#20215;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24066;&#22330;&#32463;&#33829;&#32773;&#23547;&#27714;&#36890;&#36807;&#20174;&#25104;&#26412;&#20989;&#25968;&#26410;&#30693;&#30340;&#31454;&#20105;&#20379;&#24212;&#21830;&#36141;&#20080;&#25152;&#38656;&#25968;&#37327;&#26469;&#28385;&#36275;&#23458;&#25143;&#38656;&#27714;&#12290;&#22312;&#36825;&#31181;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24179;&#34913;&#20215;&#26684;&#65292;&#21516;&#26102;&#32852;&#21512;&#20248;&#21270;&#19977;&#20010;&#24615;&#33021;&#25351;&#26631;&#8212;&#8212;&#26410;&#28385;&#36275;&#30340;&#38656;&#27714;&#12289;&#25104;&#26412;&#22833;&#35823;&#21644;&#20184;&#27454;&#22833;&#35823;&#8212;&#8212;&#36825;&#26159;&#22312;&#23450;&#20215;&#22343;&#34913;&#30340;&#24773;&#20917;&#19979;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of market equilibria is central to economic theory, particularly in efficiently allocating scarce resources. However, the computation of equilibrium prices at which the supply of goods matches their demand typically relies on having access to complete information on private attributes of agents, e.g., suppliers' cost functions, which are often unavailable in practice. Motivated by this practical consideration, we consider the problem of setting equilibrium prices in the incomplete information setting wherein a market operator seeks to satisfy the customer demand for a commodity by purchasing the required amount from competing suppliers with privately known cost functions unknown to the market operator. In this incomplete information setting, we consider the online learning problem of learning equilibrium prices over time while jointly optimizing three performance metrics -- unmet demand, cost regret, and payment regret -- pertinent in the context of equilibrium pricing over a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35757;&#32451;&#30340;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#65288;TPGM&#65289;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#38024;&#23545;&#27599;&#20010;&#23618;&#30340;&#26045;&#21152;&#32422;&#26463;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#24494;&#35843;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26131;&#29992;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24494;&#35843;&#22330;&#26223;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10720</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Trainable Projected Gradient Method for Robust Fine-tuning. (arXiv:2303.10720v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35757;&#32451;&#30340;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#65288;TPGM&#65289;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#38024;&#23545;&#27599;&#20010;&#23618;&#30340;&#26045;&#21152;&#32422;&#26463;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#24494;&#35843;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26131;&#29992;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24494;&#35843;&#22330;&#26223;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#23376;&#38598;&#23618;&#25110;&#20026;&#27599;&#20010;&#23618;&#33258;&#23450;&#20041;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#23545;&#20110;&#21306;&#20998;&#24230;&#25968;&#25454;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#20445;&#30041;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#37117;&#37319;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#26114;&#36149;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#36825;&#38450;&#27490;&#20102;&#23427;&#20204;&#22312;&#22823;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#32593;&#32476;&#19978;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#35757;&#32451;&#30340;&#25237;&#24433;&#26799;&#24230;&#26041;&#27861;&#65288;TPGM&#65289;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#38024;&#23545;&#27599;&#20010;&#23618;&#30340;&#26045;&#21152;&#32422;&#26463;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#24494;&#35843;&#27491;&#21017;&#21270;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#24494;&#35843;&#34920;&#36848;&#20026;&#21452;&#23618;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;TPGM&#32500;&#25252;&#27599;&#20010;&#22270;&#23618;&#30340;&#25237;&#24433;&#21322;&#24452;&#38598;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#25237;&#24433;&#24378;&#21046;&#25191;&#34892;&#23427;&#20204;&#12290;&#20026;&#20102;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#65292;&#20854;&#21463;&#21040;&#32473;&#23450;&#25237;&#24433;&#21322;&#24452;&#38598;&#30340;&#32422;&#26463;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24494;&#35843;&#22330;&#26223;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on transfer learning have shown that selectively fine-tuning a subset of layers or customizing different learning rates for each layer can greatly improve robustness to out-of-distribution (OOD) data and retain generalization capability in the pre-trained models. However, most of these methods employ manually crafted heuristics or expensive hyper-parameter searches, which prevent them from scaling up to large datasets and neural networks. To solve this problem, we propose Trainable Projected Gradient Method (TPGM) to automatically learn the constraint imposed for each layer for a fine-grained fine-tuning regularization. This is motivated by formulating fine-tuning as a bi-level constrained optimization problem. Specifically, TPGM maintains a set of projection radii, i.e., distance constraints between the fine-tuned model and the pre-trained model, for each layer, and enforces them through weight projections. To learn the constraints, we propose a bi-level optimization to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#32771;&#34385;&#21040;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.06945</link><description>&lt;p&gt;
CoGANPPIS: &#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction. (arXiv:2303.06945v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#32771;&#34385;&#21040;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#22312;&#29983;&#21270;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#20301;&#28857;&#65288;PPIs&#65289;&#21487;&#20197;&#21152;&#28145;&#25105;&#20204;&#23545;&#29983;&#29289;&#26426;&#29702;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#26032;&#33647;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PPI&#39044;&#27979;&#23454;&#39564;&#26041;&#27861;&#25104;&#26412;&#39640;&#26114;&#65292;&#32791;&#26102;&#38271;&#65292;&#22240;&#27492;&#36817;&#24180;&#26469;&#24320;&#21457;&#20102;&#35768;&#22810;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#25366;&#25496;&#20102;&#19968;&#20123;&#26377;&#29992;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#20294;&#26410;&#32771;&#34385;&#21040;&#20849;&#36827;&#21270;&#29305;&#24449;&#65292;&#21518;&#32773;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#32447;&#32034;&#65307;&#65288;2&#65289;attention-based&#27169;&#22411;&#20165;&#20026;&#30456;&#37051;&#27531;&#22522;&#20998;&#37197;&#20851;&#27880;&#26435;&#37325;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#20998;&#37197;&#65292;&#24573;&#30053;&#20102;&#36828;&#31163;&#30446;&#26631;&#27531;&#22522;&#30340;&#19968;&#20123;&#27531;&#22522;&#21487;&#33021;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#36827;&#21270;&#22686;&#24378;&#30340;&#20840;&#23616;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PPI&#20301;&#28857;&#39044;&#27979;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#20849;&#36827;&#21270;&#29305;&#24449;&#21644;&#20840;&#23616;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#27688;&#22522;&#37240;&#27531;&#22522;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#32771;&#34385;&#21040;&#34507;&#30333;&#24207;&#21015;&#20013;&#25152;&#26377;&#27531;&#22522;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein-protein interactions are essential in biochemical processes. Accurate prediction of the protein-protein interaction sites (PPIs) deepens our understanding of biological mechanism and is crucial for new drug design. However, conventional experimental methods for PPIs prediction are costly and time-consuming so that many computational approaches, especially ML-based methods, have been developed recently. Although these approaches have achieved gratifying results, there are still two limitations: (1) Most models have excavated some useful input features, but failed to take coevolutionary features into account, which could provide clues for inter-residue relationships; (2) The attention-based models only allocate attention weights for neighboring residues, instead of doing it globally, neglecting that some residues being far away from the target residues might also matter.  We propose a coevolution-enhanced global attention neural network, a sequence-based deep learning model for P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#27491;&#21017;&#21270;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#38750;&#24179;&#31283;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24179;&#34913;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#36890;&#36807;&#33021;&#37327;&#26368;&#23567;&#21270;&#39033;&#38480;&#21046;&#20102;&#32593;&#32476;&#23545;&#25903;&#25345;&#26576;&#20010;&#25805;&#20316;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#26377;&#25928;&#24615;&#19982;&#21516;&#31867;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.06552</link><description>&lt;p&gt;
&#33021;&#37327;&#27491;&#21017;&#21270;RNN&#35299;&#20915;&#38750;&#24179;&#31283;Bandit&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Energy Regularized RNNs for Solving Non-Stationary Bandit Problems. (arXiv:2303.06552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#27491;&#21017;&#21270;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#38750;&#24179;&#31283;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24179;&#34913;&#20102;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#36890;&#36807;&#33021;&#37327;&#26368;&#23567;&#21270;&#39033;&#38480;&#21046;&#20102;&#32593;&#32476;&#23545;&#25903;&#25345;&#26576;&#20010;&#25805;&#20316;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#26377;&#25928;&#24615;&#19982;&#21516;&#31867;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#24182;&#21462;&#20915;&#20110;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#21487;&#33021;&#30340;&#36807;&#21435;&#24773;&#22659;&#12290;&#22312;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#25105;&#20204;&#37319;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;&#36825;&#20123;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#37327;&#26368;&#23567;&#21270;&#39033;&#65292;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#23545;&#25903;&#25345;&#26576;&#20010;&#25805;&#20316;&#36807;&#20110;&#33258;&#20449;&#12290;&#36890;&#36807;&#36825;&#20010;&#26415;&#35821;&#65292;&#32593;&#32476;&#25152;&#20998;&#37197;&#30340;&#26368;&#22823;&#21644;&#26368;&#23567;&#27010;&#29575;&#20043;&#38388;&#30340;&#24046;&#36317;&#21463;&#21040;&#21512;&#29702;&#38480;&#21046;&#12290;&#22312;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33267;&#23569;&#19982;&#35299;&#20915;Rotting Bandits&#23376;&#38382;&#39064;&#30340;&#26041;&#27861;&#19968;&#26679;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22522;&#20934;&#38382;&#39064;&#30340;&#30452;&#35266;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/rotmanmi/Energy-Regularized-RNN&#19978;&#20849;&#20139;&#25105;&#20204;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a Multi-Armed Bandit problem in which the rewards are non-stationary and are dependent on past actions and potentially on past contexts. At the heart of our method, we employ a recurrent neural network, which models these sequences. In order to balance between exploration and exploitation, we present an energy minimization term that prevents the neural network from becoming too confident in support of a certain action. This term provably limits the gap between the maximal and minimal probabilities assigned by the network. In a diverse set of experiments, we demonstrate that our method is at least as effective as methods suggested to solve the sub-problem of Rotting Bandits, and can solve intuitive extensions of various benchmark problems. We share our implementation at https://github.com/rotmanmi/Energy-Regularized-RNN.
&lt;/p&gt;</description></item><item><title>HyT-NAS&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#38024;&#23545;&#24494;&#22411;&#35774;&#22791;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#23569;&#20110;5&#20493;&#30340;&#35757;&#32451;&#35780;&#20272;&#27425;&#25968;&#19979;&#23454;&#29616;&#31867;&#20284;&#30340;&#36229;&#20307;&#31215;&#65292;&#24182;&#22312;Visu&#19978;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;3.5&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;6.3%&#12290;</title><link>http://arxiv.org/abs/2303.04440</link><description>&lt;p&gt;
HyT-NAS: &#38754;&#21521;&#36793;&#32536;&#35774;&#22791;&#30340;&#28151;&#21512;&#21464;&#21387;&#22120;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices. (arXiv:2303.04440v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04440
&lt;/p&gt;
&lt;p&gt;
HyT-NAS&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#38024;&#23545;&#24494;&#22411;&#35774;&#22791;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#21487;&#20197;&#22312;&#23569;&#20110;5&#20493;&#30340;&#35757;&#32451;&#35780;&#20272;&#27425;&#25968;&#19979;&#23454;&#29616;&#31867;&#20284;&#30340;&#36229;&#20307;&#31215;&#65292;&#24182;&#22312;Visu&#19978;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;3.5&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;6.3%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#20351;&#24471;&#36817;&#26399;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20123;&#26550;&#26500;&#24456;&#23569;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24179;&#21488;&#19978;&#23454;&#29616;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#28151;&#21512;&#25163;&#24037;&#21367;&#31215;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HyT-NAS&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#21253;&#25324;&#38024;&#23545;&#24494;&#22411;&#35774;&#22791;&#30340;&#28151;&#21512;&#26550;&#26500;&#12290;HyT-NAS&#36890;&#36807;&#20016;&#23500;&#25628;&#32034;&#31354;&#38388;&#12289;&#22686;&#24378;&#25628;&#32034;&#31574;&#30053;&#21644;&#24615;&#33021;&#39044;&#27979;&#22120;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;HW-NAS&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;HyT-NAS&#22312;&#23569;&#20110;5&#20493;&#30340;&#35757;&#32451;&#35780;&#20272;&#27425;&#25968;&#19979;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#36229;&#20307;&#31215;&#12290;&#26368;&#32456;&#30340;&#26550;&#26500;&#22312;Visu&#19978;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;3.5&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;6.3%&#65292;&#32988;&#36807;&#20102;MLPerf MobileNetV1&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers have enabled recent attention-based Deep Learning (DL) architectures to achieve remarkable results in Computer Vision (CV) tasks. However, due to the extensive computational resources required, these architectures are rarely implemented on resource-constrained platforms. Current research investigates hybrid handcrafted convolution-based and attention-based models for CV tasks such as image classification and object detection. In this paper, we propose HyT-NAS, an efficient Hardware-aware Neural Architecture Search (HW-NAS) including hybrid architectures targeting vision tasks on tiny devices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search space and enhancing the search strategy as well as the performance predictors. Our experiments show that HyT-NAS achieves a similar hypervolume with less than ~5x training evaluations. Our resulting architecture outperforms MLPerf MobileNetV1 by 6.3% accuracy improvement with 3.5x less number of parameters on Visu
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2302.12906</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Invertible Quantum Neural Networks. (arXiv:2302.12906v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#27169;&#25311;&#21644;&#29983;&#25104;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#38376;&#31639;&#27861;&#29992;&#20110;&#37327;&#23376;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;QINN&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#34928;&#21464;&#20026;&#36731;&#23376;&#30340;Z&#29627;&#33394;&#23376;&#30340;&#21943;&#27880;&#30456;&#20851;&#20135;&#29983;&#30340;LHC&#25968;&#25454;&#65292;&#36825;&#26159;&#31890;&#23376;&#23545;&#25758;&#26426;&#31934;&#23494;&#27979;&#37327;&#30340;&#26631;&#20934;&#36807;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;QINN&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#28151;&#21512;&#30340;QINN&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#19968;&#20010;&#26174;&#33879;&#26356;&#22823;&#30340;&#23436;&#20840;&#32463;&#20856;&#30340;INN&#30340;&#34920;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invertible Neural Networks (INN) have become established tools for the simulation and generation of highly complex data. We propose a quantum-gate algorithm for a Quantum Invertible Neural Network (QINN) and apply it to the LHC data of jet-associated production of a Z-boson that decays into leptons, a standard candle process for particle collider precision measurements. We compare the QINN's performance for different loss functions and training scenarios. For this task, we find that a hybrid QINN matches the performance of a significantly larger purely classical INN in learning and generating complex data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21040;&#21487;&#20197;&#27867;&#21270;&#22320;&#20248;&#21270;&#22120;&#21644;&#20248;&#21270;&#23545;&#35937;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#29616;&#26377;&#30340; L2O &#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20248;&#21270;&#22120;&#21644;&#20248;&#21270;&#23545;&#35937;&#30340;&#19968;&#33324;&#21270;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.11085</link><description>&lt;p&gt;
&#22312;&#12300;&#23398;&#20064;&#20248;&#21270;&#12301;&#20013;&#23398;&#20064;&#19968;&#33324;&#21270;&#30340;&#20445;&#35777;&#65288;Learning to Generalize Provably in Learning to Optimize&#65289;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize Provably in Learning to Optimize. (arXiv:2302.11085v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21040;&#21487;&#20197;&#27867;&#21270;&#22320;&#20248;&#21270;&#22120;&#21644;&#20248;&#21270;&#23545;&#35937;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#29616;&#26377;&#30340; L2O &#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20248;&#21270;&#22120;&#21644;&#20248;&#21270;&#23545;&#35937;&#30340;&#19968;&#33324;&#21270;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#23398;&#20064;&#20248;&#21270;&#12301;&#65288;Learning to optimize&#65292;L2O&#65289;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#33258;&#21160;&#21270;&#35774;&#35745;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340; L2O &#26041;&#27861;&#22312;&#33267;&#23569;&#20004;&#20010;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65306;&#65288;i&#65289;&#23558; L2O &#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#20248;&#21270;&#23545;&#35937;&#26102;&#38477;&#20302;&#20854;&#25439;&#22833;&#20989;&#25968;&#20540;&#65288;&#20248;&#21270;&#22120;&#19968;&#33324;&#21270;&#25110;&#8220;&#21487;&#27867;&#21270;&#30340;&#20248;&#21270;&#22120;&#23398;&#20064;&#8221;&#65289;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#30001;&#20248;&#21270;&#22120;&#35757;&#32451;&#30340;&#20248;&#21270;&#23545;&#35937;&#65288;&#26412;&#36523;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#22312;&#20934;&#30830;&#24615;&#19978;&#38754;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#34920;&#29616;&#30340;&#27979;&#35797;&#24615;&#33021;&#65288;&#20248;&#21270;&#23545;&#35937;&#19968;&#33324;&#21270;&#25110;&#8220;&#23398;&#20064;&#19968;&#33324;&#21270;&#8221;&#65289;&#12290;&#34429;&#28982;&#20248;&#21270;&#22120;&#19968;&#33324;&#21270;&#26368;&#36817;&#24050;&#34987;&#30740;&#31350;&#65292;&#20294;&#20248;&#21270;&#23545;&#35937;&#19968;&#33324;&#21270;&#22312; L2O &#19978;&#23578;&#26410;&#24471;&#21040;&#20005;&#26684;&#30740;&#31350;&#65292;&#36825;&#26159;&#26412;&#25991;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#23616;&#37096;&#29109;&#19982; Hessian &#20043;&#38388;&#30340;&#38544;&#24335;&#32852;&#31995;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#23427;&#20204;&#22312;&#36890;&#29992;&#20248;&#21270;&#25216;&#26415;&#30340;&#25163;&#24037;&#35774;&#35745;&#20013;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340; &#8220;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;" &#26469;&#23398;&#20064; L2O &#30340;&#19968;&#33324;&#21270;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#19968;&#37096;&#20998;&#26159; GO &#27169;&#22359;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26368;&#22823;&#21270;&#36890;&#29992;&#21270;&#30446;&#26631;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#21270;&#31639;&#27861;&#65307;&#21478;&#19968;&#37096;&#20998;&#26159;&#20248;&#21270;&#23545;&#35937;&#27169;&#22359;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20248;&#21270;&#22120;&#21644;&#26631;&#20934;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#19982;&#29616;&#26377;&#30340; L2O &#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20248;&#21270;&#22120;&#21644;&#20248;&#21270;&#23545;&#35937;&#30340;&#19968;&#33324;&#21270;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to optimize (L2O) has gained increasing popularity, which automates the design of optimizers by data-driven approaches. However, current L2O methods often suffer from poor generalization performance in at least two folds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms of lowering their loss function values (optimizer generalization, or ``generalizable learning of optimizers"); and (ii) the test performance of an optimizee (itself as a machine learning model), trained by the optimizer, in terms of the accuracy over unseen data (optimizee generalization, or ``learning to generalize"). While the optimizer generalization has been recently studied, the optimizee generalization (or learning to generalize) has not been rigorously studied in the L2O context, which is the aim of this paper. We first theoretically establish an implicit connection between the local entropy and the Hessian, and hence unify their roles in the handcrafted design of generalizable optim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#20197;&#21450;Circuit Training (CT)&#23454;&#29616;&#30340;&#24320;&#28304;&#20195;&#30721;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#20102;CT&#30456;&#23545;&#20110;&#22810;&#20010;&#21487;&#26367;&#20195;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23398;&#26415;&#24615;&#28151;&#21512;&#23610;&#23544;&#24067;&#23616;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#21644;&#31283;&#23450;&#24615;&#30740;&#31350;&#65292;&#20026;&#26410;&#26469;&#30340;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.11014</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessment of Reinforcement Learning for Macro Placement. (arXiv:2302.11014v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#20197;&#21450;Circuit Training (CT)&#23454;&#29616;&#30340;&#24320;&#28304;&#20195;&#30721;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#20102;CT&#30456;&#23545;&#20110;&#22810;&#20010;&#21487;&#26367;&#20195;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23398;&#26415;&#24615;&#28151;&#21512;&#23610;&#23544;&#24067;&#23616;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#21644;&#31283;&#23450;&#24615;&#30740;&#31350;&#65292;&#20026;&#26410;&#26469;&#30340;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;Google Brain&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23439;&#35266;&#24067;&#23616;&#21450;&#20854;Circuit Training (CT)&#23454;&#29616;&#30340;&#24320;&#25918;&#36879;&#26126;&#23454;&#29616;&#21644;&#35780;&#20272;&#65292;&#24182;&#22312;GitHub&#20013;&#23454;&#29616;&#20102;CT&#30340;&#20851;&#38190;"&#40657;&#30418;"&#20803;&#32032;&#65292;&#28548;&#28165;&#20102;CT&#19982;Nature&#35770;&#25991;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#21457;&#24067;&#20102;&#26032;&#30340;&#23545;&#24320;&#25918;&#23454;&#29616;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;CT&#21450;&#22810;&#20010;&#21487;&#26367;&#20195;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#65292;&#25152;&#26377;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#30456;&#20851;&#33050;&#26412;&#37117;&#22312;GitHub&#19978;&#20844;&#24320;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#21253;&#25324;&#20102;&#23398;&#26415;&#24615;&#28151;&#21512;&#23610;&#23544;&#24067;&#23616;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#28040;&#34701;&#21644;&#31283;&#23450;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#35780;&#35770;&#20102;Nature&#21644;CT&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide open, transparent implementation and assessment of Google Brain's deep reinforcement learning approach to macro placement and its Circuit Training (CT) implementation in GitHub. We implement in open source key "blackbox" elements of CT, and clarify discrepancies between CT and Nature paper. New testcases on open enablements are developed and released. We assess CT alongside multiple alternative macro placers, with all evaluation flows and related scripts public in GitHub. Our experiments also encompass academic mixed-size placement benchmarks, as well as ablation and stability studies. We comment on the impact of Nature and CT, as well as directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10296</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21151;&#33021;&#32806;&#21512;&#27700;&#21360;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#34920;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#28023;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#27700;&#21360;&#25216;&#26415;&#65292;&#20854;&#20013;DNN&#25552;&#20379;&#21830;&#23558;&#31192;&#23494;&#20449;&#24687;&#26893;&#20837;&#27169;&#22411;&#20013;&#65292;&#20197;&#20415;&#22312;&#31245;&#21518;&#36890;&#36807;&#19968;&#20123;&#19987;&#29992;&#35302;&#21457;&#36755;&#20837;&#26816;&#32034;&#23884;&#20837;&#30340;&#27700;&#21360;&#32034;&#26435;&#65307;&#34429;&#28982;&#25991;&#29486;&#20013;&#25253;&#21578;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36973;&#21463;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65292;&#20363;&#22914;&#27169;&#22411;&#24494;&#35843;&#21644;&#27169;&#22411;&#20462;&#21098;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#19978;&#36848;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#36825;&#26679;&#21024;&#38500;&#27700;&#21360;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#31192;&#23494;&#29305;&#24449;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; JANA &#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#36817;&#20284;&#35745;&#31639;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#20998;&#25674;&#30340;&#36817;&#20284;&#21518;&#39564;&#21644;&#20284;&#28982;&#65292;&#20026;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36884;&#24452;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#25311;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#26657;&#20934;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09125</link><description>&lt;p&gt;
JANA&#65306;&#22797;&#26434;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#32852;&#21512;&#20998;&#25674;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models. (arXiv:2302.09125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; JANA &#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#36817;&#20284;&#35745;&#31639;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#20998;&#25674;&#30340;&#36817;&#20284;&#21518;&#39564;&#21644;&#20284;&#28982;&#65292;&#20026;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36884;&#24452;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#25311;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#26657;&#20934;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#32852;&#21512;&#20998;&#25674;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#8221;&#65288;JANA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36125;&#21494;&#26031;&#20195;&#29702;&#24314;&#27169;&#21644;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#20013;&#20986;&#29616;&#30340;&#38590;&#20197;&#35745;&#31639;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#23494;&#24230;&#12290;&#25105;&#20204;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#19977;&#20010;&#30456;&#20114;&#34917;&#20805;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;1&#65289;&#19968;&#20010;&#24635;&#32467;&#32593;&#32476;&#65292;&#23558;&#20010;&#21035;&#25968;&#25454;&#28857;&#12289;&#38598;&#21512;&#25110;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#25104;&#20449;&#24687;&#23884;&#20837;&#21521;&#37327;&#65307;2&#65289;&#19968;&#20010;&#21518;&#39564;&#32593;&#32476;&#65292;&#23398;&#20064;&#20998;&#25674;&#30340;&#36817;&#20284;&#21518;&#39564;&#65307;3&#65289;&#19968;&#20010;&#20284;&#28982;&#32593;&#32476;&#65292;&#23398;&#20064;&#20998;&#25674;&#30340;&#36817;&#20284;&#20284;&#28982;&#12290;&#23427;&#20204;&#30340;&#20132;&#20114;&#20026;&#20998;&#25674;&#36793;&#32536;&#20284;&#28982;&#21644;&#21518;&#39564;&#39044;&#27979;&#20272;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36825;&#26159;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24120;&#24120;&#23545;&#20110;&#26631;&#20934;&#26041;&#27861;&#26469;&#35828;&#22826;&#26114;&#36149;&#20102;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#25311;&#27169;&#22411;&#20013;&#23545;JANA&#30340;&#20445;&#30495;&#24230;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#21487;&#35299;&#37322;&#30340;&#32852;&#21512;&#26657;&#20934;&#35786;&#26029;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24490;&#29615;&#20284;&#28982;&#32593;&#32476;&#27169;&#25311;&#22797;&#26434;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes ''jointly amortized neural approximation'' (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation -- two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state-of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SN-Net&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20415;&#23452;&#22320;&#20135;&#29983;&#35768;&#22810;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#26435;&#34913;&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#20316;&#20026;&#38170;&#28857;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#30340;&#32541;&#21512;&#23618;&#23558;&#23427;&#20204;&#25340;&#25509;&#22312;&#19968;&#36215;&#20197;&#23454;&#29616;&#21160;&#24577;&#30340;&#31934;&#24230;-&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.06586</link><description>&lt;p&gt;
&#21487;&#32541;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stitchable Neural Networks. (arXiv:2302.06586v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SN-Net&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20415;&#23452;&#22320;&#20135;&#29983;&#35768;&#22810;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#26435;&#34913;&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#20316;&#20026;&#38170;&#28857;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#30340;&#32541;&#21512;&#23618;&#23558;&#23427;&#20204;&#25340;&#25509;&#22312;&#19968;&#36215;&#20197;&#23454;&#29616;&#21160;&#24577;&#30340;&#31934;&#24230;-&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24040;&#22823;&#23041;&#21147;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#38598;&#32676;(&#22914;ResNet/DeiT)&#25152;&#26500;&#25104;&#30340;&#20844;&#20849;&#27169;&#22411;&#24211;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33539;&#22260;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20419;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27169;&#22411;&#31995;&#21015;&#37117;&#21253;&#21547;&#30528;&#19981;&#21516;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;(&#27604;&#22914;DeiT-Ti/S/B)&#65292;&#36825;&#33258;&#28982;&#22320;&#24341;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#36816;&#34892;&#26102;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#21487;&#29992;&#30340;&#27169;&#22411;&#31995;&#21015;&#20197;&#23454;&#29616;&#21160;&#24577;&#30340;&#31934;&#24230;-&#25928;&#29575;&#26435;&#34913;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Stitchable Neural Networks (SN-Net)&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#30340;&#27169;&#22411;&#37096;&#32626;&#26694;&#26550;&#12290;&#22312;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#20013;&#65292;&#23427;&#21487;&#20197;&#20415;&#23452;&#22320;&#20135;&#29983;&#35768;&#22810;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#26435;&#34913;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38170;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SN-Net&#23558;&#38170;&#28857;&#20998;&#25955;&#22312;&#22359;/&#23618;&#20043;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#31616;&#21333;&#30340;&#32541;&#21512;&#23618;&#23558;&#23427;&#20204;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#26144;&#23556;&#19968;&#20010;&#38170;&#28857;&#30340;&#28608;&#27963;&#21040;&#21478;&#19968;&#20010;&#38170;&#28857;&#12290;&#20165;&#20165;&#36890;&#36807;&#20960;&#20010;&#36718;&#27425;&#30340;&#35757;&#32451;&#65292;SN-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#25554;&#20540;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The public model zoo containing enormous powerful pretrained model families (e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which significantly contributes to the success of deep learning. As each model family consists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it naturally arises a fundamental question of how to efficiently assemble these readily available models in a family for dynamic accuracy-efficiency trade-offs at runtime. To this end, we present Stitchable Neural Networks (SN-Net), a novel scalable and efficient framework for model deployment. It cheaply produces numerous networks with different complexity and performance trade-offs given a family of pretrained neural networks, which we call anchors. Specifically, SN-Net splits the anchors across the blocks/layers and then stitches them together with simple stitching layers to map the activations from one anchor to another. With only a few epochs of training, SN-Net effectively interpolates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#38544;&#31169;&#30340;&#31227;&#21160;&#28216;&#25103;&#24212;&#29992;&#23433;&#35013;&#39044;&#27979;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.03332</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#38544;&#31169;&#30340;&#31227;&#21160;&#28216;&#25103;&#24212;&#29992;&#23433;&#35013;&#39044;&#27979;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards a User Privacy-Aware Mobile Gaming App Installation Prediction Model. (arXiv:2302.03332v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#38544;&#31169;&#30340;&#31227;&#21160;&#28216;&#25103;&#24212;&#29992;&#23433;&#35013;&#39044;&#27979;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#31243;&#24207;&#21270;&#24191;&#21578;&#22312;&#22312;&#32447;&#24191;&#21578;&#34892;&#19994;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#23454;&#26102;&#31454;&#20215;&#31995;&#32479;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#36141;&#20080;&#21644;&#20986;&#21806;&#22312;&#32447;&#24191;&#21578;&#23637;&#29616;&#37327;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#22312;&#23454;&#26102;&#31454;&#20215;&#31995;&#32479;&#20013;&#65292;&#38656;&#27714;&#26041;&#24179;&#21488;&#26088;&#22312;&#26377;&#25928;&#22320;&#25903;&#37197;&#24191;&#21578;&#21830;&#30340;&#25512;&#24191;&#39044;&#31639;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21033;&#28070;&#65292;&#23547;&#27714;&#20135;&#29983;&#39640;&#29992;&#25143;&#21709;&#24212;&#65288;&#22914;&#28857;&#20987;&#25110;&#23433;&#35013;&#65289;&#30340;&#23637;&#31034;&#37327;&#12290;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29305;&#23450;&#38656;&#27714;&#26041;&#24179;&#21488;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#39044;&#27979;&#31227;&#21160;&#28216;&#25103;&#24212;&#29992;&#23433;&#35013;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#27880;&#24847;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#25506;&#35752;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#32423;&#21035;&#30340;&#28508;&#22312;&#29992;&#25143;&#38544;&#31169;&#23041;&#32961;&#65292;&#36825;&#21462;&#20915;&#20110;&#19982;&#25968;&#25454;&#20849;&#20139;&#27969;&#31243;&#30456;&#20851;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#20363;&#22914;&#25968;&#25454;&#36716;&#25442;&#25110;&#21435;&#21311;&#21517;&#21270;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#20363;&#22914;&#23494;&#30721;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, programmatic advertising has received a great deal of attention in the online advertising industry. A real-time bidding (RTB) system is rapidly becoming the most popular method to buy and sell online advertising impressions. Within the RTB system, demand-side platforms (DSP) aim to spend advertisers' campaign budgets efficiently while maximizing profit, seeking impressions that result in high user responses, such as clicks or installs. In the current study, we investigate the process of predicting a mobile gaming app installation from the point of view of a particular DSP, while paying attention to user privacy, and exploring the trade-off between privacy preservation and model performance. There are multiple levels of potential threats to user privacy, depending on the privacy leaks associated with the data-sharing process, such as data transformation or de-anonymization. To address these concerns, privacy-preserving techniques were proposed, such as cryptographi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#20102;&#19968;&#20010;&#20195;&#29702;&#35299;&#31639;&#22120;&#65292;&#21487;&#20197;&#24212;&#29992;&#22312;&#33258;&#30001;&#24418;&#30005;&#30913;&#21453;&#28436;&#35774;&#35745;&#20013;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.01934</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#30340;&#33258;&#30001;&#24418;&#30005;&#30913;&#21453;&#28436;&#35774;&#35745;&#20195;&#29702;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A neural operator-based surrogate solver for free-form electromagnetic inverse design. (arXiv:2302.01934v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#20102;&#19968;&#20010;&#20195;&#29702;&#35299;&#31639;&#22120;&#65292;&#21487;&#20197;&#24212;&#29992;&#22312;&#33258;&#30001;&#24418;&#30005;&#30913;&#21453;&#28436;&#35774;&#35745;&#20013;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#31070;&#32463;&#31639;&#23376;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23454;&#29616;&#21644;&#35757;&#32451;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;&#30005;&#30913;&#25955;&#23556;&#38382;&#39064;&#30340;&#20195;&#29702;&#35299;&#31639;&#22120;&#65292;&#24182;&#23558;&#20854;&#25968;&#25454;&#25928;&#29575;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20854;&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#33258;&#30001;&#24418;&#20840;&#19977;&#32500;&#30005;&#30913;&#25955;&#23556;&#20307;&#30340;&#32435;&#31859;&#20809;&#23376;&#23398;&#21453;&#28436;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#21040;&#30446;&#21069;&#20026;&#27490;&#23578;&#26410;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have emerged as a powerful tool for solving partial differential equations in the context of scientific machine learning. Here, we implement and train a modified Fourier neural operator as a surrogate solver for electromagnetic scattering problems and compare its data efficiency to existing methods. We further demonstrate its application to the gradient-based nanophotonic inverse design of free-form, fully three-dimensional electromagnetic scatterers, an area that has so far eluded the application of deep learning techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05785</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#24314;&#27169;&#23454;&#29616;&#39640;&#25928;&#30340;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#31354;&#38388;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#35774;&#35745;&#30340;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24456;&#38590;&#26500;&#24314;&#26368;&#20248;&#28608;&#27963;&#20989;&#25968;&#65292;&#32780;&#24403;&#21069;&#30340;&#28608;&#27963;&#20989;&#25968;&#25628;&#32034;&#31639;&#27861;&#36807;&#20110;&#26114;&#36149;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;2,913&#20010;&#31995;&#32479;&#29983;&#25104;&#30340;&#28608;&#27963;&#20989;&#25968;&#20174;&#22836;&#35757;&#32451;&#21367;&#31215;&#12289;&#27531;&#24046;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#21019;&#24314; Act-Bench-CNN&#12289;Act-Bench-ResNet &#21644; Act-Bench-ViT &#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;&#22522;&#20934;&#31354;&#38388;&#65292;&#21457;&#29616;&#19982;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#21644;&#28608;&#27963;&#20989;&#25968;&#36755;&#20986;&#20998;&#24067;&#30456;&#20851;&#32852;&#30340; Fisher &#20449;&#24687;&#30697;&#38453;&#30340;&#39057;&#35889;&#23545;&#24615;&#33021;&#30340;&#39044;&#27979;&#24615;&#24456;&#39640;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#20195;&#29702;&#22312;&#36739;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#30340;&#28608;&#27963;&#20989;&#25968;&#26550;&#26500;&#65292;&#21516;&#26102;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33203;&#39135;&#33829;&#20859;&#36741;&#21161;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#30340;&#26032;&#21152;&#22369;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#30417;&#31649;&#21644;&#30417;&#30563;&#20154;&#20204;&#30340;&#33829;&#20859;&#25668;&#20837;&#65292;&#20026;&#26032;&#21152;&#22369;&#20154;&#30340;&#20581;&#24247;&#20419;&#36827;&#25552;&#20379;&#21307;&#23398;&#32423;&#21035;&#30340;&#33829;&#20859;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.03829</link><description>&lt;p&gt;
&#20174;&#39184;&#30424;&#21040;&#39044;&#38450;&#65306;&#26032;&#21152;&#22369;&#20581;&#24247;&#20419;&#36827;&#30340;&#33203;&#39135;&#33829;&#20859;&#36741;&#21161;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore. (arXiv:2301.03829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33203;&#39135;&#33829;&#20859;&#36741;&#21161;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#30340;&#26032;&#21152;&#22369;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#30417;&#31649;&#21644;&#30417;&#30563;&#20154;&#20204;&#30340;&#33829;&#20859;&#25668;&#20837;&#65292;&#20026;&#26032;&#21152;&#22369;&#20154;&#30340;&#20581;&#24247;&#20419;&#36827;&#25552;&#20379;&#21307;&#23398;&#32423;&#21035;&#30340;&#33829;&#20859;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#21152;&#22369;&#19968;&#30452;&#33268;&#21147;&#20110;&#25913;&#21892;&#20154;&#27665;&#30340;&#21307;&#30103;&#20445;&#20581;&#26381;&#21153;&#12290;&#25919;&#24220;&#27880;&#24847;&#21040;&#20102;&#30417;&#31649;&#21644;&#30417;&#30563;&#20154;&#20204;&#25668;&#20837;&#33829;&#20859;&#30340;&#19981;&#36275;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#24930;&#24615;&#30142;&#30149;&#21457;&#23637;&#30340;&#19968;&#20010;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#26032;&#21152;&#22369;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21644;&#33719;&#21462;&#21307;&#30103;&#32423;&#21035;&#33829;&#20859;&#25668;&#20837;&#20449;&#24687;&#20197;&#22312;&#19981;&#21516;&#26041;&#38754;&#36896;&#31119;&#26032;&#21152;&#22369;&#20154;&#30340;&#32463;&#39564;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FoodSG&#24179;&#21488;&#26469;&#23413;&#21270;&#22810;&#26679;&#21270;&#30340;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#30340;&#24212;&#29992;&#26381;&#21153;&#22312;&#26032;&#21152;&#22369;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23427;&#20204;&#30340;&#20849;&#21516;&#35201;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#26412;&#22320;&#21270;&#39135;&#21697;&#25968;&#25454;&#38598;&#30340;&#28145;&#36828;&#24847;&#20041;&#65292;&#24182;&#31995;&#32479;&#22320;&#28165;&#29702;&#21644;&#31579;&#36873;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#30340;&#26032;&#21152;&#22369;&#39135;&#21697;&#25968;&#25454;&#38598;FoodSG-233&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#26032;&#21152;&#22369;&#22810;&#26679;&#21270;&#39135;&#21697;&#33756;&#32948;&#24102;&#26469;&#30340;&#35782;&#21035;&#24615;&#33021;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#35758;&#25972;&#21512;&#30417;&#30563;&#24335;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singapore has been striving to improve the provision of healthcare services to her people. In this course, the government has taken note of the deficiency in regulating and supervising people's nutrient intake, which is identified as a contributing factor to the development of chronic diseases. Consequently, this issue has garnered significant attention. In this paper, we share our experience in addressing this issue and attaining medical-grade nutrient intake information to benefit Singaporeans in different aspects. To this end, we develop the FoodSG platform to incubate diverse healthcare-oriented applications as a service in Singapore, taking into account their shared requirements. We further identify the profound meaning of localized food datasets and systematically clean and curate a localized Singaporean food dataset FoodSG-233. To overcome the hurdle in recognition performance brought by Singaporean multifarious food dishes, we propose to integrate supervised contrastive learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#36739;&#23569;&#21807;&#19968;&#20803;&#32032;&#30340;&#29305;&#27530;&#32467;&#26500;&#26469;&#23454;&#29616;&#35774;&#22791;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#23454;&#29616;&#26368;&#39640;19&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.00330</link><description>&lt;p&gt;
&#26799;&#24230;&#36807;&#28388;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#30340;&#35774;&#22791;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient On-device Training via Gradient Filtering. (arXiv:2301.00330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#36739;&#23569;&#21807;&#19968;&#20803;&#32032;&#30340;&#29305;&#27530;&#32467;&#26500;&#26469;&#23454;&#29616;&#35774;&#22791;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#23454;&#29616;&#26368;&#39640;19&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#32852;&#37030;&#23398;&#20064;&#12289;&#36830;&#32493;&#23398;&#20064;&#21644;&#20854;&#20182;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#35774;&#22791;&#31471;&#35757;&#32451;&#20173;&#28982;&#26159;EdgeAI&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#36807;&#28388;&#25216;&#26415;&#65292;&#20351;&#24471;&#35774;&#22791;&#31471;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#36739;&#23569;&#21807;&#19968;&#20803;&#32032;&#30340;&#29305;&#27530;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#26399;&#38388;&#21453;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#22312;&#22810;&#20010;CNN&#27169;&#22411;&#65288;&#20363;&#22914;MobileNet&#12289;DeepLabV3&#12289;UPerNet&#65289;&#21644;&#35774;&#22791;&#65288;&#20363;&#22914;Raspberry Pi&#21644;Jetson Nano&#65289;&#19978;&#36827;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;&#19982;SOTA&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36798;19&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\times$ speedu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#23545;EEG&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#31471;&#21040;&#31471;&#33021;&#21147;&#12289;&#27169;&#22411;&#35757;&#32451;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10426</link><description>&lt;p&gt;
EEG&#35299;&#30721;&#30340;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#23545;EEG&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#31471;&#21040;&#31471;&#33021;&#21147;&#12289;&#27169;&#22411;&#35757;&#32451;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22312;&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35299;&#30721;&#20219;&#21153;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#30001;&#28145;&#24230;&#23398;&#20064;&#25110;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#35299;&#30721;&#22120;&#23454;&#29616;&#30340;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#65288;DRNs&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#21487;&#33021;&#32467;&#21512;&#20102;&#20043;&#21069;&#20004;&#31867;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#36824;&#26377;&#19968;&#31995;&#21015;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#27934;&#23519;&#65292;&#20197;&#38138;&#24179;DRNs&#22312;EEG&#20013;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36947;&#36335;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#38382;&#39064;&#65292;&#22914;&#32593;&#32476;&#22823;&#23567;&#21644;&#31471;&#21040;&#31471;&#33021;&#21147;&#65292;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#38382;&#39064;&#12290;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#25968;&#25454;&#22914;&#20309;&#36716;&#25442;&#65292;&#20197;&#21450;&#26159;&#21542;&#19982;&#20256;&#32479;&#30340;EEG&#35299;&#30721;&#30456;&#20851;&#20063;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#20855;&#26377;&#24191;&#27867;&#36229;&#21442;&#25968;&#30340;DRNs&#26469;&#22880;&#23450;&#36825;&#20123;&#20027;&#39064;&#39046;&#22495;&#30340;&#22522;&#30784;&#12290;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;EEG&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#32593;&#32476;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning or Riemannian-Geometry-based decoders. Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability as well as model training questions. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on two public EEG datasets and compared with sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#26102;&#65292;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#65292;&#32780;&#20351;&#29992;&#21512;&#25104; ImageNet &#20811;&#38534;&#20307;&#33021;&#21542;&#24357;&#34917;&#24046;&#36317;&#12290;&#36890;&#36807;&#26368;&#23567;&#21644;&#31867;&#19981;&#21487;&#30693;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#35777;&#26126;&#20102;&#21512;&#25104;&#22270;&#20687;&#21046;&#20316;&#27169;&#22411;&#21644;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#30340;&#24040;&#22823;&#24046;&#36317;&#24471;&#21040;&#20102;&#24357;&#21512;&#12290;</title><link>http://arxiv.org/abs/2212.08420</link><description>&lt;p&gt;
&#36870;&#22659;&#27714;&#29983;&#65306;&#20174;&#21512;&#25104; ImageNet &#20811;&#38534;&#20307;&#20013;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fake it till you make it: Learning transferable representations from synthetic ImageNet clones. (arXiv:2212.08420v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#26102;&#65292;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#65292;&#32780;&#20351;&#29992;&#21512;&#25104; ImageNet &#20811;&#38534;&#20307;&#33021;&#21542;&#24357;&#34917;&#24046;&#36317;&#12290;&#36890;&#36807;&#26368;&#23567;&#21644;&#31867;&#19981;&#21487;&#30693;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#35777;&#26126;&#20102;&#21512;&#25104;&#22270;&#20687;&#21046;&#20316;&#27169;&#22411;&#21644;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#30340;&#24040;&#22823;&#24046;&#36317;&#24471;&#21040;&#20102;&#24357;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914; Stable Diffusion&#65289;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20174;&#31616;&#21333;&#30340;&#25991;&#26412;&#25552;&#31034;&#24320;&#22987;&#29983;&#25104;&#30456;&#24403;&#30495;&#23454;&#30340;&#22270;&#20687;&#12290;&#24403;&#35757;&#32451;&#22270;&#20687;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#33021;&#21542;&#20351;&#30495;&#23454;&#22270;&#20687;&#21464;&#24471;&#36807;&#26102;&#65311; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24314;&#31435;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#21517;&#31216;&#24320;&#22987;&#65292;&#25506;&#32034;&#20102;&#20165;&#25552;&#20379; ImageNet &#20811;&#38534;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;Stable Diffusion &#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#26368;&#23567;&#19988;&#31867;&#19981;&#21487;&#30693;&#30340;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;ImageNet &#20811;&#38534;&#20307;&#33021;&#22815;&#24357;&#21512;&#21512;&#25104;&#22270;&#20687;&#21046;&#20316;&#27169;&#22411;&#21644;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#65292;&#23545;&#20110;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#32771;&#34385;&#30340;&#22810;&#20010;&#26631;&#20934;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by investigating the need for real images when training models for ImageNet classification. Provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful these are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering, ImageNet clones are able to close a large part of the gap between models produced by synthetic images and models trained with real images, for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39640;&#26031;&#27169;&#22411;&#20272;&#35745;&#29305;&#24449;&#20998;&#24067;&#21442;&#25968;&#36827;&#34892;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#31867;&#26465;&#20214;&#23494;&#24230;&#36317;&#31163;&#20272;&#35745;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.06461</link><description>&lt;p&gt;
&#19968;&#31181;&#39044;&#27979;Few-Shot&#20998;&#31867;&#27867;&#21270;&#30340;&#32479;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Statistical Model for Predicting Generalization in Few-Shot Classification. (arXiv:2212.06461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06461
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39640;&#26031;&#27169;&#22411;&#20272;&#35745;&#29305;&#24449;&#20998;&#24067;&#21442;&#25968;&#36827;&#34892;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#31867;&#26465;&#20214;&#23494;&#24230;&#36317;&#31163;&#20272;&#35745;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#36890;&#24120;&#20381;&#36182;&#20110;&#39564;&#35777;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;Few-Shot&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#24456;&#38590;&#33719;&#24471;&#36825;&#26679;&#30340;&#39564;&#35777;&#38598;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#19968;&#20010;&#39640;&#24230;&#34987;&#24573;&#35270;&#30340;&#32570;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29305;&#24449;&#20998;&#24067;&#30340;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#20272;&#35745;&#36825;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#22312;&#26032;&#30340;Few-Shot&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31867;&#26465;&#20214;&#23494;&#24230;&#20043;&#38388;&#20934;&#30830;&#30340;&#36317;&#31163;&#20272;&#35745;&#26159;&#20934;&#30830;&#35780;&#20272;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#20559;&#20272;&#35745;&#22120;&#26469;&#35745;&#31639;&#36825;&#20123;&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#25968;&#20540;&#20998;&#26512;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#20363;&#22914;&#30041;&#19968;&#27861;-Cross Validation &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation of the generalization error of classifiers often relies on a validation set. Such a set is hardly available in few-shot learning scenarios, a highly disregarded shortcoming in the field. In these scenarios, it is common to rely on features extracted from pre-trained neural networks combined with distance-based classifiers such as nearest class mean. In this work, we introduce a Gaussian model of the feature distribution. By estimating the parameters of this model, we are able to predict the generalization error on new classification tasks with few samples. We observe that accurate distance estimates between class-conditional densities are the key to accurate estimates of the generalization performance. Therefore, we propose an unbiased estimator for these distances and integrate it in our numerical analysis. We empirically show that our approach outperforms alternatives such as the leave-one-out cross-validation strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCurvEd&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21487;&#20132;&#25442;&#30340;&#26041;&#24335;&#30830;&#23450;&#28508;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20132;&#38169;&#30690;&#37327;&#22330;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.14573</link><description>&lt;p&gt;
&#28145;&#24230;&#26354;&#32447;&#32534;&#36753;&#65306;&#38024;&#23545;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20132;&#25442;&#21644;&#38750;&#32447;&#24615;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model. (arXiv:2211.14573v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCurvEd&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21487;&#20132;&#25442;&#30340;&#26041;&#24335;&#30830;&#23450;&#28508;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20132;&#38169;&#30690;&#37327;&#22330;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#30340;&#35821;&#20041;&#32534;&#36753;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#26412;&#30446;&#26631;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#20294;&#36890;&#24120;&#23427;&#20204;&#19981;&#20855;&#22791;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#32534;&#36753;&#30340;&#20869;&#22312;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#25805;&#32437;&#28508;&#21464;&#37327;&#26469;&#30830;&#23450;&#35201;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#32447;&#24615;&#35821;&#20041;&#31639;&#26415;&#30340;&#26041;&#27861;&#22312;&#22270;&#20687;&#32534;&#36753;&#26041;&#38754;&#20855;&#26377;&#26576;&#20123;&#23616;&#38480;&#24615;&#65292;&#32780;&#21457;&#29616;&#38750;&#32447;&#24615;&#30340;&#35821;&#20041;&#36335;&#24452;&#25552;&#20379;&#20102;&#19981;&#21487;&#20132;&#25442;&#30340;&#32534;&#36753;&#65292;&#36825;&#22312;&#20197;&#19981;&#21516;&#30340;&#39034;&#24207;&#24212;&#29992;&#26102;&#19981;&#19968;&#33268;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#26354;&#32447;&#32534;&#36753;&#65288;DeCurvEd&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#28508;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20132;&#38169;&#30690;&#37327;&#22330;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#30001;&#20110;&#21487;&#20132;&#25442;&#24615;&#65292;&#22810;&#20010;&#23646;&#24615;&#30340;&#32534;&#36753;&#20165;&#21462;&#20915;&#20110;&#25968;&#37327;&#32780;&#19981;&#26159;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic editing of images is the fundamental goal of computer vision. Although deep learning methods, such as generative adversarial networks (GANs), are capable of producing high-quality images, they often do not have an inherent way of editing generated images semantically. Recent studies have investigated a way of manipulating the latent variable to determine the images to be generated. However, methods that assume linear semantic arithmetic have certain limitations in terms of the quality of image editing, whereas methods that discover nonlinear semantic pathways provide non-commutative editing, which is inconsistent when applied in different orders. This study proposes a novel method called deep curvilinear editing (DeCurvEd) to determine semantic commuting vector fields on the latent space. We theoretically demonstrate that owing to commutativity, the editing of multiple attributes depends only on the quantities and not on the order. Furthermore, we experimentally demonstrate th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#30340;&#20998;&#31867;&#22836;&#35757;&#32451;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#20998;&#31867;&#22836;&#20351;&#27169;&#22411;&#24615;&#33021;&#31283;&#23450;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.16771</link><description>&lt;p&gt;
&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#20351;&#20998;&#31867;&#22836;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16771
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#30340;&#20998;&#31867;&#22836;&#35757;&#32451;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#20998;&#31867;&#22836;&#20351;&#27169;&#22411;&#24615;&#33021;&#31283;&#23450;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33539;&#24335;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20043;&#21518;&#38468;&#21152;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#20998;&#31867;&#22836;&#65292;&#24182;&#24494;&#35843;&#25972;&#20010;&#27169;&#22411;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#20027;&#24178;&#27169;&#22411;&#23545;&#24615;&#33021;&#30340;&#36129;&#29486;&#24456;&#22823;&#65292;&#22240;&#27492;&#25105;&#20204;&#33258;&#28982;&#26399;&#26395;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20998;&#31867;&#22836;&#20063;&#33021;&#21463;&#30410;&#20110;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20027;&#24178;&#27169;&#22411;&#26368;&#21518;&#19968;&#23618;&#30340;&#36755;&#20986;&#65288;&#21363;&#20998;&#31867;&#22836;&#30340;&#36755;&#20837;&#65289;&#22312;&#24494;&#35843;&#26399;&#38388;&#20250;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#23548;&#33268;&#36890;&#24120;&#30340;&#22836;&#37096;&#21333;&#29420;&#39044;&#35757;&#32451;&#65288;LP-FT&#65289;&#22833;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#20351;&#20998;&#31867;&#22836;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#26367;&#25442;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22836;&#37096;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312; GLUE &#21644; SuperGLUE &#30340; 9 &#39033;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#32852;&#21512;&#39044;&#35757;&#32451;&#30340;&#20998;&#31867;&#22836;&#21487;&#20197;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#31034;&#33539;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#30340;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#24207;&#21015;&#65292;&#36880;&#27493;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24230;&#65292;&#20197;&#24110;&#21161;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#22312;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.10999</link><description>&lt;p&gt;
&#20219;&#21153;&#38454;&#27573;&#21270;&#65306;&#26469;&#33258;&#31034;&#33539;&#30340;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Phasing: Automated Curriculum Learning from Demonstrations. (arXiv:2210.10999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#31034;&#33539;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#30340;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#24207;&#21015;&#65292;&#36880;&#27493;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24230;&#65292;&#20197;&#24110;&#21161;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#22312;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#22495;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#36275;&#22815;&#30340;&#24341;&#23548;&#20449;&#21495;&#12290;&#35299;&#20915;&#27492;&#31867;&#39046;&#22495;&#30340;&#24120;&#35265;RL&#25216;&#26415;&#21253;&#25324;&#65288;1&#65289;&#20174;&#31034;&#33539;&#23398;&#20064;&#21644;&#65288;2&#65289;&#35838;&#31243;&#23398;&#20064;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35814;&#32454;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#34987;&#21516;&#26102;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#31034;&#33539;&#30340;&#21407;&#21017;&#24615;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#35813;&#30446;&#30340;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31034;&#33539;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#65288;&#27425;&#20248;&#65289;&#28436;&#31034;&#30340;&#36870;RL&#23450;&#20041;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21021;&#22987;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#30452;&#21040;&#30446;&#26631;&#20219;&#21153;&#65292;&#21516;&#26102;&#22312;&#27599;&#20010;&#38454;&#27573;&#36845;&#20195;&#20013;&#37325;&#26032;&#35843;&#25972;RL&#20195;&#29702;&#12290;&#32771;&#34385;&#20102;&#20004;&#31181;&#20998;&#38454;&#27573;&#26041;&#27861;&#65306;&#65288;1&#65289;&#36880;&#27493;&#22686;&#21152;RL&#20195;&#29702;&#22788;&#20110;&#25511;&#21046;&#19979;&#30340;&#26102;&#38388;&#27493;&#25968;&#30340;&#27604;&#20363;&#65292;&#20197;&#21450;&#65288;2&#65289;&#36880;&#27493;&#28120;&#27760;&#24341;&#23548;&#24615;&#20449;&#24687;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#35777;&#25910;&#25947;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying reinforcement learning (RL) to sparse reward domains is notoriously challenging due to insufficient guiding signals. Common RL techniques for addressing such domains include (1) learning from demonstrations and (2) curriculum learning. While these two approaches have been studied in detail, they have rarely been considered together. This paper aims to do so by introducing a principled task phasing approach that uses demonstrations to automatically generate a curriculum sequence. Using inverse RL from (suboptimal) demonstrations we define a simple initial task. Our task phasing approach then provides a framework to gradually increase the complexity of the task all the way to the target task, while retuning the RL agent in each phasing iteration. Two approaches for phasing are considered: (1) gradually increasing the proportion of time steps an RL agent is in control, and (2) phasing out a guiding informative reward function. We present conditions that guarantee the convergence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35889;&#26041;&#27861;SPORADIC&#65292;&#22312;&#20960;&#20046;&#32447;&#24615;&#31232;&#30095;&#24230;&#19979;&#30340;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#20013;&#21487;&#20197;&#24674;&#22797;&#36229;&#23436;&#22791;&#23383;&#20856;&#12290;</title><link>http://arxiv.org/abs/2210.10855</link><description>&lt;p&gt;
&#20960;&#20046;&#32447;&#24615;&#31232;&#30095;&#24230;&#19979;&#30340;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dictionary Learning for the Almost-Linear Sparsity Regime. (arXiv:2210.10855v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35889;&#26041;&#27861;SPORADIC&#65292;&#22312;&#20960;&#20046;&#32447;&#24615;&#31232;&#30095;&#24230;&#19979;&#30340;&#23383;&#20856;&#23398;&#20064;&#38382;&#39064;&#20013;&#21487;&#20197;&#24674;&#22797;&#36229;&#23436;&#22791;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#25351;&#30340;&#26159;&#20174;&#24418;&#22914;$\mathbf{y}_i = \mathbf{D}\mathbf{x}_i$&#30340;&#26679;&#26412;&#20013;&#24674;&#22797;&#19968;&#20010;&#30697;&#38453;$\mathbf{D} \in \mathbb{R}^{M \times K}$&#21644;$N$&#20010;$s$-&#31232;&#30095;&#21521;&#37327;$\mathbf{x}_i \in \mathbb{R}^{K}$&#30340;&#38382;&#39064;&#12290;&#22312;&#23383;&#20856;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#31232;&#30095;&#24230;&#32447;&#24615;&#22686;&#38271;&#21040;&#23610;&#23544;$M$&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;$x_i$&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#21807;&#19968;&#33021;&#22312;&#32447;&#24615;&#31232;&#30095;&#24230;&#33539;&#22260;&#20869;&#26377;&#20445;&#35777;&#25104;&#21151;&#30340;&#31639;&#27861;&#26159;&#40654;&#26364;&#20449;&#36182;&#21306;&#22495;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#27491;&#20132;&#23383;&#20856;&#65292;&#24182;&#19988;&#22522;&#20110;&#24179;&#26041;&#21644;&#23618;&#27425;&#30340;&#26041;&#27861;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#25165;&#33021;&#33719;&#24471;&#22312;$M$&#20013;&#34928;&#20943;&#30340;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SPORADIC&#65288;SPectral ORAcle DICtionary Learning&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#19968;&#31995;&#21015;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#25928;&#35889;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36275;&#22815;&#39640;&#30340;&#32500;&#24230;&#19979;&#65292;SPORADIC&#21487;&#20197;&#24674;&#22797;&#28385;&#36275;$K&gt;M$&#30340;&#36229;&#23436;&#22791;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dictionary learning, the problem of recovering a sparsely used matrix $\mathbf{D} \in \mathbb{R}^{M \times K}$ and $N$ $s$-sparse vectors $\mathbf{x}_i \in \mathbb{R}^{K}$ from samples of the form $\mathbf{y}_i = \mathbf{D}\mathbf{x}_i$, is of increasing importance to applications in signal processing and data science. When the dictionary is known, recovery of $\mathbf{x}_i$ is possible even for sparsity linear in dimension $M$, yet to date, the only algorithms which provably succeed in the linear sparsity regime are Riemannian trust-region methods, which are limited to orthogonal dictionaries, and methods based on the sum-of-squares hierarchy, which requires super-polynomial time in order to obtain an error which decays in $M$. In this work, we introduce SPORADIC (SPectral ORAcle DICtionary Learning), an efficient spectral method on family of reweighted covariance matrices. We prove that in high enough dimensions, SPORADIC can recover overcomplete ($K &gt; M$) dictionaries satisfying the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#26679;&#26412;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#21512;&#25104;&#20102;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#23433;&#20840;&#38598;&#30340;&#38646;&#38556;&#30861;&#20989;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.05596</link><description>&lt;p&gt;
&#22522;&#20110;&#24452;&#21521;&#22522;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#20559;&#20506;&#36924;&#36817;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Geometry of Radial Basis Neural Networks for Safety Biased Approximation of Unsafe Regions. (arXiv:2210.05596v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#26679;&#26412;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#21512;&#25104;&#20102;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#23433;&#20840;&#38598;&#30340;&#38646;&#38556;&#30861;&#20989;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23631;&#38556;&#20989;&#25968;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#26159;&#23454;&#26045;&#25511;&#21046;&#31995;&#32479;&#23433;&#20840;&#35268;&#33539;&#30340;&#19968;&#31181;&#25163;&#27573;&#12290;&#24403;&#19982;&#20984;&#20248;&#21270;&#31243;&#24207;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#25511;&#21046;&#20223;&#23556;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#21069;&#25552;&#20043;&#19968;&#26159;&#20808;&#39564;&#30693;&#35782;&#65292;&#21363;&#23545;&#23631;&#38556;&#20989;&#25968;&#26412;&#36523;&#65292;&#21363;&#23433;&#20840;&#38598;&#30340;&#20102;&#35299;&#12290;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#20013;&#65292;&#26412;&#22320;&#23433;&#20840;&#38598;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#22522;&#20110;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#26679;&#26412;&#27979;&#37327;&#65288;&#20363;&#22914;&#20174;&#23548;&#33322;&#24212;&#29992;&#20013;&#30340;&#24863;&#30693;&#25968;&#25454;&#20013;&#33719;&#21462;&#65289;&#32508;&#21512;&#25551;&#36848;&#23433;&#20840;&#38598;&#30340;&#38646;&#38556;&#30861;&#20989;&#25968;&#30340;&#21512;&#25104;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#21046;&#23450;&#20102;&#19968;&#20010;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#20445;&#35777;&#20102;&#20855;&#26377;&#29305;&#23450;&#32423;&#38598;&#23646;&#24615;&#30340;&#38646;&#38556;&#30861;&#20989;&#25968;&#30340;&#26500;&#24314;&#12290;&#28982;&#32780;&#65292;&#23427;&#27809;&#26377;&#25506;&#35752;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Barrier function-based inequality constraints are a means to enforce safety specifications for control systems. When used in conjunction with a convex optimization program, they provide a computationally efficient method to enforce safety for the general class of control-affine systems. One of the main assumptions when taking this approach is the a priori knowledge of the barrier function itself, i.e., knowledge of the safe set. In the context of navigation through unknown environments where the locally safe set evolves with time, such knowledge does not exist. This manuscript focuses on the synthesis of a zeroing barrier function characterizing the safe set based on safe and unsafe sample measurements, e.g., from perception data in navigation applications. Prior work formulated a supervised machine learning algorithm whose solution guaranteed the construction of a zeroing barrier function with specific level-set properties. However, it did not explore the geometry of the neural networ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#20998;&#23376;&#27169;&#22411;&#65292;&#21517;&#20026;Transformer-M&#65292;&#21487;&#20197;&#22788;&#29702;2D&#21644;3D&#26684;&#24335;&#30340;&#20998;&#23376;&#25968;&#25454;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2210.01765</link><description>&lt;p&gt;
&#19968;&#20010;Transformer&#27169;&#22411;&#21487;&#21516;&#26102;&#22788;&#29702;2D&#21644;3D&#20998;&#23376;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
One Transformer Can Understand Both 2D &amp; 3D Molecular Data. (arXiv:2210.01765v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#20998;&#23376;&#27169;&#22411;&#65292;&#21517;&#20026;Transformer-M&#65292;&#21487;&#20197;&#22788;&#29702;2D&#21644;3D&#26684;&#24335;&#30340;&#20998;&#23376;&#25968;&#25454;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#36890;&#24120;&#26377;&#21807;&#19968;&#26684;&#24335;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#19981;&#21516;&#65292;&#20998;&#23376;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#19981;&#21516;&#30340;&#21270;&#23398;&#20844;&#24335;&#36827;&#34892;&#34920;&#24449;&#12290;&#23545;&#20110;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#35774;&#35745;&#20102;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#26684;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#24471;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#20854;&#20182;&#25968;&#25454;&#26684;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21270;&#23398;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24212;&#33021;&#22815;&#22788;&#29702;&#36328;&#25968;&#25454;&#27169;&#24577;&#30340;&#20998;&#23376;&#20219;&#21153;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;Transformer&#30340;&#20998;&#23376;&#27169;&#22411;&#65292;&#31216;&#20026;Transformer-M&#65292;&#23427;&#21487;&#20197;&#23558;2D&#25110;3D&#26684;&#24335;&#30340;&#20998;&#23376;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#20351;&#29992;&#26631;&#20934;Transformer&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;Transformer-M&#24320;&#21457;&#20102;&#20004;&#20010;&#20998;&#31163;&#30340;&#36890;&#36947;&#26469;&#32534;&#30721;2D&#21644;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#32593;&#32476;&#27169;&#22359;&#20013;&#30340;&#21407;&#23376;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. Whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#35270;&#35282;&#20449;&#24687;&#34701;&#21512;&#26469;&#39044;&#27979;&#32929;&#39592;&#36817;&#31471;&#24378;&#24230;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MVAE)&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#21644;&#19987;&#23478;&#20043;&#31215;&#27169;&#22411;(PoE)&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#12290;&#30740;&#31350;&#37319;&#29992;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;(GWAS)&#36873;&#25321;&#21464;&#24322;&#20307;&#65292;&#25972;&#21512;WGS&#21644;DXA&#25104;&#20687;&#29305;&#24449;&#65292;&#24471;&#21040;&#20102;&#26368;&#22909;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.00674</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#35270;&#35282;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#35270;&#35282;&#20449;&#24687;&#34701;&#21512;&#20197;&#39044;&#27979;&#32929;&#39592;&#36817;&#31471;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Multi-view information fusion using multi-view variational autoencoders to predict proximal femoral strength. (arXiv:2210.00674v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#35270;&#35282;&#20449;&#24687;&#34701;&#21512;&#26469;&#39044;&#27979;&#32929;&#39592;&#36817;&#31471;&#24378;&#24230;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(MVAE)&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#21644;&#19987;&#23478;&#20043;&#31215;&#27169;&#22411;(PoE)&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#12290;&#30740;&#31350;&#37319;&#29992;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;(GWAS)&#36873;&#25321;&#21464;&#24322;&#20307;&#65292;&#25972;&#21512;WGS&#21644;DXA&#25104;&#20687;&#29305;&#24449;&#65292;&#24471;&#21040;&#20102;&#26368;&#22909;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#35270;&#35282;&#20449;&#24687;&#34701;&#21512;&#26469;&#39044;&#27979;&#32929;&#39592;&#36817;&#31471;&#24378;&#24230;&#12290;&#25105;&#20204;&#37319;&#29992;&#22810;&#35270;&#35282;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;MVAE&#65289;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#20043;&#31215;&#65288;PoE&#65289;&#27169;&#22411;&#36827;&#34892;&#22810;&#35270;&#35282;&#20449;&#24687;&#34701;&#21512;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;931&#21517;&#30007;&#24615;&#21463;&#35797;&#32773;&#32452;&#25104;&#30340;&#36335;&#26131;&#26031;&#23433;&#37027;&#39592;&#36136;&#30095;&#26494;&#30151;&#30740;&#31350;&#65288;LOS&#65289;&#38431;&#21015;&#20013;&#65292;&#21253;&#25324;345&#21517;&#38750;&#35028;&#32654;&#22269;&#20154;&#21644;586&#21517;&#30333;&#20154;&#12290;&#36890;&#36807;&#39640;&#26031;&#20998;&#24067;&#20056;&#31215;&#30340;&#35299;&#26512;&#35299;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#35757;&#32451;&#35774;&#35745;&#30340;MVAE-PoE&#27169;&#22411;&#20197;&#25191;&#34892;&#36890;&#29992;&#28508;&#22312;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#65288;GWAS&#65289;&#26469;&#36873;&#21462;&#27599;&#20010;&#32929;&#39592;&#36817;&#31471;&#24378;&#24230;&#30340;&#26368;&#20302;p&#20540;&#30340;256&#20010;&#36951;&#20256;&#21464;&#24322;&#20307;&#65292;&#24182;&#25972;&#21512;&#20102;&#20840;&#22522;&#22240;&#32452;&#24207;&#21015;&#65288;WGS&#65289;&#29305;&#24449;&#21644;DXA&#25104;&#20687;&#29305;&#24449;&#26469;&#39044;&#27979;&#32929;&#39592;&#36817;&#31471;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to design a deep learning-based model to predict proximal femoral strength using multi-view information fusion. Method: We developed new models using multi-view variational autoencoder (MVAE) for feature representation learning and a product of expert (PoE) model for multi-view information fusion. We applied the proposed models to an in-house Louisiana Osteoporosis Study (LOS) cohort with 931 male subjects, including 345 African Americans and 586 Caucasians. With an analytical solution of the product of Gaussian distribution, we adopted variational inference to train the designed MVAE-PoE model to perform common latent feature extraction. We performed genome-wide association studies (GWAS) to select 256 genetic variants with the lowest p-values for each proximal femoral strength and integrated whole genome sequence (WGS) features and DXA-derived imaging features to predict proximal femoral strength. Results: The best prediction model for fall fracture load was 
&lt;/p&gt;</description></item><item><title>FINDE&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21457;&#29616;&#21644;&#20445;&#25345;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#21464;&#37327;&#65292;&#26377;&#21161;&#20110;&#31185;&#23398;&#21457;&#29616;&#21644;&#26410;&#30693;&#31995;&#32479;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2210.00272</link><description>&lt;p&gt;
FINDE: &#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#30340;&#19981;&#21464;&#37327;&#21457;&#29616;&#19982;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities. (arXiv:2210.00272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00272
&lt;/p&gt;
&lt;p&gt;
FINDE&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21457;&#29616;&#21644;&#20445;&#25345;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#21464;&#37327;&#65292;&#26377;&#21161;&#20110;&#31185;&#23398;&#21457;&#29616;&#21644;&#26410;&#30693;&#31995;&#32479;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#21160;&#21147;&#23398;&#31995;&#32479;&#37117;&#19982;&#39318;&#27425;&#31215;&#20998;&#65288;&#20063;&#31216;&#20026;&#19981;&#21464;&#37327;&#65289;&#30456;&#20851;&#32852;&#65292;&#23427;&#20204;&#26159;&#38543;&#26102;&#38388;&#20445;&#25345;&#19981;&#21464;&#30340;&#37327;&#12290;&#39318;&#27425;&#31215;&#20998;&#30340;&#21457;&#29616;&#21644;&#29702;&#35299;&#26159;&#33258;&#28982;&#31185;&#23398;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#22522;&#26412;&#21644;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;&#39318;&#27425;&#31215;&#20998;&#28304;&#33258;&#31995;&#32479;&#33021;&#37327;&#12289;&#21160;&#37327;&#21644;&#36136;&#37327;&#30340;&#23432;&#24658;&#23450;&#24459;&#20197;&#21450;&#23545;&#29366;&#24577;&#30340;&#38480;&#21046;&#65307;&#36825;&#20123;&#36890;&#24120;&#19982;&#25511;&#21046;&#26041;&#31243;&#30340;&#29305;&#23450;&#20960;&#20309;&#32467;&#26500;&#30456;&#20851;&#12290;&#29616;&#26377;&#30340;&#35774;&#35745;&#29992;&#20110;&#30830;&#20445;&#36825;&#20123;&#39318;&#27425;&#31215;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21253;&#21547;&#20102;&#28508;&#22312;&#30340;&#32467;&#26500;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26410;&#30693;&#31995;&#32479;&#30340;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26500;&#20063;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#31185;&#23398;&#21457;&#29616;&#21644;&#26410;&#30693;&#31995;&#32479;&#24314;&#27169;&#65292;&#38656;&#35201;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#27425;&#31215;&#20998;&#20445;&#25345;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;FINDE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#25237;&#23556;&#26041;&#27861;&#65292;FINDE&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21457;&#29616;&#21644;&#20445;&#25345;&#39318;&#27425;&#31215;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FINDE&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#20960;&#20010;&#32463;&#20856;&#31034;&#20363;&#30340;&#28508;&#22312;&#21160;&#24577;&#21644;&#19981;&#21464;&#37327;&#65292;&#21253;&#25324;Duffing&#21644;van der Pol&#25391;&#33633;&#22120;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#39318;&#27425;&#31215;&#20998;&#21457;&#29616;&#21644;&#21160;&#24577;&#31995;&#32479;&#28145;&#24230;&#23398;&#20064;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world dynamical systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time. The discovery and understanding of first integrals are fundamental and important topics both in the natural sciences and in industrial applications. First integrals arise from the conservation laws of system energy, momentum, and mass, and from constraints on states; these are typically related to specific geometric structures of the governing equations. Existing neural networks designed to ensure such first integrals have shown excellent accuracy in modeling from data. However, these models incorporate the underlying structures, and in most situations where neural networks learn unknown systems, these structures are also unknown. This limitation needs to be overcome for scientific discovery and modeling of unknown systems. To this end, we propose first integral-preserving neural differential equation (FINDE). By leveraging the proje
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30830;&#20445;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#32852;&#37030;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#25308;&#21344;&#24237;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14547</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Secure Federated Learning Framework for Residential Short Term Load Forecasting. (arXiv:2209.14547v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30830;&#20445;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#32852;&#37030;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#25308;&#21344;&#24237;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30005;&#34920;&#26159;&#20934;&#30830;&#38656;&#27714;&#39044;&#27979;&#30340;&#20851;&#38190;&#65292;&#20294;&#30001;&#20110;&#28040;&#36153;&#32773;&#38544;&#31169;&#21644;&#25968;&#25454;&#27844;&#38706;&#31561;&#38382;&#39064;&#65292;&#23384;&#22312;&#33509;&#24178;&#32570;&#28857;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#25506;&#32034;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#26292;&#38706;&#31169;&#26377;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#29992;&#20110;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#12290;&#23613;&#31649;FL&#20855;&#26377;&#20248;&#28857;&#65292;&#20294;&#26631;&#20934;FL&#20173;&#28982;&#26131;&#21463;&#21517;&#20026;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#38590;&#20197;&#22788;&#29702;&#30340;&#32593;&#32476;&#23041;&#32961;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#26159;&#30001;&#26377;&#32570;&#38519;&#21644;/&#25110;&#24694;&#24847;&#23458;&#25143;&#21457;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#32852;&#37030;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#25308;&#21344;&#24237;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#23433;&#20840;&#30340;FL-based&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30830;&#20445;&#20102;&#20010;&#20154;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;FL&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#23433;&#20840;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;&#21033;&#29992;&#20102;&#36890;&#36807;Sign Stochastic Gradient Descent&#65288;SignSGD&#65289;&#31639;&#27861;&#36827;&#34892;&#26799;&#24230;&#37327;&#21270;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart meter measurements, though critical for accurate demand forecasting, face several drawbacks including consumers' privacy, data breach issues, to name a few. Recent literature has explored Federated Learning (FL) as a promising privacy-preserving machine learning alternative which enables collaborative learning of a model without exposing private raw data for short term load forecasting. Despite its virtue, standard FL is still vulnerable to an intractable cyber threat known as Byzantine attack carried out by faulty and/or malicious clients. Therefore, to improve the robustness of federated short-term load forecasting against Byzantine threats, we develop a state-of-the-art differentially private secured FL-based framework that ensures the privacy of the individual smart meter's data while protect the security of FL models and architecture. Our proposed framework leverages the idea of gradient quantization through the Sign Stochastic Gradient Descent (SignSGD) algorithm, where the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2209.13020</link><description>&lt;p&gt;
&#27861;&#24459;&#24341;&#23548;&#20195;&#30721;&#65306;&#19968;&#31181;&#27861;&#24459;&#20449;&#24687;&#23398;&#26041;&#27861;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20445;&#25345;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13020
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#25105;&#20204;&#26080;&#27861;&#21487;&#38752;&#22320;&#25351;&#23450;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#65292;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#12290;&#21046;&#23450;&#27861;&#24459;&#21644;&#35299;&#37322;&#27861;&#24459;&#26500;&#25104;&#20102;&#19968;&#31181;&#35745;&#31639;&#24341;&#25806;&#65292;&#23558;&#19981;&#36879;&#26126;&#30340;&#20154;&#31867;&#20215;&#20540;&#36716;&#21270;&#20026;&#26131;&#35835;&#30340;&#25351;&#20196;&#12290; &#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#26159;&#23884;&#20837;&#20102;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35758;&#31243;&#12290;&#31867;&#20284;&#20110;&#21512;&#21516;&#24403;&#20107;&#20154;&#26080;&#27861;&#39044;&#35265;&#20182;&#20204;&#26410;&#26469;&#20851;&#31995;&#30340;&#27599;&#20010;&#28508;&#22312;&#21464;&#25968;&#65292;&#31435;&#27861;&#32773;&#26080;&#27861;&#39044;&#27979;&#20854;&#25552;&#20986;&#30340;&#27861;&#26696;&#23558;&#36866;&#29992;&#30340;&#25152;&#26377;&#24773;&#20917;&#65292;&#25105;&#20204;&#26080;&#27861;&#25552;&#21069;&#26126;&#30830;&#35268;&#21017;&#65292;&#20197;&#21487;&#38752;&#22320;&#24341;&#23548;&#33391;&#22909;&#30340;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#12290;&#27861;&#24459;&#29702;&#35770;&#21644;&#23454;&#36341;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20123;&#35268;&#23450;&#38382;&#39064;&#12290;&#19982;&#27861;&#24459;&#26356;&#20026;&#26222;&#36890;&#30340;&#29992;&#36884;&#65288;&#20363;&#22914;&#36890;&#36807;&#21046;&#35009;&#23041;&#32961;&#26469;&#38459;&#27490;&#19981;&#33391;&#34892;&#20026;&#65289;&#30456;&#21453;&#65292;&#27861;&#24459;&#20316;&#20026;&#19968;&#31181;&#34920;&#36798;&#20154;&#31867;&#27807;&#36890;&#30446;&#26631;&#21644;&#20215;&#20540;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#20195;&#30721;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27861;&#24459;&#20449;&#24687;&#23398;&#26159;&#35745;&#31639;&#35268;&#21017;&#21644;&#31995;&#32479;&#29992;&#20110;&#34920;&#31034;&#65292;&#20998;&#26512;&#21644;&#25805;&#20316;&#27861;&#24459;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#19968;&#31181;&#36879;&#26126;&#65292;&#36127;&#36131;&#65292;&#28789;&#27963;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32534;&#30721;&#26041;&#27861;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#29305;&#28857;&#23545;&#19981;&#21516;&#30340;&#26679;&#26412;&#36827;&#34892;&#32534;&#30721;&#29983;&#25104;&#19981;&#21516;&#30340;&#32534;&#30721;&#21521;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.11056</link><description>&lt;p&gt;
&#22686;&#24378;&#32534;&#30721;&#65306;&#36890;&#36807;&#23545;&#35757;&#32451;&#26631;&#31614;&#36827;&#34892;&#32534;&#30721;&#30340;&#26032;&#22411;&#19981;&#24179;&#34913;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancement Encoding: A Novel Imbalanced Classification Approach via Encoding the Training Labels. (arXiv:2208.11056v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#32534;&#30721;&#26041;&#27861;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#29305;&#28857;&#23545;&#19981;&#21516;&#30340;&#26679;&#26412;&#36827;&#34892;&#32534;&#30721;&#29983;&#25104;&#19981;&#21516;&#30340;&#32534;&#30721;&#21521;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20063;&#31216;&#20026;&#38271;&#23614;&#20998;&#24067;&#65292;&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#24120;&#35265;&#38382;&#39064;&#12290;&#22914;&#26524;&#20986;&#29616;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#37027;&#20040;&#23569;&#25968;&#31867;&#25968;&#25454;&#23558;&#34987;&#22810;&#25968;&#31867;&#28153;&#27809;&#65292;&#36825;&#23545;&#25968;&#25454;&#31185;&#23398;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65306;&#19968;&#20123;&#20154;&#20351;&#25968;&#25454;&#38598;&#24179;&#34913;&#65288;SMOTE&#65289;&#65292;&#19968;&#20123;&#20154;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#65288;Focal Loss&#65289;&#65292;&#29978;&#33267;&#26377;&#20154;&#27880;&#24847;&#21040;&#26631;&#31614;&#20284;&#20046;&#24433;&#21709;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#65288;Yang&#21644;Xu&#12290;&#37325;&#22609;&#26631;&#35760;&#30340;&#20215;&#20540;&#65292;&#20197;&#25552;&#39640;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#12290;&#22312;NeurIPS 2020&#20013;&#65289;&#65292;&#20294;&#36824;&#27809;&#26377;&#20154;&#25913;&#21464;&#25968;&#25454;&#26631;&#31614;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;&#30446;&#21069;&#65292;&#32534;&#30721;&#26631;&#31614;&#30340;&#26368;&#27969;&#34892;&#25216;&#26415;&#26159;&#19968;&#20301;&#32534;&#30721;&#65292;&#22240;&#20026;&#23427;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#23545;&#20110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#65292;&#23427;&#24182;&#19981;&#26159;&#19968;&#20010;&#22909;&#36873;&#25321;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#23558;&#24179;&#31561;&#23545;&#24453;&#22810;&#25968;&#21644;&#23569;&#25968;&#26679;&#26412;&#12290;&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#22686;&#24378;&#32534;&#30721;&#26041;&#27861;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#26631;&#31614;&#30340;&#20540;&#20026;&#22810;&#25968;&#21644;&#23569;&#25968;&#26679;&#26412;&#29983;&#25104;&#19981;&#21516;&#30340;&#32534;&#30721;&#21521;&#37327;&#65292;&#36825;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#26377;&#25928;&#24179;&#34913;&#19981;&#21516;&#26679;&#26412;&#30340;&#36129;&#29486;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;F1&#24471;&#20998;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance, which is also called long-tailed distribution, is a common problem in classification tasks based on machine learning. If it happens, the minority data will be overwhelmed by the majority, which presents quite a challenge for data science. To address the class imbalance problem, researchers have proposed lots of methods: some people make the data set balanced (SMOTE), some others refine the loss function (Focal Loss), and even someone has noticed the value of labels influences class-imbalanced learning (Yang and Xu. Rethinking the value of labels for improving class-imbalanced learning. In NeurIPS 2020), but no one changes the way to encode the labels of data yet. Nowadays, the most prevailing technique to encode labels is the one-hot encoding due to its nice performance in the general situation. However, it is not a good choice for imbalanced data, because the classifier will treat majority and minority samples equally. In this paper, we innovatively propose the enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#36712;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#29289;&#29702;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25512;&#23548;&#26410;&#27979;&#37327;&#29289;&#20307;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#36712;&#36947;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.08993</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#36712;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning in Orbit Estimation: a Survey. (arXiv:2207.08993v3 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#36712;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#29289;&#29702;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25512;&#23548;&#26410;&#27979;&#37327;&#29289;&#20307;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#36712;&#36947;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#19978;&#19990;&#32426;50&#24180;&#20195;&#21457;&#23556;&#20102;&#31532;&#19968;&#39063;&#20154;&#36896;&#21355;&#26143;&#20197;&#26469;&#65292;&#36712;&#36947;&#19978;&#30340;&#31354;&#38388;&#29289;&#20307;&#25968;&#37327;&#25345;&#32493;&#22686;&#21152;&#12290;&#30446;&#21069;&#20272;&#35745;&#22320;&#29699;&#19978;&#26377;&#19968;&#30334;&#19975;&#20010;&#22823;&#23567;&#36229;&#36807;&#19968;&#21400;&#31859;&#30340;&#29289;&#20307;&#27491;&#22312;&#32469;&#22320;&#36816;&#34892;&#65292;&#20854;&#20013;&#21482;&#26377;&#19977;&#19975;&#20010;&#22823;&#23567;&#36229;&#36807;&#21313;&#21400;&#31859;&#30340;&#29289;&#20307;&#34987;&#36319;&#36394;&#12290;&#20026;&#20102;&#36991;&#20813;&#30896;&#25758;&#38142;&#21453;&#24212;&#65292;&#21363;&#22522;&#26031;&#21202;&#30151;&#20505;&#32676;&#30340;&#21457;&#29983;&#65292;&#24517;&#39035;&#20934;&#30830;&#22320;&#36319;&#36394;&#21644;&#39044;&#27979;&#30862;&#29255;&#21644;&#21355;&#26143;&#30340;&#36712;&#36947;&#12290;&#24403;&#21069;&#30340;&#36817;&#20284;&#29289;&#29702;&#26041;&#27861;&#23545;&#20110;&#19971;&#22825;&#30340;&#39044;&#27979;&#35823;&#24046;&#22312;&#20844;&#37324;&#32423;&#21035;&#65292;&#36825;&#22312;&#32771;&#34385;&#36890;&#24120;&#23567;&#20110;&#19968;&#31859;&#30340;&#31354;&#38388;&#30862;&#29255;&#26102;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#36825;&#31181;&#22833;&#36133;&#36890;&#24120;&#26159;&#30001;&#20110;&#22312;&#36712;&#36947;&#36215;&#22987;&#28857;&#38468;&#36817;&#30340;&#31354;&#38388;&#29289;&#20307;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#22823;&#27668;&#38459;&#21147;&#65289;&#30340;&#39044;&#27979;&#35823;&#24046;&#20197;&#21450;&#31354;&#38388;&#29289;&#20307;&#30340;&#26410;&#30693;&#29305;&#24449;&#65288;&#22914;&#36136;&#37327;&#25110;&#20960;&#20309;&#24418;&#29366;&#65289;&#25152;&#33268;&#12290;&#25805;&#20316;&#21592;&#21487;&#20197;&#36890;&#36807;&#25512;&#23548;&#26410;&#27979;&#37327;&#29289;&#20307;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#36712;&#36947;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the late 1950s, when the first artificial satellite was launched, the number of Resident Space Objects has steadily increased. It is estimated that around one million objects larger than one cm are currently orbiting the Earth, with only thirty thousand larger than ten cm being tracked. To avert a chain reaction of collisions, known as Kessler Syndrome, it is essential to accurately track and predict debris and satellites' orbits. Current approximate physics-based methods have errors in the order of kilometers for seven-day predictions, which is insufficient when considering space debris, typically with less than one meter. This failure is usually due to uncertainty around the state of the space object at the beginning of the trajectory, forecasting errors in environmental conditions such as atmospheric drag, and unknown characteristics such as the mass or geometry of the space object. Operators can enhance Orbit Prediction accuracy by deriving unmeasured objects' characteristics
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.13508</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#19968;&#20221;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#20986;&#33394;&#24615;&#33021;&#30340;&#26041;&#24335;&#24182;&#19981;&#38656;&#35201;&#22826;&#38271;&#26102;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#20016;&#23500;&#65292;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#21644;&#38656;&#35201;&#20445;&#35777;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#25968;&#25454;&#37327;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#25110;&#32622;&#25442;&#36824;&#26159;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#29992;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#30456;&#20851;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#19981;&#21516;&#21464;&#20307;&#30340;&#25928;&#29575;&#23558;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#20013;&#24515;&#37096;&#20998;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#23558;&#35780;&#20272;&#19981;&#21516;&#30340;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#27599;&#20010;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#32593;&#32476;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#20840;&#23610;&#24230;&#22270;&#20687;&#37329;&#23383;&#22612;&#65292;&#35299;&#20915;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#34701;&#21512;&#20013;&#26410;&#27880;&#24847;&#21040;&#30340;&#35821;&#20041;&#24046;&#36317;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.05782</link><description>&lt;p&gt;
DSCA&#65306;&#21452;&#27969;&#32593;&#32476;&#19982;&#20840;&#23610;&#24230;&#22270;&#20687;&#37329;&#23383;&#22612;&#19978;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#30284;&#30151;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis. (arXiv:2206.05782v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#32593;&#32476;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#20840;&#23610;&#24230;&#22270;&#20687;&#37329;&#23383;&#22612;&#65292;&#35299;&#20915;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#34701;&#21512;&#20013;&#26410;&#27880;&#24847;&#21040;&#30340;&#35821;&#20041;&#24046;&#36317;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21315;&#20159;&#32423;&#21035;&#30340;&#20840;&#23610;&#24230;&#22270;&#20687;&#20013;&#36827;&#34892;&#30284;&#30151;&#39044;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20840;&#23610;&#24230;&#22270;&#20687;&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20351;&#29992;&#22270;&#20687;&#37329;&#23383;&#22612;&#32780;&#38750;&#21333;&#19968;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#20204;&#20173;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#34701;&#21512;&#20013;&#26410;&#27880;&#24847;&#21040;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSCA&#30340;&#26041;&#27861;&#65292;&#21363;&#21452;&#27969;&#32593;&#32476;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#20004;&#20010;&#23376;&#27969;&#26469;&#22788;&#29702;&#20004;&#31181;&#20998;&#36776;&#29575;&#30340;WSI&#34917;&#19969;&#65292;&#20854;&#20013;&#39640;&#20998;&#36776;&#29575;&#27969;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#27491;&#26041;&#24418;&#27744;&#26469;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#26469;&#27491;&#30830;&#22788;&#29702;&#21452;&#27969;&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;DSCA&#65292;&#20849;&#35745;1,911&#21517;&#24739;&#32773;&#30340;3,101&#20010;WSI&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a challenging task. To further enhance WSI visual representations, existing methods have explored image pyramids, instead of single-resolution images, in WSIs. In spite of this, they still face two major problems: high computational cost and the unnoticed semantical gap in multi-resolution feature fusion. To tackle these problems, this paper proposes to efficiently exploit WSI pyramids from a new perspective, the dual-stream network with cross-attention (DSCA). Our key idea is to utilize two sub-streams to process the WSI patches with two resolutions, where a square pooling is devised in a high-resolution stream to significantly reduce computational costs, and a cross-attention-based method is proposed to properly handle the fusion of dual-stream features. We validate our DSCA on three publicly-available datasets with a total number of 3,101 WSIs from 1,911 patients. Our experiments and ablation studies verify 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39057;&#29575;&#27880;&#24847;&#21147;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21033;&#29992;&#39057;&#29575;&#20449;&#24687;&#25552;&#39640;&#20102;&#33041;&#30251;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#28040;&#36153;&#32423;RGB&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#26032;&#30740;&#31350;&#26368;&#39640;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.10997</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#27880;&#24847;&#21147;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#33041;&#30251;
&lt;/p&gt;
&lt;p&gt;
Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks. (arXiv:2204.10997v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39057;&#29575;&#27880;&#24847;&#21147;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21033;&#29992;&#39057;&#29575;&#20449;&#24687;&#25552;&#39640;&#20102;&#33041;&#30251;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#28040;&#36153;&#32423;RGB&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#26032;&#30740;&#31350;&#26368;&#39640;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30251;(CP)&#30340;&#26089;&#26399;&#35786;&#26029;&#21644;&#24178;&#39044;&#26159;&#27835;&#30103;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35774;&#35745;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#39044;&#27979;CP&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#21457;&#29616;CP&#23156;&#20799;&#30340;&#20154;&#20307;&#36816;&#21160;&#39057;&#29575;&#19982;&#20581;&#24247;&#32452;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#23156;&#20799;&#36816;&#21160;&#30340;&#39057;&#29575;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#29575;&#27880;&#24847;&#21147;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#22312;&#20004;&#20010;&#28040;&#36153;&#32423;RGB&#35270;&#39057;&#25968;&#25454;&#38598;(MINI-RGBD &#21644; RVI-38&#25968;&#25454;&#38598;)&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#39057;&#29575;&#27880;&#24847;&#21147;&#27169;&#22359;&#26377;&#21161;&#20110;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#21644;&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39057;&#29575;&#20998;&#26742;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#30041;&#20154;&#20307;&#20851;&#33410;&#20301;&#32622;&#25968;&#25454;&#30340;&#20851;&#38190;&#39057;&#29575;&#65292;&#21516;&#26102;&#36807;&#28388;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#39044;&#27979;&#24615;&#33021;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36798;&#21040;&#20102;&#26368;&#26032;&#30740;&#31350;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis and intervention are clinically considered the paramount part of treating cerebral palsy (CP), so it is essential to design an efficient and interpretable automatic prediction system for CP. We highlight a significant difference between CP infants' frequency of human movement and that of the healthy group, which improves prediction performance. However, the existing deep learning-based methods did not use the frequency information of infants' movement for CP prediction. This paper proposes a frequency attention informed graph convolutional network and validates it on two consumer-grade RGB video datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency attention module aids in improving both classification performance and system interpretability. In addition, we design a frequency-binning method that retains the critical frequency of the human joint position data while filtering the noise. Our prediction performance achieves state-of-the-art research on bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#24066;&#27835;&#29702;&#20013;&#23621;&#27665;&#20247;&#21253;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#20351;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#19981;&#20877;&#25104;&#20026;&#22478;&#24066;&#27835;&#29702;&#19979;&#28216;&#35299;&#20915;&#20107;&#20214;&#36895;&#24230;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#26681;&#28304;&#12290;</title><link>http://arxiv.org/abs/2204.08620</link><description>&lt;p&gt;
&#23621;&#27665;&#20247;&#21253;&#20013;&#31354;&#38388;&#27424;&#25253;&#21578;&#24046;&#24322;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing. (arXiv:2204.08620v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#24066;&#27835;&#29702;&#20013;&#23621;&#27665;&#20247;&#21253;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#20351;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#19981;&#20877;&#25104;&#20026;&#22478;&#24066;&#27835;&#29702;&#19979;&#28216;&#35299;&#20915;&#20107;&#20214;&#36895;&#24230;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#26681;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22478;&#24066;&#27835;&#29702;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20247;&#21253;(&#8220;&#21327;&#21516;&#29983;&#20135;&#8221;)&#65292;&#20197;&#35782;&#21035; downed trees &#21644; power lines &#31561;&#38382;&#39064;&#12290;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#23621;&#27665;&#19981;&#20197;&#30456;&#21516;&#30340;&#36895;&#29575;&#25253;&#21578;&#38382;&#39064;&#65292;&#19981;&#21516;&#30340;&#25253;&#21578;&#24322;&#36136;&#24615;&#30452;&#25509;&#36716;&#21270;&#20026;&#19979;&#28216;&#22312;&#35299;&#20915;&#20107;&#20214;&#30340;&#36895;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#27979;&#37327;&#36825;&#26679;&#30340;&#27424;&#25253;&#21578;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#32479;&#35745;&#20219;&#21153;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#25105;&#20204;&#19981;&#33021;&#35266;&#23519;&#21040;&#27809;&#26377;&#34987;&#25253;&#21578;&#30340;&#20107;&#20214;&#25110;&#25253;&#21578;&#20107;&#20214;&#31532;&#19968;&#27425;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#19981;&#33021;&#21333;&#32431;&#22320;&#21306;&#20998;&#20302;&#25253;&#36947;&#29575;&#21644;&#20302;&#22522;&#20934;&#30495;&#23454;&#20107;&#20214;&#29575;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;(&#24322;&#36136;&#30340;)&#25253;&#36947;&#29575;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#22312;&#30456;&#21516;&#20107;&#20214;&#30340; $\textit{duplicate}$ &#25253;&#21578;&#20013;&#30340;&#27604;&#29575;&#21487;&#20197;&#21033;&#29992;&#26469;&#28040;&#38500;&#25253;&#21578;&#29575;&#38543;&#20107;&#20214;&#21457;&#29983;&#32780;&#21457;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20511;&#21161;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#26631;&#20934;&#30340;&#27850;&#26494;&#29575;&#20272;&#35745;&#20219;&#21153;&#65292;&#23613;&#31649;&#26631;&#39064;&#26377;&#24456;&#22810;&#25216;&#26415;&#26415;&#35821;&#65292;&#20294;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#20247;&#21253;&#26469;&#24110;&#21161;&#22478;&#24066;&#27835;&#29702;&#65292;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#22914;&#20309;&#23548;&#33268;&#38382;&#39064;&#35299;&#20915;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#25968;&#25454;&#12290;&#20182;&#20204;&#21033;&#29992;&#22810;&#27425;&#25253;&#21578;&#30340;&#20107;&#20214;&#21487;&#20197;&#24110;&#21161;&#21306;&#20998;&#20107;&#20214;&#26159;&#21542;&#21457;&#29983;&#20197;&#21450;&#20854;&#25253;&#36947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern city governance relies heavily on crowdsourcing ("co-production") to identify problems such as downed trees and power lines. A major concern is that residents do not report problems at the same rates, with reporting heterogeneity directly translating to downstream disparities in how quickly incidents can be addressed. Measuring such under-reporting is a difficult statistical task, as, by definition, we do not observe incidents that are not reported or when reported incidents first occurred. Thus, low reporting rates and low ground-truth incident rates cannot be naively distinguished. We develop a method to identify (heterogeneous) reporting rates, without using external ground truth data. Our insight is that rates on $\textit{duplicate}$ reports about the same incident can be leveraged to disambiguate whether an incident has occurred with its reporting rate once it has occurred. Using this idea, we reduce the question to a standard Poisson rate estimation task -- even though the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2204.08504</link><description>&lt;p&gt;
CGC: &#23545;&#27604;&#22270;&#32858;&#31867;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22270;&#32858;&#31867;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25506;&#35752;&#22312;&#32593;&#32476;&#25968;&#25454;&#20013;&#21457;&#29616;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20197;&#21450;&#23545;&#23427;&#20204;&#36827;&#34892;&#31038;&#21306;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;CGC&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#20102;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#22270;&#24418;&#25299;&#25169;&#12290;&#22312;&#21508;&#20010;&#30495;&#23454;&#22330;&#26223;&#21644;&#21512;&#25104;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#23545;CGC&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21160;&#24577;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given entities and their interactions in the web data, which may have occurred at different time, how can we find communities of entities and track their evolution? In this paper, we approach this important task from graph clustering perspective. Recently, state-of-the-art clustering performance in various domains has been achieved by deep clustering methods. Especially, deep graph clustering (DGC) methods have successfully extended deep clustering to graph-structured data by learning node representations and cluster assignments in a joint optimization framework. Despite some differences in modeling choices (e.g., encoder architectures), existing DGC methods are mainly based on autoencoders and use the same clustering objective with relatively minor adaptations. Also, while many real-world graphs are dynamic, previous DGC methods considered only static graphs. In this work, we develop CGC, a novel end-to-end framework for graph clustering, which fundamentally differs from existing meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#26679;&#21270;&#21040;&#31070;&#32463;&#20803;&#26469;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#65292;&#26500;&#24314;&#20986;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#33258;&#36523;&#28608;&#27963;&#20989;&#25968;&#24555;&#36895;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#32988;&#36807;&#20256;&#32479;&#30340;&#21516;&#26500;&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20248;&#65292;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#24615;&#20026;&#21160;&#24577;&#31995;&#32479;&#36873;&#25321;&#22810;&#26679;&#24615;&#32780;&#38750;&#22343;&#21248;&#24615;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26679;&#24615;&#22312;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2204.04348</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#33021;&#22815;&#25552;&#39640;&#29289;&#29702;&#23398;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuronal diversity can improve machine learning for physics and beyond. (arXiv:2204.04348v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#26679;&#21270;&#21040;&#31070;&#32463;&#20803;&#26469;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#65292;&#26500;&#24314;&#20986;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#33258;&#36523;&#28608;&#27963;&#20989;&#25968;&#24555;&#36895;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#32988;&#36807;&#20256;&#32479;&#30340;&#21516;&#26500;&#31070;&#32463;&#20803;&#32593;&#32476;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20248;&#65292;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#24615;&#20026;&#21160;&#24577;&#31995;&#32479;&#36873;&#25321;&#22810;&#26679;&#24615;&#32780;&#38750;&#22343;&#21248;&#24615;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26679;&#24615;&#22312;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#34920;&#29616;&#20986;&#22810;&#26679;&#24615;&#30340;&#20248;&#28857;&#65292;&#20294;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#36890;&#24120;&#26159;&#30001;&#21516;&#26500;&#31070;&#32463;&#20803;&#26500;&#25104;&#30340;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#24314;&#31435;&#36215;&#33021;&#22815;&#23398;&#20064;&#33258;&#36523;&#28608;&#27963;&#20989;&#25968;&#12289;&#24555;&#36895;&#22810;&#26679;&#21270;&#24182;&#19988;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#32988;&#36807;&#21516;&#26500;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23376;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#31070;&#32463;&#20803;&#65292;&#32780;&#20803;&#23398;&#20064;&#23588;&#20854;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#21709;&#24212;&#38598;&#21512;&#12290;&#20363;&#23376;&#21253;&#25324;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#25968;&#23383;&#21644;&#39044;&#27979;&#19968;&#20010; van der Pol &#25391;&#33633;&#22120;&#21644;&#19968;&#31181;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340; Hamiltonian &#31070;&#32463;&#32593;&#32476;&#23398;&#20064; H&#233;nond-Heiles &#36712;&#36947;&#12290;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#24615;&#20026;&#21160;&#24577;&#31995;&#32479;&#36873;&#25321;&#22810;&#26679;&#24615;&#32780;&#38750;&#22343;&#21248;&#24615;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#24182;&#38416;&#26126;&#20102;&#22810;&#26679;&#24615;&#22312;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diversity conveys advantages in nature, yet homogeneous neurons typically comprise the layers of artificial neural networks. Here we construct neural networks from neurons that learn their own activation functions, quickly diversify, and subsequently outperform their homogeneous counterparts on image classification and nonlinear regression tasks. Sub-networks instantiate the neurons, which meta-learn especially efficient sets of nonlinear responses. Examples include conventional neural networks classifying digits and forecasting a van der Pol oscillator and a physics-informed Hamiltonian neural network learning H\'enon-Heiles orbits. Such learned diversity provides examples of dynamical systems selecting diversity over uniformity and elucidates the role of diversity in natural and artificial systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.11740</link><description>&lt;p&gt;
&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#23545;&#20851;&#38190;&#26399;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#23454;&#29616;&#31361;&#35302;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#12290;&#65288;arXiv: 2203.11740v12 [cs.NE] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#26143;&#24418;&#32454;&#32990;&#20316;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31361;&#35302;&#30340;&#31454;&#20105;&#21644;&#24378;&#24230;&#24179;&#34913;&#23454;&#29616;&#29616;&#26377;&#21644;&#35760;&#24518;&#24615;&#30340;&#22823;&#33041;&#21487;&#22609;&#24615;&#21644;&#31361;&#35302;&#24418;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#20851;&#38190;&#26399;&#30456;&#20851;&#30340;&#31070;&#32463;&#32010;&#20081;&#21644;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#23545;&#31361;&#35302;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#31361;&#35302;&#20849;&#20139;&#36830;&#25509;&#26435;&#37325;&#20043;&#22806;&#65292;PNN&#36824;&#21253;&#25324;&#31361;&#35302;&#26377;&#25928;&#33539;&#22260;&#30340;&#26435;&#37325;[14-25]&#12290;PNN&#32771;&#34385;&#31361;&#35302;&#24378;&#24230;&#24179;&#34913;&#22312;&#31361;&#35302;&#21534;&#22124;&#30340;&#21160;&#24577;&#21644;&#38271;&#24230;&#24120;&#25968;&#20043;&#21644;&#30340;&#38745;&#24577;&#20013;[14]&#65292;&#24182;&#21253;&#21547;&#20102;&#40060;&#32676;&#34892;&#20026;&#30340;&#20808;&#23548;&#34892;&#20026;&#12290;&#31361;&#35302;&#24418;&#25104;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#20250;&#25233;&#21046;&#26641;&#31361;&#29983;&#25104;[15]&#12290;&#31867;&#20284;&#20110;Spring Boot&#20013;&#30340;&#24378;&#21046;&#38887;&#24615;&#65292;&#21453;&#21521;&#22238;&#36335;&#30340;&#35760;&#24518;&#25345;&#20037;&#24230;&#26799;&#24230;&#20063;&#23384;&#22312;&#12290;&#30456;&#23545;&#36739;&#22909;&#21644;&#36739;&#24046;&#30340;&#26799;&#24230;&#20449;&#24687;&#23384;&#20648;&#22312;&#31867;&#20284;&#20110;&#33041;&#35126;&#30340;&#35760;&#24518;&#30165;&#36857;&#32454;&#32990;&#20013;&#65292;&#22312;&#21453;&#21521;&#22238;&#36335;&#30340;&#31361;&#35302;&#24418;&#25104;&#20013;&#12290;&#20105;&#35758;&#35748;&#20026;&#20154;&#31867;&#28023;&#39532;&#31070;&#32463;&#20803;&#30340;&#20877;&#29983;&#33021;&#21147;&#26159;&#21542;&#25345;&#32493;&#21040;&#32769;&#24180;&#65292;&#24182;&#21487;&#33021;&#22312;&#21518;&#26399;&#36845;&#20195;&#20013;&#24418;&#25104;&#26032;&#30340;&#26356;&#38271;&#30340;&#22238;&#36335;[17,18]&#12290;&#20851;&#38381;&#20851;&#38190;&#26399;&#20250;&#23548;&#33268;&#31070;&#32463;&#32010;&#20081;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;[19]&#12290;&#32771;&#34385;&#21040;&#36127;&#38754;&#21644;&#27491;&#38754;&#35760;&#24518;&#30340;&#25345;&#20037;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#28608;&#27963;&#31361;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.08147</link><description>&lt;p&gt;
&#22522;&#20110;&#28023;&#32501;&#27602;&#21270;&#30340;&#33021;&#32791;&#24310;&#36831;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Latency Attacks via Sponge Poisoning. (arXiv:2203.08147v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#26679;&#26412;&#26159;&#22312;&#27979;&#35797;&#26102;&#31934;&#24515;&#20248;&#21270;&#30340;&#36755;&#20837;&#65292;&#21487;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#26102;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#28023;&#32501;&#26679;&#26412;&#20063;&#21487;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#28023;&#32501;&#27602;&#21270;&#30340;&#25915;&#20987;&#27880;&#20837;&#21040;&#35757;&#32451;&#20013;&#12290;&#35813;&#25915;&#20987;&#20801;&#35768;&#22312;&#27599;&#20010;&#27979;&#35797;&#26102;&#36755;&#20837;&#20013;&#19981;&#21152;&#21306;&#20998;&#22320;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28023;&#32501;&#27602;&#21270;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#19982;&#20248;&#21270;&#27979;&#35797;&#26102;&#28023;&#32501;&#26679;&#26412;&#30456;&#20851;&#30340;&#38480;&#21046;&#65292;&#24182;&#34920;&#26126;&#21363;&#20351;&#25915;&#20987;&#32773;&#20165;&#25511;&#21046;&#20960;&#20010;&#27169;&#22411;&#26356;&#26032;&#65292;&#20363;&#22914;&#27169;&#22411;&#35757;&#32451;&#34987;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#25110;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20998;&#24067;&#24335;&#36827;&#34892;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#36825;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#34920;&#26126;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27602;&#21270;&#27169;&#22411;&#30340;&#28608;&#27963;&#65292;&#30830;&#23450;&#20102;&#21738;&#20123;&#35745;&#31639;&#23545;&#23548;&#33268;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#22686;&#21152;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge examples are test-time inputs carefully optimized to increase energy consumption and latency of neural networks when deployed on hardware accelerators. In this work, we are the first to demonstrate that sponge examples can also be injected at training time, via an attack that we call sponge poisoning. This attack allows one to increase the energy consumption and latency of machine-learning models indiscriminately on each test-time input. We present a novel formalization for sponge poisoning, overcoming the limitations related to the optimization of test-time sponge examples, and show that this attack is possible even if the attacker only controls a few model updates; for instance, if model training is outsourced to an untrusted third-party or distributed via federated learning. Our extensive experimental analysis shows that sponge poisoning can almost completely vanish the effect of hardware accelerators. We also analyze the activations of poisoned models, identifying which comp
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#19981;&#21516;&#20256;&#24863;&#22120;&#27880;&#20876;&#30340;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#22810;&#35270;&#35282;&#19977;&#32500;&#37325;&#24314;&#12290;&#23427;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29289;&#36136;&#23646;&#24615;&#36827;&#34892;&#20102;&#36873;&#25321;&#24182;&#25552;&#20379;&#20102;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#33719;&#21462;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#26088;&#22312;&#24110;&#21161;&#31639;&#27861;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2203.06111</link><description>&lt;p&gt;
&#22810;&#20256;&#24863;&#22120;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#29992;&#20110;&#22810;&#35270;&#35282;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Multi-sensor large-scale dataset for multi-view 3D reconstruction. (arXiv:2203.06111v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06111
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#19981;&#21516;&#20256;&#24863;&#22120;&#27880;&#20876;&#30340;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#22810;&#35270;&#35282;&#19977;&#32500;&#37325;&#24314;&#12290;&#23427;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29289;&#36136;&#23646;&#24615;&#36827;&#34892;&#20102;&#36873;&#25321;&#24182;&#25552;&#20379;&#20102;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#33719;&#21462;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#26088;&#22312;&#24110;&#21161;&#31639;&#27861;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#35270;&#35282;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#20998;&#36776;&#29575;&#21644;&#27169;&#24577;&#20256;&#24863;&#22120;&#30340;&#27880;&#20876;RGB&#21644;&#28145;&#24230;&#25968;&#25454;:&#26234;&#33021;&#25163;&#26426;&#12289;&#33521;&#29305;&#23572;RealSense&#12289;&#24494;&#36719;Kinect&#12289;&#24037;&#19994;&#30456;&#26426;&#21644;&#32467;&#26500;&#20809;&#25195;&#25551;&#20202;&#12290;&#22330;&#26223;&#30340;&#36873;&#25321;&#24378;&#35843;&#20102;&#29616;&#26377;&#31639;&#27861;&#25361;&#25112;&#30340;&#21508;&#31181;&#29289;&#36136;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32422;107&#20010;&#19981;&#21516;&#22330;&#26223;&#30340;100&#20010;&#35270;&#35282;&#26041;&#21521;&#19979;&#30340;1.4&#30334;&#19975;&#24352;&#22270;&#20687;&#65292;&#28085;&#30422;&#20102;14&#31181;&#29031;&#26126;&#26465;&#20214;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#23545;&#35780;&#20272;&#21644;&#22521;&#35757;&#19977;&#32500;&#37325;&#24314;&#31639;&#27861;&#21450;&#30456;&#20851;&#20219;&#21153;&#26377;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;skoltech3d.appliedai.tech&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from sensors of different resolutions and modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are selected to emphasize a diverse set of material properties challenging for existing algorithms. We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions. We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks. The dataset is available at skoltech3d.appliedai.tech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;&#65292;&#22312;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#21644;&#20445;&#35777;&#37325;&#26500;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#31561;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2203.02194</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#22806;&#26679;&#26412;&#26816;&#27979;&#65292;&#22312;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#21644;&#20445;&#35777;&#37325;&#26500;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#31561;&#31574;&#30053;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20998;&#31867;&#22120;&#38656;&#35201;&#26816;&#27979;&#36828;&#31163;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#22806;&#26679;&#26412;&#12290;&#37325;&#26500;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#21033;&#29992;&#36755;&#20837;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#26032;&#39062;&#24615;&#19982;&#27491;&#24120;&#24615;&#30340;&#24230;&#37327;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#30340;&#26412;&#36136;&#34920;&#36848;&#20026;&#20855;&#26377;&#23545;&#26465;&#20214;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#26597;&#35810;&#30340;&#22235;&#20803;&#32452;&#22495;&#36716;&#25442;&#65292;&#20854;&#26377;&#20869;&#22312;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#25913;&#36827;&#26041;&#21521;&#34987;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#65292;&#21516;&#26102;&#30830;&#20445;&#20854;&#37325;&#26500;&#33021;&#21147;&#65292;&#20197;&#20805;&#24403;&#25152;&#25551;&#36848;&#30340;&#22495;&#36716;&#25442;&#22120;&#12290;&#20174;&#20013;&#65292;&#24341;&#20837;&#20102;&#31574;&#30053;&#65292;&#21253;&#25324;&#35821;&#20041;&#37325;&#26500;&#12289;&#25968;&#25454;&#30830;&#23450;&#24615;&#20998;&#35299;&#21644;&#26631;&#20934;&#21270;L2&#36317;&#31163;&#65292;&#20197;&#23454;&#36136;&#24615;&#25913;&#21892;&#21407;&#22987;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20849;&#21516;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#65292;&#22312;Wide-ResNet&#19978;&#65292;CIFAR-100&#19982;TinyImagenet-crop&#30340;FPR@95%TPR&#20026;0.2%&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#26631;&#35760;&#22806;&#26679;&#26412;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With desirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruction error as a metric of novelty vs. normality. We formulate the essence of such approach as a quadruplet domain translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an improvement direction is formalized as maximumly compressing the autoencoder's latent space while ensuring its reconstructive power for acting as a described domain translator. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normalized L2 distance to substantially improve original methods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method works without any addit
&lt;/p&gt;</description></item><item><title>FedREP &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#24179;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#20010; REP &#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#12289;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23454;&#29616;&#33021;&#28304;&#36127;&#36733;&#28040;&#32791;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2203.00219</link><description>&lt;p&gt;
FedREP&#65306;&#38754;&#21521;&#38646;&#21806;&#33021;&#28304;&#20379;&#24212;&#21830;&#30340;&#27700;&#24179;&#32852;&#37030;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers. (arXiv:2203.00219v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00219
&lt;/p&gt;
&lt;p&gt;
FedREP &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#24179;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#20010; REP &#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#12289;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23454;&#29616;&#33021;&#28304;&#36127;&#36733;&#28040;&#32791;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#30005;&#34920;&#25910;&#38598;&#21644;&#20256;&#36755;&#23478;&#24237;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#32473;&#38646;&#21806;&#33021;&#28304;&#20379;&#24212;&#21830;&#65288;REP&#65289;&#65292;&#26368;&#22823;&#30340;&#25361;&#25112;&#26159;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#28040;&#36153;&#32773;&#25968;&#25454;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#38024;&#23545;&#33021;&#28304;&#36127;&#36733;&#28040;&#32791;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#33021;&#28304;&#38656;&#27714;&#31649;&#29702;&#12289;&#36127;&#36733;&#20999;&#25442;&#21644;&#22522;&#30784;&#35774;&#26045;&#21457;&#23637;&#38750;&#24120;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;&#33021;&#28304;&#36127;&#36733;&#39044;&#27979;&#26159;&#38598;&#20013;&#30340;&#65292;&#19981;&#21487;&#25193;&#23637;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#23481;&#26131;&#36973;&#21463;&#25968;&#25454;&#38544;&#31169;&#23041;&#32961;&#12290;&#27492;&#22806;&#65292;REP &#26159;&#21508;&#33258;&#30340;&#24066;&#22330;&#21442;&#19982;&#32773;&#65292;&#24182;&#26377;&#36131;&#20219;&#20445;&#25252;&#33258;&#24049;&#23458;&#25143;&#30340;&#38544;&#31169;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#24179;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363; FedREP&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#30001;&#19968;&#20010;&#25511;&#21046;&#20013;&#24515;&#21644;&#22810;&#20010;&#38646;&#21806;&#21830;&#32452;&#25104;&#65292;&#36890;&#36807;&#21551;&#29992;&#22810;&#20010; REP &#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#12289;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Smart Meters are collecting and transmitting household energy consumption data to Retail Energy Providers (REP), the main challenge is to ensure the effective use of fine-grained consumer data while ensuring data privacy. In this manuscript, we tackle this challenge for energy load consumption forecasting in regards to REPs which is essential to energy demand management, load switching and infrastructure development. Specifically, we note that existing energy load forecasting is centralized, which are not scalable and most importantly, vulnerable to data privacy threats. Besides, REPs are individual market participants and liable to ensure the privacy of their own customers. To address this issue, we propose a novel horizontal privacy-preserving federated learning framework for REPs energy load forecasting, namely FedREP. We consider a federated learning system consisting of a control centre and multiple retailers by enabling multiple REPs to build a common, robust machine learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;-&#20248;&#21270;&#26694;&#26550;&#26469;&#33258;&#21160;&#21270;&#20248;&#21270;LiDAR&#20809;&#26463;&#37197;&#32622;&#20197;&#25552;&#39640;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#24615;&#33021;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2201.03860</link><description>&lt;p&gt;
&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#23450;&#20301;&#20013;&#30340;LiDAR&#20809;&#26463;&#37197;&#32622;&#31471;&#21040;&#31471;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-To-End Optimization of LiDAR Beam Configuration for 3D Object Detection and Localization. (arXiv:2201.03860v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;-&#20248;&#21270;&#26694;&#26550;&#26469;&#33258;&#21160;&#21270;&#20248;&#21270;LiDAR&#20809;&#26463;&#37197;&#32622;&#20197;&#25552;&#39640;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#24615;&#33021;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;LiDAR&#30340;&#24212;&#29992;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#22312;&#39044;&#20808;&#30830;&#23450;&#30340;&#20809;&#26463;&#37197;&#32622;&#19979;&#25195;&#25551;&#30340;&#19977;&#32500;&#28857;&#20113;&#65292;&#20363;&#22914;&#65292;&#20809;&#26463;&#30340;&#20208;&#35282;&#36890;&#24120;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#12290;&#36825;&#20123;&#22266;&#23450;&#37197;&#32622;&#26159;&#38754;&#21521;&#20219;&#21153;&#30340;&#65292;&#22240;&#27492;&#31616;&#21333;&#22320;&#20351;&#29992;&#23427;&#20204;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#26032;&#36335;&#32447;&#65292;&#23398;&#20064;&#22914;&#20309;&#38024;&#23545;&#32473;&#23450;&#30340;&#24212;&#29992;&#31243;&#24207;&#20248;&#21270;LiDAR&#20809;&#26463;&#37197;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;-&#20248;&#21270;&#65288;RL-L2O&#65289;&#26694;&#26550;&#65292;&#20197;&#33258;&#21160;&#31471;&#21040;&#31471;&#22320;&#20248;&#21270;&#19981;&#21516;&#22522;&#20110;LiDAR&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#20809;&#26463;&#37197;&#32622;&#12290;&#20248;&#21270;&#26159;&#30001;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#32456;&#24615;&#33021;&#25351;&#23548;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#20316;&#20026;&#31616;&#21333;&#30340;&#25554;&#20837;&#24335;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;&#22522;&#20110;LiDAR&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#24403;&#38656;&#35201;&#20302;&#20998;&#36776;&#29575;&#65288;&#20302;&#25104;&#26412;&#65289;&#30340;LiDAR&#26102;&#65292;&#35813;&#26041;&#27861;&#23588;&#20854;&#26377;&#29992;&#65292;&#20363;&#22914;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#37096;&#32626;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25628;&#32034;&#20102;&#20302;&#20998;&#36776;&#29575;LiDAR&#30340;&#20809;&#26463;&#37197;&#32622;&#65292;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#20809;&#26463;&#37197;&#32622;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing learning methods for LiDAR-based applications use 3D points scanned under a pre-determined beam configuration, e.g., the elevation angles of beams are often evenly distributed. Those fixed configurations are task-agnostic, so simply using them can lead to sub-optimal performance. In this work, we take a new route to learn to optimize the LiDAR beam configuration for a given application. Specifically, we propose a reinforcement learning-based learning-to-optimize (RL-L2O) framework to automatically optimize the beam configuration in an end-to-end manner for different LiDAR-based applications. The optimization is guided by the final performance of the target task and thus our method can be integrated easily with any LiDAR-based application as a simple drop-in module. The method is especially useful when a low-resolution (low-cost) LiDAR is needed, for instance, for system deployment at a massive scale. We use our method to search for the beam configuration of a low-resolution Li
&lt;/p&gt;</description></item><item><title>AtteSTNet&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#26816;&#27979;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#19982;&#22797;&#26434;&#32593;&#32476;&#30456;&#24403;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#65292;&#20854;&#26497;&#22823;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#32500;&#25252;&#24615;&#26159;&#20854;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2112.11479</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection. (arXiv:2112.11479v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11479
&lt;/p&gt;
&lt;p&gt;
AtteSTNet&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#26816;&#27979;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#19982;&#22797;&#26434;&#32593;&#32476;&#30456;&#24403;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#65292;&#20854;&#26497;&#22823;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#32500;&#25252;&#24615;&#26159;&#20854;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#37327;&#22686;&#21152;&#65292;&#20063;&#23548;&#33268;&#22823;&#37327;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#20196;&#20154;&#35752;&#21388;&#21644;&#20882;&#29359;&#30340;&#35328;&#35770;&#12290;&#31038;&#20132;&#23186;&#20307;&#19978;&#20351;&#29992;&#30340;&#35821;&#35328;&#36890;&#24120;&#26159;&#33521;&#35821;&#21644;&#32946;&#22320;&#26041;&#35821;&#35328;&#30340;&#32452;&#21512;&#12290;&#22312;&#21360;&#24230;&#65292;&#21360;&#22320;&#35821;&#26159;&#20027;&#35201;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#24182;&#32463;&#24120;&#19982;&#33521;&#35821;&#20999;&#25442;&#65292;&#24418;&#25104;&#21360;&#22320;&#33521;&#35821;&#65288;Hinglish&#65289;&#35821;&#35328;&#12290;&#36807;&#21435;&#24050;&#32463;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#23545;&#28151;&#21512;&#26102;&#30340;&#21360;&#22320;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20351;&#29992;&#30340;&#24490;&#29615;&#25110;&#21367;&#31215;&#26426;&#21046;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#20869;&#23384;&#38656;&#27714;&#22823;&#12290;&#36807;&#21435;&#30340;&#25216;&#26415;&#36824;&#20351;&#29992;&#22797;&#26434;&#30340;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29616;&#26377;&#25216;&#26415;&#38750;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#25913;&#21464;&#25968;&#25454;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#19982;&#36825;&#20123;&#22797;&#26434;&#32593;&#32476;&#19968;&#26679;&#65292;&#24182;&#19988;&#22312;&#22914;HASOC&#65288;&#21360;&#27431;&#35821;&#35328;&#30340;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#20869;&#23481;&#35782;&#21035;&#65289;&#27492;&#31867;&#28151;&#21512;&#21360;&#22320;&#33521;&#35821;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;AtteSTNet&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#26469;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#26356;&#31616;&#21333;&#26131;&#20110;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in technology have led to a boost in social media usage which has ultimately led to large amounts of user-generated data which also includes hateful and offensive speech. The language used in social media is often a combination of English and the native language in the region. In India, Hindi is used predominantly and is often code-switched with English, giving rise to the Hinglish (Hindi+English) language. Various approaches have been made in the past to classify the code-mixed Hinglish hate speech using different machine learning and deep learning-based techniques. However, these techniques make use of recurrence on convolution mechanisms which are computationally expensive and have high memory requirements. Past techniques also make use of complex data processing making the existing techniques very complex and non-sustainable to change in data. Proposed work gives a much simpler approach which is not only at par with these complex networks but also exceeds perfor
&lt;/p&gt;</description></item><item><title>CoReS&#26159;&#19968;&#31181;&#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#65292;&#24182;&#20351;&#20854;&#19982;&#20043;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#8220;&#20860;&#23481;&#8221;&#12290;&#36825;&#20351;&#24471;&#22312;&#36880;&#27493;&#21319;&#32423;&#34920;&#31034;&#27169;&#22411;&#26102;&#65292;&#26080;&#38656;&#20026;&#25152;&#26377;&#20043;&#21069;&#30475;&#21040;&#30340;&#22270;&#20687;&#25552;&#21462;&#26032;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2111.07632</link><description>&lt;p&gt;
CoReS: &#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#20860;&#23481;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoReS: Compatible Representations via Stationarity. (arXiv:2111.07632v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07632
&lt;/p&gt;
&lt;p&gt;
CoReS&#26159;&#19968;&#31181;&#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#65292;&#24182;&#20351;&#20854;&#19982;&#20043;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#8220;&#20860;&#23481;&#8221;&#12290;&#36825;&#20351;&#24471;&#22312;&#36880;&#27493;&#21319;&#32423;&#34920;&#31034;&#27169;&#22411;&#26102;&#65292;&#26080;&#38656;&#20026;&#25152;&#26377;&#20043;&#21069;&#30475;&#21040;&#30340;&#22270;&#20687;&#25552;&#21462;&#26032;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#20854;&#19982;&#20808;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#8220;&#20860;&#23481;&#8221;&#12290;&#20860;&#23481;&#29305;&#24449;&#20351;&#26087;&#21644;&#26032;&#30340;&#29305;&#24449;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#65292;&#20801;&#35768;&#23427;&#20204;&#22312;&#26102;&#38388;&#19978;&#20114;&#25442;&#20351;&#29992;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#36880;&#27493;&#21319;&#32423;&#34920;&#31034;&#27169;&#22411;&#26102;&#65292;&#22312;&#22270;&#24211;&#20013;&#25552;&#21462;&#25152;&#26377;&#20808;&#21069;&#30475;&#21040;&#30340;&#22270;&#20687;&#30340;&#26032;&#29305;&#24449;&#30340;&#38656;&#35201;&#65292;&#36825;&#36890;&#24120;&#22312;&#24222;&#22823;&#30340;&#22270;&#24211;&#38598;&#21644;/&#25110;&#23454;&#26102;&#31995;&#32479;&#20013;&#38750;&#24120;&#26114;&#36149;&#25110;&#19981;&#21487;&#34892;&#65288;&#21363;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#12289;&#31038;&#20132;&#32593;&#32476;&#12289;&#32456;&#36523;&#23398;&#20064;&#31995;&#32479;&#12289;&#26426;&#22120;&#20154;&#21644;&#30417;&#25511;&#31995;&#32479;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#20860;&#23481;&#34920;&#31034;&#65288;CoReS&#65289;&#65292;&#36890;&#36807;&#40723;&#21169;&#34920;&#31034;&#27169;&#22411;&#30340;&#24179;&#31283;&#24615;&#26469;&#23454;&#29616;&#20860;&#23481;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#24179;&#31283;&#24615;&#20351;&#29305;&#24449;&#30340;&#32479;&#35745;&#29305;&#24615;&#22312;&#26102;&#38388;&#20559;&#31227;&#19979;&#19981;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method to learn internal feature representation models that are \textit{compatible} with previously learned ones. Compatible features enable for direct comparison of old and new learned features, allowing them to be used interchangeably over time. This eliminates the need for visual search systems to extract new features for all previously seen images in the gallery-set when sequentially upgrading the representation model. Extracting new features is typically quite expensive or infeasible in the case of very large gallery-sets and/or real time systems (i.e., face-recognition systems, social networks, life-long learning systems, robotics and surveillance systems). Our approach, called Compatible Representations via Stationarity (CoReS), achieves compatibility by encouraging stationarity to the learned representation model without relying on previously learned models. Stationarity allows features' statistical properties not to change under time shift so 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#23558;&#21508;&#31181;&#25513;&#30721;&#26426;&#21046;&#32435;&#20837;Transformers&#26550;&#26500;&#20013;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26410;&#23631;&#34109;&#30340;&#27880;&#24847;&#21147;&#30340;&#25299;&#25169;&#65288;&#22522;&#20110;&#22270;&#24418;&#65289;&#35843;&#21046;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;d&#32500;RPE&#25513;&#30721;&#21644;&#22270;&#20869;&#26680;&#25513;&#30721;&#12290;&#35813;&#26041;&#27861;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2107.07999</link><description>&lt;p&gt;
&#20174;&#22359;-Toeplitz&#30697;&#38453;&#21040;&#22270;&#19978;&#30340;&#24494;&#20998;&#26041;&#31243;&#65306;&#36808;&#21521;&#21487;&#25193;&#23637;&#30340;Masked Transformers&#30340;&#36890;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.07999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#23558;&#21508;&#31181;&#25513;&#30721;&#26426;&#21046;&#32435;&#20837;Transformers&#26550;&#26500;&#20013;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26410;&#23631;&#34109;&#30340;&#27880;&#24847;&#21147;&#30340;&#25299;&#25169;&#65288;&#22522;&#20110;&#22270;&#24418;&#65289;&#35843;&#21046;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;d&#32500;RPE&#25513;&#30721;&#21644;&#22270;&#20869;&#26680;&#25513;&#30721;&#12290;&#35813;&#26041;&#27861;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#23558;&#21508;&#31181;&#25513;&#30721;&#26426;&#21046;&#32435;&#20837;Transformers&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#20851;&#20110;&#32447;&#24615;&#22240;&#26524;&#27880;&#24847;&#21147;&#65288;Choromanski&#31561;&#20154;&#65292;2021&#65289;&#21644;&#23545;&#25968;-&#32447;&#24615;RPE-&#27880;&#24847;&#21147;&#65288;Luo&#31561;&#20154;&#65292;2021&#65289;&#30340;&#32467;&#26524;&#26159;&#36825;&#31181;&#19968;&#33324;&#26426;&#21046;&#30340;&#29305;&#20363;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26410;&#23631;&#34109;&#30340;&#27880;&#24847;&#21147;&#30340;&#25299;&#25169;&#65288;&#22522;&#20110;&#22270;&#24418;&#65289;&#35843;&#21046;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#30693;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#25928;&#30340;d&#32500;RPE&#25513;&#30721;&#21644;&#22270;&#20869;&#26680;&#25513;&#30721;&#12290;&#25105;&#20204;&#21033;&#29992;&#35768;&#22810;&#25968;&#23398;&#25216;&#26415;&#65292;&#20174;&#35889;&#20998;&#26512;&#12289;&#21160;&#24577;&#35268;&#21010;&#21644;&#38543;&#26426;&#28216;&#36208;&#21040;&#35299;&#20915;&#22270;&#19978;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#35201;&#31034;&#20363;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26631;&#20934;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#20998;&#25968;&#12290;&#22312;&#20462;&#21098;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#19981;&#29306;&#29298;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;EL2N&#20998;&#25968;&#22312;&#20960;&#20010;&#26102;&#26399;&#30340;&#35757;&#32451;&#20013;&#33021;&#22815;&#20462;&#21098;CIFAR10&#35757;&#32451;&#38598;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2107.07075</link><description>&lt;p&gt;
&#25968;&#25454;&#39278;&#39135;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#22312;&#35757;&#32451;&#26089;&#26399;&#25214;&#21040;&#37325;&#35201;&#30340;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#35201;&#31034;&#20363;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26631;&#20934;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#20998;&#25968;&#12290;&#22312;&#20462;&#21098;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#19981;&#29306;&#29298;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;EL2N&#20998;&#25968;&#22312;&#20960;&#20010;&#26102;&#26399;&#30340;&#35757;&#32451;&#20013;&#33021;&#22815;&#20462;&#21098;CIFAR10&#35757;&#32451;&#38598;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#36817;&#25104;&#21151;&#37096;&#20998;&#22320;&#26469;&#33258;&#20110;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;&#65306;&#26377;&#22810;&#23569;&#25968;&#25454;&#26159;&#22810;&#20313;&#30340;&#65292;&#21738;&#20123;&#31034;&#20363;&#23545;&#20110;&#25512;&#24191;&#26159;&#37325;&#35201;&#30340;&#65292;&#22914;&#20309;&#25214;&#21040;&#23427;&#20204;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#24778;&#20154;&#30340;&#35266;&#23519;&#65306;&#22312;&#26631;&#20934;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#22810;&#20010;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#31616;&#21333;&#20998;&#25968;&#21487;&#20197;&#29992;&#20110;&#38750;&#24120;&#26089;&#26399;&#22320;&#35782;&#21035;&#37325;&#35201;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#36825;&#26679;&#30340;&#20998;&#25968;&#8212;&#8212;&#26799;&#24230;&#24402;&#19968;&#21270;&#65288;GraNd&#65289;&#21644;&#35823;&#24046;L2&#33539;&#25968;&#65288;EL2N&#65289;&#20998;&#25968;&#8212;&#8212;&#24182;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#20307;&#31995;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#20462;&#21098;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#32780;&#27809;&#26377;&#29306;&#29298;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#21151;&#25928;&#12290;&#23454;&#38469;&#19978;&#65292;&#20351;&#29992;&#22312;&#20960;&#20010;&#26102;&#26399;&#20013;&#35745;&#31639;&#30340;EL2N&#20998;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#20462;&#21098;CIFAR10&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#65292;&#21516;&#26102;&#30053;&#24494;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#19968;&#31181;&#20307;&#31995;&#32467;&#26500;&#25110;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;EL2N&#20998;&#25968;&#26222;&#36941;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores -- the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generaliz
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#28145;&#24230;&#38598;&#25104;&#30340;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#26680;&#21270;&#25490;&#26021;&#39033;&#65292;&#21487;&#20197;&#24378;&#21046;&#24182;&#32500;&#25252;&#25104;&#21592;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#20351;&#38598;&#25104;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2106.11642</link><description>&lt;p&gt;
&#25298;&#32477;&#24615;&#28145;&#24230;&#38598;&#25104;&#26159;&#36125;&#21494;&#26031;&#30340;
&lt;/p&gt;
&lt;p&gt;
Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#28145;&#24230;&#38598;&#25104;&#30340;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#26680;&#21270;&#25490;&#26021;&#39033;&#65292;&#21487;&#20197;&#24378;&#21046;&#24182;&#32500;&#25252;&#25104;&#21592;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#20351;&#38598;&#25104;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22240;&#20854;&#27010;&#24565;&#19978;&#30340;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#30028;&#30340;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29420;&#31435;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#30340;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#32500;&#25252;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#28155;&#21152;&#26356;&#22810;&#38598;&#25104;&#25104;&#21592;&#26102;&#20986;&#29616;&#30149;&#24577;&#65292;&#20363;&#22914;&#38598;&#25104;&#24615;&#33021;&#30340;&#39281;&#21644;&#65292;&#23427;&#20250;&#25910;&#25947;&#21040;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#19981;&#20165;&#24433;&#21709;&#20854;&#39044;&#27979;&#30340;&#36136;&#37327;&#65292;&#32780;&#19988;&#26356;&#21152;&#24433;&#21709;&#38598;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#22312;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#38459;&#27490;&#19981;&#21516;&#38598;&#25104;&#25104;&#21592;&#22349;&#22604;&#21040;&#30456;&#21516;&#21151;&#33021;&#26469;&#20811;&#26381;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#28145;&#24230;&#38598;&#25104;&#30340;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26680;&#21270;&#30340;&#25490;&#26021;&#26415;&#35821;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#19981;&#20165;&#24378;&#21046;&#24182;&#32500;&#25252;&#25104;&#21592;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23558;&#26368;&#22823;&#30340;&#21518;&#39564;&#20540;&#36716;&#21270;&#20026;...&#65288;&#21407;&#25991;&#25130;&#27490;&#27492;&#22788;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles have recently gained popularity in the deep learning community for their conceptual simplicity and efficiency. However, maintaining functional diversity between ensemble members that are independently trained with gradient descent is challenging. This can lead to pathologies when adding more ensemble members, such as a saturation of the ensemble performance, which converges to the performance of a single model. Moreover, this does not only affect the quality of its predictions, but even more so the uncertainty estimates of the ensemble, and thus its performance on out-of-distribution data. We hypothesize that this limitation can be overcome by discouraging different ensemble members from collapsing to the same function. To this end, we introduce a kernelized repulsive term in the update rule of the deep ensembles. We show that this simple modification not only enforces and maintains diversity among the members but, even more importantly, transforms the maximum a posterio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;NoiseGrad&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#26435;&#37325;&#30340;&#38543;&#26426;&#21464;&#21270;&#25200;&#21160;&#20915;&#31574;&#36793;&#30028;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.10185</link><description>&lt;p&gt;
NoiseGrad&#65306;&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#26435;&#37325;&#30340;&#38543;&#26426;&#21464;&#21270;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights. (arXiv:2106.10185v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;NoiseGrad&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#26435;&#37325;&#30340;&#38543;&#26426;&#21464;&#21270;&#25200;&#21160;&#20915;&#31574;&#36793;&#30028;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#40657;&#21283;&#23376;&#23398;&#20064;&#26426;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24456;&#22810;&#24037;&#20316;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26377;&#29992;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NoiseGrad&#65292;&#36890;&#36807;&#22312;&#26435;&#37325;&#21442;&#25968;&#31354;&#38388;&#20013;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20174;&#32780;&#25200;&#21160;&#20915;&#31574;&#36793;&#30028;&#65292;&#22686;&#24378;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;NoiseGrad&#19982;&#20854;&#19982;SmoothGrad&#30340;&#34701;&#21512;&#26041;&#27861;&#65288;FusionGrad&#65289;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad -FusionGrad -- qualitatively and quantitatively with several evaluation criteria, and show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22855;&#24322;&#20998;&#24067;&#38750;&#21442;&#25968;&#20272;&#35745;&#30340;&#32479;&#35745;&#23398;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#26679;&#26412;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#25200;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#20998;&#24067;&#30340;&#19968;&#33268;&#20272;&#35745;&#21644;&#33391;&#22909;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2105.04046</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#20284;&#28982;&#26041;&#27861;&#36827;&#34892;&#22855;&#24322;&#20998;&#24067;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
A likelihood approach to nonparametric estimation of a singular distribution using deep generative models. (arXiv:2105.04046v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.04046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22855;&#24322;&#20998;&#24067;&#38750;&#21442;&#25968;&#20272;&#35745;&#30340;&#32479;&#35745;&#23398;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#26679;&#26412;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#25200;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#20998;&#24067;&#30340;&#19968;&#33268;&#20272;&#35745;&#21644;&#33391;&#22909;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22855;&#24322;&#20998;&#24067;&#38750;&#21442;&#25968;&#20272;&#35745;&#30340;&#32479;&#35745;&#23398;&#24615;&#36136;&#12290;&#22312;&#32771;&#34385;&#30340;&#27169;&#22411;&#20013;&#65292;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#20551;&#35774;&#25968;&#25454;&#38598;&#20013;&#22312;&#19968;&#20123;&#20302;&#32500;&#32467;&#26500;&#21608;&#22260;&#12290;&#30001;&#20110;&#25903;&#25745;&#20302;&#32500;&#32467;&#26500;&#30340;&#20998;&#24067;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#23545;&#21202;&#36125;&#26684;&#27979;&#24230;&#20855;&#26377;&#22855;&#24322;&#24615;&#65292;&#22240;&#27492;&#29992;&#36890;&#24120;&#30340;&#20284;&#28982;&#26041;&#27861;&#26469;&#20272;&#35745;&#30446;&#26631;&#20998;&#24067;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#26679;&#26412;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#25200;&#21160;&#21487;&#20197;&#24471;&#21040;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#28508;&#22312;&#20998;&#24067;&#30340;&#19968;&#33268;&#20272;&#35745;&#21644;&#33391;&#22909;&#25910;&#25947;&#29575;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#34920;&#24449;&#20102;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26377;&#25928;&#20272;&#35745;&#30340;&#20998;&#24067;&#31867;&#12290;&#36825;&#20010;&#31867;&#36275;&#22815;&#26222;&#36941;&#65292;&#21487;&#20197;&#23481;&#32435;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate statistical properties of a likelihood approach to nonparametric estimation of a singular distribution using deep generative models. More specifically, a deep generative model is used to model high-dimensional data that are assumed to concentrate around some low-dimensional structure. Estimating the distribution supported on this low-dimensional structure, such as a low-dimensional manifold, is challenging due to its singularity with respect to the Lebesgue measure in the ambient space. In the considered model, a usual likelihood approach can fail to estimate the target distribution consistently due to the singularity. We prove that a novel and effective solution exists by perturbing the data with an instance noise, which leads to consistent estimation of the underlying distribution with desirable convergence rates. We also characterize the class of distributions that can be efficiently estimated via deep generative models. This class is sufficiently general to contain v
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#22270;&#12289;&#26102;&#38388;&#28982;&#21518;&#22270;&#31561;&#20004;&#31181;&#19981;&#21516;&#30340;&#33410;&#28857;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#19981;&#26159;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#32452;&#20214;GNN&#26102;&#65292;&#26102;&#38388;&#28982;&#21518;&#22270;&#20855;&#26377;&#27604;&#26102;&#38388;&#21644;&#22270;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2103.07016</link><description>&lt;p&gt;
&#20851;&#20110;&#35266;&#27979;&#39044;&#27979;&#20013;&#26102;&#24577;&#22270;&#34920;&#31034;&#21644;&#38745;&#24577;&#22270;&#34920;&#31034;&#30340;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Equivalence Between Temporal and Static Graph Representations for Observational Predictions. (arXiv:2103.07016v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#22270;&#12289;&#26102;&#38388;&#28982;&#21518;&#22270;&#31561;&#20004;&#31181;&#19981;&#21516;&#30340;&#33410;&#28857;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#19981;&#26159;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#32452;&#20214;GNN&#26102;&#65292;&#26102;&#38388;&#28982;&#21518;&#22270;&#20855;&#26377;&#27604;&#26102;&#38388;&#21644;&#22270;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#27491;&#24335;&#21270;&#20102;&#39044;&#27979;&#26102;&#38388;&#22270;&#20013;&#33410;&#28857;&#23646;&#24615;&#28436;&#21270;&#30340;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26102;&#38388;&#22270;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#26694;&#26550;&#65306;(a)&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26102;&#38388;&#21644;&#22270;&#65292;&#20854;&#20013;&#31561;&#21464;&#22270;(&#22914;GNN)&#21644;&#24207;&#21015;(&#22914;RNN)&#34920;&#31034;&#30456;&#20114;&#20132;&#32455;&#65292;&#20197;&#34920;&#31034;&#22270;&#20013;&#33410;&#28857;&#23646;&#24615;&#30340;&#26102;&#38388;&#28436;&#21270;&#65307;(b)&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#26102;&#38388;&#28982;&#21518;&#22270;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#34920;&#31034;&#25551;&#36848;&#33410;&#28857;&#21644;&#36793;&#21160;&#24577;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#39304;&#36865;&#21040;&#38745;&#24577;&#31561;&#21464;&#22270;&#34920;&#31034;&#20013;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#24403;&#20004;&#31181;&#26041;&#27861;&#20351;&#29992;&#19981;&#26159;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#32452;&#20214;GNN (&#22914;1-Weisfeiler-Lehman GNNs)&#26102;&#65292;&#26102;&#38388;&#28982;&#21518;&#22270;&#34920;&#31034;&#20855;&#26377;&#27604;&#26102;&#38388;&#21644;&#22270;&#34920;&#31034;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#19968;&#23450;&#26159;&#33719;&#24471;&#26368;&#20339;&#39044;&#27979;&#25928;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26102;&#38388;&#28982;&#21518;&#22270;&#34920;&#31034;&#21487;&#20197;&#20174;&#38745;&#24577;&#22270;&#20013;&#23398;&#20064;&#31561;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work formalizes the associational task of predicting node attribute evolution in temporal graphs from the perspective of learning equivariant representations. We show that node representations in temporal graphs can be cast into two distinct frameworks: (a) The most popular approach, which we denote as time-and-graph, where equivariant graph (e.g., GNN) and sequence (e.g., RNN) representations are intertwined to represent the temporal evolution of node attributes in the graph; and (b) an approach that we denote as time-then-graph, where the sequences describing the node and edge dynamics are represented first, then fed as node and edge attributes into a static equivariant graph representation that comes after. Interestingly, we show that time-then-graph representations have an expressivity advantage over time-and-graph representations when both use component GNNs that are not most-expressive (e.g., 1-Weisfeiler-Lehman GNNs). Moreover, while our goal is not necessarily to obtain st
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#27969;&#24418;&#32467;&#26500;&#26469;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#22312;&#31867;&#21035;&#24179;&#34913;&#21644;&#36873;&#25321;&#24178;&#20928;&#26631;&#31614;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36845;&#20195;&#28165;&#27927;&#26631;&#31614;&#20197;&#25552;&#39640;&#20266;&#26631;&#31614;&#36136;&#37327;&#30340;&#31639;&#27861;&#65292;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2012.07962</link><description>&lt;p&gt;
&#36845;&#20195;&#26631;&#31614;&#28165;&#27927;&#29992;&#20110;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.07962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#27969;&#24418;&#32467;&#26500;&#26469;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#22312;&#31867;&#21035;&#24179;&#34913;&#21644;&#36873;&#25321;&#24178;&#20928;&#26631;&#31614;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36845;&#20195;&#28165;&#27927;&#26631;&#31614;&#20197;&#25552;&#39640;&#20266;&#26631;&#31614;&#36136;&#37327;&#30340;&#31639;&#27861;&#65292;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#28041;&#21450;&#21040;&#23398;&#20064;&#34920;&#24449;&#21644;&#33719;&#21462;&#30693;&#35782;&#65292;&#20197;&#20351;&#26032;&#20219;&#21153;&#21487;&#20197;&#22312;&#30417;&#30563;&#21644;&#25968;&#25454;&#37117;&#24456;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#35299;&#20915;&#12290;&#36890;&#36807;&#27178;&#21521;&#25512;&#26029;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#27969;&#24418;&#32467;&#26500;&#26469;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#21516;&#26102;&#24179;&#34913;&#31867;&#21035;&#24182;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20540;&#20998;&#24067;&#26469;&#36873;&#25321;&#26368;&#24178;&#20928;&#30340;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#25552;&#39640;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#21363; miniImageNet&#12289;tieredImageNet&#12289;CUB &#21644; CIFAR-FS&#65289;&#19978;&#36229;&#36807;&#25110;&#21305;&#37197;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#21516;&#26102;&#22312;&#29305;&#24449;&#31354;&#38388;&#39044;&#22788;&#29702;&#21644;&#21487;&#29992;&#25968;&#25454;&#30340;&#25968;&#37327;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.c &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solution surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https://github.c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340; ECGAN &#29992;&#20110;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#65292;&#20351;&#29992;&#36793;&#32536;&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#65292;&#24182;&#19988;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22788;&#29702;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2003.13898</link><description>&lt;p&gt;
&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#36793;&#32536;&#24341;&#23548; GAN &#29992;&#20110;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis. (arXiv:2003.13898v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.13898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340; ECGAN &#29992;&#20110;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#65292;&#20351;&#29992;&#36793;&#32536;&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#65292;&#24182;&#19988;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22788;&#29702;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; ECGAN &#29992;&#20110;&#25361;&#25112;&#24615;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#12290;&#34429;&#28982;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#20173;&#28982;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#21407;&#22240;&#26377;&#19977;&#20010;&#65306;1&#65289;&#35821;&#20041;&#26631;&#31614;&#27809;&#26377;&#25552;&#20379;&#35814;&#32454;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#24471;&#21512;&#25104;&#23616;&#37096;&#32454;&#33410;&#21644;&#32467;&#26500;&#21464;&#24471;&#22256;&#38590;&#65307;2&#65289;&#24191;&#27867;&#37319;&#29992;&#30340; CNN &#25805;&#20316;&#65292;&#22914;&#21367;&#31215;&#12289;&#19979;&#37319;&#26679;&#21644;&#24402;&#19968;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#25439;&#22833;&#65292;&#19981;&#33021;&#23436;&#20840;&#20445;&#30041;&#21407;&#22987;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#65307;3&#65289;&#29616;&#26377;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20174;&#21333;&#20010;&#36755;&#20837;&#35821;&#20041;&#24067;&#23616;&#24314;&#27169;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#22810;&#20010;&#36755;&#20837;&#35821;&#20041;&#24067;&#23616;&#30340;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#21363;&#36328;&#19981;&#21516;&#36755;&#20837;&#24067;&#23616;&#30340;&#20687;&#32032;&#20043;&#38388;&#30340;&#35821;&#20041;&#20132;&#21449;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36793;&#32536;&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#65292;&#24182;&#36827;&#19968;&#27493;&#37319;&#29992;&#20854;&#26469;&#24341;&#23548;&#22270;&#20687;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel ECGAN for the challenging semantic image synthesis task. Although considerable improvement has been achieved, the quality of synthesized images is far from satisfactory due to three largely unresolved challenges. 1) The semantic labels do not provide detailed structural information, making it difficult to synthesize local details and structures. 2) The widely adopted CNN operations such as convolution, down-sampling, and normalization usually cause spatial resolution loss and thus cannot fully preserve the original semantic information, leading to semantically inconsistent results. 3) Existing semantic image synthesis methods focus on modeling local semantic information from a single input semantic layout. However, they ignore global semantic information of multiple input semantic layouts, i.e., semantic cross-relations between pixels across different input layouts. To tackle 1), we propose to use edge as an intermediate representation which is further adopted to gui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#21270; SVM &#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#39640;&#30456;&#20851;&#24615;&#26679;&#26412;&#30340;&#25490;&#21517;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2002.11436</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#21270; SVM &#30340;&#25490;&#21517;&#38382;&#39064;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nonlinear classifiers for ranking problems based on kernelized SVM. (arXiv:2002.11436v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#21270; SVM &#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#39640;&#30456;&#20851;&#24615;&#26679;&#26412;&#30340;&#25490;&#21517;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20998;&#31867;&#38382;&#39064;&#20165;&#20851;&#27880;&#26368;&#20855;&#30456;&#20851;&#24615;&#30340;&#26679;&#26412;&#30340;&#24615;&#33021;&#32780;&#38750;&#25152;&#26377;&#26679;&#26412;&#12290;&#20363;&#22914;&#65292;&#25490;&#21517;&#38382;&#39064;&#12289;&#39030;&#37096;&#20934;&#30830;&#24230;&#25110;&#25628;&#32034;&#24341;&#25806;&#20165;&#20851;&#27880;&#21069;&#20960;&#20010;&#26597;&#35810;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20043;&#21069;&#24050;&#32463;&#25512;&#23548;&#20986;&#21253;&#25324;&#22810;&#20010;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#31867;&#21035;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#26412;&#25991;&#23558;&#35813;&#26694;&#26550;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#21033;&#29992; SVM &#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#23545;&#20598;&#21270;&#22788;&#29702;&#65292;&#28155;&#21152;&#20102;&#26680;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#37327;&#23545;&#20598;&#19978;&#21319;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many classification problems focus on maximizing the performance only on the samples with the highest relevance instead of all samples. As an example, we can mention ranking problems, accuracy at the top or search engines where only the top few queries matter. In our previous work, we derived a general framework including several classes of these linear classification problems. In this paper, we extend the framework to nonlinear classifiers. Utilizing a similarity to SVM, we dualize the problems, add kernels and propose a componentwise dual ascent method.
&lt;/p&gt;</description></item></channel></rss>