<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#38145;&#26356;&#22810;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#65292;&#26089;&#26399;&#34701;&#21512;&#31574;&#30053;&#21644;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#21644;&#31934;&#30830;&#22320;&#25191;&#34892;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2304.15010</link><description>&lt;p&gt;
LLaMA-Adapter V2: &#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.15010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#38145;&#26356;&#22810;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#65292;&#26089;&#26399;&#34701;&#21512;&#31574;&#30053;&#21644;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#21644;&#31934;&#30830;&#22320;&#25191;&#34892;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#25928;&#22320;&#36716;&#21270;&#20026;&#25351;&#20196;&#36319;&#38543;&#32773;&#65292;&#32780;&#20026;&#22810;&#27169;&#24577;&#25512;&#29702;&#35757;&#32451;LLM&#30340;&#30740;&#31350;&#20173;&#28982;&#36739;&#23569;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;LLaMA-Adapter&#35777;&#26126;&#20102;&#29992;LLM&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#65292;&#24182;&#19988;&#33853;&#21518;&#20110;GPT-4&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14997</link><description>&lt;p&gt;
&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861; ACDC&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20498;&#25512;&#20102;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#34892;&#20026;&#12290;&#36825;&#20123;&#21457;&#29616;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#21644;&#30740;&#31350;&#32773;&#30340;&#30452;&#35273;&#65292;&#36825;&#20351;&#24471;&#24212;&#29992;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#20102;&#35299;&#24403;&#21069;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#22797;&#26434;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#26680;&#24515;&#24037;&#20316;&#27969;&#31243;&#38750;&#24120;&#30456;&#20284;&#12290;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#24230;&#37327;&#65292;&#35825;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#36866;&#24403;&#30340;&#25277;&#35937;&#21333;&#20803;&#65292;&#26367;&#25442;&#36825;&#20123;&#21333;&#20803;&#30340;&#28608;&#27963;&#20197;&#30830;&#23450;&#21738;&#20123;&#21442;&#19982;&#20102;&#34892;&#20026;&#65292;&#28982;&#21518;&#35299;&#37322;&#36825;&#20123;&#21333;&#20803;&#23454;&#26045;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#38598;&#12289;&#24230;&#37327;&#21644;&#24453;&#30740;&#31350;&#30340;&#21333;&#20803;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#29702;&#35299;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#21306;&#22495;&#30340;&#21151;&#33021;&#21644;&#23427;&#20204;&#32452;&#25104;&#30340;&#30005;&#36335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#65288;ACDC&#65289;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has reverse-engineered nontrivial behaviors of transformer models. These contributions required considerable effort and researcher intuition, which makes it difficult to apply the same methods to understand the complex behavior that current models display. At their core however, the workflow for these discoveries is surprisingly similar. Researchers create a data set and metric that elicit the desired model behavior, subdivide the network into appropriate abstract units, replace activations of those units to identify which are involved in the behavior, and then interpret the functions that these units implement. By varying the data set, metric, and units under investigation, researchers can understand the functionality of each neural network region and the circuits they compose. This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Given a model's comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21021;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20840;&#23616;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340; PDE &#27531;&#24046;&#20013;&#36935;&#21040;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644; ODE &#26041;&#27861;&#20013;&#38543;&#30528;&#25968;&#37327;&#21576;&#31435;&#26041;&#32423;&#21035;&#25193;&#23637;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14994</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21021;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21487;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21021;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#23450;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#20840;&#23616;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340; PDE &#27531;&#24046;&#20013;&#36935;&#21040;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644; ODE &#26041;&#27861;&#20013;&#38543;&#30528;&#25968;&#37327;&#21576;&#31435;&#26041;&#32423;&#21035;&#25193;&#23637;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#32593;&#26684;&#21644;&#22522;&#20110;&#32593;&#26684;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#26377;&#21487;&#33021;&#25171;&#30772;&#32500;&#25968;&#28798;&#38590;&#65292;&#22312;&#20351;&#29992;&#32463;&#20856;&#27714;&#35299;&#22120;&#22256;&#38590;&#25110;&#19981;&#21487;&#33021;&#30340;&#38382;&#39064;&#20013;&#25552;&#20379;&#36817;&#20284;&#35299;&#12290;&#20840;&#23616;&#26368;&#23567;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340; PDE &#27531;&#24046;&#23545;&#20110;&#36793;&#30028;&#20540;&#38382;&#39064;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#26159;&#28798;&#38590;&#24615;&#24536;&#21364;&#25439;&#23475;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#21021;&#20540;&#38382;&#39064;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26367;&#20195;&#30340;&#23616;&#37096;&#26102;&#38388;&#26041;&#27861;&#20013;&#65292;&#21487;&#20197;&#23558;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#32593;&#32476;&#21442;&#25968;&#19978;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#24182;&#23558;&#35299;&#21521;&#21069;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30446;&#21069;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36981;&#24490; ODE &#20250;&#23548;&#33268;&#38382;&#39064;&#26465;&#20214;&#22686;&#38271;&#26080;&#27861;&#25511;&#21046;&#65292;&#26368;&#32456;&#23548;&#33268;&#19981;&#21487;&#25509;&#21463;&#30340;&#22823;&#25968;&#20540;&#35823;&#24046;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528; ODE &#26041;&#27861;&#38543;&#30528; m &#30340;&#25968;&#37327;&#21576;&#31435;&#26041;&#32423;&#21035;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14989</link><description>&lt;p&gt;
Kullback-Leibler Maillard&#37319;&#26679;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#20998;&#24067;&#38598;&#20013;&#22312;&#21306;&#38388;$[0,1]$&#20869;&#30340;$K$&#33218;&#25968;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback-Leibler Maillard Sampling (KL-MS)&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#26159;Maillard&#37319;&#26679;&#22312;KL&#31354;&#38388;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;KL-MS&#22312;Bernoulli&#22870;&#21169;&#26102;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#33021;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#24230;&#19978;&#30028;&#20026;$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$&#65292;&#20854;&#20013;$\mu^*$&#26159;&#26368;&#20248;&#33218;&#30340;&#26399;&#26395;&#22870;&#21169;&#65292;$T$&#26159;&#26102;&#27573;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#23558;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#25193;&#23637;&#21040;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#24179;&#34913;&#20113;-&#36793;&#32536;&#35745;&#31639;&#30340;&#22788;&#29702;&#20248;&#21183;&#65292;&#23558;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#30340;&#20851;&#38190;&#25903;&#25745;&#12290;</title><link>http://arxiv.org/abs/2304.14982</link><description>&lt;p&gt;
&#20998;&#23618;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical and Decentralised Federated Learning. (arXiv:2304.14982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14982
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#23558;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#25193;&#23637;&#21040;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#24179;&#34913;&#20113;-&#36793;&#32536;&#35745;&#31639;&#30340;&#22788;&#29702;&#20248;&#21183;&#65292;&#23558;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#30340;&#20851;&#38190;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#29289;&#32852;&#32593;&#31561;&#32593;&#32476;&#31995;&#32479;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#24212;&#23545;&#30340;&#26032;&#25361;&#25112;&#12290;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#23558;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#25193;&#23637;&#21040;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#20197;&#20805;&#20998;&#32771;&#34385;&#24212;&#29992;&#38656;&#27714;&#25110;&#37096;&#32626;&#29615;&#22659;&#30340;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#36164;&#28304;&#33021;&#21147;&#21644;/&#25110;&#32593;&#32476;&#36830;&#25509;&#24615;&#65289;&#65292;&#24182;&#24179;&#34913;&#20113;-&#36793;&#32536;&#35745;&#31639;&#30340;&#22788;&#29702;&#20248;&#21183;&#12290;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#24456;&#21487;&#33021;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#65288;&#22914;&#26234;&#33021;&#20892;&#19994;&#21644;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#65289;&#30340;&#20851;&#38190;&#25903;&#25745;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#38477;&#20302;&#25104;&#26412;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#22312;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#19981;&#36866;&#21512;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#32852;&#37030;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#12290;&#27169;&#22411;&#32858;&#21512;&#31639;&#27861;&#12289;&#36719;&#20214;&#26694;&#26550;&#31561;&#25216;&#26415;&#37117;&#22312;&#19981;&#26029;&#22320;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has shown enormous promise as a way of training ML models in distributed environments while reducing communication costs and protecting data privacy. However, the rise of complex cyber-physical systems, such as the Internet-of-Things, presents new challenges that are not met with traditional FL methods. Hierarchical Federated Learning extends the traditional FL process to enable more efficient model aggregation based on application needs or characteristics of the deployment environment (e.g., resource capabilities and/or network connectivity). It illustrates the benefits of balancing processing across the cloud-edge continuum. Hierarchical Federated Learning is likely to be a key enabler for a wide range of applications, such as smart farming and smart energy management, as it can improve performance and reduce costs, whilst also enabling FL workflows to be deployed in environments that are not well-suited to traditional FL. Model aggregation algorithms, software fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;MLCopilot&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;MLCopilot&#22312;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14979</link><description>&lt;p&gt;
MLCopilot&#65306;&#37322;&#25918;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks. (arXiv:2304.14979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;MLCopilot&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;MLCopilot&#22312;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22240;&#27492;&#36880;&#28176;&#24341;&#21457;&#20102;&#23558;ML&#24212;&#29992;&#20110;&#29305;&#23450;&#22330;&#26223;&#30340;&#38656;&#27714;&#65292;&#20294;&#23454;&#29616;&#36215;&#26469;&#32791;&#26102;&#19988;&#19981;&#26131;&#12290; &#33258;&#21160;&#21270;&#35299;&#20915;ML&#20219;&#21153;&#65288;&#20363;&#22914;AutoML&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#36890;&#24120;&#32791;&#36153;&#26102;&#38388;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290; &#32780;&#19982;&#20043;&#30456;&#21453;&#65292;&#34429;&#28982;&#20154;&#31867;&#24037;&#31243;&#24072;&#20855;&#26377;&#29702;&#35299;&#20219;&#21153;&#21644;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#30340;&#38590;&#20197;&#32622;&#20449;&#30340;&#33021;&#21147;&#65292;&#20294;&#20182;&#20204;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#24448;&#24448;&#19981;&#20805;&#20998;&#19988;&#38590;&#20197;&#20511;&#21161;&#23450;&#37327;&#26041;&#27861;&#21033;&#29992;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;MLCopilot&#26469;&#24357;&#21512;&#26426;&#22120;&#26234;&#33021;&#21644;&#20154;&#31867;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#24320;&#21457;&#26032;&#22411;&#20219;&#21153;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#23637;LLM&#30340;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290; &#32463;&#36807;&#19968;&#20123;&#19987;&#38376;&#35774;&#35745;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#21487;&#20197;&#65288;i&#65289;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#20214;&#20013;&#35266;&#23519;&#29616;&#26377;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#21046;&#23450;&#35299;&#20915;ML&#20219;&#21153;&#30340;&#20855;&#20307;&#27493;&#39588;&#12290; &#25105;&#20204;&#23545;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MLCopilot&#22312;&#35299;&#20915;&#23454;&#38469;ML&#38382;&#39064;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#36136;&#37327;&#33258;&#36866;&#24212;&#30340;&#24179;&#22343;&#31574;&#30053;QA-SplitFed&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#23458;&#25143;&#31471;&#26631;&#31614;&#30340;&#20559;&#35265;&#12289;&#19981;&#20934;&#30830;&#21644;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#19981;&#20934;&#30830;&#27880;&#37322;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22810;&#20010;&#19981;&#20934;&#30830;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14976</link><description>&lt;p&gt;
&#19981;&#20934;&#30830;&#27880;&#37322;&#19979;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#20998;&#35010;&#24335;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quality-Adaptive Split-Federated Learning for Segmenting Medical Images with Inaccurate Annotations. (arXiv:2304.14976v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#36136;&#37327;&#33258;&#36866;&#24212;&#30340;&#24179;&#22343;&#31574;&#30053;QA-SplitFed&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#23458;&#25143;&#31471;&#26631;&#31614;&#30340;&#20559;&#35265;&#12289;&#19981;&#20934;&#30830;&#21644;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#19981;&#20934;&#30830;&#27880;&#37322;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22810;&#20010;&#19981;&#20934;&#30830;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SplitFed Learning&#26159;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#21457;&#23637;&#20043;&#19968;&#65292;&#26159;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#35010;&#24335;&#23398;&#20064;&#30340;&#32452;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#33258;&#36866;&#24212;&#24179;&#22343;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;QA-SplitFed&#12290;&#35813;&#31574;&#30053;&#19982;5&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#24179;&#22343;&#26041;&#27861;&#22312;&#32986;&#32974;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QA-SplitFed&#22312;&#22788;&#29702;&#22810;&#20010;&#19981;&#20934;&#30830;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
SplitFed Learning, a combination of Federated and Split Learning (FL and SL), is one of the most recent developments in the decentralized machine learning domain. In SplitFed learning, a model is trained by clients and a server collaboratively. For image segmentation, labels are created at each client independently and, therefore, are subject to clients' bias, inaccuracies, and inconsistencies. In this paper, we propose a data quality-based adaptive averaging strategy for SplitFed learning, called QA-SplitFed, to cope with the variation of annotated ground truth (GT) quality over multiple clients. The proposed method is compared against five state-of-the-art model averaging methods on the task of learning human embryo image segmentation. Our experiments show that all five baseline methods fail to maintain accuracy as the number of corrupted clients increases. QA-SplitFed, however, copes effectively with corruption as long as there is at least one uncorrupted client.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26469;&#20462;&#25913;&#20998;&#24067;&#65292;&#19981;&#20381;&#36182;&#20110;&#20998;&#32452;&#36873;&#25321;&#21644;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21305;&#37197;&#30446;&#26631;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.14963</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#37325;&#21152;&#26435;&#30340;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Flow Away your Differences: Conditional Normalizing Flows as an Improvement to Reweighting. (arXiv:2304.14963v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26469;&#20462;&#25913;&#20998;&#24067;&#65292;&#19981;&#20381;&#36182;&#20110;&#20998;&#32452;&#36873;&#25321;&#21644;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21305;&#37197;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#20110;&#37325;&#37325;&#26032;&#21152;&#26435;&#25216;&#26415;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#20462;&#25913;&#20998;&#24067;&#20197;&#32771;&#34385;&#22522;&#30784;&#26465;&#20214;&#20998;&#24067;&#30340;&#26399;&#26395;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26469;&#23398;&#20064;&#20174;&#20013;&#37319;&#26679;&#26032;&#20107;&#20214;&#30340;&#23436;&#25972;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#20854;&#30446;&#26631;&#20998;&#24067;&#20026;&#26465;&#20214;&#20540;&#20135;&#29983;&#25152;&#38656;&#30340;&#20462;&#25913;&#20998;&#24067;&#12290;&#19982;&#24120;&#35265;&#30340;&#37325;&#26032;&#21152;&#26435;&#25216;&#26415;&#30456;&#27604;&#65292;&#27492;&#36807;&#31243;&#19981;&#20381;&#36182;&#20110;&#20998;&#32452;&#36873;&#25321;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an alternative to reweighting techniques for modifying distributions to account for a desired change in an underlying conditional distribution, as is often needed to correct for mis-modelling in a simulated sample. We employ conditional normalizing flows to learn the full conditional probability distribution from which we sample new events for conditional values drawn from the target distribution to produce the desired, altered distribution. In contrast to common reweighting techniques, this procedure is independent of binning choice and does not rely on an estimate of the density ratio between two distributions.  In several toy examples we show that normalizing flows outperform reweighting approaches to match the distribution of the target.We demonstrate that the corrected distribution closes well with the ground truth, and a statistical uncertainty on the training dataset can be ascertained with bootstrapping. In our examples, this leads to a statistical precision up to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#12289;&#21487;&#35299;&#37322;&#30340;PSO&#21464;&#20307;&#31639;&#27861;&#65292;&#21363;&#31890;&#23376;&#21560;&#24341;&#23376;&#31639;&#27861;&#65288;PAO&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#24102;&#26377;&#31934;&#30830;&#21160;&#21147;&#23398;&#21644;&#38381;&#24335;&#36716;&#31227;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.14956</link><description>&lt;p&gt;
PAO&#65306;&#19968;&#31181;&#24102;&#26377;&#31934;&#30830;&#21160;&#21147;&#23398;&#21644;&#38381;&#24335;&#36716;&#31227;&#23494;&#24230;&#30340;&#36890;&#29992;&#31890;&#23376;&#32676;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PAO: A general particle swarm algorithm with exact dynamics and closed-form transition densities. (arXiv:2304.14956v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#12289;&#21487;&#35299;&#37322;&#30340;PSO&#21464;&#20307;&#31639;&#27861;&#65292;&#21363;&#31890;&#23376;&#21560;&#24341;&#23376;&#31639;&#27861;&#65288;PAO&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#24102;&#26377;&#31934;&#30830;&#21160;&#21147;&#23398;&#21644;&#38381;&#24335;&#36716;&#31227;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#26041;&#27861;&#30340;&#32771;&#34385;&#20013;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#20256;&#32479;&#26799;&#24230;&#20248;&#21270;&#22120;&#38590;&#20197;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#20854;&#20013;&#65292;&#25152;&#35859;&#30340;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#37492;&#20110;PSO&#39046;&#22495;&#30340;&#25104;&#29087;&#65292;&#26032;&#39062;&#30340;PSO&#31639;&#27861;&#30340;&#24615;&#33021;&#21482;&#33021;&#24102;&#26469;&#24494;&#23567;&#30340;&#25910;&#30410;&#12290;&#30456;&#21453;&#65292;&#22312;&#36861;&#27714;&#20855;&#26377;&#20854;&#20182;&#26377;&#29992;&#24615;&#36136;&#30340;&#31639;&#27861;&#26041;&#38754;&#65292;&#30740;&#31350;&#24037;&#20316;&#26356;&#22909;&#34987;&#25918;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36890;&#29992;&#12289;&#21487;&#35299;&#37322;&#30340;PSO&#21464;&#20307;&#31639;&#27861;&#8212;&#8212;&#31890;&#23376;&#21560;&#24341;&#23376;&#31639;&#27861;&#65288;PAO&#65289;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#30340;&#35774;&#35745;&#26159;&#20026;&#20102;&#20351;&#31890;&#23376;&#20174;&#19968;&#20010;&#20840;&#23616;&#26368;&#20248;&#35299;&#21040;&#21478;&#19968;&#20010;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#36816;&#21160;&#36807;&#31243;&#33021;&#22815;&#25551;&#36848;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
A great deal of research has been conducted in the consideration of meta-heuristic optimisation methods that are able to find global optima in settings that gradient based optimisers have traditionally struggled. Of these, so-called particle swarm optimisation (PSO) approaches have proven to be highly effective in a number of application areas. Given the maturity of the PSO field, it is likely that novel variants of the PSO algorithm stand to offer only marginal gains in terms of performance -- there is, after all, no free lunch. Instead of only chasing performance on suites of benchmark optimisation functions, it is argued herein that research effort is better placed in the pursuit of algorithms that also have other useful properties. In this work, a highly-general, interpretable variant of the PSO algorithm -- particle attractor algorithm (PAO) -- is proposed. Furthermore, the algorithm is designed such that the transition densities (describing the motions of the particles from one g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;</title><link>http://arxiv.org/abs/2304.14933</link><description>&lt;p&gt;
&#19968;&#39033;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#34701;&#21512;&#65288;&#20363;&#22914;&#25554;&#20540;&#25110;&#20219;&#21153;&#31639;&#26415;&#65289;&#23558;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#21512;&#24182;&#20197;&#29983;&#25104;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25216;&#26415;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#26159;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21270;&#19979;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#34701;&#21512;&#65292;&#23558;&#27492;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#65292;&#22312;&#35813;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#30340;transformer&#21512;&#24182;&#21040;&#29305;&#23450;&#27169;&#24577;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#24433;&#21709;&#27169;&#22411;&#34701;&#21512;&#21518;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#21021;&#22987;&#21270;&#12289;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#26469;&#21305;&#37197;&#27169;&#24577;&#19981;&#21487;&#30693;&#22522;&#32447;&#30340;&#24615;&#33021;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#30456;&#20284;&#26679;&#26412;&#19982;&#25935;&#24863;&#24615;&#24863;&#30693;&#65292;&#26377;&#25928;&#22320;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14925</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#21644;&#25935;&#24863;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Aware Neural Network from Similarity and Sensitivity. (arXiv:2304.14925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#30456;&#20284;&#26679;&#26412;&#19982;&#25935;&#24863;&#24615;&#24863;&#30693;&#65292;&#26377;&#25928;&#22320;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#24050;&#26377;&#22810;&#31181;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#24378;&#20551;&#35774;&#30340;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#23578;&#19981;&#26126;&#30830;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#30456;&#20284;&#26679;&#26412;&#19982;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28857;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35745;&#31639;&#39044;&#27979;&#20540;&#21644;&#30446;&#26631;&#20540;&#20043;&#38388;&#30340;&#32477;&#23545;&#24046;&#20540;&#65292;&#24182;&#35757;&#32451;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#36825;&#20123;&#32477;&#23545;&#24046;&#20540;&#25110;&#32773;&#32477;&#23545;&#35823;&#24046;&#12290;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#36739;&#39640;&#30340;&#39046;&#22495;&#34920;&#31034;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#36880;&#20010;&#36873;&#25321;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#65292;&#24182;&#35745;&#31639;&#39044;&#27979;&#21644;&#35823;&#24046;&#30340;&#25935;&#24863;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#36873;&#25321;&#25935;&#24863;&#24615;&#32771;&#34385;&#30340;&#30456;&#20284;&#26679;&#26412;&#24182;&#20445;&#23384;&#30456;&#20284;&#26679;&#26412;&#30340;&#32034;&#24341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have proposed several approaches for neural network (NN) based uncertainty quantification (UQ). However, most of the approaches are developed considering strong assumptions. Uncertainty quantification algorithms often perform poorly in an input domain and the reason for poor performance remains unknown. Therefore, we present a neural network training method that considers similar samples with sensitivity awareness in this paper. In the proposed NN training method for UQ, first, we train a shallow NN for the point prediction. Then, we compute the absolute differences between prediction and targets and train another NN for predicting those absolute differences or absolute errors. Domains with high average absolute errors represent a high uncertainty. In the next step, we select each sample in the training set one by one and compute both prediction and error sensitivities. Then we select similar samples with sensitivity consideration and save indexes of similar samples. The ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;&#65288;ICS&#65289;&#29992;&#20110;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#36873;&#25321;&#20851;&#38190;&#30340;&#36129;&#29486;&#36890;&#36947;&#65292;&#22312;&#25552;&#39640;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14920</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#24615;&#24341;&#23548;&#36827;&#34892;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#30340;&#33041;&#30005;&#20449;&#21495;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An EEG Channel Selection Framework for Driver Drowsiness Detection via Interpretability Guidance. (arXiv:2304.14920v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;&#65288;ICS&#65289;&#29992;&#20110;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#36873;&#25321;&#20851;&#38190;&#30340;&#36129;&#29486;&#36890;&#36947;&#65292;&#22312;&#25552;&#39640;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30130;&#21171;&#39550;&#39542;&#23545;&#34892;&#36710;&#23433;&#20840;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#20419;&#20351;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#36827;&#34892;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#12290;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#31934;&#31070;&#30130;&#21171;&#29366;&#24577;&#65292;&#22240;&#27492;&#22312;&#30130;&#21171;&#30417;&#27979;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;EEG&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#22024;&#26434;&#19988;&#20887;&#20313;&#30340;&#65292;&#36825;&#34987;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#20351;&#29992;&#21333;&#36890;&#36947;EEG&#25968;&#25454;&#25110;&#20840;&#22836;&#36890;&#36947;EEG&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23548;&#33268;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#24615;&#33021;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#24341;&#23548;&#30340;&#36890;&#36947;&#36873;&#25321;&#26694;&#26550;&#65288;ICS&#65289;&#29992;&#20110;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#35299;&#37322;&#24615;&#24341;&#23548;&#36880;&#27493;&#36873;&#25321;&#20851;&#38190;&#30340;&#36129;&#29486;&#36890;&#36947;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#20840;&#22836;&#36890;&#36947;EEG&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#25945;&#24072;&#32593;&#32476;&#65292;&#28982;&#21518;&#23545;&#35757;&#32451;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#24212;&#29992;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;CAM&#65289;&#26469;&#31361;&#20986;&#26174;&#31034;&#23545;&#20110;&#30130;&#21171;&#26816;&#27979;&#20219;&#21153;&#26377;&#39640;&#36129;&#29486;&#30340;&#36890;&#36947;&#12290;&#22522;&#20110;&#36873;&#25321;&#30340;&#20851;&#38190;&#36890;&#36947;&#65292;&#25105;&#20204;&#22312;&#31532;&#20108;&#38454;&#27573;&#35757;&#32451;&#19968;&#20010;&#26356;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#30340;&#36890;&#36947;&#25968;&#37327;&#23569;&#24471;&#22810;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ICS&#26694;&#26550;&#22312;&#25552;&#39640;&#21496;&#26426;&#30130;&#21171;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drowsy driving has a crucial influence on driving safety, creating an urgent demand for driver drowsiness detection. Electroencephalogram (EEG) signal can accurately reflect the mental fatigue state and thus has been widely studied in drowsiness monitoring. However, the raw EEG data is inherently noisy and redundant, which is neglected by existing works that just use single-channel EEG data or full-head channel EEG data for model training, resulting in limited performance of driver drowsiness detection. In this paper, we are the first to propose an Interpretability-guided Channel Selection (ICS) framework for the driver drowsiness detection task. Specifically, we design a two-stage training strategy to progressively select the key contributing channels with the guidance of interpretability. We first train a teacher network in the first stage using full-head channel EEG data. Then we apply the class activation mapping (CAM) to the trained teacher model to highlight the high-contributing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#33033;&#25615;&#27874;&#20998;&#26512;&#39044;&#27979;&#34880;&#21387;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#35768;&#22810;&#35770;&#25991;&#24120;&#24120;&#20986;&#29616;&#25968;&#25454;&#27844;&#28431;&#21644;&#23545;&#20219;&#21153;&#21450;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#19981;&#20999;&#23454;&#38469;&#38480;&#21046;&#12290;&#25552;&#20986;&#20102;&#26032;&#30340;&#24037;&#20855;&#26469;&#30830;&#23450;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;PPG&#65289;&#26159;&#21542;&#33021;&#39044;&#27979;&#25152;&#38656;&#30340;&#37327;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.14916</link><description>&lt;p&gt;
&#36890;&#36807;&#33033;&#25615;&#27874;&#20998;&#26512;&#20272;&#35745;&#34880;&#21387;&#30340;&#25361;&#25112;&#65306;&#33021;&#21542;&#25215;&#21463;&#65311;
&lt;/p&gt;
&lt;p&gt;
"Can't Take the Pressure?": Examining the Challenges of Blood Pressure Estimation via Pulse Wave Analysis. (arXiv:2304.14916v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#33033;&#25615;&#27874;&#20998;&#26512;&#39044;&#27979;&#34880;&#21387;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#35768;&#22810;&#35770;&#25991;&#24120;&#24120;&#20986;&#29616;&#25968;&#25454;&#27844;&#28431;&#21644;&#23545;&#20219;&#21153;&#21450;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#19981;&#20999;&#23454;&#38469;&#38480;&#21046;&#12290;&#25552;&#20986;&#20102;&#26032;&#30340;&#24037;&#20855;&#26469;&#30830;&#23450;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;PPG&#65289;&#26159;&#21542;&#33021;&#39044;&#27979;&#25152;&#38656;&#30340;&#37327;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#22914;&#20809;&#30005;&#23481;&#31215;&#27874;&#24418;&#22270;[PPG]&#65289;&#25512;&#23548;&#20581;&#24247;&#37327;&#24230;&#65288;&#22914;&#33889;&#33796;&#31958;&#27700;&#24179;&#25110;&#34880;&#21387;&#65289;&#26159;&#30446;&#21069;&#38750;&#24120;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#23545;&#20581;&#24247;&#31579;&#26597;&#12289;&#24930;&#24615;&#30149;&#31649;&#29702;&#21644;&#36828;&#31243;&#30417;&#27979;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20174;PPG&#33033;&#25615;&#27874;&#20998;&#26512;&#20013;&#39044;&#27979;&#34880;&#21387;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35768;&#22810;&#35770;&#25991;&#23481;&#26131;&#20986;&#29616;&#25968;&#25454;&#27844;&#28431;&#20197;&#21450;&#23545;&#20219;&#21153;&#21644;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#19981;&#20999;&#23454;&#38469;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#30830;&#23450;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;PPG&#65289;&#26159;&#21542;&#33021;&#22815;&#33391;&#22909;&#22320;&#39044;&#27979;&#25152;&#38656;&#30340;&#37327;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of observed wearable sensor data (e.g., photoplethysmograms [PPG]) to infer health measures (e.g., glucose level or blood pressure) is a very active area of research. Such technology can have a significant impact on health screening, chronic disease management and remote monitoring. A common approach is to collect sensor data and corresponding labels from a clinical grade device (e.g., blood pressure cuff), and train deep learning models to map one to the other. Although well intentioned, this approach often ignores a principled analysis of whether the input sensor data has enough information to predict the desired metric. We analyze the task of predicting blood pressure from PPG pulse wave analysis. Our review of the prior work reveals that many papers fall prey data leakage, and unrealistic constraints on the task and the preprocessing steps. We propose a set of tools to help determine if the input signal in question (e.g., PPG) is indeed a good predictor of the desired label
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#33719;&#24471;&#20102;&#36739;&#24378;&#24615;&#33021;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14912</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#35782;&#21035;&#31359;&#25140;&#25968;&#25454;&#30340;&#20154;&#31867;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition Using Self-Supervised Representations of Wearable Data. (arXiv:2304.14912v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#33719;&#24471;&#20102;&#36739;&#24378;&#24615;&#33021;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20329;&#25140;&#20256;&#24863;&#22120;&#36827;&#34892;&#33258;&#21160;&#21270;&#21644;&#20934;&#30830;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#21487;&#20197;&#23454;&#29616;&#23454;&#29992;&#21644;&#32463;&#27982;&#25928;&#30410;&#30340;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#65288;ADL&#65289;&#30340;&#36828;&#31243;&#30417;&#27979;&#65292;&#36825;&#20123;&#27963;&#21160;&#24050;&#34987;&#35777;&#26126;&#21487;&#25552;&#20379;&#36328;&#22810;&#20010;&#27835;&#30103;&#39046;&#22495;&#30340;&#20020;&#24202;&#27934;&#35265;&#12290;&#20934;&#30830;&#22320;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#65288;HAR&#65289;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21463;&#21040;&#32570;&#20047;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38459;&#30861;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#24456;&#23569;&#33021;&#22815;&#36229;&#36234;&#23427;&#20204;&#21407;&#22411;&#19978;&#30340;&#29305;&#23450;&#20256;&#24863;&#22120;&#65292;&#24341;&#21457;&#20102;&#26377;&#20851;&#26159;&#21542;&#21487;&#33021;&#22522;&#20110;&#21152;&#36895;&#24230;&#35745;&#30340;HAR&#30340;&#20105;&#35758;[Tong&#31561;&#20154;&#65292;2020]&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20845;&#31867;HAR&#27169;&#22411;&#65292;&#24403;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#30340;&#20923;&#32467;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#65292;&#32467;&#21512;&#20855;&#26377;&#26102;&#38388;&#24179;&#28369;&#30340;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#22120;&#12290;&#35813;&#27169;&#22411;&#22312;Capture24&#25968;&#25454;&#38598;[$\kappa$=0.86]&#20869;&#37096;&#25968;&#25454;&#38598;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Automated and accurate human activity recognition (HAR) using body-worn sensors enables practical and cost efficient remote monitoring of Activity of DailyLiving (ADL), which are shown to provide clinical insights across multiple therapeutic areas. Development of accurate algorithms for human activity recognition(HAR) is hindered by the lack of large real-world labeled datasets. Furthermore, algorithms seldom work beyond the specific sensor on which they are prototyped, prompting debate about whether accelerometer-based HAR is even possible [Tong et al., 2020]. Here we develop a 6-class HAR model with strong performance when evaluated on real-world datasets not seen during training. Our model is based on a frozen self-supervised representation learned on a large unlabeled dataset, combined with a shallow multi-layer perceptron with temporal smoothing. The model obtains in-dataset state-of-the art performance on the Capture24 dataset ($\kappa= 0.86$). Out-of-distribution (OOD) performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#20869;&#28857;&#31639;&#27861;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#25628;&#32034;&#26041;&#21521;&#21644;&#20869;&#37096;&#37051;&#22495;&#65292;&#33021;&#22815;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14907</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#20869;&#28857;&#31639;&#27861;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#20809;&#28369;&#26377;&#30028;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#20869;&#28857;&#31639;&#27861;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#25628;&#32034;&#26041;&#21521;&#21644;&#20869;&#37096;&#37051;&#22495;&#65292;&#33021;&#22815;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#30340;&#20869;&#28857;&#31639;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#23384;&#22312;&#32422;&#26463;&#30340;&#36830;&#32493;&#21487;&#24494;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#20809;&#28369;&#65288;&#38750;&#20984;&#65289;&#20248;&#21270;&#38382;&#39064;&#26102;&#19982;&#20854;&#20182;&#20869;&#28857;&#26041;&#27861;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#25628;&#32034;&#26041;&#21521;&#26159;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#35745;&#31639;&#24471;&#21040;&#30340;&#12290;&#23427;&#22312;&#20351;&#29992;&#21487;&#34892;&#22495;&#30340;&#20869;&#37096;&#37051;&#22495;&#65288;&#30001;&#27491;&#19988;&#28040;&#22833;&#30340;&#37051;&#22495;&#21442;&#25968;&#24207;&#21015;&#23450;&#20041;&#65289;&#30340;&#36807;&#31243;&#20013;&#20063;&#24456;&#29420;&#29305;&#65292;&#36890;&#36807;&#23558;&#36845;&#20195;&#24378;&#21046;&#20445;&#30041;&#22312;&#35813;&#37051;&#22495;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#31934;&#24515;&#24179;&#34913;&#23631;&#38556;&#12289;&#27493;&#38271;&#21644;&#37051;&#22495;&#24207;&#21015;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#28385;&#36275;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#35774;&#32622;&#19979;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#22312;&#20004;&#31181;&#35774;&#32622;&#19979;&#65292;&#25968;&#20540;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20248;&#20110;&#25237;&#24433;-&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20197;&#36890;&#29992;&#30005;&#27668; (GE) &#29123;&#27668;&#21160;&#21147;&#20844;&#21496;&#30340;&#27668;&#20307;&#21644;&#33976;&#27773;&#28065;&#36718;&#26381;&#21153;&#21644;&#21046;&#36896;&#19994;&#21153;&#20026;&#20363;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20135;&#21697;&#21487;&#29992;&#24615;&#26085;&#26399;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26799;&#24230;&#25552;&#21319;&#26426; (GBM) &#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026; 4.93 &#22825;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#20026; 6.47 &#22825;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#30830;&#23450;&#20102;&#24433;&#21709;&#20135;&#21697;&#21487;&#29992;&#24615;&#26085;&#26399;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.14902</link><description>&lt;p&gt;
&#25552;&#39640;&#20379;&#24212;&#38142;&#24377;&#24615;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#21463;&#24178;&#25200;&#24433;&#21709;&#19979;&#20135;&#21697;&#21487;&#29992;&#24615;&#26085;&#26399;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Supply Chain Resilience: A Machine Learning Approach for Predicting Product Availability Dates Under Disruption. (arXiv:2304.14902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14902
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20197;&#36890;&#29992;&#30005;&#27668; (GE) &#29123;&#27668;&#21160;&#21147;&#20844;&#21496;&#30340;&#27668;&#20307;&#21644;&#33976;&#27773;&#28065;&#36718;&#26381;&#21153;&#21644;&#21046;&#36896;&#19994;&#21153;&#20026;&#20363;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20135;&#21697;&#21487;&#29992;&#24615;&#26085;&#26399;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26799;&#24230;&#25552;&#21319;&#26426; (GBM) &#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026; 4.93 &#22825;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#20026; 6.47 &#22825;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#30830;&#23450;&#20102;&#24433;&#21709;&#20135;&#21697;&#21487;&#29992;&#24615;&#26085;&#26399;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20896;&#30123;&#24773;&#21644;&#25345;&#32493;&#23384;&#22312;&#30340;&#25919;&#27835;&#21644;&#22320;&#21306;&#20914;&#31361;&#23545;&#20840;&#29699;&#20379;&#24212;&#38142;&#20135;&#29983;&#20102;&#26497;&#20026;&#19981;&#21033;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#29289;&#27969;&#25805;&#20316;&#21644;&#22269;&#38469;&#36135;&#36816;&#20013;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#24310;&#35823;&#12290;&#20854;&#20013;&#26368;&#36843;&#20999;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#20135;&#21697;&#21487;&#29992;&#24615;&#26085;&#26399;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#20844;&#21496;&#21046;&#23450;&#26377;&#25928;&#29289;&#27969;&#21644;&#36816;&#36755;&#35745;&#21010;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#39044;&#27979;&#21487;&#29992;&#24615;&#26085;&#26399;&#22312;&#25191;&#34892;&#25104;&#21151;&#30340;&#29289;&#27969;&#25805;&#20316;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26368;&#32456;&#21487;&#20197;&#23558;&#24635;&#36816;&#36755;&#21644;&#24211;&#23384;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#36890;&#29992;&#30005;&#27668;&#65288;GE&#65289;&#29123;&#27668;&#21160;&#21147;&#20844;&#21496;&#30340;&#27668;&#20307;&#21644;&#33976;&#27773;&#28065;&#36718;&#26381;&#21153;&#21644;&#21046;&#36896;&#19994;&#21153;&#30340;&#36135;&#29289;&#21487;&#29992;&#24615;&#26085;&#26399;&#39044;&#27979;&#65292;&#21033;&#29992;&#25968;&#23383;&#29305;&#24449;&#21644;&#31867;&#21035;&#29305;&#24449;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#22238;&#24402;&#27169;&#22411;&#65292;&#21253;&#25324;&#31616;&#21333;&#22238;&#24402;&#12289;&#22871;&#32034;&#22238;&#24402;&#12289;&#23725;&#22238;&#24402;&#12289;&#24377;&#24615;&#32593;&#32476;&#12289;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBM&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NM&#65289;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GBM&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;4.93&#22825;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;6.47&#22825;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#20135;&#21697;&#21487;&#29992;&#24615;&#26085;&#26399;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#36890;&#36807;&#20934;&#30830;&#30340;&#20135;&#21697;&#21487;&#29992;&#24615;&#39044;&#27979;&#25552;&#39640;&#20379;&#24212;&#38142;&#24377;&#24615;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#36825;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#34892;&#19994;&#21644;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID 19 pandemic and ongoing political and regional conflicts have a highly detrimental impact on the global supply chain, causing significant delays in logistics operations and international shipments. One of the most pressing concerns is the uncertainty surrounding the availability dates of products, which is critical information for companies to generate effective logistics and shipment plans. Therefore, accurately predicting availability dates plays a pivotal role in executing successful logistics operations, ultimately minimizing total transportation and inventory costs. We investigate the prediction of product availability dates for General Electric (GE) Gas Power's inbound shipments for gas and steam turbine service and manufacturing operations, utilizing both numerical and categorical features. We evaluate several regression models, including Simple Regression, Lasso Regression, Ridge Regression, Elastic Net, Random Forest (RF), Gradient Boosting Machine (GBM), and Neural 
&lt;/p&gt;</description></item><item><title>TADS&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#25104;&#26377;&#20851;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#30340;&#35777;&#26126;&#25110;&#32773;&#35823;&#24046;&#29305;&#24449;&#65292;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;TADS&#21487;&#20197;&#29992;&#20110;&#25552;&#20379;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#38169;&#35823;&#30340;&#31934;&#30830;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2304.14888</link><description>&lt;p&gt;
&#26377;&#31867;&#22411;&#20223;&#23556;&#20915;&#31574;&#32467;&#26500;&#30340;&#23041;&#21147;&#65306;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Power of Typed Affine Decision Structures: A Case Study. (arXiv:2304.14888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14888
&lt;/p&gt;
&lt;p&gt;
TADS&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#25104;&#26377;&#20851;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#30340;&#35777;&#26126;&#25110;&#32773;&#35823;&#24046;&#29305;&#24449;&#65292;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;TADS&#21487;&#20197;&#29992;&#20110;&#25552;&#20379;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#38169;&#35823;&#30340;&#31934;&#30830;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TADS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#27905;&#30340;&#31070;&#32463;&#32593;&#32476;&#30333;&#30418;&#34920;&#31034;&#12290;&#26412;&#25991;&#23558;TADS&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#38382;&#39064;&#65292;&#20351;&#29992;TADS&#29983;&#25104;&#26377;&#20851;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#30340;&#35777;&#26126;&#25110;&#31616;&#27905;&#35823;&#24046;&#29305;&#24449;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#65288;&#21363;&#36755;&#20837;&#24494;&#23567;&#21464;&#21270;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#30693;&#35273;&#22823;&#24133;&#24230;&#25913;&#21464;&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;TADS&#21487;&#20197;&#29992;&#20110;&#25552;&#20379;&#20851;&#20110;&#40065;&#26834;&#24615;&#38169;&#35823;&#22914;&#20309;&#21450;&#20854;&#21457;&#29983;&#22312;&#20309;&#22788;&#30340;&#31934;&#30830;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
TADS are a novel, concise white-box representation of neural networks. In this paper, we apply TADS to the problem of neural network verification, using them to generate either proofs or concise error characterizations for desirable neural network properties. In a case study, we consider the robustness of neural networks to adversarial attacks, i.e., small changes to an input that drastically change a neural networks perception, and show that TADS can be used to provide precise diagnostics on how and where robustness errors a occur. We achieve these results by introducing Precondition Projection, a technique that yields a TADS describing network behavior precisely on a given subset of its input space, and combining it with PCA, a traditional, well-understood dimensionality reduction technique. We show that PCA is easily compatible with TADS. All analyses can be implemented in a straightforward fashion using the rich algebraic properties of TADS, demonstrating the utility of the TADS fr
&lt;/p&gt;</description></item><item><title>ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#28041;&#21450;&#24773;&#24863;&#20849;&#20139;&#21644;&#35831;&#27714;&#26816;&#27979;&#65292;&#25552;&#20379;&#20102;&#22522;&#32447;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#22120;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14882</link><description>&lt;p&gt;
ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#65306;&#24773;&#24863;&#20849;&#20139;&#19982;&#35831;&#27714;
&lt;/p&gt;
&lt;p&gt;
The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share &amp; Requests. (arXiv:2304.14882v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14882
&lt;/p&gt;
&lt;p&gt;
ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#28041;&#21450;&#24773;&#24863;&#20849;&#20139;&#21644;&#35831;&#27714;&#26816;&#27979;&#65292;&#25552;&#20379;&#20102;&#22522;&#32447;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#22120;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#39318;&#27425;&#22312;&#26126;&#30830;&#23450;&#20041;&#30340;&#26465;&#20214;&#19979;&#35299;&#20915;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#22312;&#24773;&#24863;&#20849;&#20139;&#23376;&#25361;&#25112;&#20013;&#38656;&#35201;&#23545;&#35821;&#38899;&#36827;&#34892;&#22238;&#24402;&#65292;&#32780;&#22312;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#38656;&#35201;&#26816;&#27979;&#35831;&#27714;&#21644;&#25265;&#24616;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23376;&#25361;&#25112;&#12289;&#22522;&#32447;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#36890;&#24120;&#30340; ComPaRE &#29305;&#24449;&#12289;auDeep &#24037;&#20855;&#21253;&#21644;&#20351;&#29992;&#39044;&#35757;&#32451; CNN &#30340;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450; wav2vec2 &#27169;&#22411;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ACM Multimedia 2023 Computational Paralinguistics Challenge addresses two different problems for the first time in a research competition under well-defined conditions: In the Emotion Share Sub-Challenge, a regression on speech has to be made; and in the Requests Sub-Challenges, requests and complaints need to be detected. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the usual ComPaRE features, the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the DeepSpectRum toolkit; in addition, wav2vec2 models are used.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;&#65292;DeepStock&#36890;&#36807;&#26597;&#30475;&#32929;&#31080;&#20215;&#26684;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Resnet&#21644;logits&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#26410;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#65292;&#24182;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#24066;&#22330;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#21033;&#28070;&#12290;</title><link>http://arxiv.org/abs/2304.14870</link><description>&lt;p&gt;
Deep Stock: &#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Deep Stock: training and trading scheme using deep learning. (arXiv:2304.14870v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;&#65292;DeepStock&#36890;&#36807;&#26597;&#30475;&#32929;&#31080;&#20215;&#26684;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Resnet&#21644;logits&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#26410;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#65292;&#24182;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#24066;&#22330;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#25928;&#24066;&#22330;&#20551;&#35828;&#23384;&#22312;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#32929;&#31080;&#24066;&#22330;&#23384;&#22312;&#22833;&#28789;&#29616;&#35937;&#65292;&#23548;&#33268;&#20986;&#29616;&#20102;&#19968;&#20123;&#33021;&#22815;&#33719;&#24471;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#25216;&#26415;&#65292;&#21363;alpha&#12290;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#31995;&#32479;&#24615;&#20132;&#26131;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#20998;&#26512;&#21644;&#39044;&#27979;&#24066;&#22330;&#34892;&#20026;&#30340;&#24378;&#22823;&#24037;&#20855;&#24050;&#32463;&#24320;&#22987;&#23853;&#38706;&#22836;&#35282;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#19987;&#19994;&#20132;&#26131;&#21592;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26597;&#30475;&#20808;&#21069;&#30340;600&#22825;&#30340;&#32929;&#31080;&#20215;&#26684;&#65292;&#24182;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#25509;&#19979;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;DeepStock&#65292;&#20351;&#29992;Resnet&#30340;&#36339;&#36291;&#36830;&#25509;&#21644;logits&#26469;&#22686;&#21152;&#27169;&#22411;&#22312;&#20132;&#26131;&#26041;&#26696;&#20013;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#32929;&#31080;&#24066;&#22330;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#38889;&#22269;&#24066;&#22330;&#19978;&#33719;&#24471;&#20102;N&#65285;&#30340;&#21033;&#28070;&#65292;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;M&#65285;&#65292;&#24182;&#22312;&#32654;&#22269;&#24066;&#22330;&#19978;&#33719;&#24471;&#20102;A&#65285;&#30340;&#21033;&#28070;&#65292;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;B&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the efficient market hypothesis, many studies suggest the existence of inefficiencies in the stock market, leading to the development of techniques to gain above-market returns, known as alpha. Systematic trading has undergone significant advances in recent decades, with deep learning emerging as a powerful tool for analyzing and predicting market behavior. In this paper, we propose a model inspired by professional traders that look at stock prices of the previous 600 days and predicts whether the stock price rises or falls by a certain percentage within the next D days. Our model, called DeepStock, uses Resnet's skip connections and logits to increase the probability of a model in a trading scheme. We test our model on both the Korean and US stock markets and achieve a profit of N\% on Korea market, which is M\% above the market return, and profit of A\% on US market, which is B\% above the market return.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#22312;CNN&#20013;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#26412;&#25991;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;CNN&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65306;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#21644;&#27010;&#24565;&#24402;&#23646;&#31283;&#23450;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#35299;&#20915;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14864</link><description>&lt;p&gt;
&#35780;&#20272;CNN&#20013;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Stability of Semantic Concept Representations in CNNs for Robust Explainability. (arXiv:2304.14864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#22312;CNN&#20013;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#26412;&#25991;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;CNN&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65306;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#21644;&#27010;&#24565;&#24402;&#23646;&#31283;&#23450;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#35299;&#20915;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#65292;&#20998;&#26512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20013;&#35821;&#20041;&#27010;&#24565;&#30340;&#34920;&#31034;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#21160;&#26426;&#26159;&#22240;&#20026;&#21508;&#20010;&#39046;&#22495;&#22914;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#30340;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#38656;&#35201;&#36879;&#26126;&#24230;&#12290;&#28982;&#32780;&#65292;&#35201;&#23558;&#36825;&#20123;&#27010;&#24565;&#34920;&#31034;&#29992;&#20110;&#23433;&#20840;&#30456;&#20851;&#30446;&#30340;&#65292;&#20363;&#22914;&#26816;&#26597;&#25110;&#38169;&#35823;&#26816;&#32034;&#65292;&#36825;&#20123;&#34920;&#31034;&#24517;&#39035;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;CNN&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#31283;&#23450;&#24615;&#65306;&#27010;&#24565;&#26816;&#32034;&#31283;&#23450;&#24615;&#21644;&#27010;&#24565;&#24402;&#23646;&#31283;&#23450;&#24615;&#12290;&#20197;&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;CNN&#30340;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#20026;&#25351;&#23548;&#30446;&#26631;&#65292;&#25104;&#21151;&#22320;&#23558;&#29616;&#26377;&#30340;&#27010;&#24565;&#20998;&#26512;&#65288;CA&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#20854;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#27010;&#24565;&#26816;&#32034;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#32771;&#34385;&#27010;&#24565;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#65292;&#19982;&#23618;&#21644;&#27010;&#24565;&#34920;&#31034;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of how semantic concepts are represented within Convolutional Neural Networks (CNNs) is a widely used approach in Explainable Artificial Intelligence (XAI) for interpreting CNNs. A motivation is the need for transparency in safety-critical AI-based systems, as mandated in various domains like automated driving. However, to use the concept representations for safety-relevant purposes, like inspection or error retrieval, these must be of high quality and, in particular, stable. This paper focuses on two stability goals when working with concept representations in computer vision CNNs: stability of concept retrieval and of concept attribution. The guiding use-case is a post-hoc explainability framework for object detection (OD) CNNs, towards which existing concept analysis (CA) methods are successfully adapted. To address concept retrieval stability, we propose a novel metric that considers both concept separation and consistency, and is agnostic to layer and concept representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#23383;&#20856;&#30340;&#25345;&#20037;&#22270;&#30340;&#32039;&#20945;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#25968;&#25454;&#38477;&#32500;&#21644;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2304.14852</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#23383;&#20856;&#30340;&#25345;&#20037;&#22270;&#30340;&#32039;&#20945;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Dictionaries of Persistence Diagrams. (arXiv:2304.14852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#23383;&#20856;&#30340;&#25345;&#20037;&#22270;&#30340;&#32039;&#20945;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#25968;&#25454;&#38477;&#32500;&#21644;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#20197;&#21407;&#23376;&#22270;&#23383;&#20856;&#30340;&#21152;&#26435;Wasserstein barycenters [99]&#65292;[101] &#30340;&#24418;&#24335;&#23545;&#19968;&#32452;&#25345;&#20037;&#22270;&#36827;&#34892;&#31616;&#27905;&#32534;&#30721;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35299;&#20915;&#30456;&#24212;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;Barycenter&#26435;&#37325;&#30340;&#20248;&#21270;&#19982;Atom&#22270;&#30340;&#20248;&#21270;&#20132;&#38169;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#20010;&#23376;&#38382;&#39064;&#26799;&#24230;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#30830;&#20445;&#24555;&#36895;&#36845;&#20195;&#65292;&#24182;&#19988;&#36824;&#21033;&#29992;&#20102;&#20849;&#20139;&#20869;&#23384;&#24182;&#34892;&#24615;&#12290;&#23545;&#20844;&#20849;&#21512;&#22863;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#26368;&#22823;&#31034;&#20363;&#30340;Wasserstein&#23383;&#20856;&#35745;&#31639;&#26102;&#38388;&#22312;&#25968;&#20998;&#38047;&#20043;&#20869;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#25928;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;Wassserstein&#23383;&#20856;&#24212;&#29992;&#20110;&#25968;&#25454;&#38477;&#32500;&#65292;&#24182;&#36890;&#36807;&#20165;&#29992;&#20854;&#37325;&#37327;&#26469;&#32039;&#20945;&#22320;&#34920;&#31034;Persistence&#22270;&#26469;&#21487;&#38752;&#22320;&#21387;&#32553;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a computational framework for the concise encoding of an ensemble of persistence diagrams, in the form of weighted Wasserstein barycenters [99], [101] of a dictionary of atom diagrams. We introduce a multi-scale gradient descent approach for the efficient resolution of the corresponding minimization problem, which interleaves the optimization of the barycenter weights with the optimization of the atom diagrams. Our approach leverages the analytic expressions for the gradient of both sub-problems to ensure fast iterations and it additionally exploits shared-memory parallelism. Extensive experiments on public ensembles demonstrate the efficiency of our approach, with Wasserstein dictionary computations in the orders of minutes for the largest examples. We show the utility of our contributions in two applications. First, we apply Wassserstein dictionaries to data reduction and reliably compress persistence diagrams by concisely representing them with their weights in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38899;&#20048;&#20154;&#22768;&#20998;&#31163;&#30340;&#24863;&#30693;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#31526;&#20043;&#38388;&#30340;&#38142;&#25509;&#23558;&#19981;&#21516;&#30340;&#20154;&#22768;&#36827;&#34892;&#20998;&#31163;&#65292;&#20174;&#32780;&#40723;&#21169;&#21333;&#22768;&#37096;&#65288;&#20154;&#22768;&#65289;&#36712;&#36857;&#30340;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2304.14848</link><description>&lt;p&gt;
&#23558;&#38899;&#20048;&#20154;&#22768;&#20998;&#31163;&#24314;&#27169;&#20026;&#38142;&#36335;&#39044;&#27979;&#65306;&#23558;&#38899;&#20048;&#24863;&#30693;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem. (arXiv:2304.14848v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38899;&#20048;&#20154;&#22768;&#20998;&#31163;&#30340;&#24863;&#30693;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#31526;&#20043;&#38388;&#30340;&#38142;&#25509;&#23558;&#19981;&#21516;&#30340;&#20154;&#22768;&#36827;&#34892;&#20998;&#31163;&#65292;&#20174;&#32780;&#40723;&#21169;&#21333;&#22768;&#37096;&#65288;&#20154;&#22768;&#65289;&#36712;&#36857;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#22810;&#22768;&#37096;&#38899;&#20048;&#20013;&#20998;&#31163;&#19981;&#21516;&#20154;&#22768;&#65288;&#21363;&#21333;&#22768;&#37096;&#26059;&#24459;&#27969;&#65289;&#30340;&#24863;&#30693;&#38382;&#39064;&#12290;&#25105;&#20204;&#38024;&#23545;&#31526;&#21495;&#38899;&#20048;&#65292;&#21363;&#26126;&#30830;&#32534;&#30721;&#38899;&#31526;&#65292;&#23558;&#27492;&#20219;&#21153;&#24314;&#27169;&#20026;&#20174;&#31163;&#25955;&#35266;&#27979;&#65288;&#21363;&#38899;&#39640;-&#26102;&#38388;&#31354;&#38388;&#20013;&#30340;&#38899;&#31526;&#65289;&#20013;&#30340;&#22810;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#38899;&#31526;&#21019;&#24314;&#19968;&#20010;&#33410;&#28857;&#26469;&#26500;&#24314;&#38899;&#20048;&#29255;&#27573;&#30340;&#22270;&#24418;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#20004;&#20010;&#38899;&#31526;&#20043;&#38388;&#30340;&#38142;&#25509;&#26469;&#20998;&#31163;&#26059;&#24459;&#36712;&#36857;&#65292;&#22914;&#26524;&#23427;&#20204;&#22312;&#21516;&#19968;&#22768;&#37096;/&#27969;&#20013;&#36830;&#32493;&#12290;&#36825;&#31181;&#23616;&#37096;&#65292;&#36138;&#24515;&#30340;&#39044;&#27979;&#26159;&#30001;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#30340;&#33410;&#28857;&#23884;&#20837;&#25152;&#23454;&#29616;&#30340;&#65292;&#35813;&#33410;&#28857;&#23884;&#20837;&#21487;&#20197;&#25429;&#25417;&#36712;&#36857;&#20043;&#38388;&#21644;&#36712;&#36857;&#20869;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#40723;&#21169;&#36755;&#20986;&#36981;&#23432;&#27599;&#20010;&#33410;&#28857;&#26368;&#22810;&#26377;&#19968;&#20010;&#20837;&#21475;&#21644;&#19968;&#20010;&#20986;&#21475;&#38142;&#25509;&#30340;&#22810;&#36712;&#36857;&#36319;&#36394;&#21069;&#25552;&#65292;&#25903;&#25345;&#21333;&#22768;&#37096;&#65288;&#20154;&#22768;&#65289;&#36712;&#36857;&#65307;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#22312;&#20854;&#20182;&#29983;&#25104;&#24207;&#21015;&#35774;&#32622;&#20013;&#20063;&#21487;&#33021;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper targets the perceptual task of separating the different interacting voices, i.e., monophonic melodic streams, in a polyphonic musical piece. We target symbolic music, where notes are explicitly encoded, and model this task as a Multi-Trajectory Tracking (MTT) problem from discrete observations, i.e., notes in a pitch-time space. Our approach builds a graph from a musical piece, by creating one node for every note, and separates the melodic trajectories by predicting a link between two notes if they are consecutive in the same voice/stream. This kind of local, greedy prediction is made possible by node embeddings created by a heterogeneous graph neural network that can capture inter- and intra-trajectory information. Furthermore, we propose a new regularization loss that encourages the output to respect the MTT premise of at most one incoming and one outgoing link for every node, favouring monophonic (voice) trajectories; this loss function might also be useful in other gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14836</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#23545;&#22823;&#35268;&#27169;CNN&#36827;&#34892;&#25935;&#24863;&#35843;&#25972;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#23433;&#20840;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption. (arXiv:2304.14836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#36817;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36235;&#21183;&#26159;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#25191;&#34892;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#35757;&#32451;&#36866;&#29992;&#20110;HE&#30340;&#21152;&#23494;&#25110;&#26410;&#21152;&#23494;&#30340;&#28145;&#23618;CNN&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#22522;&#26412;&#21644;&#29616;&#20195;CNN&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#20363;&#22914;ResNet&#21644;ConvNeXt&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;HELayers SDK&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#20135;&#29983;&#20102;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#26102;&#65292;&#25105;&#20204;&#30340;ResNet-18/50/101&#23454;&#29616;&#20165;&#38656;&#35201;7&#12289;31&#21644;57&#20998;&#38047;&#65292;&#36825;&#34920;&#26126;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#22312;HE&#19979;&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#21644;&#36339;&#36291;&#36830;&#25509;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning solutions have recently gained significant attention. One promising research trend is using Homomorphic Encryption (HE), a method for performing computation over encrypted data. One major challenge in this approach is training HE-friendly, encrypted or unencrypted, deep CNNs with decent accuracy. We propose a novel training method for HE-friendly models, and demonstrate it on fundamental and modern CNNs, such as ResNet and ConvNeXt. After training, we evaluate our models by running encrypted samples using HELayers SDK and proving that they yield the desired results. When running on a GPU over the ImageNet dataset, our ResNet-18/50/101 implementations take only 7, 31 and 57 minutes, respectively, which shows that this solution is practical. Furthermore, we present several insights on handling the activation functions and skip-connections under HE. Finally, we demonstrate in an unprecedented way how to perform secure zero-shot prediction using a CLIP m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EXPECTED&#30340;&#25361;&#25112;&#65292;&#35299;&#20915;&#27169;&#22411;&#35843;&#25972;&#38382;&#39064;&#65292;&#27169;&#22411;&#25552;&#20379;&#32773;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#26412;&#22320;&#29992;&#25143;&#30340;&#21453;&#39304;&#22810;&#27425;&#35775;&#38382;&#20505;&#36873;&#27169;&#22411;&#30340;&#25805;&#20316;&#24615;&#33021;&#65292;&#20174;&#32780;&#20248;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.14831</link><description>&lt;p&gt;
&#20174;&#38480;&#21046;&#24615;&#21453;&#39304;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Earning Extra Performance from Restrictive Feedbacks. (arXiv:2304.14831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EXPECTED&#30340;&#25361;&#25112;&#65292;&#35299;&#20915;&#27169;&#22411;&#35843;&#25972;&#38382;&#39064;&#65292;&#27169;&#22411;&#25552;&#20379;&#32773;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#26412;&#22320;&#29992;&#25143;&#30340;&#21453;&#39304;&#22810;&#27425;&#35775;&#38382;&#20505;&#36873;&#27169;&#22411;&#30340;&#25805;&#20316;&#24615;&#33021;&#65292;&#20174;&#32780;&#20248;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#38754;&#20020;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65306;&#27169;&#22411;&#25552;&#20379;&#32773;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#26412;&#22320;&#29992;&#25143;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#22914;&#26524;&#21487;&#20197;&#23558;&#30446;&#26631;&#25968;&#25454;&#20256;&#36882;&#32473;&#27169;&#22411;&#65292;&#37027;&#20040;&#36825;&#20010;&#38382;&#39064;&#23601;&#36716;&#21270;&#20026;&#26631;&#20934;&#30340;&#27169;&#22411;&#35843;&#25972;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30446;&#26631;&#25968;&#25454;&#24182;&#19981;&#20849;&#20139;&#32473;&#27169;&#22411;&#25552;&#20379;&#32773;&#65292;&#32780;&#21482;&#26159;&#19968;&#20123;&#20851;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#21487;&#20379;&#35775;&#38382;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EXPECTED&#65288;Earning eXtra PerformancE from restriCTive feEDdbacks&#65289;&#30340;&#25361;&#25112;&#65292;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#24418;&#24335;&#30340;&#27169;&#22411;&#35843;&#25972;&#38382;&#39064;&#65292;&#20801;&#35768;&#27169;&#22411;&#25552;&#20379;&#32773;&#36890;&#36807;&#26469;&#33258;&#26412;&#22320;&#29992;&#25143;&#65288;&#25110;&#19968;&#32452;&#29992;&#25143;&#65289;&#30340;&#21453;&#39304;&#22810;&#27425;&#35775;&#38382;&#20505;&#36873;&#27169;&#22411;&#30340;&#25805;&#20316;&#24615;&#33021;&#12290;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21033;&#29992;&#21453;&#39304;&#26368;&#32456;&#21521;&#26412;&#22320;&#29992;&#25143;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#19981;&#21516;&#65292;EXPECTED&#22312;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning applications encounter a situation where model providers are required to further refine the previously trained model so as to gratify the specific need of local users. This problem is reduced to the standard model tuning paradigm if the target data is permissibly fed to the model. However, it is rather difficult in a wide range of practical cases where target data is not shared with model providers but commonly some evaluations about the model are accessible. In this paper, we formally set up a challenge named \emph{Earning eXtra PerformancE from restriCTive feEDdbacks} (EXPECTED) to describe this form of model tuning problems. Concretely, EXPECTED admits a model provider to access the operational performance of the candidate model multiple times via feedback from a local user (or a group of users). The goal of the model provider is to eventually deliver a satisfactory model to the local user(s) by utilizing the feedbacks. Unlike existing model tuning methods wher
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14824</link><description>&lt;p&gt;
&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
A noise-robust acoustic method for recognition of foraging activities of grazing cattle. (arXiv:2304.14824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#20083;&#21046;&#21697;&#24066;&#22330;&#20013;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#20892;&#27665;&#24517;&#39035;&#19981;&#26029;&#25913;&#36827;&#20182;&#20204;&#30340;&#30044;&#29287;&#29983;&#20135;&#31995;&#32479;&#12290;&#31934;&#30830;&#30044;&#29287;&#19994;&#25216;&#26415;&#25552;&#20379;&#20102;&#21830;&#19994;&#20892;&#22330;&#21160;&#29289;&#20010;&#20307;&#21270;&#30417;&#27979;&#65292;&#20248;&#21270;&#30044;&#29287;&#29983;&#20135;&#12290;&#36830;&#32493;&#30340;&#22768;&#23398;&#30417;&#27979;&#26159;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#24863;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#33258;&#30001;&#25918;&#29287;&#29275;&#30340;&#26085;&#21453;&#21005;&#21644;&#21507;&#33609;&#26102;&#38388;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#29287;&#22330;&#19978;&#30340;&#20856;&#22411;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26126;&#26174;&#24433;&#21709;&#24403;&#21069;&#22768;&#23398;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#26041;&#27861;&#65292;&#31216;&#20026;&#25239;&#22122;&#22768;&#35269;&#39135;&#27963;&#21160;&#35782;&#21035;&#22120; (NRFAR)&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#30830;&#23450;&#35269;&#39135;&#27963;&#21160;&#30340;&#31361;&#21457;&#12290;NRFAR &#30340;&#21152;&#24615;&#22122;&#22768;&#40065;&#26834;&#24615;&#20351;&#29992;&#38745;&#24577;&#39640;&#26031;&#30333;&#22122;&#22768;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#38750;&#38745;&#24577;&#33258;&#28982;&#22122;&#22768;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
To stay competitive in the growing dairy market, farmers must continuously improve their livestock production systems. Precision livestock farming technologies provide individualised monitoring of animals on commercial farms, optimising livestock production. Continuous acoustic monitoring is a widely accepted sensing technique used to estimate the daily rumination and grazing time budget of free-ranging cattle. However, typical environmental and natural noises on pasture noticeably affect the performance and generalisation of current acoustic methods. In this study, we present an acoustic method called Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method determines foraging activity bouts by analysing fixed-length segments of identified jaw movement events associated with grazing and rumination. The additive noise robustness of NRFAR was evaluated for several signal-to-noise ratios, using stationary Gaussian white noise and four different non-stationary natural noise 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.14807</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation. (arXiv:2304.14807v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#23494;&#24230;&#26159;&#34920;&#24449;&#20219;&#20309;&#31561;&#31163;&#23376;&#20307;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#24212;&#29992;&#21644;&#30740;&#31350;&#37117;&#22522;&#20110;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#21644;&#31561;&#31163;&#23376;&#20307;&#28201;&#24230;&#12290;&#20256;&#32479;&#30340;&#30005;&#23376;&#23494;&#24230;&#27979;&#37327;&#26041;&#27861;&#38024;&#23545;&#32473;&#23450;&#32447;&#24615;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#35774;&#22791;&#25552;&#20379;&#36724;&#21521;&#21644;&#24452;&#21521;&#21078;&#38754;&#12290;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#25805;&#20316;&#33539;&#22260;&#36739;&#23567;&#12289;&#20202;&#22120;&#27785;&#37325;&#20197;&#21450;&#25968;&#25454;&#20998;&#26512;&#36807;&#31243;&#22797;&#26434;&#31561;&#20027;&#35201;&#32570;&#28857;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#30830;&#23450;&#31561;&#31163;&#23376;&#20307;&#20869;&#30005;&#23376;&#23494;&#24230;&#21078;&#38754;&#12290;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;&#35813;&#31574;&#30053;&#38024;&#23545;&#19968;&#20010;&#27169;&#25311;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#65292;&#20854;&#20013;&#21253;&#25324;&#20302;&#28201;&#12289;&#38750;&#30913;&#21270;&#21644;&#30896;&#25758;&#24615;&#31561;&#31163;&#23376;&#20307;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#39640;&#26031;&#24418;&#29366;&#23494;&#24230;&#21078;&#38754;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electron density is a key parameter to characterize any plasma. Most of the plasma applications and research in the area of low-temperature plasmas (LTPs) is based on plasma density and plasma temperature. The conventional methods for electron density measurements offer axial and radial profiles for any given linear LTP device. These methods have major disadvantages of operational range (not very wide), cumbersome instrumentation, and complicated data analysis procedures. To address such practical concerns, the article proposes a novel machine learning (ML) assisted microwave-plasma interaction based strategy which is capable enough to determine the electron density profile within the plasma. The electric field pattern due to microwave scattering is measured to estimate the density profile. The proof of concept is tested for a simulated training data set comprising a low-temperature, unmagnetized, collisional plasma. Different types of Gaussian-shaped density profiles, in the range
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Pre-Post-LN&#21452;&#37325;&#27531;&#24046;&#36830;&#25509;&#30340;&#26032;&#22411;Transformer&#26550;&#26500;ResiDual&#65292;&#35299;&#20915;&#20102;Post-LN&#21644;Pre-LN&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14802</link><description>&lt;p&gt;
ResiDual&#65306;&#20855;&#26377;&#21452;&#37325;&#27531;&#24046;&#36830;&#25509;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Pre-Post-LN&#21452;&#37325;&#27531;&#24046;&#36830;&#25509;&#30340;&#26032;&#22411;Transformer&#26550;&#26500;ResiDual&#65292;&#35299;&#20915;&#20102;Post-LN&#21644;Pre-LN&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;Transformer&#32593;&#32476;&#24050;&#25104;&#20026;&#35768;&#22810;&#20219;&#21153;&#30340;&#39318;&#36873;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#20248;&#21270;&#22320;&#23454;&#29616;Transformer&#20013;&#30340;&#27531;&#24046;&#36830;&#25509;&#20173;&#23384;&#22312;&#20105;&#35758;&#65292;&#32780;&#36825;&#20123;&#27531;&#24046;&#36830;&#25509;&#23545;&#20110;&#26377;&#25928;&#35757;&#32451;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#20307;&#26159;Post-Layer-Normalization(Post-LN)&#21644;Pre-Layer-Normalization(Pre-LN) Transformers&#65292;&#23427;&#20204;&#20998;&#21035;&#22312;&#27599;&#20010;&#27531;&#24046;&#22359;&#30340;&#36755;&#20986;&#20043;&#21518;&#25110;&#36755;&#20837;&#20043;&#21069;&#24212;&#29992;&#23618;&#35268;&#33539;&#21270;&#12290;&#23613;&#31649;&#20004;&#31181;&#21464;&#20307;&#37117;&#26377;&#23427;&#20204;&#30340;&#20248;&#28857;&#65292;&#20294;&#20063;&#23384;&#22312;&#20005;&#37325;&#30340;&#23616;&#38480;&#24615;&#65306;Post-LN&#20250;&#23548;&#33268;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#20174;&#32780;&#38459;&#30861;&#35757;&#32451;&#28145;&#23618;Transformer&#65292;&#32780;Pre-LN&#20250;&#23548;&#33268;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#38480;&#21046;&#27169;&#22411;&#23481;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#26550;&#26500;ResiDual&#65292;&#20855;&#26377;Pre-Post-LN(PPLN)&#65292;&#23427;&#23558;Post-LN&#21644;Pre-LN&#20013;&#30340;&#36830;&#25509;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#32487;&#25215;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#34920;&#26126;ResiDual&#27604;&#29616;&#26377;&#26041;&#27861;&#20248;&#36234;&#65292;&#23588;&#20854;&#26159;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empiric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MCPrioQ&#31639;&#27861;&#65292;&#19968;&#31181;&#26080;&#38145;&#31232;&#30095;Markov&#38142;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(1)$&#29992;&#20110;&#26356;&#26032;&#21644;$O(CDF^{-1}(t))$&#29992;&#20110;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.14801</link><description>&lt;p&gt;
MCPrioQ: &#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#31232;&#30095;Markov&#38142;&#30340;&#26080;&#38145;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
MCPrioQ: A lock-free algorithm for online sparse markov-chains. (arXiv:2304.14801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MCPrioQ&#31639;&#27861;&#65292;&#19968;&#31181;&#26080;&#38145;&#31232;&#30095;Markov&#38142;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(1)$&#29992;&#20110;&#26356;&#26032;&#21644;$O(CDF^{-1}(t))$&#29992;&#20110;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#31995;&#32479;&#20013;&#65292;&#26500;&#24314;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22823;&#22411;&#22270;&#24418;&#26377;&#26102;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Markov-chain-priority-queue(MCPrioQ)&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#23427;&#26159;&#19968;&#31181;&#26080;&#38145;&#31232;&#30095;Markov&#38142;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#32447;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O&#65288;1&#65289;$&#29992;&#20110;&#26356;&#26032;&#21644;$O(CDF^{-1}(t))$&#29992;&#20110;&#25512;&#29702;&#12290;MCPrioQ&#29305;&#21035;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#38477;&#24207;&#27010;&#29575;&#39034;&#24207;&#26597;&#25214;n&#20010;&#39033;&#30446;&#12290;&#24182;&#21457;&#26356;&#26032;&#26159;&#36890;&#36807;&#21704;&#24076;&#34920;&#21644;&#21407;&#23376;&#25351;&#20196;&#23454;&#29616;&#30340;&#65292;&#26597;&#25214;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#20808;&#32423;&#38431;&#21015;&#23454;&#29616;&#30340;&#65292;&#21363;&#20351;&#22312;&#24182;&#21457;&#26356;&#26032;&#26399;&#38388;&#65292;&#20063;&#21487;&#20197;&#24471;&#21040;&#36817;&#20284;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;&#36817;&#20284;&#27491;&#30830;&#21644;&#26080;&#38145;&#23450;&#23646;&#24615;&#30001;&#35835;&#21462;-&#22797;&#21046;-&#26356;&#26032;&#26041;&#26696;&#32500;&#25252;&#65292;&#20294;&#35821;&#20041;&#24050;&#30053;&#24494;&#26356;&#26032;&#65292;&#20197;&#20801;&#35768;&#20803;&#32032;&#30340;&#20132;&#25442;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#24377;&#20986;-&#25554;&#20837;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high performance systems it is sometimes hard to build very large graphs that are efficient both with respect to memory and compute. This paper proposes a data structure called Markov-chain-priority-queue (MCPrioQ), which is a lock-free sparse markov-chain that enables online and continuous learning with time-complexity of $O(1)$ for updates and $O(CDF^{-1}(t))$ inference. MCPrioQ is especially suitable for recommender-systems for lookups of $n$-items in descending probability order. The concurrent updates are achieved using hash-tables and atomic instructions and the lookups are achieved through a novel priority-queue which allows for approximately correct results even during concurrent updates. The approximatly correct and lock-free property is maintained by a read-copy-update scheme, but where the semantics have been slightly updated to allow for swap of elements rather than the traditional pop-insert scheme.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#26469;&#20943;&#23569;&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#31561;&#25928;&#30340;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14793</link><description>&lt;p&gt;
&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Graph Neural Networks using Exact Compression. (arXiv:2304.14793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#26469;&#20943;&#23569;&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#31561;&#25928;&#30340;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23398;&#20064;&#36825;&#31181;&#32593;&#32476;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#20869;&#23384;&#21463;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;GPU&#65289;&#26469;&#35828;&#26159;&#19968;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31934;&#30830;&#21387;&#32553;&#26469;&#20943;&#23569;&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#35777;&#26126;&#31561;&#25928;&#30340;&#21387;&#32553;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#21021;&#27493;&#30340;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#21487;&#20197;&#33719;&#24471;&#30340;&#21387;&#32553;&#27604;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a form of deep learning that enable a wide range of machine learning applications on graph-structured data. The learning of GNNs, however, is known to pose challenges for memory-constrained devices such as GPUs. In this paper, we study exact compression as a way to reduce the memory requirements of learning GNNs on large graphs. In particular, we adopt a formal approach to compression and propose a methodology that transforms GNN learning problems into provably equivalent compressed GNN learning problems. In a preliminary experimental evaluation, we give insights into the compression ratios that can be obtained on real-world graphs and apply our methodology to an existing GNN benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#27491;&#30830;&#30340;&#36793;&#32536;&#32422;&#26463;&#30340;&#26465;&#20214;&#19979;&#65292;&#21033;&#29992;&#23567;&#25209;&#37327;&#32806;&#21512;&#23558;&#27969;&#36827;&#34892;&#30699;&#27491;&#65292;&#20174;&#32780;&#20351;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#20302;&#32500;&#20195;&#20215;&#30340;&#36816;&#36755;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.14772</link><description>&lt;p&gt;
&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#65306;&#21033;&#29992;&#23567;&#25209;&#37327;&#32806;&#21512;&#23558;&#27969;&#36827;&#34892;&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
Multisample Flow Matching: Straightening Flows with Minibatch Couplings. (arXiv:2304.14772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14772
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#27491;&#30830;&#30340;&#36793;&#32536;&#32422;&#26463;&#30340;&#26465;&#20214;&#19979;&#65292;&#21033;&#29992;&#23567;&#25209;&#37327;&#32806;&#21512;&#23558;&#27969;&#36827;&#34892;&#30699;&#27491;&#65292;&#20174;&#32780;&#20351;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#20302;&#32500;&#20195;&#20215;&#30340;&#36816;&#36755;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#27169;&#25311;&#30340;&#36830;&#32493;&#26102;&#38388;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#26500;&#24314;&#20102;&#20174;&#22122;&#22768;&#20998;&#24067;&#21040;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#27010;&#29575;&#36335;&#24452;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#65292;&#22914;&#27969;&#21305;&#37197;&#65292;&#23548;&#20986;&#20102;&#26368;&#36866;&#21512;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#25968;&#25454;&#21644;&#22122;&#22768;&#26679;&#26412;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#22522;&#30784;&#32467;&#26500;&#26469;&#26500;&#24314;&#27010;&#29575;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#26412;&#27969;&#21305;&#37197;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#25968;&#25454;&#21644;&#22122;&#22768;&#26679;&#26412;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#32806;&#21512;&#65292;&#21516;&#26102;&#28385;&#36275;&#27491;&#30830;&#30340;&#36793;&#32536;&#32422;&#26463;&#12290;&#22312;&#38750;&#24120;&#23567;&#30340;&#24320;&#38144;&#19979;&#65292;&#36825;&#31181;&#27867;&#21270;&#20351;&#25105;&#20204;&#33021;&#22815;(i) &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38477;&#20302;&#26799;&#24230;&#26041;&#24046;&#65292;(ii) &#33719;&#24471;&#26356;&#21152;&#30452;&#25509;&#30340;&#27969;&#65292;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;(iii) &#33719;&#24471;&#26356;&#20302;&#32500;&#20195;&#20215;&#30340;&#36816;&#36755;&#22270;&#65292;&#36825;&#22312;&#29983;&#25104;&#27169;&#22411;&#20043;&#22806;&#20063;&#26377;&#24212;&#29992;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20197;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#23567;&#25209;&#37327;&#32806;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. At very small overhead costs, this generalization allows us to (i) reduce gradient variance during training, (ii) obtain straighter flows for the learned vector field, which allows us to generate high-quality samples using fewer function evaluations, and (iii) obtain transport maps with lower cost in high dimensions, which has applications beyond generative modeling. Importantly, we do so in a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35757;&#32451;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#20998;&#21306;&#19982;&#29305;&#23450;&#30340;&#25968;&#25454;&#29255;&#27573;&#20851;&#32852;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#20998;&#21306;&#30340;&#23376;&#32593;&#32476;&#30340;&#8220;&#35757;&#32451;&#20043;&#22806;&#30340;&#26679;&#26412;&#8221;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#38477;&#20302;&#36229;&#21442;&#25968;&#20248;&#21270;&#20195;&#20215;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14766</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20998;&#21106;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Optimization through Neural Network Partitioning. (arXiv:2304.14766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35757;&#32451;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#20998;&#21306;&#19982;&#29305;&#23450;&#30340;&#25968;&#25454;&#29255;&#27573;&#20851;&#32852;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#20998;&#21306;&#30340;&#23376;&#32593;&#32476;&#30340;&#8220;&#35757;&#32451;&#20043;&#22806;&#30340;&#26679;&#26412;&#8221;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#38477;&#20302;&#36229;&#21442;&#25968;&#20248;&#21270;&#20195;&#20215;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#24688;&#24403;&#30340;&#36229;&#21442;&#23545;&#20110;&#33719;&#24471;&#31070;&#32463;&#32593;&#32476;&#20013;&#33391;&#22909;&#30340;&#27867;&#21270;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#21487;&#20197;&#24378;&#21046;&#36866;&#24403;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#27491;&#21017;&#21270;&#27169;&#22411;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#21463;&#36793;&#32536;&#20284;&#28982;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#25968;&#25454;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35757;&#32451;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#20026; K &#20010;&#25968;&#25454;&#20998;&#29255;&#21644;&#21442;&#25968;&#20998;&#21306;&#12290;&#27599;&#20010;&#20998;&#21306;&#20165;&#19982;&#29305;&#23450;&#30340;&#25968;&#25454;&#29255;&#27573;&#20851;&#32852;&#24182;&#36827;&#34892;&#20248;&#21270;&#12290;&#23558;&#36825;&#20123;&#20998;&#21306;&#32452;&#21512;&#25104;&#23376;&#32593;&#32476;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#23376;&#32593;&#32476;&#30340;&#8220;&#35757;&#32451;&#20043;&#22806;&#30340;&#26679;&#26412;&#8221;&#25439;&#22833;&#23450;&#20041;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#30446;&#26631;&#65292;&#21363;&#22312;&#23376;&#32593;&#32476;&#30475;&#19981;&#21040;&#30340;&#25968;&#25454;&#29255;&#27573;&#19978;&#35745;&#31639;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#20010;&#30446;&#26631;&#24212;&#29992;&#21040;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#65292;&#21516;&#26102;&#26174;&#30528;&#38477;&#20302;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance -- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#24341;&#20837;&#25200;&#21160;&#65292;&#25913;&#36827;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#36136;&#20294;&#28151;&#21512;&#27604;&#20363;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#20302;&#21151;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.14762</link><description>&lt;p&gt;
&#21033;&#29992;&#25200;&#21160;&#26469;&#25913;&#21892;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy. (arXiv:2304.14762v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#24341;&#20837;&#25200;&#21160;&#65292;&#25913;&#36827;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#36136;&#20294;&#28151;&#21512;&#27604;&#20363;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#20302;&#21151;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#21270;&#26031;&#22374;&#36317;&#65288;KSD&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#24046;&#24322;&#24230;&#37327;&#12290;&#21363;&#20351;&#30446;&#26631;&#20998;&#24067;&#20855;&#26377;&#26410;&#30693;&#30340;&#26631;&#20934;&#21270;&#22240;&#23376;&#65292;&#20363;&#22914;&#22312;&#36125;&#21494;&#26031;&#20998;&#26512;&#20013;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#23427;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#30446;&#26631;&#20998;&#24067;&#21644;&#26367;&#20195;&#20998;&#24067;&#20855;&#26377;&#30456;&#21516;&#19988;&#30456;&#36317;&#36739;&#36828;&#30340;&#27169;&#24335;&#20294;&#22312;&#28151;&#21512;&#27604;&#20363;&#19978;&#26377;&#25152;&#19981;&#21516;&#26102;&#65292;KSD&#26816;&#39564;&#21487;&#33021;&#20250;&#20986;&#29616;&#20302;&#21151;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#26680;&#23545;&#35266;&#27979;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#65292;&#20351;&#20854;&#30456;&#23545;&#20110;&#30446;&#26631;&#20998;&#24067;&#19981;&#21464;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#22312;&#25200;&#21160;&#26679;&#26412;&#19978;&#20351;&#29992;KSD&#26816;&#39564;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#25968;&#20540;&#35777;&#25454;&#34920;&#26126;&#65292;&#20351;&#29992;&#36866;&#24403;&#36873;&#25321;&#30340;&#26680;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;KSD&#26816;&#39564;&#20855;&#26377;&#26356;&#39640;&#30340;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distribution have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen kernels the proposed approach can lead to a substantially higher power than the KSD test.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.14760</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Class of Explanations for Classifiers with Non-Binary Features. (arXiv:2304.14760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#30340;&#26032;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24403;&#20998;&#26512;&#20998;&#31867;&#22120;&#20915;&#31574;&#26102;&#65292;&#24050;&#32463;&#26377;&#20004;&#31181;&#31867;&#22411;&#30340;&#35299;&#37322;&#21463;&#21040;&#20102;&#25991;&#29486;&#20013;&#30340;&#37325;&#35270;&#12290;&#31532;&#19968;&#31181;&#35299;&#37322;&#26159;&#20026;&#20915;&#31574;&#25552;&#20379;&#20805;&#20998;&#29702;&#30001;&#30340;&#35299;&#37322;&#65292;&#21363;&#32553;&#20889;&#20026;PI&#35299;&#37322;&#30340;&#35825;&#23548;&#24335;&#35299;&#37322;&#65307;&#31532;&#20108;&#31181;&#35299;&#37322;&#26159;&#20026;&#20309;&#19981;&#20570;&#20986;&#20854;&#20182;&#20915;&#31574;&#30340;&#35299;&#37322;&#65292;&#21363;&#23545;&#29031;&#24335;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24517;&#35201;&#29702;&#30001;&#12290;&#36825;&#20123;&#35299;&#37322;&#26159;&#20026;&#20108;&#20803;&#12289;&#31163;&#25955;&#21644;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20026;&#36830;&#32493;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23384;&#22312;&#38750;&#20108;&#20803;&#29305;&#24449;&#26102;&#65292;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#24471;&#21040;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#20915;&#31574;&#21644;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#20449;&#24687;&#12290;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#20063;&#34987;&#35777;&#26126;&#26159;&#23436;&#25972;&#21407;&#22240;&#30340;&#20027;&#35201;&#34164;&#21547;&#39033;&#21644;&#34987;&#34164;&#21547;&#39033;&#65292;&#21487;&#20197;&#20351;&#29992;&#37327;&#21270;&#31639;&#23376;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#24565;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#20108;&#20803;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two types of explanations have received significant attention in the literature recently when analyzing the decisions made by classifiers. The first type explains why a decision was made and is known as a sufficient reason for the decision, also an abductive or PI-explanation. The second type explains why some other decision was not made and is known as a necessary reason for the decision, also a contrastive or counterfactual explanation. These explanations were defined for classifiers with binary, discrete and, in some cases, continuous features. We show that these explanations can be significantly improved in the presence of non-binary features, leading to a new class of explanations that relay more information about decisions and the underlying classifiers. Necessary and sufficient reasons were also shown to be the prime implicates and implicants of the complete reason for a decision, which can be obtained using a quantification operator. We show that our improved notions of necessa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;&#65288;CSST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#65292;&#20026;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14738</link><description>&lt;p&gt;
&#38754;&#21521;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#30340;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics. (arXiv:2304.14738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;&#65288;CSST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#65292;&#20026;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20351;&#24471;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#23601;&#33021;&#23398;&#20064;&#21040;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#35757;&#32451;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#25552;&#39640;&#31934;&#24230;&#30340;&#30446;&#26631;&#19978;&#65292;&#32780;&#23454;&#38469;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20855;&#26377;&#19981;&#21487;&#20998;&#35299;&#30340;&#22797;&#26434;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#26368;&#22823;&#21270;&#19981;&#21516;&#31867;&#21035;&#21484;&#22238;&#29575;&#30340;&#26368;&#23567;&#20540;&#31561;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;&#65288;CSST&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25512;&#24191;&#20102;&#29992;&#20110;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#30340;&#22522;&#20110;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20248;&#21270;&#25152;&#38656;&#30340;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#65292;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#19982;&#33258;&#25105;&#35757;&#32451;&#30340;&#20998;&#26512;&#25152;&#20570;&#30340;&#19968;&#26679;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;CSST&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#20102;&#38024;&#23545;&#19981;&#21516;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#30340;&#23454;&#38469;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;&#29992;&#20110;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;CSST&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy, whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training. Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23558;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#20225;&#19994;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#26367;&#20195;&#25163;&#21160;&#21019;&#24314;&#30340;ML&#31649;&#36947;&#65292;&#20026;&#20013;&#23567;&#22411;&#20225;&#19994;&#23454;&#29616;&#33258;&#21160;&#21270;&#20215;&#26684;&#39044;&#27979;&#25552;&#20379;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14735</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20215;&#26684;&#39044;&#27979;&#24212;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Automated Machine Learning Methods for Price Forecasting Applications. (arXiv:2304.14735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23558;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#19982;&#20225;&#19994;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#26367;&#20195;&#25163;&#21160;&#21019;&#24314;&#30340;ML&#31649;&#36947;&#65292;&#20026;&#20013;&#23567;&#22411;&#20225;&#19994;&#23454;&#29616;&#33258;&#21160;&#21270;&#20215;&#26684;&#39044;&#27979;&#25552;&#20379;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#20215;&#26684;&#27874;&#21160;&#65292;&#22312;&#20108;&#25163;&#24314;&#31569;&#35774;&#22791;&#30340;&#20215;&#26684;&#39044;&#27979;&#26041;&#38754;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24403;&#21069;&#24066;&#22330;&#25968;&#25454;&#33258;&#21160;&#21270;&#39044;&#27979;&#36807;&#31243;&#20855;&#26377;&#26497;&#39640;&#30340;&#20852;&#36259;&#12290;&#21363;&#20351;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#20110;&#36825;&#20123;&#25968;&#25454;&#26159;&#39044;&#27979;&#26576;&#20123;&#24037;&#20855;&#27531;&#20540;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20013;&#23567;&#22411;&#20225;&#19994;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#19981;&#36275;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#35299;&#20915;&#26041;&#26696;&#20195;&#26367;&#25163;&#21160;&#21019;&#24314;&#30340;ML&#31649;&#36947;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24213;&#23618;&#31649;&#36947;&#12290;&#25105;&#20204;&#23558;AutoML&#26041;&#27861;&#19982;&#20844;&#21496;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#22522;&#20110;CRISP-DM&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#25163;&#21160;ML&#31649;&#36947;&#20998;&#20026;&#26426;&#22120;&#23398;&#20064;&#37096;&#20998;&#21644;&#38750;&#26426;&#22120;&#23398;&#20064;&#37096;&#20998;&#12290;&#20026;&#20102;&#32771;&#34385;&#25152;&#26377;&#22797;&#26434;&#30340;&#24037;&#19994;&#35201;&#27714;&#24182;&#23637;&#31034;&#25105;&#20204;&#26032;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#20026;&#8220;&#26041;&#27861;&#35780;&#20272;&#8221;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price forecasting for used construction equipment is a challenging task due to spatial and temporal price fluctuations. It is thus of high interest to automate the forecasting process based on current market data. Even though applying machine learning (ML) to these data represents a promising approach to predict the residual value of certain tools, it is hard to implement for small and medium-sized enterprises due to their insufficient ML expertise. To this end, we demonstrate the possibility of substituting manually created ML pipelines with automated machine learning (AutoML) solutions, which automatically generate the underlying pipelines. We combine AutoML methods with the domain knowledge of the companies. Based on the CRISP-DM process, we split the manual ML pipeline into a machine learning and non-machine learning part. To take all complex industrial requirements into account and to demonstrate the applicability of our new approach, we designed a novel metric named method evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#21327;&#21516;&#20248;&#21270;&#22810;&#38142;&#36335;Wi-Fi&#32593;&#32476;&#20013;&#30340;&#38142;&#36335;&#28608;&#27963;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14720</link><description>&lt;p&gt;
&#38750;&#32852;&#32593;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#38142;&#36335;Wi-Fi&#32593;&#32476;&#20013;&#30340;&#38142;&#36335;&#28608;&#27963;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Federated Reinforcement Learning Framework for Link Activation in Multi-link Wi-Fi Networks. (arXiv:2304.14720v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#21327;&#21516;&#20248;&#21270;&#22810;&#38142;&#36335;Wi-Fi&#32593;&#32476;&#20013;&#30340;&#38142;&#36335;&#28608;&#27963;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;Wi-Fi&#32593;&#32476;&#27491;&#22312;&#24341;&#20837;&#26032;&#21151;&#33021;&#65292;&#22914;&#22810;&#38142;&#36335;&#25805;&#20316;&#65288;MLO&#65289;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#29992;&#20449;&#36947;&#25968;&#37327;&#26377;&#38480;&#65292;&#19968;&#32452;&#31454;&#20105;&#22522;&#26412;&#26381;&#21153;&#38598;&#65288;BSS&#65289;&#30340;&#22810;&#20010;&#38142;&#36335;&#30340;&#20351;&#29992;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#24178;&#25200;&#21644;&#20449;&#36947;&#20105;&#29992;&#65292;&#20174;&#32780;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#38477;&#20302;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#25152;&#26377;&#31454;&#20105;&#30340;BSS&#37117;&#20351;&#29992;&#26356;&#23569;&#30340;&#38142;&#36335;&#20197;&#20943;&#23569;&#20449;&#36947;&#35775;&#38382;&#20105;&#29992;&#65292;&#21017;&#21487;&#33021;&#26356;&#22909;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#26080;&#32447;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#20248;&#21270;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#26080;&#32447;&#32593;&#32476;&#30340;&#29420;&#31435;&#36816;&#34892;&#20351;&#24471;&#27599;&#20010;&#21333;&#29420;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#37197;&#32622;&#21464;&#24471;&#22256;&#38590; - &#22914;&#26524;&#19981;&#26159;&#20960;&#20046;&#19981;&#21487;&#33021;&#30340;&#35805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;--&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next-generation Wi-Fi networks are looking forward to introducing new features like multi-link operation (MLO) to both achieve higher throughput and lower latency. However, given the limited number of available channels, the use of multiple links by a group of contending Basic Service Sets (BSSs) can result in higher interference and channel contention, thus potentially leading to lower performance and reliability. In such a situation, it could be better for all contending BSSs to use less links if that contributes to reduce channel access contention. Recently, reinforcement learning (RL) has proven its potential for optimizing resource allocation in wireless networks. However, the independent operation of each wireless network makes difficult -- if not almost impossible -- for each individual network to learn a good configuration. To solve this issue, in this paper, we propose the use of a Federated Reinforcement Learning (FRL) framework, i.e., a collaborative machine learning approac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;X-RLflow&#65292;&#29992;&#20110;&#26367;&#25442;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#22270;&#65292;&#20197;&#27714;&#24471;&#26356;&#20248;&#30340;&#35745;&#31639;&#22270;&#32467;&#26500;&#65292;&#21487;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#36229;&#20248;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.14698</link><description>&lt;p&gt;
X-RLflow&#65306;&#38754;&#21521;&#31070;&#32463;&#32593;&#32476;&#23376;&#22270;&#36716;&#25442;&#30340;&#22270;&#24418;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation. (arXiv:2304.14698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;X-RLflow&#65292;&#29992;&#20110;&#26367;&#25442;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#22270;&#65292;&#20197;&#27714;&#24471;&#26356;&#20248;&#30340;&#35745;&#31639;&#22270;&#32467;&#26500;&#65292;&#21487;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#36229;&#20248;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#22270;&#36229;&#20248;&#21270;&#31995;&#32479;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31995;&#21015;&#23376;&#22270;&#26367;&#25442;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#35745;&#31639;&#22270;&#32467;&#26500;&#12290;&#36825;&#20010;&#22270;&#36716;&#25442;&#36807;&#31243;&#33258;&#28982;&#32780;&#28982;&#22320;&#33853;&#20837;&#20102;&#24207;&#21015;&#20915;&#31574;&#26694;&#26550;&#20013;, &#29616;&#26377;&#31995;&#32479;&#36890;&#24120;&#37319;&#29992;&#36138;&#24515;&#25628;&#32034;&#26041;&#27861;&#65292;&#26080;&#27861;&#25506;&#32034;&#25972;&#20010;&#25628;&#32034;&#31354;&#38388;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#23481;&#24525;&#20020;&#26102;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#26367;&#20195;&#25628;&#32034;&#26041;&#27861;&#26469;&#35299;&#20915;&#24352;&#37327;&#22270;&#36229;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;X-RLflow&#65292;&#21487;&#20197;&#23398;&#20064;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25968;&#25454;&#27969;&#22270;&#37325;&#20889;&#65292;&#19968;&#27425;&#26367;&#25442;&#19968;&#20010;&#23376;&#22270;&#12290;X-RLflow &#22522;&#20110;&#19968;&#31181;&#26080;&#27169;&#22411; RL &#20195;&#29702;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#23545;&#30446;&#26631;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#36845;&#20195;&#36755;&#20986;&#36716;&#25442;&#21518;&#30340;&#35745;&#31639;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#36229;&#20248;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor graph superoptimisation systems perform a sequence of subgraph substitution to neural networks, to find the optimal computation graph structure. Such a graph transformation process naturally falls into the framework of sequential decision-making, and existing systems typically employ a greedy search approach, which cannot explore the whole search space as it cannot tolerate a temporary loss of performance. In this paper, we address the tensor graph superoptimisation problem by exploring an alternative search approach, reinforcement learning (RL). Our proposed approach, X-RLflow, can learn to perform neural network dataflow graph rewriting, which substitutes a subgraph one at a time. X-RLflow is based on a model-free RL agent that uses a graph neural network (GNN) to encode the target computation graph and outputs a transformed computation graph iteratively. We show that our approach can outperform state-of-the-art superoptimisation systems over a range of deep learning models an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22240;&#23376;&#22270;&#19978;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;PMU&#30005;&#21387;&#21644;&#30005;&#27969;&#27979;&#37327;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#27597;&#32447;&#30005;&#21387;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14680</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;PMU&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#40065;&#26834;&#12289;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on Factor Graphs for Robust, Fast, and Scalable Linear State Estimation with PMUs. (arXiv:2304.14680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22240;&#23376;&#22270;&#19978;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;PMU&#30005;&#21387;&#21644;&#30005;&#27969;&#27979;&#37327;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#27597;&#32447;&#30005;&#21387;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30456;&#37327;&#27979;&#37327;&#35774;&#22791;(PMU)&#22312;&#36755;&#30005;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#39640;&#37319;&#26679;&#29575;&#30340;&#24555;&#36895;&#29366;&#24577;&#20272;&#35745;&#65288;SE&#65289;&#31639;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20174;PMU&#30005;&#21387;&#21644;&#30005;&#27969;&#27979;&#37327;&#20013;&#23398;&#20064;&#22797;&#26434;&#30340;&#27597;&#32447;&#30005;&#21387;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;GNN&#23454;&#29616;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#30340;&#22240;&#23376;&#22270;&#19978;&#65292;&#20197;&#31616;&#21270;&#23545;&#30005;&#21147;&#31995;&#32479;&#27597;&#32447;&#21644;&#20998;&#25903;&#19978;&#21508;&#31181;&#31867;&#22411;&#21644;&#25968;&#37327;&#30340;&#27979;&#37327;&#30340;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22240;&#23376;&#22270;&#65292;&#20197;&#25552;&#39640;GNN&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#27169;&#22411;&#38750;&#24120;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#30005;&#21147;&#31995;&#32479;&#33410;&#28857;&#25968;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#30005;&#21147;&#31995;&#32479;&#27979;&#37327;&#38598;&#29983;&#25104;&#35757;&#32451;&#21644;&#27979;&#35797;&#31034;&#20363;&#65292;&#24182;&#29992;&#24102;&#26377;PMU&#30340;&#32447;&#24615;SE&#30340;&#30830;&#20999;&#35299;&#36827;&#34892;&#27880;&#37322;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;GNN&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As phasor measurement units (PMUs) become more widely used in transmission power systems, a fast state estimation (SE) algorithm that can take advantage of their high sample rates is needed. To accomplish this, we present a method that uses graph neural networks (GNNs) to learn complex bus voltage estimates from PMU voltage and current measurements. We propose an original implementation of GNNs over the power system's factor graph to simplify the integration of various types and quantities of measurements on power system buses and branches. Furthermore, we augment the factor graph to improve the robustness of GNN predictions. This model is highly efficient and scalable, as its computational complexity is linear with respect to the number of nodes in the power system. Training and test examples were generated by randomly sampling sets of power system measurements and annotated with the exact solutions of linear SE with PMUs. The numerical results demonstrate that the GNN model provides 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#20934;&#21017;&#12289;&#30456;&#20284;&#24615;&#21644;&#32858;&#31867;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14663</link><description>&lt;p&gt;
&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Client Recruitment for Federated Learning in ICU Length of Stay Prediction. (arXiv:2304.14663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#20934;&#21017;&#12289;&#30456;&#20284;&#24615;&#21644;&#32858;&#31867;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21307;&#30103;&#21644;&#20445;&#20581;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#21644;&#25913;&#36827;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21307;&#30103;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#30340;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#36825;&#20123;&#25968;&#25454;&#26159;&#20998;&#25955;&#30340;&#12290;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#38750;&#24120;&#36866;&#21512;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;ICU&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25307;&#21215;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#21253;&#25324;&#20960;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#22522;&#20110;&#20934;&#21017;&#30340;&#23458;&#25143;&#31471;&#39044;&#36873;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#65307;&#21644;&#65288;iii&#65289;&#23458;&#25143;&#31471;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine and deep learning methods for medical and healthcare applications have shown significant progress and performance improvement in recent years. These methods require vast amounts of training data which are available in the medical sector, albeit decentralized. Medical institutions generate vast amounts of data for which sharing and centralizing remains a challenge as the result of data and privacy regulations. The federated learning technique is well-suited to tackle these challenges. However, federated learning comes with a new set of open problems related to communication overhead, efficient parameter aggregation, client selection strategies and more. In this work, we address the step prior to the initiation of a federated network for model training, client recruitment. By intelligently recruiting clients, communication overhead and overall cost of training can be reduced without sacrificing predictive performance. Client recruitment aims at pre-excluding potential clients fro
&lt;/p&gt;</description></item><item><title>&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14660</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#8220;Segment Anything Model&#8221;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14660
&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#20998;&#21106;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#20004;&#31181;&#27169;&#24335;&#23454;&#29616;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#24577;&#12289;&#32454;&#24494;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#23545;&#35937;&#36793;&#30028;&#21644;&#24191;&#27867;&#30340;&#23545;&#35937;&#23610;&#24230;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;MIS&#65289;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#38646;&#26679;&#26412;&#21644;&#39640;&#25928;&#30340;MIS&#21487;&#20197;&#24456;&#22909;&#22320;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#24182;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;SAM&#20284;&#20046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#24037;&#20855;&#65292;&#24182;&#19988;&#20854;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24212;&#35813;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#25972;&#29702;&#20102;52&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;16&#20010;&#27169;&#24577;&#21644;68&#20010;&#23545;&#35937;&#30340;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26174;&#24615;&#27807;&#36890;&#21040;&#38544;&#24615;&#21512;&#20316;&#30340;&#26032;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#20139;&#20449;&#24687;&#21644;&#20351;&#29992;&#26412;&#22320;&#36712;&#36857;&#37325;&#24314;&#20449;&#24687;&#26469;&#20419;&#36827;&#21327;&#20316;&#65292;&#24182;&#36880;&#28176;&#20943;&#23569;&#26174;&#24335;&#20256;&#36798;&#20449;&#24687;&#30340;&#27604;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#19979;&#27604;&#20256;&#32479;&#30340; CTDE &#33539;&#24335;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.14656</link><description>&lt;p&gt;
&#20174;&#26174;&#24615;&#27807;&#36890;&#21040;&#38544;&#24615;&#21512;&#20316;&#65306;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
From Explicit Communication to Tacit Cooperation:A Novel Paradigm for Cooperative MARL. (arXiv:2304.14656v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26174;&#24615;&#27807;&#36890;&#21040;&#38544;&#24615;&#21512;&#20316;&#30340;&#26032;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#20139;&#20449;&#24687;&#21644;&#20351;&#29992;&#26412;&#22320;&#36712;&#36857;&#37325;&#24314;&#20449;&#24687;&#26469;&#20419;&#36827;&#21327;&#20316;&#65292;&#24182;&#36880;&#28176;&#20943;&#23569;&#26174;&#24335;&#20256;&#36798;&#20449;&#24687;&#30340;&#27604;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#19979;&#27604;&#20256;&#32479;&#30340; CTDE &#33539;&#24335;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#22521;&#35757;&#19982;&#20998;&#25955;&#25191;&#34892;&#65288;CTDE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#38382;&#39064;&#21644;&#20195;&#29702;&#20043;&#38388;&#32570;&#20047;&#26377;&#25928;&#20849;&#20139;&#20449;&#21495;&#30340;&#23384;&#22312;&#32463;&#24120;&#38480;&#21046;&#20102;&#23427;&#22312;&#20419;&#36827;&#21327;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;&#36890;&#20449;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20294;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#21463;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#20174;&#26174;&#24615;&#36890;&#20449;&#21040;&#38544;&#24615;&#21512;&#20316;&#30340;&#36880;&#28176;&#36716;&#21464;&#12290;&#22312;&#21021;&#22987;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20195;&#29702;&#20043;&#38388;&#20998;&#20139;&#30456;&#20851;&#20449;&#24687;&#21644;&#21516;&#26102;&#20351;&#29992;&#27599;&#20010;&#20195;&#29702;&#30340;&#26412;&#22320;&#36712;&#36857;&#37325;&#24314;&#35813;&#20449;&#24687;&#26469;&#20419;&#36827;&#21327;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26174;&#24335;&#20256;&#36798;&#30340;&#20449;&#24687;&#19982;&#37325;&#24314;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#28151;&#21512;&#20449;&#24687;&#12290;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#38544;&#21547;&#21512;&#20316;&#36880;&#28176;&#20943;&#23569;&#26174;&#24335;&#20256;&#36798;&#20449;&#24687;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized training with decentralized execution (CTDE) is a widely-used learning paradigm that has achieved significant success in complex tasks. However, partial observability issues and the absence of effectively shared signals between agents often limit its effectiveness in fostering cooperation. While communication can address this challenge, it simultaneously reduces the algorithm's practicality. Drawing inspiration from human team cooperative learning, we propose a novel paradigm that facilitates a gradual shift from explicit communication to tacit cooperation. In the initial training stage, we promote cooperation by sharing relevant information among agents and concurrently reconstructing this information using each agent's local trajectory. We then combine the explicitly communicated information with the reconstructed information to obtain mixed information. Throughout the training process, we progressively reduce the proportion of explicitly communicated information, facilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#24418;&#29366;&#65292;&#29992;&#20110;&#35774;&#35745;&#39640;&#25928;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;SAM&#12290;&#37319;&#29992;AE-SAM&#21644;AE-LookSAM&#20004;&#31181;&#31639;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;AE-SAM&#20855;&#26377;&#19982;SAM&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#19968;&#31574;&#30053;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.14647</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#29992;&#20110;&#21033;&#29992;SAM&#31639;&#27861;&#36827;&#34892;&#38160;&#24230;&#24863;&#30693;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Policy to Employ Sharpness-Aware Minimization. (arXiv:2304.14647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#24418;&#29366;&#65292;&#29992;&#20110;&#35774;&#35745;&#39640;&#25928;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;SAM&#12290;&#37319;&#29992;AE-SAM&#21644;AE-LookSAM&#20004;&#31181;&#31639;&#27861;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;AE-SAM&#20855;&#26377;&#19982;SAM&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#19968;&#31574;&#30053;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#22411;&#20248;&#21270;(SAM)&#36890;&#36807;&#26368;&#23567;&#21270;&#36807;&#31243;&#20013;&#30340;&#38160;&#24230;&#23547;&#27714;&#24179;&#22374;&#26497;&#23567;&#20540;&#65292;&#22312;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#24615;&#12290;&#20294;&#26159;&#65292;&#19982;&#26631;&#20934;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#30456;&#27604;&#65292;&#27599;&#27425;SAM&#26356;&#26032;&#38656;&#35201;&#35745;&#31639;&#20004;&#20010;&#26799;&#24230;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#21644;&#35757;&#32451;&#26102;&#38388;&#37117;&#22686;&#21152;&#20102;&#19968;&#20493;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#25110;&#23450;&#26399;&#22312;SAM&#26356;&#26032;&#21644;ERM&#26356;&#26032;&#20043;&#38388;&#36827;&#34892;&#20999;&#25442;&#65292;&#20174;&#32780;&#20943;&#23569;SAM&#26356;&#26032;&#30340;&#27604;&#20363;&#65292;&#21152;&#36895;SAM&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20960;&#20309;&#24418;&#29366;&#26469;&#24212;&#29992;SAM&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;AE-SAM&#21644;AE-LookSAM&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;AE-SAM&#20855;&#26377;&#19982;SAM&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM), which searches for flat minima by min-max optimization, has been shown to be useful in improving model generalization. However, since each SAM update requires computing two gradients, its computational cost and training time are both doubled compared to standard empirical risk minimization (ERM). Recent state-of-the-arts reduce the fraction of SAM updates and thus accelerate SAM by switching between SAM and ERM updates randomly or periodically. In this paper, we design an adaptive policy to employ SAM based on the loss landscape geometry. Two efficient algorithms, AE-SAM and AE-LookSAM, are proposed. We theoretically show that AE-SAM has the same convergence rate as SAM. Experimental results on various datasets and architectures demonstrate the efficiency and effectiveness of the adaptive policy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27424;&#38459;&#23612;Nesterov&#21152;&#36895;&#65292;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#26500;&#24314;&#20102;&#26032;&#30340;Lyapunov&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#26799;&#24230;&#33539;&#25968;&#24179;&#26041;&#21644;&#30446;&#26631;&#20540;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14642</link><description>&lt;p&gt;
&#20851;&#20110;&#27424;&#38459;&#23612;Nesterov&#21152;&#36895;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Underdamped Nesterov's Acceleration. (arXiv:2304.14642v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27424;&#38459;&#23612;Nesterov&#21152;&#36895;&#65292;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#26500;&#24314;&#20102;&#26032;&#30340;Lyapunov&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#26799;&#24230;&#33539;&#25968;&#24179;&#26041;&#21644;&#30446;&#26631;&#20540;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#38750;&#24120;&#36866;&#21512;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;\texttt{NAG}&#65289;&#20197;&#21450;&#23427;&#30340;&#36817;&#31471;&#23545;&#24212;&#29289;&#8212;&#8212;&#26356;&#24555;&#30340;&#36845;&#20195;&#38408;&#20540;&#25910;&#32553;&#31639;&#27861;&#65288;FISTA&#65289;&#12290;&#20294;&#26159;&#65292;&#35813;&#29702;&#35770;&#20307;&#31995;&#20173;&#28982;&#19981;&#23436;&#25972;&#65292;&#22240;&#20026;&#27424;&#38459;&#23612;&#24773;&#20917;&#65288;$r&lt;2$&#65289;&#23578;&#26410;&#21253;&#25324;&#22312;&#20869;&#12290;&#26412;&#25991;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#27424;&#38459;&#23612;&#24773;&#20917;&#19979;&#30340;&#26032;&#30340;Lyapunov&#20989;&#25968;&#65292;&#28608;&#21457;&#20110;&#26102;&#38388;$t^{\gamma}$&#25110;&#36845;&#20195;$k^{\gamma}$&#30340;&#28151;&#21512;&#39033;&#30340;&#24433;&#21709;&#12290;&#24403;&#21160;&#37327;&#21442;&#25968;$r$&#20026;$2$&#26102;&#65292;&#26032;&#30340;Lyapunov&#20989;&#25968;&#19982;&#20043;&#21069;&#30340;&#20989;&#25968;&#30456;&#21516;&#12290;&#36825;&#20123;&#26032;&#30340;&#35777;&#26126;&#19981;&#20165;&#21253;&#25324;&#20102;&#20197;&#21069;&#26681;&#25454;&#20302;&#20998;&#36776;&#29575;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#24471;&#20986;&#30340;&#30446;&#26631;&#20540;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36824;&#34920;&#24449;&#20102;&#26368;&#23567;&#26799;&#24230;&#33539;&#25968;&#24179;&#26041;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high-resolution differential equation framework has been proven to be tailor-made for Nesterov's accelerated gradient descent method~(\texttt{NAG}) and its proximal correspondence -- the class of faster iterative shrinkage thresholding algorithms (FISTA). However, the systems of theories is not still complete, since the underdamped case ($r &lt; 2$) has not been included. In this paper, based on the high-resolution differential equation framework, we construct the new Lyapunov functions for the underdamped case, which is motivated by the power of the time $t^{\gamma}$ or the iteration $k^{\gamma}$ in the mixed term. When the momentum parameter $r$ is $2$, the new Lyapunov functions are identical to the previous ones. These new proofs do not only include the convergence rate of the objective value previously obtained according to the low-resolution differential equation framework but also characterize the convergence rate of the minimal gradient norm square. All the convergence rates o
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14633</link><description>&lt;p&gt;
CVRecon: &#37325;&#26032;&#24605;&#32771;&#31070;&#32463;&#37325;&#24314;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14633
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#37325;&#24314;&#30340;&#36827;&#23637;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#25216;&#26415;&#20165;&#27839;&#25972;&#20010;&#30456;&#26426;&#20809;&#32447;&#22797;&#21046;&#23545;&#35937;&#34920;&#38754;&#30340;2D&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#22797;&#21046;&#20250;&#22312;&#31354;&#27934;&#21644;&#36974;&#25377;&#31354;&#38388;&#20013;&#24341;&#20837;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#20960;&#20309;&#20307;&#25104;&#24418;&#26041;&#38754;&#20135;&#29983;&#25361;&#25112;&#12290;&#21463;&#20256;&#32479;&#22810;&#35270;&#35282;&#31435;&#20307;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#26088;&#22312;&#21033;&#29992;&#20195;&#20215;&#20307;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#34920;&#31034;&#27861;&#8212;&#8212;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#32534;&#30721;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#24674;&#22797;&#20102;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
&lt;/p&gt;</description></item><item><title>MUDiff &#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14621</link><description>&lt;p&gt;
MUDiff: &#32479;&#19968;&#25193;&#25955;&#29983;&#25104;&#23436;&#25972;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
MUDiff: Unified Diffusion for Complete Molecule Generation. (arXiv:2304.14621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14621
&lt;/p&gt;
&lt;p&gt;
MUDiff &#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20998;&#23376;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#20998;&#23376;&#30340;&#20840;&#38754;&#34920;&#31034;&#65292;&#21253;&#25324;&#21407;&#23376;&#29305;&#24449;&#12289;&#20108;&#32500;&#31163;&#25955;&#20998;&#23376;&#32467;&#26500;&#21644;&#19977;&#32500;&#36830;&#32493;&#20998;&#23376;&#22352;&#26631;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#36716;&#25442;&#22120;&#23545;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#26159;&#31561;&#21464;&#30340;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#19981;&#21464;&#30340;&#21407;&#23376;&#21644;&#36793;&#30028;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#23376;&#22352;&#26631;&#30340;&#31561;&#21464;&#24615;&#12290;&#36825;&#31181;&#36716;&#25442;&#22120;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#23545;&#20960;&#20309;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21644;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#29983;&#25104;&#26356;&#31283;&#23450;&#21644;&#26377;&#25928;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new model for generating molecular data by combining discrete and continuous diffusion processes. Our model generates a comprehensive representation of molecules, including atom features, 2D discrete molecule structures, and 3D continuous molecule coordinates. The use of diffusion processes allows for capturing the probabilistic nature of molecular processes and the ability to explore the effect of different factors on molecular structures and properties. Additionally, we propose a novel graph transformer architecture to denoise the diffusion process. The transformer is equivariant to Euclidean transformations, allowing it to learn invariant atom and edge representations while preserving the equivariance of atom coordinates. This transformer can be used to learn molecular representations robust to geometric transformations. We evaluate the performance of our model through experiments and comparisons with existing methods, showing its ability to generate more stable and val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35782;&#21035;&#20449;&#24687;&#29942;&#39048;&#65288;RIB&#65289;&#65292;&#20854;&#36890;&#36807;&#21487;&#35782;&#21035;&#24615;&#35780;&#35770;&#23478;&#26469;&#35268;&#33539;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#25512;&#24191;&#26041;&#24335;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#20449;&#24687;&#29942;&#39048;&#33021;&#22815;&#26356;&#22909;&#22320;&#20445;&#35777;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14618</link><description>&lt;p&gt;
&#21487;&#35782;&#21035;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Recognizable Information Bottleneck. (arXiv:2304.14618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35782;&#21035;&#20449;&#24687;&#29942;&#39048;&#65288;RIB&#65289;&#65292;&#20854;&#36890;&#36807;&#21487;&#35782;&#21035;&#24615;&#35780;&#35770;&#23478;&#26469;&#35268;&#33539;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#25512;&#24191;&#26041;&#24335;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#20449;&#24687;&#29942;&#39048;&#33021;&#22815;&#26356;&#22909;&#22320;&#20445;&#35777;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#36890;&#36807;&#20449;&#24687;&#21387;&#32553;&#23398;&#20064;&#34920;&#31034;&#65292;&#20174;&#32780;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#26080;&#30340;&#25512;&#24191;&#36793;&#30028;&#65292;&#29616;&#26377;&#30340;IB&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#26080;&#27861;&#20445;&#35777;&#25512;&#24191;&#24615;&#12290;&#26368;&#36817;&#30340;PAC-Bayes IB&#20351;&#29992;&#20449;&#24687;&#22797;&#26434;&#24230;&#32780;&#19981;&#26159;&#20449;&#24687;&#21387;&#32553;&#26469;&#24314;&#31435;&#19982;&#30456;&#20114;&#20449;&#24687;&#25512;&#24191;&#30028;&#38480;&#30340;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#20108;&#38454;&#26354;&#29575;&#65292;&#38459;&#30861;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;&#19982;&#36817;&#26399;&#30340;&#20989;&#25968;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;f-CMI&#65289;&#25512;&#24191;&#36793;&#30028;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35813;&#36793;&#30028;&#30340;&#20272;&#35745;&#35201;&#31616;&#21333;&#24471;&#22810;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35782;&#21035;&#20449;&#24687;&#29942;&#39048;&#65288;RIB&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;Bregman&#25955;&#24230;&#19979;&#30340;&#23494;&#24230;&#27604;&#21305;&#37197;&#20248;&#21270;&#30340;&#21487;&#35782;&#21035;&#24615;&#35780;&#35770;&#23478;&#26469;&#35268;&#33539;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Bottlenecks (IBs) learn representations that generalize to unseen data by information compression. However, existing IBs are practically unable to guarantee generalization in real-world scenarios due to the vacuous generalization bound. The recent PAC-Bayes IB uses information complexity instead of information compression to establish a connection with the mutual information generalization bound. However, it requires the computation of expensive second-order curvature, which hinders its practical application. In this paper, we establish the connection between the recognizability of representations and the recent functional conditional mutual information (f-CMI) generalization bound, which is significantly easier to estimate. On this basis we propose a Recognizable Information Bottleneck (RIB) which regularizes the recognizability of representations through a recognizability critic optimized by density ratio matching under the Bregman divergence. Extensive experiments on sev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65288;CEPIA&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#34892;&#21160;&#24314;&#35758;&#24182;&#20102;&#35299;&#32570;&#22833;&#20540;&#23545;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14606</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#19979;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation with Missing Values. (arXiv:2304.14606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65288;CEPIA&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#34892;&#21160;&#24314;&#35758;&#24182;&#20102;&#35299;&#32570;&#22833;&#20540;&#23545;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#25552;&#20379;&#25200;&#21160;&#20197;&#25913;&#21464;&#20998;&#31867;&#22120;&#39044;&#27979;&#32467;&#26524;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#23436;&#25972;&#20449;&#24687;&#30340;&#36755;&#20837;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#20013;&#24448;&#24448;&#20250;&#26377;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;CE&#26694;&#26550;&#65288;&#31216;&#20026;CEPIA&#65289;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#26377;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26377;&#25928;&#30340;&#25805;&#20316;&#65292;&#24182;&#38416;&#26126;&#32570;&#22833;&#20540;&#23545;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. Users can interpret the perturbation as an "action" to obtain their desired decision results. Existing CE methods require complete information on the features of an input instance. However, we often encounter missing values in a given instance, and the previous methods do not work in such a practical situation. In this paper, we first empirically and theoretically show the risk that missing value imputation methods affect the validity of an action, as well as the features that the action suggests changing. Then, we propose a new framework of CE, named Counterfactual Explanation by Pairs of Imputation and Action (CEPIA), that enables users to obtain valid actions even with missing values and clarifies how actions are affected by imputation of the missing values. Specifically, our CEPIA provides a representative set of pairs of an imputation ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#31561;&#21516;&#20110;&#22522;&#20110;&#20869;&#26680;&#23725;&#22238;&#24402;&#30340;&#21333;&#20010;&#27424;&#24179;&#28369;&#23725;&#22238;&#24402;&#65292;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.14545</link><description>&lt;p&gt;
&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Augmented balancing weights as linear regression. (arXiv:2304.14545v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#31561;&#21516;&#20110;&#22522;&#20110;&#20869;&#26680;&#23725;&#22238;&#24402;&#30340;&#21333;&#20010;&#27424;&#24179;&#28369;&#23725;&#22238;&#24402;&#65292;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;(AutoDML)&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#12290;&#36825;&#20123;&#20272;&#31639;&#22120;&#23558;&#32467;&#26524;&#24314;&#27169;&#19982;&#37325;&#37197;&#30456;&#32467;&#21512;&#65292;&#30452;&#25509;&#20272;&#35745;&#21453;&#21521;&#20542;&#21521;&#31215;&#20998;&#26435;&#37325;&#12290;&#24403;&#32467;&#26524;&#19982;&#26435;&#37325;&#27169;&#22411;&#37117;&#26159;&#26576;&#20123;&#65288;&#21487;&#33021;&#26159;&#26080;&#38480;&#30340;&#65289;&#22522;&#30784;&#20013;&#30340;&#32447;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;&#22686;&#24378;&#30340;&#20272;&#31639;&#22120;&#31561;&#21516;&#20110;&#20855;&#26377;&#23558;&#21407;&#22987;&#32467;&#26524;&#27169;&#22411;&#31995;&#25968;&#21644;OLS&#30456;&#32467;&#21512;&#30340;&#31995;&#25968;&#30340;&#21333;&#20010;&#32447;&#24615;&#27169;&#22411;&#65307;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#65292;&#22686;&#24378;&#20272;&#31639;&#22120;&#21512;&#24182;&#20026;&#20165;&#20351;&#29992;OLS. &#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#20351;&#29992;(&#20869;&#26680;)&#23725;&#22238;&#24402;&#20316;&#20026;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#30340;&#32852;&#21512;&#20272;&#31639;&#22120;&#31561;&#21516;&#20110;&#21333;&#20010;&#12289;&#27424;&#24179;&#28369;(&#20869;&#26680;)&#23725;&#22238;&#24402;&#65307;&#24403;&#32771;&#34385;&#21040;&#28176;&#36817;&#36895;&#29575;&#26102;&#65292;&#36825;&#19968;&#32467;&#26524;&#20063;&#25104;&#31435;&#12290;&#24403;&#20195;&#26367;&#26435;&#37325;&#27169;&#22411;&#20026;&#22871;&#32034;&#22238;&#24402;&#26102;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#29305;&#27530;&#24773;&#20917;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#24182;&#19988;&#28436;&#31034;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;Deep Spatiotemporal Clustering (DSC) &#65292;&#29992;&#20110;&#20351;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#26102;&#38388;&#32858;&#31867;&#65292;DSC&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#38598;&#25104;CNN-RNN&#23618;&#23398;&#20064;&#26102;&#31354;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#32858;&#31867;&#25439;&#22833;&#21644;&#25968;&#25454;&#37325;&#24314;&#25439;&#22833;&#20197;&#25913;&#21892;&#32858;&#31867;&#20998;&#37197;&#21644;&#38750;&#32447;&#24615;&#37325;&#24314;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14541</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#31354;&#32858;&#31867;&#65306;&#22810;&#32500;&#27668;&#20505;&#25968;&#25454;&#30340;&#20020;&#26102;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data. (arXiv:2304.14541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;Deep Spatiotemporal Clustering (DSC) &#65292;&#29992;&#20110;&#20351;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#26102;&#38388;&#32858;&#31867;&#65292;DSC&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#38598;&#25104;CNN-RNN&#23618;&#23398;&#20064;&#26102;&#31354;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#32858;&#31867;&#25439;&#22833;&#21644;&#25968;&#25454;&#37325;&#24314;&#25439;&#22833;&#20197;&#25913;&#21892;&#32858;&#31867;&#20998;&#37197;&#21644;&#38750;&#32447;&#24615;&#37325;&#24314;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#23545;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26159;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#21644;&#36317;&#31163;&#20989;&#25968;&#65292;&#20294;&#38598;&#20013;&#20110;&#25968;&#25454;&#30340;&#31354;&#38388;&#25110;&#26102;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#38598;&#20013;&#20110;&#20351;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#32852;&#21512;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;Deep Spatiotemporal Clustering (DSC)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#26102;&#38388;&#32858;&#31867;&#12290;&#21463;U-net&#26550;&#26500;&#21551;&#21457;&#65292;DSC&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#38598;&#25104;CNN-RNN&#23618;&#23398;&#20064;&#26102;&#31354;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290; DSC&#36824;&#21253;&#25324;&#19968;&#20010;&#29420;&#29305;&#30340;&#28508;&#22312;&#34920;&#31034;&#23618;&#65292;&#29992;&#20110;&#20351;&#29992;&#23398;&#29983;t&#20998;&#24067;&#36827;&#34892;&#32858;&#31867;&#20998;&#37197;&#12290;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#32858;&#31867;&#25439;&#22833;&#21644;&#25968;&#25454;&#37325;&#24314;&#25439;&#22833;&#65292;&#35813;&#31639;&#27861;&#36880;&#28176;&#25913;&#21892;&#32858;&#31867;&#20998;&#37197;&#21644;&#38750;&#32447;&#24615;&#37325;&#24314;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering high-dimensional spatiotemporal data using an unsupervised approach is a challenging problem for many data-driven applications. Existing state-of-the-art methods for unsupervised clustering use different similarity and distance functions but focus on either spatial or temporal features of the data. Concentrating on joint deep representation learning of spatial and temporal features, we propose Deep Spatiotemporal Clustering (DSC), a novel algorithm for the temporal clustering of high-dimensional spatiotemporal data using an unsupervised deep learning method. Inspired by the U-net architecture, DSC utilizes an autoencoder integrating CNN-RNN layers to learn latent representations of the spatiotemporal data. DSC also includes a unique layer for cluster assignment on latent representations that uses the Student's t-distribution. By optimizing the clustering loss and data reconstruction loss simultaneously, the algorithm gradually improves clustering assignments and the nonlinea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14537</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#26368;&#20248;&#20998;&#21306;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal partition of feature using Bayesian classifier. (arXiv:2304.14537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#26159;&#19968;&#31181;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#27969;&#34892;&#20998;&#31867;&#26041;&#27861;&#65292;&#23613;&#31649;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#21548;&#36215;&#26469;&#24456;&#22909;&#65292;&#20294;&#23454;&#38469;&#19978;&#20250;&#23548;&#33268;&#22823;&#22810;&#25968;&#25237;&#31080;&#39118;&#26684;&#30340;&#34892;&#20026;&#12290;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#34987;&#31216;&#20026;&#29420;&#31435;&#29305;&#24449;&#65292;&#22240;&#20026;&#22312;&#39044;&#27979;&#20998;&#31867;&#26102;&#23427;&#20204;&#27809;&#26377;&#26465;&#20214;&#30456;&#20851;&#24615;&#25110;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#26126;&#30830;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#38169;&#35823;&#29575;&#26356;&#20302;&#12289;&#20934;&#30830;&#29575;&#26356;&#39640;&#25110;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Naive Bayesian classifier is a popular classification method employing the Bayesian paradigm. The concept of having conditional dependence among input variables sounds good in theory but can lead to a majority vote style behaviour. Achieving conditional independence is often difficult, and they introduce decision biases in the estimates. In Naive Bayes, certain features are called independent features as they have no conditional correlation or dependency when predicting a classification. In this paper, we focus on the optimal partition of features by proposing a novel technique called the Comonotone-Independence Classifier (CIBer) which is able to overcome the challenges posed by the Naive Bayes method. For different datasets, we clearly demonstrate the efficacy of our technique, where we achieve lower error rates and higher or equivalent accuracy compared to models such as Random Forests and XGBoost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14535</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26041;&#38754;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#22495;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#21644;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#29616;&#23454;&#19990;&#30028;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#26080;&#27861;&#36866;&#29992;&#30340;&#12290;DTL&#34987;&#24341;&#20837;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#23427;&#26377;&#21161;&#20110;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#23454;&#38469;&#25968;&#25454;&#38598;&#21363;&#20351;&#24456;&#23567;&#25110;&#31245;&#26377;&#19981;&#21516;&#65292;&#20294;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20197;&#38416;&#26126;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which can not meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#25200;&#21160;&#32593;&#32476;&#36890;&#36807;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#25191;&#34892;&#19981;&#21516;&#21160;&#20316;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29366;&#24577;&#30340;&#25197;&#26354;&#65292;&#20197;&#20943;&#36731;&#25968;&#25454;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14533</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Policy Optimization in Deep Reinforcement Learning. (arXiv:2304.14533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#25200;&#21160;&#32593;&#32476;&#36890;&#36807;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#25191;&#34892;&#19981;&#21516;&#21160;&#20316;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29366;&#24577;&#30340;&#25197;&#26354;&#65292;&#20197;&#20943;&#36731;&#25968;&#25454;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#31574;&#30053;&#21487;&#20197;&#36807;&#24230;&#25311;&#21512;&#35266;&#27979;&#20013;&#30340;&#34920;&#38754;&#29305;&#24449;&#65292;&#36825;&#20250;&#22952;&#30861;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#22312;&#39640;&#32500;&#29366;&#24577;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#26234;&#33021;&#20307;&#38590;&#20197;&#23398;&#20064;&#26377;&#29992;&#30340;&#31574;&#30053;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#26469;&#25552;&#20379;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#26524;&#22312;&#29615;&#22659;&#20013;&#31616;&#21333;&#22320;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20943;&#36731;&#19978;&#36848;&#38382;&#39064;&#24182;&#25552;&#39640;&#23398;&#20064;&#31574;&#30053;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21338;&#24328;&#29702;&#35770;&#30446;&#26631;&#65292;&#22312;&#36825;&#20010;&#30446;&#26631;&#20013;&#65292;&#25200;&#21160;&#32593;&#32476;&#20462;&#25913;&#29366;&#24577;&#65292;&#20197;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#25191;&#34892;&#19981;&#21516;&#21160;&#20316;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29366;&#24577;&#30340;&#25197;&#26354;&#12290;&#30456;&#21453;&#65292;&#31574;&#30053;&#32593;&#32476;&#26356;&#26032;&#20854;&#21442;&#25968;&#65292;&#20197;&#26368;&#23567;&#21270;&#25200;&#21160;&#25928;&#26524;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#26469;&#22870;&#21169;&#30340;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The policy represented by the deep neural network can overfit the spurious features in observations, which hamper a reinforcement learning agent from learning effective policy. This issue becomes severe in high-dimensional state, where the agent struggles to learn a useful policy. Data augmentation can provide a performance boost to RL agents by mitigating the effect of overfitting. However, such data augmentation is a form of prior knowledge, and naively applying them in environments might worsen an agent's performance. In this paper, we propose a novel RL algorithm to mitigate the above issue and improve the efficiency of the learned policy. Our approach consists of a max-min game theoretic objective where a perturber network modifies the state to maximize the agent's probability of taking a different action while minimizing the distortion in the state. In contrast, the policy network updates its parameters to minimize the effect of perturbation while maximizing the expected future r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65306;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14530</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#31181;&#23376;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
It is all about where you start: Text-to-image generation with seed selection. (arXiv:2304.14530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65306;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#26032;&#30340;&#32452;&#21512;&#21644;&#22330;&#26223;&#20013;&#21512;&#25104;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#29983;&#25104;&#19981;&#24120;&#35265;&#30340;&#27010;&#24565;&#12289;&#32597;&#35265;&#30340;&#19981;&#23547;&#24120;&#32452;&#21512;&#25110;&#32467;&#26500;&#21270;&#27010;&#24565;&#65288;&#22914;&#25163;&#25484;&#65289;&#26041;&#38754;&#26377;&#22256;&#38590;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#37096;&#20998;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#38271;&#23614;&#24615;&#65306;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#38598;&#20005;&#37325;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#20998;&#24067;&#23614;&#37096;&#30340;&#27010;&#24565;&#19978;&#34920;&#29616;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#19981;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#22122;&#22768;&#31354;&#38388;&#20013;&#31934;&#24515;&#36873;&#25321;&#36866;&#24403;&#30340;&#29983;&#25104;&#31181;&#23376;&#65292;&#21487;&#20197;&#27491;&#30830;&#29983;&#25104;&#32597;&#35265;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#25216;&#26415;&#34987;&#31216;&#20026;SeedSelect&#12290;SeedSelect&#26159;&#39640;&#25928;&#30340;&#65292;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;SeedSelect&#30340;&#25928;&#30410;&#12290;&#39318;&#20808;&#65292;&#22312;&#23569;&#26679;&#26412;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#20013;&#65292;&#25105;&#20204;&#20026;&#23569;&#26679;&#26412;&#21644;&#38271;&#23614;&#22522;&#20934;&#29983;&#25104;&#20102;&#35821;&#20041;&#27491;&#30830;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models can synthesize a large variety of concepts in new compositions and scenarios. However, they still struggle with generating uncommon concepts, rare unusual combinations, or structured concepts like hand palms. Their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. Here we characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, a technique that we call SeedSelect. SeedSelect is efficient and does not require retraining the diffusion model. We evaluate the benefit of SeedSelect on a series of problems. First, in few-shot semantic data augmentation, where we generate semantically correct images for few-shot and long-tail benchmarks. We show classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20803;&#20998;&#24067;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2304.14522</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20803;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20803;&#20998;&#24067;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#36890;&#24120;&#37319;&#29992;&#21521;&#37327;&#34920;&#31034;&#65292;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#36890;&#24120;&#20351;&#29992;&#28857;&#31215;&#20989;&#25968;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#26159;&#23398;&#20064;&#27599;&#20010;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#21521;&#37327;&#65292;&#32780;&#26159;&#23398;&#20064;&#22810;&#20803;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#36127;&#22810;&#20803;KL&#25955;&#24230;&#35745;&#31639;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#31616;&#21270;&#21644;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#20998;&#24067;&#26159;&#22810;&#32500;&#27491;&#24577;&#20998;&#24067;&#65292;&#28982;&#21518;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#36825;&#20123;&#20998;&#24067;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#21521;&#37327;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35206;&#30422;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25991;&#26412;&#25972;&#21512;&#36827;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#26080;&#35821;&#38899;&#22495;&#33258;&#36866;&#24212;&#21644;&#28608;&#27963;&#30340;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#25345;&#32493;&#27169;&#22411;&#23545;&#20849;&#20139;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#24456;&#37325;&#35201;&#65292;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#19968;&#20010;&#27604;&#21333;&#27169;&#24577;&#26356;&#32039;&#20945;&#37325;&#21472;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#65292;&#36825;&#37096;&#20998;&#35299;&#37322;&#20102;Maestro&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.14514</link><description>&lt;p&gt;
&#29702;&#35299;&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Understanding Shared Speech-Text Representations. (arXiv:2304.14514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25991;&#26412;&#25972;&#21512;&#36827;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#26080;&#35821;&#38899;&#22495;&#33258;&#36866;&#24212;&#21644;&#28608;&#27963;&#30340;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#25345;&#32493;&#27169;&#22411;&#23545;&#20849;&#20139;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#24456;&#37325;&#35201;&#65292;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#19968;&#20010;&#27604;&#21333;&#27169;&#24577;&#26356;&#32039;&#20945;&#37325;&#21472;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#65292;&#36825;&#37096;&#20998;&#35299;&#37322;&#20102;Maestro&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#23558;&#25991;&#26412;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#27169;&#22411;&#20013;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;Maestro&#25512;&#36827;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#31867;&#20998;&#26512;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;&#20135;&#29983;&#30340;&#20849;&#20139;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#30340;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#35821;&#38899;&#22495;&#33258;&#36866;&#24212;&#30340;&#26497;&#38480;&#65292;&#21457;&#29616;&#20026;&#20102;&#23398;&#20064;&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#65292;&#20855;&#26377;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#30340;&#29305;&#23450;&#35821;&#26009;&#24211;&#25345;&#32493;&#27169;&#22411;&#26159;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;&#35821;&#38899;&#25110;&#25991;&#26412;&#65289;&#30340;&#28608;&#27963;&#19982;&#20849;&#20139;&#32534;&#30721;&#22120;&#30340;&#28608;&#27963;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#19968;&#20010;&#27604;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#26356;&#32039;&#20945;&#21644;&#37325;&#21472;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#37096;&#20998;&#35299;&#37322;&#20102;Maestro&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a number of approaches to train speech models by incorpo-rating text into end-to-end models have been developed, with Mae-stro advancing state-of-the-art automatic speech recognition (ASR)and Speech Translation (ST) performance. In this paper, we expandour understanding of the resulting shared speech-text representationswith two types of analyses. First we examine the limits of speech-free domain adaptation, finding that a corpus-specific duration modelfor speech-text alignment is the most important component for learn-ing a shared speech-text representation. Second, we inspect the sim-ilarities between activations of unimodal (speech or text) encodersas compared to the activations of a shared encoder. We find that theshared encoder learns a more compact and overlapping speech-textrepresentation than the uni-modal encoders. We hypothesize that thispartially explains the effectiveness of the Maestro shared speech-textrepresentations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D Brainformer&#30340;&#19977;&#32500;&#34701;&#21512;Transformer&#32467;&#26500;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#23545;&#33041;MRI&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#36827;&#34892;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.14508</link><description>&lt;p&gt;
3D Brainformer&#65306;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#19977;&#32500;&#34701;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
3D Brainformer: 3D Fusion Transformer for Brain Tumor Segmentation. (arXiv:2304.14508v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D Brainformer&#30340;&#19977;&#32500;&#34701;&#21512;Transformer&#32467;&#26500;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#23545;&#33041;MRI&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#36827;&#34892;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#25216;&#26415;&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#20020;&#24202;&#30740;&#31350;&#20013;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#33041;&#37096;&#26144;&#23556;&#24037;&#20855;&#12290;&#33041;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;&#26377;&#21161;&#20110;&#20020;&#24202;&#35786;&#26029;&#65292;&#35780;&#20272;&#21644;&#25163;&#26415;&#35745;&#21010;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36817;&#24180;&#26469;&#22312;&#22823;&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21367;&#31215;&#32593;&#32476;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#20854;&#20165;&#33021;&#34920;&#31034;MRI&#22270;&#20687;&#20013;&#21306;&#22495;&#20869;&#23616;&#37096;&#20687;&#32032;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D Brainformer&#30340;&#19977;&#32500;&#34701;&#21512;Transformer&#32467;&#26500;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;MRI&#22270;&#20687;&#20013;&#33041;&#32959;&#30244;&#20998;&#21106;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;3D Brainformer&#22312;BraTS 2019&#21644;2020&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic resonance imaging (MRI) is critically important for brain mapping in both scientific research and clinical studies. Precise segmentation of brain tumors facilitates clinical diagnosis, evaluations, and surgical planning. Deep learning has recently emerged to improve brain tumor segmentation and achieved impressive results. Convolutional architectures are widely used to implement those neural networks. By the nature of limited receptive fields, however, those architectures are subject to representing long-range spatial dependencies of the voxel intensities in MRI images. Transformers have been leveraged recently to address the above limitations of convolutional networks. Unfortunately, the majority of current Transformers-based methods in segmentation are performed with 2D MRI slices, instead of 3D volumes. Moreover, it is difficult to incorporate the structures between layers because each head is calculated independently in the Multi-Head Self-Attention mechanism (MHSA). In th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#30382;&#32932;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2304.14505</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#29992;&#20110;&#30382;&#32932;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21487;&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#30382;&#32932;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#23450;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#22240;&#32032;&#12290;&#22312;&#20154;&#31867;&#20013;&#24515;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#30382;&#32932;&#30149;&#20998;&#31867;&#22312;&#30382;&#32932;&#31185;&#20013;&#65292;&#20173;&#22788;&#20110;&#20854;&#21021;&#32423;&#38454;&#27573;&#30340;DL&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#30001;&#20110;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#33021;&#22815;&#35299;&#37322;&#35757;&#32451;&#30340;DL&#31639;&#27861;&#34892;&#20026;&#30340;&#31243;&#24207;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#20020;&#24202;&#21307;&#24072;&#30340;&#20449;&#20219;&#12290;&#20026;&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#65292;&#30382;&#32932;&#31185;&#21307;&#29983;&#20381;&#38752;&#30142;&#30149;&#30340;&#35270;&#35273;&#35780;&#20272;&#21644;&#24739;&#32773;&#30149;&#21490;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#21463;&#38480;&#20110;&#21367;&#31215;&#32467;&#26500;&#25152;&#38656;&#30340;&#29305;&#24449;&#32423;&#21644;&#20915;&#31574;&#32423;&#34701;&#21512;&#31243;&#24207;&#30340;&#20998;&#31163;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#30340;&#27880;&#24847;&#26426;&#21046;&#23454;&#29616;&#21333;&#38454;&#27573;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#65292;&#20197;&#24110;&#21161;&#35786;&#26029;&#30382;&#32932;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
A lot of deep learning (DL) research these days is mainly focused on improving on quantitative metrics regardless of other factors. In human centered applications, like skin lesion classification in dermatology, DL-driven clinical decision support systems are still in their infancy due to the limited transparency of their decision-making process. Moreover, the lack of procedures that can explain the behavior of trained DL algorithms leads to almost no trust from the clinical physicians. To diagnose skin lesions, dermatologists rely on both visual assessment of the disease and the data gathered from the anamnesis of the patient. Data-driven algorithms dealing with multi-modal data are limited by the separation of feature-level and decision-level fusion procedures required by convolutional architectures. To address this issue, we enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures to aid in the diagnosis of skin diseases. Our method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;MLP&#21644;LSTM&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#24335;&#65292;&#26088;&#22312;&#26816;&#27979;&#24182;&#38450;&#27490;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2304.14504</link><description>&lt;p&gt;
&#28151;&#21512;MLP&#21644;LSTM&#27169;&#22411;&#30340;&#28145;&#24230;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hybrid Deepfake Detection Utilizing MLP and LSTM. (arXiv:2304.14504v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;MLP&#21644;LSTM&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#24335;&#65292;&#26088;&#22312;&#26816;&#27979;&#24182;&#38450;&#27490;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20250;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30495;&#23454;&#20449;&#24687;&#30340;&#20381;&#36182;&#19981;&#26029;&#22686;&#38271;&#65292;&#21033;&#29992;&#28145;&#24230;&#20266;&#36896;&#65288;deepfake&#65289;&#27450;&#39575;&#29992;&#25143;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#21033;&#29992;&#26368;&#26032;&#25216;&#26415;&#21457;&#26126;&#65292;&#20351;&#32593;&#32476;&#29992;&#25143;&#33021;&#22815;&#23558;&#33258;&#24049;&#30340;&#38754;&#23380;&#26367;&#25442;&#20026;&#35768;&#22810;&#37325;&#35201;&#25919;&#27835;&#21644;&#25991;&#21270;&#20154;&#29289;&#30340;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#21512;&#25104;&#38754;&#23380;&#65292;&#20174;&#32780;&#20256;&#25773;&#22823;&#37327;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#38450;&#27490;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#65292;&#29616;&#22312;&#38750;&#24120;&#38656;&#35201;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#24335;&#65292;&#37319;&#29992;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65306;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;140k Real and Fake&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing reliance of society on social media for authentic information has done nothing but increase over the past years. This has only raised the potential consequences of the spread of misinformation. One of the growing methods in popularity is to deceive users using a deepfake. A deepfake is an invention that has come with the latest technological advancements, which enables nefarious online users to replace their face with a computer generated, synthetic face of numerous powerful members of society. Deepfake images and videos now provide the means to mimic important political and cultural figures to spread massive amounts of false information. Models that can detect these deepfakes to prevent the spread of misinformation are now of tremendous necessity. In this paper, we propose a new deepfake detection schema utilizing two deep learning algorithms: long short term memory and multilayer perceptron. We evaluate our model using a publicly available dataset named 140k Real and Fake
&lt;/p&gt;</description></item><item><title>MWaste&#26159;&#19968;&#27454;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22403;&#22334;&#26448;&#26009;&#20998;&#31867;&#20026;&#22403;&#22334;&#12289;&#22609;&#26009;&#12289;&#32440;&#24352;&#12289;&#37329;&#23646;&#12289;&#29627;&#29827;&#25110;&#30828;&#32440;&#26495;&#65292;&#21487;&#24110;&#21161;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14498</link><description>&lt;p&gt;
MWaste&#65306;&#31649;&#29702;&#23478;&#24237;&#22403;&#22334;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MWaste: A Deep Learning Approach to Manage Household Waste. (arXiv:2304.14498v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14498
&lt;/p&gt;
&lt;p&gt;
MWaste&#26159;&#19968;&#27454;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22403;&#22334;&#26448;&#26009;&#20998;&#31867;&#20026;&#22403;&#22334;&#12289;&#22609;&#26009;&#12289;&#32440;&#24352;&#12289;&#37329;&#23646;&#12289;&#29627;&#29827;&#25110;&#30828;&#32440;&#26495;&#65292;&#21487;&#24110;&#21161;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20998;&#31867;&#22403;&#22334;&#22788;&#29702;&#30340;&#22238;&#25910;&#20998;&#31867;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#25104;&#26412;&#39640;&#12289;&#19981;&#31934;&#30830;&#19988;&#19981;&#28165;&#26224;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;MWaste&#65292;&#19968;&#27454;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23558;&#22403;&#22334;&#26448;&#26009;&#20998;&#31867;&#20026;&#22403;&#22334;&#12289;&#22609;&#26009;&#12289;&#32440;&#24352;&#12289;&#37329;&#23646;&#12289;&#29627;&#29827;&#25110;&#30828;&#32440;&#26495;&#12290;&#20854;&#26377;&#25928;&#24615;&#24050;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;92&#65285;&#30340;&#24179;&#22343;&#31934;&#24230;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#36890;&#36807;&#20351;&#22403;&#22334;&#22788;&#29702;&#26356;&#26377;&#25928;&#24182;&#20943;&#23569;&#22240;&#19981;&#27491;&#30830;&#30340;&#22403;&#22334;&#22788;&#29702;&#32780;&#24341;&#36215;&#30340;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#26469;&#24110;&#21161;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision methods have shown to be effective in classifying garbage into recycling categories for waste processing, existing methods are costly, imprecise, and unclear. To tackle this issue, we introduce MWaste, a mobile application that uses computer vision and deep learning techniques to classify waste materials as trash, plastic, paper, metal, glass or cardboard. Its effectiveness was tested on various neural network architectures and real-world images, achieving an average precision of 92\% on the test set. This app can help combat climate change by enabling efficient waste processing and reducing the generation of greenhouse gases caused by incorrect waste disposal.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36824;&#21407;&#21472;&#21152;&#20449;&#21495;&#30340;&#21407;&#22987;&#20449;&#21495;&#65292;&#25552;&#39640;&#20102;&#29289;&#29702;&#25968;&#25454;&#30340;&#33021;&#37327;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#36866;&#29992;&#20110;&#31867;&#20284;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.14496</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36824;&#21407;&#21472;&#21152;&#20449;&#21495;&#30340;&#21407;&#22987;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Restoring Original Signal From Pile-up Signal using Deep Learning. (arXiv:2304.14496v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36824;&#21407;&#21472;&#21152;&#20449;&#21495;&#30340;&#21407;&#22987;&#20449;&#21495;&#65292;&#25552;&#39640;&#20102;&#29289;&#29702;&#25968;&#25454;&#30340;&#33021;&#37327;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#36866;&#29992;&#20110;&#31867;&#20284;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#29289;&#29702;&#23398;&#39046;&#22495;&#65292;&#21472;&#21152;&#20449;&#21495;&#32463;&#24120;&#34987;&#20135;&#29983;&#65292;&#23548;&#33268;&#29289;&#29702;&#25968;&#25454;&#19981;&#20934;&#30830;&#65292;&#24102;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#24182;&#24341;&#36215;&#21508;&#31181;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#20851;&#38190;&#22320;&#30699;&#27491;&#21472;&#21152;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#36824;&#21407;&#21472;&#21152;&#20449;&#21495;&#30340;&#21407;&#22987;&#20449;&#21495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#37325;&#24314;&#21472;&#21152;&#27874;&#24418;&#20013;&#30340;&#21407;&#22987;&#20449;&#21495;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22987;&#20449;&#21495;&#26367;&#25442;&#21472;&#21152;&#20449;&#21495;&#65292;&#25968;&#25454;&#30340;&#33021;&#37327;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#24471;&#21040;&#26174;&#30528;&#25552;&#39640;&#12290;&#35813;&#27169;&#22411;&#23454;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#31890;&#23376;&#35782;&#21035;&#22270;&#21644;&#31890;&#23376;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#31867;&#20284;&#38382;&#39064;&#65292;&#20363;&#22914;&#20998;&#31163;&#22810;&#20010;&#20449;&#21495;&#25110;&#32416;&#27491;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#21644;&#32972;&#26223;&#19979;&#30340;&#21472;&#21152;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pile-up signals are frequently produced in experimental physics. They create inaccurate physics data with high uncertainty and cause various problems. Therefore, the correction to pile-up signals is crucially required. In this study, we implemented a deep learning method to restore the original signals from the pile-up signals. We showed that a deep learning model could accurately reconstruct the original signal waveforms from the pile-up waveforms. By substituting the pile-up signals with the original signals predicted by the model, the energy and timing resolutions of the data are notably enhanced. The model implementation significantly improved the quality of the particle identification plot and particle tracks. This method is applicable to similar problems, such as separating multiple signals or correcting pile-up signals with other types of noises and backgrounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#19979;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20197;&#29983;&#25104;&#26368;&#31616;&#27905;&#32780;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21644;&#39044;&#27979;&#26032;&#30340;&#29289;&#20307;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.14493</link><description>&lt;p&gt;
&#23545;&#35937;&#20013;&#24515;&#30340;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#19982;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Symmetry and Complexity in Object-Centric Deep Active Inference Models. (arXiv:2304.14493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#19979;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20197;&#29983;&#25104;&#26368;&#31616;&#27905;&#32780;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#21644;&#39044;&#27979;&#26032;&#30340;&#29289;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#35201;&#24863;&#30693;&#21644;&#20114;&#21160;&#19978;&#30334;&#20010;&#29289;&#20307;&#12290;&#20026;&#27492;&#65292;&#20182;&#20204;&#38656;&#35201;&#20351;&#29992;&#36825;&#20123;&#29289;&#20307;&#30340;&#24515;&#29702;&#27169;&#22411;&#65292;&#24182;&#32463;&#24120;&#21033;&#29992;&#29289;&#20307;&#24418;&#29366;&#21644;&#22806;&#35266;&#30340;&#23545;&#31216;&#24615;&#26469;&#23398;&#20064;&#36890;&#29992;&#21644;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#12290;&#20027;&#21160;&#25512;&#29702;&#26159;&#29702;&#35299;&#21644;&#24314;&#27169;&#26377;&#24863;&#30693;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#12290;&#23427;&#35748;&#20026;&#20195;&#29702;&#20154;&#22312;&#29615;&#22659;&#20013;&#20135;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19978;&#38480;&#30340;&#24778;&#22855;&#65288;&#21363;&#33258;&#30001;&#33021;&#65289;&#26469;&#23398;&#20064;&#21644;&#34892;&#21160;&#12290;&#33258;&#30001;&#33021;&#20998;&#35299;&#20026;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#39033;&#65292;&#36825;&#24847;&#21619;&#30528;&#20195;&#29702;&#20542;&#21521;&#20110;&#36873;&#25321;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#35299;&#37322;&#20182;&#20204;&#30340;&#24863;&#35273;&#35266;&#23519;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#23450;&#29289;&#20307;&#22825;&#29983;&#23545;&#31216;&#24615;&#22312;&#28145;&#24230;&#20027;&#21160;&#25512;&#29702;&#19979;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#20063;&#34920;&#29616;&#20026;&#23545;&#31216;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#23545;&#35937;&#20013;&#24515;&#30340;&#34920;&#31034;&#65292;&#20854;&#20174;&#20687;&#32032;&#20013;&#35757;&#32451;&#20197;&#39044;&#27979;&#26032;&#30340;&#29289;&#20307;&#35270;&#22270;&#32780;&#24180;&#40836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive and interact with hundreds of objects every day. In doing so, they need to employ mental models of these objects and often exploit symmetries in the object's shape and appearance in order to learn generalizable and transferable skills. Active inference is a first principles approach to understanding and modeling sentient agents. It states that agents entertain a generative model of their environment, and learn and act by minimizing an upper bound on their surprisal, i.e. their Free Energy. The Free Energy decomposes into an accuracy and complexity term, meaning that agents favor the least complex model, that can accurately explain their sensory observations. In this paper, we investigate how inherent symmetries of particular objects also emerge as symmetries in the latent state space of the generative model learnt under deep active inference. In particular, we focus on object-centric representations, which are trained from pixels to predict novel object views as the age
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#20998;&#26512;&#20581;&#36523;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.14489</link><description>&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;NLP&#25216;&#26415;&#20998;&#26512;YouTube&#30340;&#23383;&#24149;&#25968;&#25454;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Automatic Generation of Labeled Data for Video-Based Human Pose Analysis via NLP applied to YouTube Subtitles. (arXiv:2304.14489v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14489
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#20998;&#26512;&#20581;&#36523;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#23621;&#23478;&#38203;&#28860;&#35780;&#20272;&#31995;&#32479;&#24050;&#25104;&#20026;&#30446;&#21069;&#30740;&#31350;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#30001;&#20110;&#19987;&#38376;&#38024;&#23545;&#36816;&#21160;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#20581;&#36523;&#35270;&#39057;&#20016;&#23500;&#36164;&#28304;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#35270;&#39057;&#36890;&#24120;&#19981;&#20165;&#23637;&#31034;&#32451;&#20064;&#20869;&#23481;&#65292;&#36824;&#25552;&#20379;&#35821;&#35328;&#20449;&#24687;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#24687;&#28304;&#30340;&#20248;&#21183;&#12290;&#20197;&#20463;&#21351;&#25745;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;NLP&#25216;&#26415;&#20998;&#26512;&#23383;&#24149;&#25968;&#25454;&#65292;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#19982;&#23039;&#21183;&#20998;&#26512;&#30456;&#20851;&#20449;&#24687;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65288;&#26080;&#20851;&#32039;&#35201;&#65292;&#30456;&#20851;&#27491;&#30830;&#65292;&#30456;&#20851;&#19981;&#27491;&#30830;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20851;&#21098;&#36753;&#65288;$n=332$&#65289;&#20855;&#26377;&#19982;&#30456;&#20851;&#21098;&#36753;&#65288;$n=298$&#65289;&#26174;&#33879;&#19981;&#21516;&#30340;&#20851;&#33410;&#21487;&#35265;&#24615;&#20540;&#12290;&#26816;&#26597;&#32858;&#31867;&#20013;&#24515;&#20063;&#23637;&#29616;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in computer vision as well as machine learning (ML), video-based at-home exercise evaluation systems have become a popular topic of current research. However, performance depends heavily on the amount of available training data. Since labeled datasets specific to exercising are rare, we propose a method that makes use of the abundance of fitness videos available online. Specifically, we utilize the advantage that videos often not only show the exercises, but also provide language as an additional source of information. With push-ups as an example, we show that through the analysis of subtitle data using natural language processing (NLP), it is possible to create a labeled (irrelevant, relevant correct, relevant incorrect) dataset containing relevant information for pose analysis. In particular, we show that irrelevant clips ($n=332$) have significantly different joint visibility values compared to relevant clips ($n=298$). Inspecting cluster centroids also show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#24615;&#26694;&#26550;&#65292;&#38024;&#23545;&#23545;&#25239;&#24615;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#21487;&#24863;&#30693;&#27169;&#24335;&#21387;&#20498;&#25915;&#20987;&#32773;&#30340;&#19981;&#21487;&#24863;&#30693;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14483</link><description>&lt;p&gt;
&#23545;&#25239;&#24863;&#30693;&#30340;&#36845;&#20195;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversary Aware Continual Learning. (arXiv:2304.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#24615;&#26694;&#26550;&#65292;&#38024;&#23545;&#23545;&#25239;&#24615;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#21487;&#24863;&#30693;&#27169;&#24335;&#21387;&#20498;&#25915;&#20987;&#32773;&#30340;&#19981;&#21487;&#24863;&#30693;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#24110;&#21161;&#27169;&#22411;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20449;&#24687;&#65288;&#31867;&#21035;&#65289;&#65292;&#21516;&#26102;&#20445;&#30041;&#20043;&#21069;&#33719;&#24471;&#30340;&#20449;&#24687;&#65288;&#31867;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#26234;&#33021;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20449;&#24687;&#35823;&#23548;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#27979;&#35797;&#26102;&#25925;&#24847;&#24536;&#35760;&#29305;&#23450;&#30340;&#20219;&#21153;&#25110;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#24615;&#26694;&#26550;&#26469;&#21453;&#20987;&#36825;&#31181;&#28508;&#22312;&#25915;&#20987;&#12290;&#25105;&#20204;&#21033;&#29992;&#25915;&#20987;&#32773;&#30340;&#20027;&#35201;&#20248;&#21183;--&#20351;&#21518;&#38376;&#27169;&#24335;&#23545;&#20154;&#19981;&#21487;&#24863;&#30693;--&#24182;&#25552;&#35758;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#21387;&#20498;&#25915;&#20987;&#32773;&#30340;&#21487;&#24863;&#30693;&#27169;&#24335;&#20197;&#25269;&#28040;&#23545;&#25239;&#24615;&#25915;&#20987;&#32773;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#21508;&#31181;&#24120;&#29992;&#30340;Replay-based&#65288;&#20004;&#32773;&#37117;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning approaches are useful as they help the model to learn new information (classes) sequentially, while also retaining the previously acquired information (classes). However, it has been shown that such approaches are extremely vulnerable to the adversarial backdoor attacks, where an intelligent adversary can introduce small amount of misinformation to the model in the form of imperceptible backdoor pattern during training to cause deliberate forgetting of a specific task or class at test time. In this work, we propose a novel defensive framework to counter such an insidious attack where, we use the attacker's primary strength-hiding the backdoor pattern by making it imperceptible to humans-against it, and propose to learn a perceptible (stronger) pattern (also during the training) that can overpower the attacker's imperceptible (weaker) pattern. We demonstrate the effectiveness of the proposed defensive mechanism through various commonly used Replay-based (both 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#40657;&#30418;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#24037;&#20855;&#30340;&#35282;&#33394;&#20197;&#21450;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;BGMAttack&#25915;&#20987;&#25991;&#26412;&#20998;&#31867;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65292;&#32780;&#19988;&#36755;&#20837;&#30340;&#25968;&#25454;&#27809;&#26377;&#26126;&#26174;&#21464;&#24418;&#12290;</title><link>http://arxiv.org/abs/2304.14475</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#25915;&#20987;&#24037;&#20855;&#65306;&#36890;&#36807;&#40657;&#30418;&#29983;&#25104;&#27169;&#22411;&#35302;&#21457;&#30340;&#38544;&#34109;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger. (arXiv:2304.14475v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#40657;&#30418;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#24037;&#20855;&#30340;&#35282;&#33394;&#20197;&#21450;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;BGMAttack&#25915;&#20987;&#25991;&#26412;&#20998;&#31867;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65292;&#32780;&#19988;&#36755;&#20837;&#30340;&#25968;&#25454;&#27809;&#26377;&#26126;&#26174;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#30528;&#29616;&#26377;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#38590;&#20197;&#24863;&#30693;&#30340;&#35302;&#21457;&#22120;&#25554;&#20837;&#36755;&#20837;&#25968;&#25454;&#24182;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26469;&#30772;&#22351;&#27169;&#22411;&#12290;&#38543;&#30528;&#23574;&#31471;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#23558;&#37325;&#20889;&#25512;&#21521;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#24230;&#65292;&#36825;&#31181;&#25915;&#20987;&#21464;&#24471;&#26356;&#21152;&#38590;&#20197;&#26816;&#27979;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#40657;&#30418;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#24037;&#20855;&#30340;&#35282;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#30740;&#31350;&#30456;&#20851;&#38450;&#24481;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#25915;&#20987;BGMAttack&#21487;&#20197;&#26377;&#25928;&#22320;&#27450;&#39575;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#25915;&#20987;&#26041;&#27861;&#65292;BGMAttack&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;&#21495;&#20351;&#21518;&#38376;&#35302;&#21457;&#22120;&#19981;&#22826;&#26174;&#30524;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25915;&#20987;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#24182;&#36741;&#20197;&#19977;&#20010;&#19981;&#21516;&#30340;&#20154;&#31867;&#35748;&#30693;&#35780;&#20272;&#65292;&#21457;&#29616;BGMAttack&#30340;&#34920;&#29616;&#30456;&#24403;&#19988;&#36755;&#20837;&#25968;&#25454;&#27809;&#26377;&#26126;&#26174;&#30340;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#23545;&#36825;&#31181;&#38544;&#34109;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, BGMAttack, could effectively deceive textual classifiers. Compared with the traditional attack methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that Figure 4 achieves comparable attack performance w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32534;&#30721;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;NeRFs&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#37319;&#26679;&#20986;&#36924;&#30495;&#30340;NeRFs&#65292;&#24182;&#20801;&#35768;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#65292;&#32473;&#23450;&#26576;&#20010;&#35266;&#23519;&#20316;&#20026;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2304.14473</link><description>&lt;p&gt;
&#23398;&#20064;&#25193;&#25955;&#20808;&#39564;&#29992;&#20110;NeRFs
&lt;/p&gt;
&lt;p&gt;
Learning a Diffusion Prior for NeRFs. (arXiv:2304.14473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32534;&#30721;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;NeRFs&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#37319;&#26679;&#20986;&#36924;&#30495;&#30340;NeRFs&#65292;&#24182;&#20801;&#35768;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#65292;&#32473;&#23450;&#26576;&#20010;&#35266;&#23519;&#20316;&#20026;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#24050;&#25104;&#20026;&#20174;2D&#25968;&#25454;&#27966;&#29983;&#20986;&#30340;&#29289;&#20307;&#21644;&#22330;&#26223;&#30340;&#24378;&#22823;&#31070;&#32463;3D&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;NeRFs&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#23569;&#37327;&#35270;&#22270;&#20316;&#20026;&#30417;&#30563;&#35757;&#32451;NeRFs&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;&#27424;&#21442;&#25968;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#19968;&#20123;&#24402;&#32435;&#20808;&#39564;&#26469;&#36807;&#28388;&#19981;&#33391;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#24341;&#20837;&#36825;&#26679;&#30340;&#24402;&#32435;&#20808;&#39564;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23398;&#20064;NeRFs&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24314;&#27169;&#26576;&#31867;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32534;&#30721;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;NeRFs&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#37319;&#26679;&#20986;&#36924;&#30495;&#30340;NeRFs&#65292;&#24182;&#21516;&#26102;&#20801;&#35768;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#65292;&#32473;&#23450;&#26576;&#20010;&#35266;&#23519;&#20316;&#20026;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D representation for objects and scenes derived from 2D data. Generating NeRFs, however, remains difficult in many scenarios. For instance, training a NeRF with only a small number of views as supervision remains challenging since it is an under-constrained problem. In such settings, it calls for some inductive prior to filter out bad local minima. One way to introduce such inductive priors is to learn a generative model for NeRFs modeling a certain class of scenes. In this paper, we propose to use a diffusion model to generate NeRFs encoded on a regularized grid. We show that our model can sample realistic NeRFs, while at the same time allowing conditional generations, given a certain observation as guidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26080;&#20154;&#26426;&#25429;&#25417;&#19981;&#21516;&#38634;&#22825;&#27668;&#20505;&#19979;&#30340;&#36710;&#36742;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#26816;&#27979;&#22823;&#38634;&#22825;&#27668;&#20013;&#36710;&#36742;&#30340;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#65292;&#23545;&#20110;&#36710;&#36742;&#22312;&#22797;&#26434;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#26816;&#27979;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.14466</link><description>&lt;p&gt;
&#21271;&#27431;&#36710;&#36742;&#25968;&#25454;&#38598;&#65288;NVD&#65289;&#65306;&#20351;&#29992;&#26032;&#25429;&#33719;&#30340;&#19981;&#21516;&#38634;&#22825;&#27668;&#20505;&#19979;&#26080;&#20154;&#26426;&#25429;&#25417;&#30340;NVD&#35780;&#20272;&#36710;&#36742;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Nordic Vehicle Dataset (NVD): Performance of vehicle detectors using newly captured NVD from UAV in different snowy weather conditions. (arXiv:2304.14466v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26080;&#20154;&#26426;&#25429;&#25417;&#19981;&#21516;&#38634;&#22825;&#27668;&#20505;&#19979;&#30340;&#36710;&#36742;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#26816;&#27979;&#22823;&#38634;&#22825;&#27668;&#20013;&#36710;&#36742;&#30340;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#65292;&#23545;&#20110;&#36710;&#36742;&#22312;&#22797;&#26434;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#26816;&#27979;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#30340;&#36710;&#36742;&#26816;&#27979;&#21644;&#35782;&#21035;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24050;&#29992;&#20110;&#19981;&#21516;&#30340;&#23433;&#20840;&#30446;&#30340;&#12290;&#36825;&#20123;&#22270;&#20687;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20197;&#26012;&#35282;&#24230;&#25429;&#33719;&#65292;&#24182;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#19981;&#22343;&#21248;&#30340;&#29031;&#26126;&#25928;&#26524;&#65292;&#36864;&#21270;&#65292;&#27169;&#31946;&#65292;&#36974;&#25377;&#65292;&#33021;&#35265;&#24230;&#20007;&#22833;&#31561;&#12290;&#27492;&#22806;&#65292;&#22825;&#27668;&#26465;&#20214;&#23545;&#23548;&#33268;&#23433;&#20840;&#38382;&#39064;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#21521;&#25910;&#38598;&#30340;&#25968;&#25454;&#28155;&#21152;&#20102;&#21478;&#19968;&#20010;&#39640;&#32423;&#21035;&#30340;&#25361;&#25112;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#38388;&#65292;&#37319;&#29992;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#26816;&#27979;&#21644;&#36319;&#36394;&#19981;&#21516;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#36710;&#36742;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#38634;&#22825;&#27668;&#19979;&#26816;&#27979;&#36710;&#36742;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#22240;&#20026;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#27809;&#26377;&#20219;&#20309;&#30740;&#31350;&#20351;&#29992;&#26080;&#20154;&#26426;&#23454;&#38469;&#25429;&#25417;&#21040;&#30340;&#22270;&#20687;&#26469;&#26816;&#27979;&#38634;&#22825;&#27668;&#20013;&#30340;&#36710;&#36742;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25429;&#25417;&#22312;&#19981;&#21516;&#35774;&#32622;&#21644;&#21508;&#31181;&#38634;&#35206;&#30422;&#26465;&#20214;&#19979;&#30340;&#36710;&#36742;&#25968;&#25454;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20197;&#26381;&#21153;&#20110;&#31185;&#23398;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle detection and recognition in drone images is a complex problem that has been used for different safety purposes. The main challenge of these images is captured at oblique angles and poses several challenges like non-uniform illumination effect, degradations, blur, occlusion, loss of visibility, etc. Additionally, weather conditions play a crucial role in causing safety concerns and add another high level of challenge to the collected data. Over the past few decades, various techniques have been employed to detect and track vehicles in different weather conditions. However, detecting vehicles in heavy snow is still in the early stages because of a lack of available data. Furthermore, there has been no research on detecting vehicles in snowy weather using real images captured by unmanned aerial vehicles (UAVs). This study aims to address this gap by providing the scientific community with data on vehicles captured by UAVs in different settings and under various snow cover conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;</title><link>http://arxiv.org/abs/2304.14463</link><description>&lt;p&gt;
Moccasin&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Moccasin: Efficient Tensor Rematerialization for Neural Networks. (arXiv:2304.14463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#36739;&#20302;&#30340;&#20869;&#23384;&#26159;&#37096;&#32626;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#32463;&#24120;&#36935;&#21040;&#30340;&#26368;&#22823;&#38480;&#21046;&#22240;&#32032;&#20043;&#19968;&#12290;&#24352;&#37327;&#37325;&#31639;&#26159;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#25152;&#38656;&#39640;&#20869;&#23384;&#38656;&#27714;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#31216;&#20026;Moccasin&#65292;&#20854;&#20013;&#21482;&#26377;$O(n)$&#20010;&#25972;&#25968;&#21464;&#37327;&#65292;$n$&#26159;&#35745;&#31639;&#22270;&#20013;&#33410;&#28857;&#30340;&#25968;&#37327;&#12290;&#36825;&#30456;&#23545;&#20110;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20855;&#26377;$O(n^2)$&#24067;&#23572;&#21464;&#37327;&#30340;&#20844;&#24335;&#25552;&#20986;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#20540;&#30740;&#31350;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#27604;&#26368;&#36817;&#30340;&#24037;&#20316;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. In this paper we consider the problem of execution time minimization of compute graphs subject to a memory budget. In particular, we develop a new constraint programming formulation called \textsc{Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph. This is a significant improvement over the works in the recent literature that propose formulations with $O(n^2)$ Boolean variables. We present numerical studies that show that our approach is up to an order of magnitude faster than recent work especially for large-scale graphs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#32622;&#20449;&#24230;&#22270;&#30340;&#40065;&#26834;&#24555;&#36895;&#36710;&#36742;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#26497;&#20540;&#31283;&#23450;&#21306;&#22495;&#65292;&#29983;&#25104;&#22686;&#24378;&#32622;&#20449;&#24230;&#22270;&#65292;&#20877;&#36890;&#36807;&#24555;&#36895;CNN&#29983;&#25104;&#20505;&#36873;&#21306;&#22495;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19981;&#21516;&#36895;&#24230;&#12289;&#24418;&#29366;&#12289;&#32467;&#26500;&#21644;&#21333;&#24352;&#22270;&#20687;&#20013;&#22810;&#20010;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#37319;&#29992;&#31895;&#31961;&#38598;&#21644;&#22522;&#20110;&#27169;&#31946;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#36710;&#36742;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.14462</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#32622;&#20449;&#24230;&#22270;&#30340;&#40065;&#26834;&#24555;&#36895;&#36710;&#36742;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust and Fast Vehicle Detection using Augmented Confidence Map. (arXiv:2304.14462v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#32622;&#20449;&#24230;&#22270;&#30340;&#40065;&#26834;&#24555;&#36895;&#36710;&#36742;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#26497;&#20540;&#31283;&#23450;&#21306;&#22495;&#65292;&#29983;&#25104;&#22686;&#24378;&#32622;&#20449;&#24230;&#22270;&#65292;&#20877;&#36890;&#36807;&#24555;&#36895;CNN&#29983;&#25104;&#20505;&#36873;&#21306;&#22495;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19981;&#21516;&#36895;&#24230;&#12289;&#24418;&#29366;&#12289;&#32467;&#26500;&#21644;&#21333;&#24352;&#22270;&#20687;&#20013;&#22810;&#20010;&#36710;&#36742;&#30340;&#24433;&#21709;&#65292;&#24182;&#37319;&#29992;&#31895;&#31961;&#38598;&#21644;&#22522;&#20110;&#27169;&#31946;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#36710;&#36742;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#22330;&#26223;&#19979;&#30340;&#36710;&#36742;&#26816;&#27979;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#22810;&#31181;&#31867;&#22411;&#30340;&#36710;&#36742;&#65288;&#36895;&#24230;&#12289;&#24418;&#29366;&#12289;&#32467;&#26500;&#31561;&#65289;&#30340;&#23384;&#22312;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#32622;&#20449;&#24230;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#40065;&#26834;&#24555;&#36895;&#30340;&#36710;&#36742;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#23569;&#19981;&#21516;&#36895;&#24230;&#12289;&#24418;&#29366;&#12289;&#32467;&#26500;&#21644;&#21333;&#24352;&#22270;&#20687;&#20013;&#22810;&#20010;&#36710;&#36742;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#21253;&#21547;&#36710;&#36742;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#22686;&#24378;&#30340;&#22320;&#22270;&#26159;&#36890;&#36807;&#25506;&#32034;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#26497;&#20540;&#31283;&#23450;&#21306;&#22495;&#65288;MR-MSER&#65289;&#30340;&#32452;&#21512;&#26469;&#29983;&#25104;&#30340;&#12290; MR-MSER&#30340;&#36755;&#20986;&#36755;&#20837;&#24555;&#36895;CNN&#20197;&#29983;&#25104;&#32622;&#20449;&#24230;&#22270;&#65292;&#20174;&#32780;&#24471;&#20986;&#20505;&#36873;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#27169;&#22411;&#30340;&#36710;&#36742;&#26816;&#27979;&#19981;&#21516;&#65292;&#25105;&#20204;&#25506;&#32034;&#31895;&#31961;&#38598;&#21644;&#22522;&#20110;&#27169;&#31946;&#30340;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#20197;&#23454;&#29616;&#40065;&#26834;&#30340;&#36710;&#36742;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle detection in real-time scenarios is challenging because of the time constraints and the presence of multiple types of vehicles with different speeds, shapes, structures, etc. This paper presents a new method relied on generating a confidence map-for robust and faster vehicle detection. To reduce the adverse effect of different speeds, shapes, structures, and the presence of several vehicles in a single image, we introduce the concept of augmentation which highlights the region of interest containing the vehicles. The augmented map is generated by exploring the combination of multiresolution analysis and maximally stable extremal regions (MR-MSER). The output of MR-MSER is supplied to fast CNN to generate a confidence map, which results in candidate regions. Furthermore, unlike existing models that implement complicated models for vehicle detection, we explore the combination of a rough set and fuzzy-based models for robust vehicle detection. To show the effectiveness of the pro
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22495;&#22686;&#37327;3D&#29289;&#20307;&#26816;&#27979;&#65292;GMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#31574;&#30053;&#65292;&#21487;&#22312;&#24494;&#35843;&#26102;&#23450;&#26399;&#20174;&#20197;&#21069;&#30340;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.14460</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#30340;&#22495;&#22686;&#37327;3D&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection. (arXiv:2304.14460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14460
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22495;&#22686;&#37327;3D&#29289;&#20307;&#26816;&#27979;&#65292;GMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#31574;&#30053;&#65292;&#21487;&#22312;&#24494;&#35843;&#26102;&#23450;&#26399;&#20174;&#20197;&#21069;&#30340;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25152;&#26377;&#22825;&#27668;&#26465;&#20214;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20173;&#28982;&#26159;&#23454;&#29616;&#33258;&#20027;&#36710;&#36742;&#24191;&#27867;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#30446;&#21069;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#22312;&#28165;&#26224;&#30340;&#22825;&#27668;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#25512;&#24191;&#21040;&#36870;&#22659;&#22825;&#27668;&#26465;&#20214;&#65292;&#30417;&#30563;&#26041;&#27861;&#22312;&#25152;&#26377;&#22825;&#27668;&#25968;&#25454;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#34920;&#29616;&#24471;&#26368;&#22909;&#65292;&#32780;&#19981;&#26159;&#22312;&#28165;&#26224;&#22825;&#27668;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20174;&#25152;&#26377;&#25968;&#25454;&#24320;&#22987;&#35757;&#32451;&#26368;&#32456;&#20250;&#22240;&#25968;&#25454;&#38598;&#19981;&#26029;&#22686;&#38271;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#22825;&#27668;&#26465;&#20214;&#32780;&#21464;&#24471;&#19981;&#21487;&#34892;&#21644;&#26114;&#36149;&#12290;&#32780;&#22312;&#19981;&#21516;&#22825;&#27668;&#39046;&#22495;&#30340;&#21407;&#22987;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#30340;&#24494;&#35843;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#20808;&#21069;&#23398;&#20064;&#39046;&#22495;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21463;&#22238;&#25918;&#24335;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#22823;&#24178;&#25200;&#24674;&#22797;&#65288;GMIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#22238;&#25918;&#36827;&#34892;&#26799;&#24230;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;GMIR&#20250;&#23450;&#26399;&#20174;&#20197;&#21069;&#30340;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D object detection in all weather conditions remains a key challenge to enable the widespread deployment of autonomous vehicles, as most work to date has been performed on clear weather data. In order to generalize to adverse weather conditions, supervised methods perform best if trained from scratch on all weather data instead of finetuning a model pretrained on clear weather data. Training from scratch on all data will eventually become computationally infeasible and expensive as datasets continue to grow and encompass the full extent of possible weather conditions. On the other hand, naive finetuning on data from a different weather domain can result in catastrophic forgetting of the previously learned domain. Inspired by the success of replay-based continual learning methods, we propose Gradient-based Maximally Interfered Retrieval (GMIR), a gradient based sampling strategy for replay. During finetuning, GMIR periodically retrieves samples from the previous domain dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#20351;&#29992;&#32463;&#20856;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#38450;&#24481;XSS&#21644;CSRF&#30340;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20986;&#20851;&#38190;&#35201;&#28857;&#65292;&#20026;&#25506;&#35752;&#35813;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2304.14451</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;Web&#28431;&#27934;&#21644;Web&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Detection and Mitigation of Web Vulnerabilities and Web Attacks. (arXiv:2304.14451v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#20351;&#29992;&#32463;&#20856;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#38450;&#24481;XSS&#21644;CSRF&#30340;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20986;&#20851;&#38190;&#35201;&#28857;&#65292;&#20026;&#25506;&#35752;&#35813;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Web&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#26816;&#27979;&#21644;&#32531;&#35299;&#36328;&#31449;&#33050;&#26412;&#65288;XSS&#65289;&#21644;&#36328;&#31449;&#35831;&#27714;&#20266;&#36896;&#65288;CSRF&#65289;&#31561;&#20851;&#38190;Web&#28431;&#27934;&#21644;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;Web&#25915;&#20987;&#19981;&#26029;&#28436;&#21464;&#65292;&#36234;&#26469;&#36234;&#38590;&#20197;&#26816;&#27979;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#38450;&#24481;XSS&#21644;CSRF&#65292;&#30001;&#20110;&#21462;&#24471;&#30340;&#31215;&#26497;&#32467;&#26524;&#65292;&#21487;&#20197;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#31616;&#35201;&#20171;&#32461;&#24050;&#32463;&#21457;&#34920;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#20123;&#30740;&#31350;&#24037;&#20316;&#37319;&#29992;&#32463;&#20856;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#35782;&#21035;&#21644;&#39044;&#38450;XSS&#21644;CSRF&#12290;&#25552;&#20379;&#36825;&#20221;&#35843;&#26597;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#25506;&#35752;&#24050;&#32463;&#23454;&#26045;&#30340;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20102;&#35299;&#20854;&#20013;&#30340;&#20851;&#38190;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection and mitigation of critical web vulnerabilities and attacks like cross-site scripting (XSS), and cross-site request forgery (CSRF) have been a great concern in the field of web security. Such web attacks are evolving and becoming more challenging to detect. Several ideas from different perspectives have been put forth that can be used to improve the performance of detecting these web vulnerabilities and preventing the attacks from happening. Machine learning techniques have lately been used by researchers to defend against XSS and CSRF, and given the positive findings, it can be concluded that it is a promising research direction. The objective of this paper is to briefly report on the research works that have been published in this direction of applying classical and advanced machine learning to identify and prevent XSS and CSRF. The purpose of providing this survey is to address different machine learning approaches that have been implemented, understand the key takeaway of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#31354;&#20013;&#20316;&#25112;&#34892;&#20026;&#30340;&#23398;&#20064;&#29615;&#22659;&#65288;LEAD&#65289;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;Gymnasium&#32534;&#31243;&#24211;&#21644;&#25509;&#21475;&#65292;&#21487;&#24212;&#29992;&#29616;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21487;&#20197;&#19982;&#31532;&#19977;&#26041;&#27169;&#25311;&#36719;&#20214;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2304.14423</link><description>&lt;p&gt;
&#38024;&#23545;&#31354;&#22495;&#30340;&#23398;&#20064;&#29615;&#22659;&#65288;LEAD&#65289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Environment for the Air Domain (LEAD). (arXiv:2304.14423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#31354;&#20013;&#20316;&#25112;&#34892;&#20026;&#30340;&#23398;&#20064;&#29615;&#22659;&#65288;LEAD&#65289;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;Gymnasium&#32534;&#31243;&#24211;&#21644;&#25509;&#21475;&#65292;&#21487;&#24212;&#29992;&#29616;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21487;&#20197;&#19982;&#31532;&#19977;&#26041;&#27169;&#25311;&#36719;&#20214;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26007;&#26426;&#39134;&#34892;&#21592;&#30340;&#35757;&#32451;&#20013;&#24456;&#22823;&#19968;&#37096;&#20998;&#26159;&#22522;&#20110;&#27169;&#25311;&#30340;&#65292;&#24182;&#28041;&#21450;&#30001;&#39044;&#23450;&#20041;&#34892;&#20026;&#27169;&#22411;&#25511;&#21046;&#30340;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#21147;&#37327;&#12290;&#36825;&#20123;&#34892;&#20026;&#27169;&#22411;&#36890;&#24120;&#26159;&#36890;&#36807;&#20174;&#32463;&#39564;&#20016;&#23500;&#30340;&#39134;&#34892;&#21592;&#20013;&#33719;&#21462;&#30693;&#35782;&#25163;&#21160;&#21019;&#24314;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#33457;&#36153;&#20102;&#24456;&#22810;&#31934;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#21487;&#39044;&#27979;&#30340;&#24615;&#36136;&#21644;&#32570;&#20047;&#36866;&#24212;&#24615;&#65292;&#34892;&#20026;&#27169;&#22411;&#36890;&#24120;&#26159;&#19981;&#23613;&#22914;&#20154;&#24847;&#30340;&#65292;&#36843;&#20351;&#25945;&#32451;&#33457;&#36153;&#26102;&#38388;&#25163;&#21160;&#30417;&#25511;&#21644;&#25511;&#21046;&#23427;&#20204;&#12290;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#25104;&#20026;&#25163;&#24037;&#21046;&#20316;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#31354;&#20013;&#20316;&#25112;&#34892;&#20026;&#30340;&#23398;&#20064;&#29615;&#22659;&#65288;LEAD&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#20891;&#20107;&#27169;&#25311;&#20013;&#21019;&#24314;&#21644;&#38598;&#25104;&#26234;&#33021;&#31354;&#20013;&#20316;&#25112;&#34892;&#20026;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#25972;&#21512;&#27969;&#34892;&#30340;&#32534;&#31243;&#24211;&#21644;&#25509;&#21475;Gymnasium&#65292;LEAD&#20801;&#35768;&#29992;&#25143;&#24212;&#29992;&#29616;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;LEAD&#21487;&#20197;&#36890;&#36807;&#20998;&#24067;&#24335;&#20223;&#30495;&#21327;&#35758;&#19982;&#31532;&#19977;&#26041;&#27169;&#25311;&#36719;&#20214;&#36827;&#34892;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
A substantial part of fighter pilot training is simulation-based and involves computer-generated forces controlled by predefined behavior models. The behavior models are typically manually created by eliciting knowledge from experienced pilots, which is a time-consuming process. Despite the work put in, the behavior models are often unsatisfactory due to their predictable nature and lack of adaptivity, forcing instructors to spend time manually monitoring and controlling them. Reinforcement and imitation learning pose as alternatives to handcrafted models. This paper presents the Learning Environment for the Air Domain (LEAD), a system for creating and integrating intelligent air combat behavior in military simulations. By incorporating the popular programming library and interface Gymnasium, LEAD allows users to apply readily available machine learning algorithms. Additionally, LEAD can communicate with third-party simulation software through distributed simulation protocols, which al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20307;&#31995;&#32467;&#26500;&#65292;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65288;MINN&#65289;&#20197;&#20801;&#35768;&#22312;&#23398;&#20064;&#31995;&#32479;&#29289;&#29702;&#21160;&#24577;&#26041;&#38754;&#36827;&#34892;&#25972;&#21512;&#65292;&#24212;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#30005;&#21270;&#23398;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#12289;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.14422</link><description>&lt;p&gt;
MINN&#65306;&#23398;&#20064;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#21160;&#24577;&#21644;&#24212;&#29992;&#20110;&#30005;&#27744;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MINN: Learning the dynamics of differential-algebraic equations and application to battery modeling. (arXiv:2304.14422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20307;&#31995;&#32467;&#26500;&#65292;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65288;MINN&#65289;&#20197;&#20801;&#35768;&#22312;&#23398;&#20064;&#31995;&#32479;&#29289;&#29702;&#21160;&#24577;&#26041;&#38754;&#36827;&#34892;&#25972;&#21512;&#65292;&#24212;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#30005;&#21270;&#23398;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#12289;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#22522;&#20110;&#29289;&#29702;&#21644;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#24314;&#27169;&#21487;&#25345;&#32493;&#33021;&#28304;&#31995;&#32479;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#29983;&#25104;&#29992;&#20110;&#26367;&#20195;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#19978;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20197;&#36895;&#24230;&#20026;&#20195;&#20215;&#25442;&#21462;&#31934;&#24230;&#65292;&#20294;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#36825;&#20123;&#29305;&#28857;&#22312;&#20248;&#21270;&#21644;&#25511;&#21046;&#23454;&#38469;&#21160;&#24577;&#31995;&#32479;&#30340;&#24314;&#27169;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#26469;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65288;MINN&#65289;&#65292;&#20197;&#20801;&#35768;&#22312;&#23398;&#20064;&#31995;&#32479;&#29289;&#29702;&#21160;&#24577;&#26041;&#38754;&#36827;&#34892;&#25972;&#21512;&#12290;&#33719;&#24471;&#30340;&#28151;&#21512;&#27169;&#22411;&#35299;&#20915;&#20102;&#25511;&#21046;&#23548;&#21521;&#24314;&#27169;&#20013;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#21516;&#26102;&#33719;&#24471;&#29289;&#29702;&#27934;&#23519;&#21147;&#12289;&#25968;&#23383;&#31934;&#24230;&#21644;&#35745;&#31639;&#21487;&#34892;&#24615;&#30340;&#26368;&#20248;&#31616;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24212;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#30005;&#21270;&#23398;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#12289;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of integrating physics-based and data-driven approaches has become popular for modeling sustainable energy systems. However, the existing literature mainly focuses on the data-driven surrogates generated to replace physics-based models. These models often trade accuracy for speed but lack the generalisability, adaptability, and interpretability inherent in physics-based models, which are often indispensable in the modeling of real-world dynamic systems for optimization and control purposes. In this work, we propose a novel architecture for generating model-integrated neural networks (MINN) to allow integration on the level of learning physics-based dynamics of the system. The obtained hybrid model solves an unsettled research problem in control-oriented modeling, i.e., how to obtain an optimally simplified model that is physically insightful, numerically accurate, and computationally tractable simultaneously. We apply the proposed neural network architecture to model the el
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#27493;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;OS-DistrRL&#65289;&#26694;&#26550;&#65292;&#20165;&#28085;&#30422;&#29615;&#22659;&#19968;&#27493;&#21160;&#24577;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14421</link><description>&lt;p&gt;
&#19968;&#27493;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Step Distributional Reinforcement Learning. (arXiv:2304.14421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#27493;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;OS-DistrRL&#65289;&#26694;&#26550;&#65292;&#20165;&#28085;&#30422;&#29615;&#22659;&#19968;&#27493;&#21160;&#24577;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;Reinforcement Learning&#65292;RL&#65289;&#20801;&#35768;&#19968;&#20010;&#33021;&#20195;&#29702;&#19982;&#29615;&#22659;&#36827;&#34892;&#36830;&#32493;&#20132;&#20114;&#30340;&#31995;&#32479;&#26368;&#22823;&#21270;&#39044;&#26399;&#25910;&#30410;&#12290;&#22312;&#20998;&#24067;&#24335;RL&#65288;DistrRL&#65289;&#33539;&#24335;&#19979;&#65292;&#20195;&#29702;&#19981;&#20165;&#23616;&#38480;&#20110;&#26399;&#26395;&#20540;&#65292;&#32780;&#26159;&#25429;&#25417;&#36328;&#36234;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#30340;&#22238;&#25253;&#27010;&#29575;&#20998;&#24067;&#12290;DistrRL&#31639;&#27861;&#30340;&#38598;&#21512;&#25552;&#39640;&#20102;&#32463;&#39564;&#24615;&#33021;&#65292;&#20294;DistrRL&#30340;&#29702;&#35770;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#23588;&#20854;&#22312;&#25511;&#21046;&#26696;&#20363;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#19968;&#27493;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;OS-DistrRL&#65289;&#26694;&#26550;&#65292;&#20165;&#28085;&#30422;&#29615;&#22659;&#19968;&#27493;&#21160;&#24577;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#12290;&#19982;DistrRL&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#38024;&#23545;&#31574;&#30053;&#35780;&#20272;&#21644;&#25511;&#21046;&#37117;&#20855;&#26377;&#32479;&#19968;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;OS-DistrRL&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20046;&#30830;&#23450;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#29615;&#22659;&#20013;&#27604;&#20998;&#31867;DistrRL&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20197;&#20462;&#25913;&#36755;&#30005;&#32447;&#36335;&#20445;&#25252;&#35774;&#32622;&#20026;&#25932;&#23545;&#25915;&#20987;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#25506;&#35752;&#20102;&#26368;&#22823;&#21270;&#32423;&#32852;&#32593;&#32476;&#36864;&#21270;&#30340;&#20445;&#25252;&#35774;&#32622;&#35268;&#24459;&#65292;&#21457;&#29616;&#23558;&#25152;&#26377;&#30005;&#32593;&#32447;&#36335;&#30340;&#20445;&#25252;&#35774;&#32622;&#26368;&#22823;&#22833;&#37197;&#24182;&#19981;&#20250;&#23548;&#33268;&#26368;&#22810;&#30340;&#32423;&#32852;&#12290;</title><link>http://arxiv.org/abs/2304.14420</link><description>&lt;p&gt;
&#20351;&#29992;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32593;&#32476;&#32423;&#32852;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Network Cascade Vulnerability using Constrained Bayesian Optimization. (arXiv:2304.14420v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20197;&#20462;&#25913;&#36755;&#30005;&#32447;&#36335;&#20445;&#25252;&#35774;&#32622;&#20026;&#25932;&#23545;&#25915;&#20987;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#25506;&#35752;&#20102;&#26368;&#22823;&#21270;&#32423;&#32852;&#32593;&#32476;&#36864;&#21270;&#30340;&#20445;&#25252;&#35774;&#32622;&#35268;&#24459;&#65292;&#21457;&#29616;&#23558;&#25152;&#26377;&#30005;&#32593;&#32447;&#36335;&#30340;&#20445;&#25252;&#35774;&#32622;&#26368;&#22823;&#22833;&#37197;&#24182;&#19981;&#20250;&#23548;&#33268;&#26368;&#22810;&#30340;&#32423;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#30005;&#32593;&#30340;&#33030;&#24369;&#24615;&#24120;&#24120;&#26159;&#36890;&#36807;&#25932;&#25163;&#33021;&#22815;&#23545;&#32593;&#32476;&#36896;&#25104;&#30340;&#25439;&#23475;&#37327;&#26469;&#34913;&#37327;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#25915;&#20987;&#30340;&#32423;&#32852;&#24433;&#21709;&#36890;&#24120;&#34987;&#24573;&#35270;&#65292;&#23613;&#31649;&#32423;&#32852;&#26159;&#22823;&#35268;&#27169;&#20572;&#30005;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#36755;&#30005;&#32447;&#36335;&#20445;&#25252;&#35774;&#32622;&#20462;&#25913;&#20026;&#25932;&#23545;&#25915;&#20987;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#21482;&#35201;&#32593;&#32476;&#24179;&#34913;&#29366;&#24577;&#19981;&#25913;&#21464;&#65292;&#25915;&#20987;&#23601;&#21487;&#20197;&#20445;&#25345;&#19981;&#34987;&#26816;&#27979;&#21040;&#12290;&#36825;&#26500;&#25104;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#40657;&#30418;&#23376;&#20989;&#25968;&#22522;&#30784;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#22823;&#21270;&#32423;&#32852;&#32593;&#32476;&#36864;&#21270;&#30340;&#20445;&#25252;&#35774;&#32622;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24120;&#35782;&#30456;&#21453;&#65292;&#23558;&#25152;&#26377;&#32593;&#32476;&#32447;&#36335;&#30340;&#20445;&#25252;&#35774;&#32622;&#26368;&#22823;&#22833;&#37197;&#24182;&#19981;&#20250;&#23548;&#33268;&#26368;&#22810;&#30340;&#32423;&#32852;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#25214;&#21040;&#33021;&#22815;&#20135;&#29983;&#19982;&#23454;&#20363;&#30456;&#24403;&#20005;&#37325;&#30340;&#32423;&#32852;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measures of power grid vulnerability are often assessed by the amount of damage an adversary can exact on the network. However, the cascading impact of such attacks is often overlooked, even though cascades are one of the primary causes of large-scale blackouts. This paper explores modifications of transmission line protection settings as candidates for adversarial attacks, which can remain undetectable as long as the network equilibrium state remains unaltered. This forms the basis of a black-box function in a Bayesian optimization procedure, where the objective is to find protection settings that maximize network degradation due to cascading. Extensive experiments reveal that, against conventional wisdom, maximally misconfiguring the protection settings of all network lines does not cause the most cascading. More surprisingly, even when the degree of misconfiguration is resource constrained, it is still possible to find settings that produce cascades comparable in severity to instanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#24103;&#20809;&#27969;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#29702;&#35299;&#20855;&#26377;&#22810;&#20110;&#20004;&#24103;&#30340;&#26356;&#38271;&#24207;&#21015;&#20013;&#26102;&#38388;&#22330;&#26223;&#30340;&#21160;&#24577;&#65292;&#25512;&#24191;&#22797;&#26434;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#20809;&#27969;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14418</link><description>&lt;p&gt;
SSTM&#65306;&#29992;&#20110;&#22810;&#24103;&#20809;&#27969;&#20272;&#35745;&#30340;&#26102;&#31354;&#24490;&#29615;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
SSTM: Spatiotemporal Recurrent Transformers for Multi-frame Optical Flow Estimation. (arXiv:2304.14418v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14418
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#24103;&#20809;&#27969;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#29702;&#35299;&#20855;&#26377;&#22810;&#20110;&#20004;&#24103;&#30340;&#26356;&#38271;&#24207;&#21015;&#20013;&#26102;&#38388;&#22330;&#26223;&#30340;&#21160;&#24577;&#65292;&#25512;&#24191;&#22797;&#26434;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#20809;&#27969;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#27969;&#20272;&#35745;&#31639;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#22312;&#34987;&#36974;&#25377;&#21306;&#22495;&#20869;&#21644;&#38752;&#36817;&#36793;&#30028;&#21306;&#22495;&#20869;&#30340;&#19981;&#20934;&#30830;&#30340;&#20809;&#27969;&#20272;&#35745;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#30340;&#20809;&#27969;&#20272;&#35745;&#31639;&#27861;&#26159;&#22522;&#20110;&#20004;&#24103;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20809;&#27969;&#22312;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#36830;&#32493;&#22270;&#20687;&#23545;&#19978;&#34987;&#39034;&#24207;&#20272;&#35745;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#33391;&#22909;&#30340;&#20809;&#27969;&#20272;&#35745;&#65292;&#20294;&#30001;&#20110;&#22330;&#26223;&#20013;&#31227;&#21160;&#20803;&#32032;&#30340;&#21463;&#38480;&#23616;&#37096;&#35777;&#25454;&#26377;&#38480;&#65292;&#23427;&#26080;&#27861;&#25512;&#24191;&#21040;&#34987;&#36974;&#25377;&#21306;&#22495;&#20013;&#30340;&#20809;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#24103;&#20809;&#27969;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22810;&#24103;&#22270;&#20687;&#24207;&#21015;&#20013;&#24182;&#34892;&#22320;&#20272;&#35745;&#20004;&#20010;&#25110;&#26356;&#22810;&#20010;&#36830;&#32493;&#20809;&#27969;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#36890;&#36807;&#29702;&#35299;&#20855;&#26377;&#22810;&#20110;&#20004;&#24103;&#30340;&#26356;&#38271;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#22330;&#26223;&#21160;&#24577;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26356;&#22823;&#30340;&#26102;&#31354;&#22495;&#20013;&#34920;&#24449;&#20687;&#32032;&#32423;&#20381;&#36182;&#24615;&#65292;&#25512;&#24191;&#22797;&#26434;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#20809;&#27969;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inaccurate optical flow estimates in and near occluded regions, and out-of-boundary regions are two of the current significant limitations of optical flow estimation algorithms. Recent state-of-the-art optical flow estimation algorithms are two-frame based methods where optical flow is estimated sequentially for each consecutive image pair in a sequence. While this approach gives good flow estimates, it fails to generalize optical flows in occluded regions mainly due to limited local evidence regarding moving elements in a scene. In this work, we propose a learning-based multi-frame optical flow estimation method that estimates two or more consecutive optical flows in parallel from multi-frame image sequences. Our underlying hypothesis is that by understanding temporal scene dynamics from longer sequences with more than two frames, we can characterize pixel-wise dependencies in a larger spatiotemporal domain, generalize complex motion patterns and thereby improve the accuracy of optica
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#31216;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#65292;&#35813;&#29616;&#35937;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#19981;&#33021;&#34987;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.13850</link><description>&lt;p&gt;
SSL&#27169;&#22411;&#26159;&#21542;&#24863;&#21040;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#65311;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Do SSL Models Have D\'ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning. (arXiv:2304.13850v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13850
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#31216;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#65292;&#35813;&#29616;&#35937;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#19981;&#33021;&#34987;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#23558;&#33258;&#28982;&#22270;&#20687;&#30340;&#19981;&#21516;&#37096;&#20998;&#30456;&#20114;&#20851;&#32852;&#26469;&#20135;&#29983;&#26377;&#29992;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#34987;&#25512;&#21521;&#26497;&#31471;&#26102;&#65292;SSL&#27169;&#22411;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20851;&#32852;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;SSL&#27169;&#22411;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;&#29616;&#35937;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#35757;&#32451;&#27169;&#22411;&#21644;&#19968;&#20010;&#20165;&#21253;&#21547;&#32972;&#26223;&#65288;&#22914;&#27700;&#12289;&#22825;&#31354;&#12289;&#33609;&#22320;&#65289;&#30340;&#35757;&#32451;&#22270;&#20687;&#35009;&#21098;&#21518;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#25110;&#29978;&#33267;&#35270;&#35273;&#37325;&#26500;&#22320;&#25512;&#26029;&#20986;&#21069;&#26223;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#26159;&#19981;&#21516;SSL&#31639;&#27861;&#30340;&#26222;&#36941;&#29616;&#35937;&#65292;&#24182;&#19988;&#20250;&#22240;&#26576;&#20123;&#35774;&#35745;&#36873;&#25321;&#32780;&#24694;&#21270;&#65292;&#32780;&#19988;&#19981;&#33021;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#34920;&#31034;&#36136;&#37327;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#12290;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize specific parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models -- which we refer to as d\'ej\`a vu memorization. Concretely, we show that given the trained model and a crop of a training image containing only the background (e.g., water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that d\'ej\`a vu memorization is common to different SSL algorithms, is exacerbated by certain design choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of d\'ej\`a vu memorizatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#26080;&#26631;&#35760;&#26102;&#38388;&#20998;&#26512;&#65292;&#20854;&#33021;&#22815;&#21033;&#29992;&#21333;&#20010;&#25506;&#27979;&#22120;&#20869;&#37096;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#26377;&#24847;&#20041;&#21644;&#20934;&#30830;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.11930</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#26102;&#38388;&#20998;&#26512;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning. (arXiv:2304.11930v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11930
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#26080;&#26631;&#35760;&#26102;&#38388;&#20998;&#26512;&#65292;&#20854;&#33021;&#22815;&#21033;&#29992;&#21333;&#20010;&#25506;&#27979;&#22120;&#20869;&#37096;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#26377;&#24847;&#20041;&#21644;&#20934;&#30830;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#26102;&#38388;&#26159;&#26680;&#20202;&#22120;&#23398;&#20013;&#37325;&#35201;&#30340;&#35805;&#39064;&#65292;&#20855;&#26377;&#20174;&#39640;&#33021;&#29289;&#29702;&#21040;&#36752;&#23556;&#25104;&#20687;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#39640;&#36895;&#27169;&#25968;&#36716;&#25442;&#22120;&#36234;&#26469;&#36234;&#21457;&#23637;&#21644;&#26131;&#20110;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#26680;&#25506;&#27979;&#22120;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#20248;&#28857;&#20173;&#19981;&#30830;&#23450;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30456;&#20851;&#30340;&#26102;&#38388;&#31639;&#27861;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#21644;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22359;&#21270;&#26680;&#25506;&#27979;&#22120;&#30340;&#26102;&#38388;&#20998;&#26512;&#65292;&#26080;&#38656;&#23545;&#20107;&#20214;&#25968;&#25454;&#36827;&#34892;&#26174;&#24335;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#21033;&#29992;&#21333;&#20010;&#25506;&#27979;&#22120;&#20869;&#37096;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24418;&#25104;&#19968;&#20010;&#26080;&#26631;&#35760;&#25439;&#22833;&#20989;&#25968;&#21644;&#19968;&#20010;&#32463;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#21644;&#20934;&#30830;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#25105;&#20204;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#25152;&#38656;&#26041;&#27861;&#30340;&#26368;&#20248;&#20989;&#25968;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#21644;&#26657;&#20934;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#22312;&#26680;&#33021;&#35889;&#23398;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulse timing is an important topic in nuclear instrumentation, with far-reaching applications from high energy physics to radiation imaging. While high-speed analog-to-digital converters become more and more developed and accessible, their potential uses and merits in nuclear detector signal processing are still uncertain, partially due to associated timing algorithms which are not fully understood and utilized. In this paper, we propose a novel method based on deep learning for timing analysis of modularized nuclear detectors without explicit needs of labelling event data. By taking advantage of the inner time correlation of individual detectors, a label-free loss function with a specially designed regularizer is formed to supervise the training of neural networks towards a meaningful and accurate mapping function. We mathematically demonstrate the existence of the optimal function desired by the method, and give a systematic algorithm for training and calibration of the model. The pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09355</link><description>&lt;p&gt;
&#21387;&#32553;&#19982;&#21542;&#8212;&#8212;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#20449;&#24687;&#35770;:&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#33539;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#20449;&#24687;&#35770;&#22312;&#29702;&#35299;&#21644;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#34987;&#24212;&#29992;&#20110;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20248;&#21270;&#21387;&#32553;&#21644;&#30456;&#20851;&#20449;&#24687;&#20445;&#23384;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#20449;&#24687;&#30446;&#26631;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#34701;&#21512;&#25104;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
&lt;/p&gt;</description></item><item><title>TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.08424</link><description>&lt;p&gt;
&#29992;TiDE&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65306;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08424
&lt;/p&gt;
&lt;p&gt;
TiDE&#26159;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20855;&#22791;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#65292;&#30456;&#36739;&#20110;&#26368;&#20339;&#30340;Transformer&#27169;&#22411;&#65292;&#36895;&#24230;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#21363;&#26102;&#38388;&#24207;&#21015;&#31264;&#23494;&#32534;&#30721;&#22120;(TiDE)&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#26082;&#20139;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#21644;&#36895;&#24230;&#65292;&#21448;&#33021;&#22788;&#29702;&#21327;&#21464;&#37327;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26368;&#31616;&#32447;&#24615;&#31867;&#27604;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;(LDS)&#30340;&#36817;&#20046;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#21305;&#37197;&#25110;&#32988;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#27604;&#26368;&#20339;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;5-10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
&lt;/p&gt;</description></item><item><title>Data-OOB&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;out-of-bag&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.07718</link><description>&lt;p&gt;
Data-OOB:&#20197;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#30340;Out-of-bag&#20272;&#35745;&#20026;&#20934;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. (arXiv:2304.07718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07718
&lt;/p&gt;
&lt;p&gt;
Data-OOB&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;out-of-bag&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#35780;&#20272;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#32479;&#35745;&#27934;&#23519;&#21147;&#65292;&#20197;&#21306;&#20998;&#21738;&#20123;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26159;&#26377;&#30410;&#30340;&#65292;&#21738;&#20123;&#26159;&#26377;&#23475;&#30340;&#12290;&#32437;&#35266;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#35768;&#22810;&#20197;Shapley&#20026;&#22522;&#30784;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#22343;&#26174;&#31034;&#20986;&#20102;&#24456;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#27492;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Data-OOB&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;bagging&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;out-of-bag&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#24369;&#23398;&#20064;&#22120;&#26469;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#35780;&#20272;100&#20010;&#36755;&#20837;&#32500;&#24230;&#19988;&#23384;&#22312;$10^6$&#20010;&#26679;&#26412;&#26102;&#65292;Data-OOB&#20165;&#38656;&#35201;&#22312;&#21333;&#20010;CPU&#22788;&#29702;&#22120;&#19978;&#25191;&#34892;&#19981;&#21040;2.25&#20010;&#23567;&#26102;&#12290;&#27492;&#22806;&#65292;Data-OOB&#22312;&#29702;&#35770;&#19978;&#26377;&#22362;&#23454;&#30340;&#35299;&#37322;&#65292;&#24403;&#20004;&#20010;&#31163;&#24046;&#20540;&#20989;&#25968;&#30456;&#21516;&#26102;&#65292;&#20854;&#35782;&#21035;&#20855;&#26377;&#30456;&#21516;&#37325;&#35201;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are $10^6$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.05351</link><description>&lt;p&gt;
ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#38646;&#26679;&#26412;&#20998;&#26512;&#65306;&#21326;&#23572;&#34903;&#26032;&#25163;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#32929;&#24066;&#36208;&#21183;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#26377;&#24453;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#25512;&#25991;&#21644;&#21382;&#21490;&#32929;&#31080;&#20215;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#31227;&#21160;&#39044;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#26159;&#19968;&#20010;&#8220;&#21326;&#23572;&#34903;&#26032;&#25163;&#8221;&#65292;&#22312;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#26041;&#38754;&#30340;&#25104;&#21151;&#26377;&#38480;&#65292;&#19981;&#20165;&#19981;&#22914;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#19981;&#22914;&#20351;&#29992;&#20215;&#26684;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#36825;&#26679;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#23613;&#31649;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#21644;&#25512;&#25991;&#30340;&#21253;&#21547;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#26356;&#19987;&#19994;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35821;&#38899;&#39537;&#21160;&#21160;&#24577;&#29983;&#25104;&#33080;&#37096;&#29305;&#24449;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20174;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21407;&#22987;&#27169;&#22411;&#20013;&#31227;&#38500;&#27531;&#24046;&#22359;&#21644;&#20943;&#23569;&#36890;&#36947;&#23485;&#24230;&#20197;&#21450;&#24212;&#29992;&#31471;&#21040;&#31471;&#30340;&#21387;&#32553;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#21644;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2304.00471</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#30340;&#35821;&#38899;&#39537;&#21160;&#21160;&#24577;&#29983;&#25104;&#33080;&#37096;&#29305;&#24449;&#30340;&#32479;&#19968;&#21387;&#32553;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation. (arXiv:2304.00471v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35821;&#38899;&#39537;&#21160;&#21160;&#24577;&#29983;&#25104;&#33080;&#37096;&#29305;&#24449;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20174;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21407;&#22987;&#27169;&#22411;&#20013;&#31227;&#38500;&#27531;&#24046;&#22359;&#21644;&#20943;&#23569;&#36890;&#36947;&#23485;&#24230;&#20197;&#21450;&#24212;&#29992;&#31471;&#21040;&#31471;&#30340;&#21387;&#32553;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#21644;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#20154;&#31867;&#24050;&#32463;&#24341;&#36215;&#20102;&#35768;&#22810;&#34892;&#19994;&#30340;&#20851;&#27880;&#65292;&#20363;&#22914;&#23089;&#20048;&#21644;&#30005;&#23376;&#21830;&#21153;&#12290;&#20154;&#20204;&#31215;&#26497;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#30446;&#26631;&#35821;&#38899;&#21644;&#38754;&#37096;&#36523;&#20221;&#21512;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;&#23613;&#31649;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#32467;&#26524;&#26174;&#30528;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36731;&#37327;&#32423;&#35821;&#38899;&#39537;&#21160;&#30340;&#21160;&#24577;&#29983;&#25104;&#33080;&#37096;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#27969;&#34892;&#30340;&#21160;&#24577;&#29983;&#25104;&#33080;&#37096;&#29305;&#24449;&#27169;&#22411;Wav2Lip&#20013;&#31227;&#38500;&#27531;&#24046;&#22359;&#21644;&#20943;&#23569;&#36890;&#36947;&#23485;&#24230;&#26469;&#26500;&#24314;&#19968;&#20010;&#32039;&#20945;&#30340;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#26696;&#65292;&#20197;&#31283;&#23450;&#32780;&#26377;&#25928;&#22320;&#35757;&#32451;&#23567;&#23481;&#37327;&#29983;&#25104;&#22120;&#32780;&#19981;&#38656;&#35201;&#23545;&#25239;&#24615;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#21442;&#25968;&#25968;&#37327;&#21644;MAC&#30340;&#25968;&#37327;&#20943;&#23569;&#20102;28&#20493;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32531;&#35299;&#23558;&#25972;&#20010;&#29983;&#25104;&#22120;&#36716;&#25442;&#20026;&#22270;&#20687;&#20449;&#21495;&#22788;&#29702;&#22120;&#26102;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#21387;&#32553;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;ISP&#24863;&#30693;&#20462;&#21098;&#65292;&#37327;&#21270;&#21644;Huffman&#32534;&#30721;&#12290;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20197;&#26174;&#33879;&#26356;&#24555;&#30340;&#36895;&#24230;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21160;&#24577;&#29983;&#25104;&#33080;&#37096;&#29305;&#24449;&#65292;&#24182;&#21344;&#29992;&#26356;&#23569;&#30340;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual humans have gained considerable attention in numerous industries, e.g., entertainment and e-commerce. As a core technology, synthesizing photorealistic face frames from target speech and facial identity has been actively studied with generative adversarial networks. Despite remarkable results of modern talking-face generation models, they often entail high computational burdens, which limit their efficient deployment. This study aims to develop a lightweight model for speech-driven talking-face synthesis. We build a compact generator by removing the residual blocks and reducing the channel width from Wav2Lip, a popular talking-face generator. We also present a knowledge distillation scheme to stably yet effectively train the small-capacity generator without adversarial learning. We reduce the number of parameters and MACs by 28$\times$ while retaining the performance of the original model. Moreover, to alleviate a severe performance drop when converting the whole generator to I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#35768;&#22810;&#20256;&#32479;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;EUPPBench&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17610</link><description>&lt;p&gt;
&#37319;&#29992;&#28789;&#27963;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#38598;&#21512;&#22825;&#27668;&#39044;&#25253;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ensemble weather forecast post-processing with a flexible probabilistic neural network approach. (arXiv:2303.17610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#35768;&#22810;&#20256;&#32479;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;EUPPBench&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#39044;&#25253;&#21518;&#22788;&#29702;&#26159;&#29983;&#25104;&#20934;&#30830;&#27010;&#29575;&#39044;&#25253;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#26159;&#26681;&#25454;&#27599;&#20010;&#20301;&#32622;&#25110;&#27599;&#20010;&#25552;&#21069;&#26399;&#20272;&#35745;&#21442;&#25968;&#32479;&#35745;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#12290;&#20026;&#20102;&#25918;&#23485;&#35768;&#22810;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#28789;&#27963;&#30340;&#21442;&#25968;&#20998;&#24067;&#20272;&#35745;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#25968;&#23398;&#30830;&#20999;&#30340;&#26041;&#24335;&#27169;&#25311;&#19981;&#21516;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;EUPPBench&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35813;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23545;&#35199;&#27431;&#22320;&#21306;&#23376;&#21306;&#22495;&#30340;&#31449;&#28857;&#36827;&#34892;&#20102;&#28201;&#24230;&#39044;&#25253;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25105;&#20204;&#20043;&#21069;&#30340;&#34920;&#29616;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble forecast post-processing is a necessary step in producing accurate probabilistic forecasts. Conventional post-processing methods operate by estimating the parameters of a parametric distribution, frequently on a per-location or per-lead-time basis. We propose a novel, neural network-based method, which produces forecasts for all locations and lead times, jointly. To relax the distributional assumption of many post-processing methods, our approach incorporates normalizing flows as flexible parametric distribution estimators. This enables us to model varying forecast distributions in a mathematically exact way. We demonstrate the effectiveness of our method in the context of the EUPPBench benchmark, where we conduct temperature forecast post-processing for stations in a sub-region of western Europe. We show that our novel method exhibits state-of-the-art performance on the benchmark, outclassing our previous, well-performing entry. Additionally, by providing a detailed compariso
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#30340;&#24191;&#20041;Hopfield&#27169;&#22411;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#30528;&#19968;&#31181;&#23398;&#20064;&#30456;&#21464;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.16880</link><description>&lt;p&gt;
&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#21450;&#20854;&#23398;&#20064;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
The Hidden-Manifold Hopfield Model and a learning phase transition. (arXiv:2303.16880v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#30340;&#24191;&#20041;Hopfield&#27169;&#22411;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#30528;&#19968;&#31181;&#23398;&#20064;&#30456;&#21464;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#27169;&#22411;&#22312;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#20256;&#32479;&#65292;&#26159;&#23569;&#25968;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#19968;&#12290;&#36890;&#36807;&#23558;Hopfield&#27169;&#22411;&#30340;&#29702;&#35770;&#25299;&#23637;&#21040;&#30456;&#20851;&#25968;&#25454;&#19978;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#25551;&#36848;&#23427;&#20204;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;Hopfield&#27169;&#22411;&#65292;&#31216;&#20026;&#38544;&#34255;&#27969;&#24418;Hopfield&#27169;&#22411;&#65306;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;&#22240;&#23376;&#30340;$D=\alpha_D N$&#20010;&#38543;&#26426;&#21521;&#37327;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#20351;&#29992;&#26469;&#33258;$P=\alpha N$&#20010;&#31034;&#20363;&#30340;Hebb&#35268;&#21017;&#29983;&#25104;&#32806;&#21512;&#65292;&#20854;&#20013;$N$&#26159;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#12290;&#20351;&#29992;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#35813;&#27169;&#22411;&#30340;&#30456;&#22270;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#30456;&#21464;&#65292;&#20854;&#20013;&#22312;&#31034;&#20363;&#20013;&#38544;&#34255;&#30340;&#22240;&#23376;&#25104;&#20026;&#21160;&#24577;&#23398;&#30340;&#21560;&#24341;&#23376;&#65307;&#36825;&#31181;&#30456;&#23384;&#22312;&#20110;&#20851;&#38190;&#30340;$\alpha$&#20540;&#20197;&#19978;&#21644;$\alpha_D$&#20851;&#38190;&#30340;&#20540;&#20197;&#19979;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#34892;&#20026;&#31216;&#20026;&#23398;&#20064;&#30456;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hopfield model has a long-standing tradition in statistical physics, being one of the few neural networks for which a theory is available. Extending the theory of Hopfield models for correlated data could help understand the success of deep neural networks, for instance describing how they extract features from data. Motivated by this, we propose and investigate a generalized Hopfield model that we name Hidden-Manifold Hopfield Model: we generate the couplings from $P=\alpha N$ examples with the Hebb rule using a non-linear transformation of $D=\alpha_D N$ random vectors that we call factors, with $N$ the number of neurons. Using the replica method, we obtain a phase diagram for the model that shows a phase transition where the factors hidden in the examples become attractors of the dynamics; this phase exists above a critical value of $\alpha$ and below a critical value of $\alpha_D$. We call this behaviour learning transition.
&lt;/p&gt;</description></item><item><title>Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.13937</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#31890;&#23376;&#29289;&#29702;&#36807;&#31243;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13937
&lt;/p&gt;
&lt;p&gt;
Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Topograph&#65292;&#23427;&#21033;&#29992;&#31890;&#23376;&#29289;&#29702;&#34928;&#21464;&#30340;&#26412;&#36136;&#21644;&#20449;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#65292;&#37325;&#24314;&#20102;&#21253;&#25324;&#20013;&#20171;&#31890;&#23376;&#22312;&#20869;&#30340;&#24213;&#23618;&#29289;&#29702;&#36807;&#31243;&#12290;Topograph&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#30340;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#23558;&#23427;&#20204;&#19982;&#23427;&#20204;&#21407;&#26469;&#30340;&#27597;&#31890;&#23376;&#20851;&#32852;&#36215;&#26469;&#65292;&#32780;&#19988;&#30452;&#25509;&#39044;&#27979;&#20102;&#30828;&#25955;&#23556;&#36807;&#31243;&#20013;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#12290;&#19982;&#26631;&#20934;&#30340;&#32452;&#21512;&#26041;&#27861;&#25110;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#30340;&#22797;&#26434;&#24230;&#19982;&#37325;&#26500;&#23545;&#35937;&#30340;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#24212;&#29992;Topograph&#20110;&#20840;&#24378;&#23376;&#34928;&#21464;&#27169;&#24335;&#19979;&#30340;&#39030;&#22840;&#20811;&#23545;&#20135;&#29983;&#38382;&#39064;&#65292;&#30456;&#23545;&#26631;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.11239</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Contextual Anomaly Detection using Quantile Regression Forests. (arXiv:2302.11239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36816;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#22320;&#35782;&#21035;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#24179;&#31561;&#23545;&#24453;&#25152;&#26377;&#29305;&#24449;&#26469;&#35782;&#21035;&#20559;&#31163;&#22823;&#22810;&#25968;&#20854;&#20182;&#23545;&#35937;&#30340;&#23545;&#35937;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#23558;&#29305;&#24449;&#21010;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#26088;&#22312;&#26816;&#27979;&#20559;&#31163;&#31867;&#20284;&#23545;&#35937;&#19978;&#19979;&#25991;&#30340;&#20854;&#20182;&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20381;&#36182;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#30001;&#27492;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#19978;&#19979;&#25991;&#24322;&#24120;&#26816;&#27979;&#12290;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#19978;&#19979;&#25991;&#24322;&#24120;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-art&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional anomaly detection methods aim to identify objects that deviate from most other objects by treating all features equally. In contrast, contextual anomaly detection methods aim to detect objects that deviate from other objects within a context of similar objects by dividing the features into contextual features and behavioral features. In this paper, we develop connections between dependency-based traditional anomaly detection methods and contextual anomaly detection methods. Based on resulting insights, we propose a novel approach to inherently interpretable contextual anomaly detection that uses Quantile Regression Forests to model dependencies between features. Extensive experiments on various synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art anomaly detection methods in identifying contextual anomalies in terms of accuracy and interpretability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2302.05326</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#36830;&#25509;&#21644;&#36873;&#25321;&#24615;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#20351;&#24471;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#21035;&#26159;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#12290;&#19982;&#20854;&#20182;&#21487;&#25193;&#23637;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#65292;&#32780;&#26159;&#36890;&#36807;&#26435;&#34913;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24863;&#30693;&#35266;&#23519;&#20013;&#26500;&#24314;&#29366;&#24577;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#31181;&#29992;&#20110;&#29366;&#24577;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290; BPTT&#21644;&#23454;&#26102;&#24490;&#29615;&#23398;&#20064;&#65288;RTRL&#65289;&#26159;&#20004;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24490;&#29615;&#23398;&#20064;&#26041;&#27861;&#12290; BPTT&#22312;&#35745;&#31639;&#26799;&#24230;&#20043;&#21069;&#38656;&#35201;&#23436;&#25972;&#30340;&#35266;&#23519;&#24207;&#21015;&#65292;&#19981;&#36866;&#21512;&#22312;&#32447;&#23454;&#26102;&#26356;&#26032;&#12290; RTRL&#21487;&#20197;&#36827;&#34892;&#22312;&#32447;&#26356;&#26032;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#32593;&#32476;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38480;&#21046;&#65292;&#20351;RTRL&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#20998;&#35299;&#20026;&#29420;&#31435;&#27169;&#22359;&#25110;&#36880;&#27493;&#23398;&#20064;&#32593;&#32476;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;RTRL&#19982;&#21442;&#25968;&#25968;&#37327;&#21576;&#32447;&#24615;&#27604;&#20363;&#20851;&#31995;&#12290;&#19982;&#20808;&#21069;&#30340;&#21487;&#25193;&#23637;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#65288;&#20363;&#22914;UORO&#21644;Truncated-BPTT&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20250;&#21521;&#26799;&#24230;&#20272;&#35745;&#28155;&#21152;&#22122;&#22768;&#25110;&#20559;&#24046;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#26435;&#34913;&#20102;&#32593;&#32476;&#30340;&#21151;&#33021;&#33021;&#21147;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;&#25216;&#26415;&#65288;LOPT&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#21040;OPT&#38382;&#39064;&#19978;&#65292;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#24182;&#19988;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03232</link><description>&lt;p&gt;
&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Linear Optimal Partial Transport Embedding. (arXiv:2302.03232v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;&#25216;&#26415;&#65288;LOPT&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#21040;OPT&#38382;&#39064;&#19978;&#65292;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#24182;&#19988;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#30001;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#32479;&#35745;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#24179;&#34913;&#36136;&#37327;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;OT&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;OT&#65292;&#26368;&#20248;&#20559;&#31227;&#20256;&#36755;&#65288;OPT&#65289;&#21644;Hellinger Kantorovich&#65288;HK&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#65288;LOPT&#65289;&#23884;&#20837;&#25216;&#26415;&#65292;&#23427;&#23558;OT&#21644;HK&#19978;&#30340;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#25193;&#23637;&#21040;OPT&#38382;&#39064;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#23884;&#20837;&#25216;&#26415;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;LOPT&#36317;&#31163;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LOPT&#23884;&#20837;&#25216;&#26415;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has gained popularity due to its various applications in fields such as machine learning, statistics, and signal processing. However, the balanced mass requirement limits its performance in practical problems. To address these limitations, variants of the OT problem, including unbalanced OT, Optimal partial transport (OPT), and Hellinger Kantorovich (HK), have been proposed. In this paper, we propose the Linear optimal partial transport (LOPT) embedding, which extends the (local) linearization technique on OT and HK to the OPT problem. The proposed embedding allows for faster computation of OPT distance between pairs of positive measures. Besides our theoretical contributions, we demonstrate the LOPT embedding technique in point-cloud interpolation and PCA analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.01222</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#30340;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#65288;TPE-VMD-TFT&#65289;&#65292;&#29992;&#20110;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#26159;&#19968;&#31181;&#24191;&#27867;&#20998;&#24067;&#12289;&#21487;&#20877;&#29983;&#21644;&#29615;&#20445;&#30340;&#33021;&#28304;&#65292;&#23545;&#32531;&#35299;&#20840;&#29699;&#21464;&#26262;&#21644;&#33021;&#28304;&#30701;&#32570;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#30830;&#23450;&#24615;&#21644;&#27874;&#21160;&#24615;&#65292;&#22823;&#35268;&#27169;&#39118;&#30005;&#31995;&#32479;&#30340;&#32593;&#26684;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20013;&#26399;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#21487;&#20197;&#20026;&#33021;&#37327;&#35843;&#24230;&#25552;&#20379;&#22522;&#26412;&#20381;&#25454;&#65292;&#22240;&#27492;&#31934;&#30830;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#21644;&#20998;&#35299;&#31639;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#21464;&#20998;&#27169;&#24335;&#20998;&#35299;&#65288;VMD&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#23450;&#20041;&#20102;24&#23567;&#26102;&#21644;48&#23567;&#26102;&#20043;&#21069;&#30340;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#30340;TPE-VMD-TFT&#26041;&#27861;&#12290;&#22312;&#27861;&#22269;&#30005;&#21147;&#20844;&#21496;Engie&#30340;&#39118;&#33021;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMLCompiler&#30340;&#32479;&#19968;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#65288;CML&#65289;&#25512;&#29702;&#12290;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#32479;&#19968;&#30340;&#25277;&#35937;&#65306;&#36816;&#31639;&#31526;&#34920;&#31034;&#21644;&#25193;&#23637;&#35745;&#31639;&#22270;&#65292;CMLCompiler&#26694;&#26550;&#21487;&#20197;&#23558;&#29992;&#20110;CML&#30340;&#24050;&#20248;&#21270;&#35745;&#31639;&#22270;&#36716;&#25442;&#24182;&#36755;&#20986;&#21040;DL&#32534;&#35793;&#22120;&#25110;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.13441</link><description>&lt;p&gt;
CMLCompiler&#65306;&#19968;&#20010;&#38754;&#21521;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#32479;&#19968;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
CMLCompiler: A Unified Compiler for Classical Machine Learning. (arXiv:2301.13441v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMLCompiler&#30340;&#32479;&#19968;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#65288;CML&#65289;&#25512;&#29702;&#12290;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#32479;&#19968;&#30340;&#25277;&#35937;&#65306;&#36816;&#31639;&#31526;&#34920;&#31034;&#21644;&#25193;&#23637;&#35745;&#31639;&#22270;&#65292;CMLCompiler&#26694;&#26550;&#21487;&#20197;&#23558;&#29992;&#20110;CML&#30340;&#24050;&#20248;&#21270;&#35745;&#31639;&#22270;&#36716;&#25442;&#24182;&#36755;&#20986;&#21040;DL&#32534;&#35793;&#22120;&#25110;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#65288;CML&#65289;&#20960;&#20046;&#21344;&#25454;&#20102;&#29983;&#20135;&#24212;&#29992;&#20013;&#36817;&#19968;&#21322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35774;&#22791;&#65292;&#24615;&#33021;&#34920;&#29616;&#36739;&#24046;&#12290;&#22312;&#27809;&#26377;&#32479;&#19968;&#26694;&#26550;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;CML&#30340;&#28151;&#21512;&#37096;&#32626;&#20063;&#20250;&#36935;&#21040;&#20005;&#37325;&#30340;&#24615;&#33021;&#21644;&#21487;&#31227;&#26893;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#32534;&#35793;&#22120;&#26694;&#26550;CMLCompiler&#65292;&#29992;&#20110;CML&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32479;&#19968;&#30340;&#25277;&#35937;&#65306;&#36816;&#31639;&#31526;&#34920;&#31034;&#21644;&#25193;&#23637;&#35745;&#31639;&#22270;&#12290;CMLCompiler&#26694;&#26550;&#22522;&#20110;&#36825;&#20004;&#31181;&#32479;&#19968;&#30340;&#25277;&#35937;&#25191;&#34892;&#36716;&#25442;&#21644;&#22270;&#20248;&#21270;&#65292;&#28982;&#21518;&#23558;&#20248;&#21270;&#21518;&#30340;&#35745;&#31639;&#22270;&#36755;&#20986;&#21040;DL&#32534;&#35793;&#22120;&#25110;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#22312;TVM&#19978;&#23454;&#29616;CMLCompiler&#12290;&#35780;&#20272;&#34920;&#26126;&#65292;CMLCompiler&#20855;&#26377;&#21331;&#36234;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#24615;&#33021;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;scikit-learn&#65292;XGBoost&#31561;&#65289;&#30456;&#27604;&#65292;&#23427;&#22312;CPU&#19978;&#23454;&#29616;&#20102;&#22810;&#36798;4.38&#20493;&#30340;&#21152;&#36895;&#65292;GPU&#19978;&#23454;&#29616;&#20102;3.31&#20493;&#30340;&#21152;&#36895;&#65292;IoT&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;5.09&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical machine learning (CML) occupies nearly half of machine learning pipelines in production applications. Unfortunately, it fails to utilize the state-of-the-practice devices fully and performs poorly. Without a unified framework, the hybrid deployments of deep learning (DL) and CML also suffer from severe performance and portability issues. This paper presents the design of a unified compiler, called CMLCompiler, for CML inference. We propose two unified abstractions: operator representations and extended computational graphs. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized computational graph to DL compilers or frameworks. We implement CMLCompiler on TVM. The evaluation shows CMLCompiler's portability and superior performance. It achieves up to 4.38$\times$ speedup on CPU, 3.31$\times$ speedup on GPU, and 5.09$\times$ speedup on IoT devices, compared to the state-of-the-art solutions -- scikit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#25805;&#20316;&#31526;&#20197;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12562</link><description>&lt;p&gt;
&#38024;&#23545;&#21487;&#25193;&#23637;&#24615;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#31616;&#21270;&#20197;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Simplifying Subgraph Representation Learning for Scalable Link Prediction. (arXiv:2301.12562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#25805;&#20316;&#31526;&#20197;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23558;&#38142;&#25509;&#39044;&#27979;&#36716;&#21270;&#20026;&#22312;&#38142;&#25509;&#21608;&#22260;&#23376;&#22270;&#19978;&#30340;&#22270;&#20998;&#31867;&#26469;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#24182;&#19988;&#30001;&#20110;&#23376;&#22270;&#27700;&#24179;&#25805;&#20316;&#30340;&#20195;&#20215;&#32780;&#19981;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#31867;&#65292;&#31216;&#20026;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;S3GRL&#31616;&#21270;&#20102;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#12290;&#20316;&#20026;&#21487;&#25193;&#23637;&#24615;&#26694;&#26550;&#65292;S3GRL&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#36816;&#31639;&#31526;&#26469;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20010;S3GRL&#23454;&#20363;&#65292;&#24182;&#22312;&#23567;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction on graphs is a fundamental problem. Subgraph representation learning approaches (SGRLs), by transforming link prediction to graph classification on the subgraphs around the links, have achieved state-of-the-art performance in link prediction. However, SGRLs are computationally expensive, and not scalable to large-scale graphs due to expensive subgraph-level operations. To unlock the scalability of SGRLs, we propose a new class of SGRLs, that we call Scalable Simplified SGRL (S3GRL). Aimed at faster training and inference, S3GRL simplifies the message passing and aggregation operations in each link's subgraph. S3GRL, as a scalability framework, accommodates various subgraph sampling strategies and diffusion operators to emulate computationally-expensive SGRLs. We propose multiple instances of S3GRL and empirically study them on small to large-scale graphs. Our extensive experiments demonstrate that the proposed S3GRL models scale up SGRLs without significant performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#21367;&#31215;&#22686;&#24378;&#36827;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27531;&#24046;&#21367;&#31215;&#27169;&#22359;&#30452;&#25509;&#27169;&#25311;&#26631;&#35760;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#65292;&#22312;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#20419;&#36827;&#20449;&#24687;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2212.08330</link><description>&lt;p&gt;
&#21367;&#31215;&#22686;&#24378;&#30340;&#19981;&#26029;&#36827;&#21270;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#21367;&#31215;&#22686;&#24378;&#36827;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27531;&#24046;&#21367;&#31215;&#27169;&#22359;&#30452;&#25509;&#27169;&#25311;&#26631;&#35760;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#65292;&#22312;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#20419;&#36827;&#20449;&#24687;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;Transformers&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22312;&#25152;&#26377;&#31181;&#31867;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#65292;&#27880;&#24847;&#21147;&#22270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#32534;&#30721;&#20102;&#36755;&#20837;&#26631;&#35760;&#20043;&#38388;&#30340;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#26159;&#22522;&#20110;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#25110;&#25512;&#29702;&#30340;&#65292;&#19981;&#21516;&#23618;&#27425;&#30340;&#27880;&#24847;&#21147;&#22270;&#22312;&#23398;&#20064;&#26102;&#26159;&#20998;&#21035;&#23398;&#20064;&#32780;&#19981;&#26159;&#36827;&#34892;&#26174;&#24335;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#36827;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#19968;&#31995;&#21015;&#27531;&#24046;&#21367;&#31215;&#27169;&#22359;&#30452;&#25509;&#27169;&#25311;&#26631;&#35760;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#12290;&#20854;&#20027;&#35201;&#21160;&#26426;&#26377;&#20004;&#26041;&#38754;&#12290;&#19968;&#26041;&#38754;&#65292;&#19981;&#21516;&#23618;&#27425;&#30340;&#27880;&#24847;&#21147;&#22270;&#20998;&#20139;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#22240;&#27492;&#28155;&#21152;&#27531;&#24046;&#36830;&#25509;&#21487;&#20197;&#20419;&#36827;&#26631;&#35760;&#20043;&#38388;&#20851;&#31995;&#22312;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#28982;&#23384;&#22312;&#30528;&#28436;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations , wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;-&#21018;&#24615;&#25240;&#32440;&#28216;&#25103;&#65292;&#35813;&#28216;&#25103;&#21487;&#20197;&#25193;&#23637;&#21018;&#24615;&#25240;&#32440;&#30340;&#28508;&#21147;&#65292;&#20351;&#20854;&#24471;&#21040;&#38024;&#23545;&#24212;&#29992;&#29305;&#23450;&#30340;&#25240;&#30165;&#22270;&#26696;&#65292;&#20174;&#32780;&#21487;&#20197;&#24471;&#21040;&#26085;&#24120;&#29289;&#21697;&#30340;&#26032;&#39062;&#12289;&#21487;&#25240;&#21472;&#21644;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2211.13219</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#21018;&#24615;&#25240;&#32440;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Automating Rigid Origami Design. (arXiv:2211.13219v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;-&#21018;&#24615;&#25240;&#32440;&#28216;&#25103;&#65292;&#35813;&#28216;&#25103;&#21487;&#20197;&#25193;&#23637;&#21018;&#24615;&#25240;&#32440;&#30340;&#28508;&#21147;&#65292;&#20351;&#20854;&#24471;&#21040;&#38024;&#23545;&#24212;&#29992;&#29305;&#23450;&#30340;&#25240;&#30165;&#22270;&#26696;&#65292;&#20174;&#32780;&#21487;&#20197;&#24471;&#21040;&#26085;&#24120;&#29289;&#21697;&#30340;&#26032;&#39062;&#12289;&#21487;&#25240;&#21472;&#21644;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21018;&#24615;&#25240;&#32440;&#22312;&#24456;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#28508;&#21147;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#21018;&#24615;&#25240;&#32440;&#35126;&#30385;&#22270;&#26696;&#35774;&#35745;&#20027;&#35201;&#20381;&#36182;&#20110;&#24050;&#30693;&#30340;&#25340;&#36148;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#21487;&#20197;&#21019;&#24314;&#30340;&#22270;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#19977;&#21333;&#20803;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#21018;&#24615;&#25240;&#32440;&#35774;&#35745;&#25104;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#21018;&#24615;&#25240;&#32440;&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20801;&#35768;&#31616;&#21333;&#23450;&#20041;&#22810;&#26679;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25193;&#23637;&#21018;&#24615;&#25240;&#32440;&#30340;&#28508;&#21147;&#65292;&#24471;&#21040;&#38024;&#23545;&#24212;&#29992;&#29305;&#23450;&#30340;&#25240;&#30165;&#22270;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#31181;&#25628;&#32034;&#26041;&#27861;&#22312;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#20844;&#24335;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#19981;&#20165;&#33021;&#22815;&#26500;&#36896;&#20986;&#36817;&#20284;&#32473;&#23450;&#30446;&#26631;&#24418;&#29366;&#30340;&#21508;&#31181;&#22270;&#26696;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25351;&#23450;&#22522;&#20110;&#20989;&#25968;&#30340;&#25277;&#35937;&#22870;&#21169;&#65292;&#20174;&#32780;&#24471;&#21040;&#26085;&#24120;&#29289;&#21697;&#30340;&#26032;&#39062;&#12289;&#21487;&#25240;&#21472;&#21644;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rigid origami has shown potential in large diversity of practical applications. However, current rigid origami crease pattern design mostly relies on known tessellations. This strongly limits the diversity and novelty of patterns that can be created. In this work, we build upon the recently developed principle of three units method to formulate rigid origami design as a discrete optimization problem, the rigid origami game. Our implementation allows for a simple definition of diverse objectives and thereby expands the potential of rigid origami further to optimized, application-specific crease patterns. We showcase the flexibility of our formulation through use of a diverse set of search methods in several illustrative case studies. We are not only able to construct various patterns that approximate given target shapes, but to also specify abstract, function-based rewards which result in novel, foldable and functional designs for everyday objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20026;&#22522;&#30784;&#30340;&#36890;&#29992;&#31283;&#23450;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20505;&#36873;&#23398;&#29983;&#27169;&#22411;&#20013;&#25628;&#32034;&#19982;&#32769;&#24072;&#27169;&#22411;&#30456;&#19968;&#33268;&#30340;&#21512;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#20010;&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#26469;&#36873;&#25321;&#19968;&#32452;&#36275;&#22815;&#22823;&#30340;&#20266;&#25968;&#25454;&#38598;&#65292;&#20197;&#36873;&#20986;&#19968;&#33268;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.12631</link><description>&lt;p&gt;
&#21487;&#22797;&#29616;&#27169;&#22411;&#33976;&#39311;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generic Approach for Reproducible Model Distillation. (arXiv:2211.12631v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20026;&#22522;&#30784;&#30340;&#36890;&#29992;&#31283;&#23450;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20505;&#36873;&#23398;&#29983;&#27169;&#22411;&#20013;&#25628;&#32034;&#19982;&#32769;&#24072;&#27169;&#22411;&#30456;&#19968;&#33268;&#30340;&#21512;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#20010;&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#26469;&#36873;&#25321;&#19968;&#32452;&#36275;&#22815;&#22823;&#30340;&#20266;&#25968;&#25454;&#38598;&#65292;&#20197;&#36873;&#20986;&#19968;&#33268;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#33976;&#39311;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#29983;&#25104;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#21435;&#27169;&#20223;&#40657;&#30418;&#8220;&#32769;&#24072;&#8221;&#27169;&#22411;&#20135;&#29983;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#23398;&#29983;&#27169;&#22411;&#23545;&#20110;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#30340;&#21464;&#24322;&#24615;&#25935;&#24863;&#26102;&#65292;&#20851;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#35299;&#37322;&#23601;&#19981;&#21487;&#38752;&#20102;&#12290;&#29616;&#26377;&#30340;&#31283;&#23450;&#27169;&#22411;&#33976;&#39311;&#31574;&#30053;&#36890;&#36807;&#26816;&#26597;&#26159;&#21542;&#29983;&#25104;&#20102;&#36275;&#22815;&#22823;&#30340;&#20266;&#25968;&#25454;&#26469;&#31283;&#23450;&#27169;&#22411;&#33976;&#39311;&#65292;&#20294;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#30340;&#36890;&#29992;&#31283;&#23450;&#27169;&#22411;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20505;&#36873;&#30340;&#23398;&#29983;&#27169;&#22411;&#38598;&#21512;&#24320;&#22987;&#65292;&#25628;&#32034;&#19982;&#32769;&#24072;&#27169;&#22411;&#30456;&#19968;&#33268;&#30340;&#21512;&#29702;&#20505;&#36873;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#65292;&#36873;&#25321;&#19968;&#32452;&#21487;&#20197;&#20351;&#24471;&#19968;&#33268;&#30340;&#23398;&#29983;&#27169;&#22411;&#34987;&#36873;&#20013;&#30340;&#20266;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model distillation has been a popular method for producing interpretable machine learning. It uses an interpretable "student" model to mimic the predictions made by the black box "teacher" model. However, when the student model is sensitive to the variability of the data sets used for training even when keeping the teacher fixed, the corresponded interpretation is not reliable. Existing strategies stabilize model distillation by checking whether a large enough corpus of pseudo-data is generated to reliably reproduce student models, but methods to do so have so far been developed for a specific student model. In this paper, we develop a generic approach for stable model distillation based on central limit theorem for the average loss. We start with a collection of candidate student models and search for candidates that reasonably agree with the teacher. Then we construct a multiple testing framework to select a corpus size such that the consistent student model would be selected under d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#26469;&#22788;&#29702;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.08604</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#19981;&#24179;&#34913;PU&#26631;&#31614;&#30340;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels. (arXiv:2211.08604v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#26469;&#22788;&#29702;P2E MMORPGs&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#22810;&#20154;&#22312;&#32447;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#20013;&#65292;&#28216;&#25103;&#28857;&#21345;&#33021;&#22815;&#30452;&#25509;&#36716;&#25442;&#20026;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#25110;Klaytn&#31561;&#21152;&#23494;&#36135;&#24065;&#65292;&#22240;&#27492;play-to-earn&#65288;P2E&#65289;&#31995;&#32479;&#30340;&#20986;&#29616;&#20351;&#24471;&#28216;&#25103;&#29289;&#21697;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20215;&#20540;&#20132;&#25442;&#27604;&#20197;&#24448;&#26356;&#21152;&#39057;&#32321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;PU GNN&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;PU&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#29609;&#23478;&#30340;&#28216;&#25103;&#34892;&#20026;&#12289;P2E&#20195;&#24065;&#20132;&#26131;&#27169;&#24335;&#65292;&#21516;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;GraphSMOTE&#31639;&#27861;&#22788;&#29702;&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#38469;&#30340;P2E MMORPGs&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of play-to-earn (P2E) systems in massively multiplayer online role-playing games (MMORPGs) has made in-game goods interchangeable with real-world values more than ever before. The goods in the P2E MMORPGs can be directly exchanged with cryptocurrencies such as Bitcoin, Ethereum, or Klaytn via blockchain networks. Unlike traditional in-game goods, once they had been written to the blockchains, P2E goods cannot be restored by the game operation teams even with chargeback fraud such as payment fraud, cancellation, or refund. To tackle the problem, we propose a novel chargeback fraud prediction method, PU GNN, which leverages graph attention networks with PU loss to capture both the players' in-game behavior with P2E token transaction patterns. With the adoption of modified GraphSMOTE, the proposed model handles the imbalanced distribution of labels in chargeback fraud datasets. The conducted experiments on three real-world P2E MMORPG datasets demonstrate that PU GNN achi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;GNN&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#24635;&#21464;&#24046;&#30340;&#26494;&#24347;&#26469;&#35745;&#31639;&#38598;&#32676;&#20998;&#37197;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#37051;&#39030;&#28857;&#20043;&#38388;&#29305;&#24449;&#30340;$\ell_1$&#36317;&#31163;&#26469;&#23454;&#29616;&#38160;&#21033;&#36716;&#25442;&#65292;&#23454;&#29616;&#39030;&#28857;&#32858;&#31867;&#21644;&#22270;&#27744;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.06218</link><description>&lt;p&gt;
&#24635;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Total Variation Graph Neural Networks. (arXiv:2211.06218v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;GNN&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#24635;&#21464;&#24046;&#30340;&#26494;&#24347;&#26469;&#35745;&#31639;&#38598;&#32676;&#20998;&#37197;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#37051;&#39030;&#28857;&#20043;&#38388;&#29305;&#24449;&#30340;$\ell_1$&#36317;&#31163;&#26469;&#23454;&#29616;&#38160;&#21033;&#36716;&#25442;&#65292;&#23454;&#29616;&#39030;&#28857;&#32858;&#31867;&#21644;&#22270;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;(GNN)&#29992;&#20110;&#39030;&#28857;&#32858;&#31867;&#26159;&#36890;&#36807;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26368;&#23567;&#21106;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20010;&#30446;&#26631;&#26159;&#36890;&#36807;&#35889;&#32858;&#31867;(SC)&#26494;&#24347;&#26469;&#36817;&#20284;&#30340;&#12290;&#28982;&#32780;&#65292;SC&#26494;&#24347;&#26159;&#23485;&#26494;&#30340;&#65292;&#34429;&#28982;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#38381;&#24335;&#35299;&#65292;&#20294;&#23427;&#20063;&#20135;&#29983;&#20102;&#36807;&#20110;&#24179;&#28369;&#30340;&#38598;&#32676;&#20998;&#37197;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#23558;&#39030;&#28857;&#20998;&#38548;&#24320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;GNN&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#22522;&#20110;&#22270;&#24635;&#21464;&#24046;(GTV)&#30340;&#26368;&#23567;&#21106;&#30340;&#19968;&#20010;&#26356;&#32039;&#23494;&#30340;&#26494;&#24347;&#26469;&#35745;&#31639;&#38598;&#32676;&#20998;&#37197;&#12290;&#36825;&#20123;&#38598;&#32676;&#20998;&#37197;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25191;&#34892;&#39030;&#28857;&#32858;&#31867;&#25110;&#22312;&#22270;&#20998;&#31867;&#26694;&#26550;&#20013;&#23454;&#29616;&#22270;&#27744;&#21270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;i)&#19968;&#23618;&#28040;&#24687;&#20256;&#36882;&#65292;&#35813;&#23618;&#26368;&#23567;&#21270;&#30456;&#37051;&#39030;&#28857;&#20043;&#38388;&#29305;&#24449;&#30340;$\ell_1$&#36317;&#31163;&#65292;&#36825;&#23545;&#23454;&#29616;&#38598;&#32676;&#20043;&#38388;&#30340;&#38160;&#21033;&#36716;&#25442;&#33267;&#20851;&#37325;&#35201;&#65307;ii)&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#20102;&#38598;&#32676;&#20998;&#37197;&#30340;GTV&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#24179;&#34913;&#30340;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed Graph Neural Networks (GNNs) for vertex clustering are trained with an unsupervised minimum cut objective, approximated by a Spectral Clustering (SC) relaxation. However, the SC relaxation is loose and, while it offers a closed-form solution, it also yields overly smooth cluster assignments that poorly separate the vertices. In this paper, we propose a GNN model that computes cluster assignments by optimizing a tighter relaxation of the minimum cut based on graph total variation (GTV). The cluster assignments can be used directly to perform vertex clustering or to implement graph pooling in a graph classification framework. Our model consists of two core components: i) a message-passing layer that minimizes the $\ell_1$ distance in the features of adjacent vertices, which is key to achieving sharp transitions between clusters; ii) an unsupervised loss function that minimizes the GTV of the cluster assignments while ensuring balanced partitions. Experimental results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2211.01317</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#27169;&#24577;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#20302;&#36164;&#28304;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming. (arXiv:2211.01317v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#25552;&#20986;&#20102;&#36755;&#20837;&#20381;&#36182;NMR&#33539;&#24335;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20219;&#21153;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243; (NMR) &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20302;&#36164;&#28304;&#38899;&#20048;&#20998;&#31867;&#12290;NMR&#26088;&#22312;&#36890;&#36807;&#20462;&#25913;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#28304;&#22495;&#37325;&#26032;&#35843;&#25972;&#29992;&#20110;&#30446;&#26631;&#22495;&#12290;&#38500;&#20102;&#24050;&#30693;&#30340;&#19982;&#36755;&#20837;&#26080;&#20851;&#30340;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#37325;&#26032;&#32534;&#31243;&#33539;&#24335;&#65306;&#36755;&#20837;&#20381;&#36182;NMR&#65292;&#20197;&#22686;&#21152;&#23545;&#22797;&#26434;&#36755;&#20837;&#25968;&#25454;&#65288;&#22914;&#38899;&#39057;&#65289;&#30340;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#25104;&#21151;&#22320;&#36827;&#34892;&#38899;&#20048;&#39118;&#26684;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#20004;&#31181;&#36755;&#20837;&#30456;&#20851;&#30340;NMR&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained models for low-resource (music) classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained model. In addition to the known, input-independent, reprogramming method, we propose an advanced reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#28155;&#21152;&#37327;&#23376;&#38543;&#26426;&#26059;&#36716;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#37327;&#23376;&#20998;&#31867;&#22120;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#25903;&#25345;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#40065;&#26834;&#24615;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2211.00887</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#22122;&#22768;&#23454;&#29616;&#37327;&#23376;&#20998;&#31867;&#22120;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness of Quantum Classifiers against Adversarial Examples through Quantum Noise. (arXiv:2211.00887v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#28155;&#21152;&#37327;&#23376;&#38543;&#26426;&#26059;&#36716;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#37327;&#23376;&#20998;&#31867;&#22120;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#25903;&#25345;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#40065;&#26834;&#24615;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#29616;&#37327;&#23376;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#23548;&#33268;&#34987;&#19981;&#33021;&#23519;&#35273;&#30340;&#22122;&#22768;&#27450;&#39575;&#65292;&#20174;&#32780;&#36896;&#25104;&#20998;&#31867;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#30740;&#31350;&#65292;&#35777;&#26126;&#28155;&#21152;&#37327;&#23376;&#38543;&#26426;&#26059;&#36716;&#22122;&#22768;&#21487;&#20197;&#25552;&#39640;&#37327;&#23376;&#20998;&#31867;&#22120;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24046;&#20998;&#31169;&#23494;&#24615;&#30340;&#23450;&#20041;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#23384;&#22312;&#21152;&#24615;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#24046;&#20998;&#31169;&#23494;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#40065;&#26834;&#24615;&#36793;&#30028;&#65292;&#29992;IBM 7&#37327;&#23376;&#20301;&#35774;&#22791;&#20135;&#29983;&#30340;&#22122;&#22768;&#36827;&#34892;&#20102;&#23454;&#39564;&#27169;&#25311;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, quantum classifiers have been found to be vulnerable to adversarial attacks, in which quantum classifiers are deceived by imperceptible noises, leading to misclassification. In this paper, we propose the first theoretical study demonstrating that adding quantum random rotation noise can improve robustness in quantum classifiers against adversarial attacks. We link the definition of differential privacy and show that the quantum classifier trained with the natural presence of additive noise is differentially private. Finally, we derive a certified robustness bound to enable quantum classifiers to defend against adversarial examples, supported by experimental results simulated with noises from IBM's 7-qubits device.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17312</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#24207;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#31361;&#21464;&#20998;&#24067;&#36716;&#25442;&#65292;&#21363;&#25152;&#35859;&#30340;&#21464;&#28857;&#26816;&#27979;&#65292;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#65292;&#24403;&#21457;&#29983;&#21464;&#28857;&#26102;&#65292;&#35813;&#37327;&#20250;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21464;&#28857;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Adversarial Discriminator Ensemble Network&#65288;ADE-Net&#65289;&#26469;&#22686;&#24378;&#39640;&#20809;&#35889;&#22270;&#20687;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#25915;&#20987;&#19987;&#23478;&#38598;&#21512;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2210.16346</link><description>&lt;p&gt;
&#22312;&#22810;&#27425;&#25915;&#20987;&#19979;&#25552;&#39640;&#39640;&#20809;&#35889;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Hyperspectral Adversarial Robustness Under Multiple Attacks. (arXiv:2210.16346v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Adversarial Discriminator Ensemble Network&#65288;ADE-Net&#65289;&#26469;&#22686;&#24378;&#39640;&#20809;&#35889;&#22270;&#20687;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#25915;&#20987;&#19987;&#23478;&#38598;&#21512;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39640;&#20809;&#35889;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#21333;&#20010;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#20294;&#26159;&#22312;&#36935;&#21040;&#22810;&#31181;&#25915;&#20987;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#27604;&#21333;&#29420;&#35757;&#32451;&#27599;&#20010;&#32593;&#32476;&#37117;&#35201;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Adversarial Discriminator Ensemble Network&#65288;ADE-Net&#65289;&#65292;&#35813;&#32593;&#32476;&#19987;&#27880;&#20110;&#25915;&#20987;&#31867;&#22411;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#19979;&#20445;&#30041;&#26368;&#20339;&#30340;&#27599;&#20010;&#25968;&#25454;&#31867;&#22411;&#26435;&#37325;&#65292;&#21516;&#26102;&#22686;&#24378;&#25972;&#20010;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#37492;&#21035;&#22120;&#32593;&#32476;&#23558;&#25968;&#25454;&#25353;&#25915;&#20987;&#31867;&#22411;&#36827;&#34892;&#20998;&#31163;&#65292;&#36827;&#32780;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#31867;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#25915;&#20987;&#19987;&#23478;&#38598;&#21512;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation models classifying hyperspectral images (HSI) are vulnerable to adversarial examples. Traditional approaches to adversarial robustness focus on training or retraining a single network on attacked data, however, in the presence of multiple attacks these approaches decrease in performance compared to networks trained individually on each attack. To combat this issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net) which focuses on attack type detection and adversarial robustness under a unified model to preserve per data-type weight optimally while robustifiying the overall network. In the proposed method, a discriminator network is used to separate data by attack type into their specific attack-expert ensemble network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;DNN&#26399;&#38388;&#20934;&#30830;&#24615;&#30340;&#31283;&#23450;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#36824;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#29575;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#31283;&#23450;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.08415</link><description>&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition. (arXiv:2210.08415v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#22312;&#35757;&#32451;DNN&#26399;&#38388;&#20934;&#30830;&#24615;&#30340;&#31283;&#23450;&#24615;&#65292;&#26080;&#35770;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#36824;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#29575;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#31283;&#23450;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#35757;&#32451;&#36807;&#31243;&#20013;&#20934;&#30830;&#24615;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;DNNs&#30340;&#35757;&#32451;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#26469;&#36827;&#34892;&#65292;&#20854;&#24615;&#33021;&#25351;&#26631;&#26159;&#20934;&#30830;&#24615;&#65288;&#27491;&#30830;&#20998;&#31867;&#30340;&#23545;&#35937;&#27604;&#20363;&#65289;&#12290;&#34429;&#28982;&#35757;&#32451;&#20250;&#23548;&#33268;&#25439;&#22833;&#20943;&#23569;&#65292;&#20294;&#26159;&#20934;&#30830;&#24615;&#19981;&#19968;&#23450;&#38543;&#30528;&#36807;&#31243;&#22686;&#21152;&#65292;&#26377;&#26102;&#29978;&#33267;&#20250;&#19979;&#38477;&#12290;&#23454;&#29616;&#20934;&#30830;&#24615;&#31283;&#23450;&#24615;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#22914;&#26524;&#21021;&#22987;&#26102;&#20934;&#30830;&#24615;&#24456;&#39640;&#65292;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#27700;&#24179;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#21152;&#20493;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;DNN&#30340;&#35757;&#32451;&#26399;&#38388;&#30340;&#20934;&#30830;&#24615;&#31283;&#23450;&#12290;&#23545;&#20110;&#22312;$R^n$&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#21152;&#20493;&#26465;&#20214;&#20351;&#29992;$R^n$&#20013;&#30340;&#26495;&#22359;&#36827;&#34892;&#21046;&#23450;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#26495;&#22359;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20004;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#24341;&#20837;&#26356;&#31616;&#21333;&#12289;&#26356;&#36890;&#29992;&#30340;&#26465;&#20214;&#8212;&#8212;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#65292;&#20351;&#21152;&#20493;&#26465;&#20214;&#26041;&#27861;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#26465;&#20214;&#35777;&#26126;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26080;&#35770;&#26159;&#20351;&#29992;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#36824;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;DNN&#65292;&#20854;&#20934;&#30830;&#24615;&#37117;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#22312;&#32479;&#19968;&#21152;&#20493;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#24615;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#20445;&#25345;&#31283;&#23450;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#27010;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the stability of accuracy during the training of deep neural networks (DNNs). In this context, the training of a DNN is performed via the minimization of a cross-entropy loss function, and the performance metric is accuracy (the proportion of objects that are classified correctly). While training results in a decrease of loss, the accuracy does not necessarily increase during the process and may sometimes even decrease. The goal of achieving stability of accuracy is to ensure that if accuracy is high at some initial time, it remains high throughout training.  A recent result by Berlyand, Jabin, and Safsten introduces a doubling condition on the training data, which ensures the stability of accuracy during training for DNNs using the absolute value activation function. For training data in $\mathbb{R}^n$, this doubling condition is formulated using slabs in $\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper is twofold. First, to make the doubling cond
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;&#20004;&#31181;&#27169;&#24335;&#30456;&#20114;&#28040;&#38500;&#27495;&#20041;&#26469;&#26377;&#25928;&#22320;&#25351;&#23450;&#21644;&#25945;&#25480;&#26426;&#22120;&#20154;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#25945;&#24072;&#24037;&#20316;&#37327;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04476</link><description>&lt;p&gt;
&#20351;&#29992;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#39640;&#25928;&#23398;&#20064;&#26426;&#22120;&#20154;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks. (arXiv:2210.04476v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;&#20004;&#31181;&#27169;&#24335;&#30456;&#20114;&#28040;&#38500;&#27495;&#20041;&#26469;&#26377;&#25928;&#22320;&#25351;&#23450;&#21644;&#25945;&#25480;&#26426;&#22120;&#20154;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#25945;&#24072;&#24037;&#20316;&#37327;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#25351;&#23450;&#21644;&#25945;&#25480;&#26426;&#22120;&#20154;&#26032;&#20219;&#21153;&#30340;&#20004;&#31181;&#24120;&#35265;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#65292;&#21333;&#29420;&#20351;&#29992;&#28436;&#31034;&#25110;&#35821;&#35328;&#25351;&#20196;&#20250;&#23384;&#22312;&#27495;&#20041;&#65292;&#23548;&#33268;&#20219;&#21153;&#26080;&#27861;&#28165;&#26224;&#22320;&#34987;&#25351;&#23450;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#21644;&#25351;&#20196;&#30340;&#32452;&#21512;&#21487;&#20197;&#26356;&#31616;&#26126;&#22320;&#26377;&#25928;&#22320;&#21521;&#26426;&#22120;&#20154;&#20256;&#36798;&#20219;&#21153;&#12290;&#20026;&#20102;&#28436;&#31034;&#36825;&#20010;&#38382;&#39064;&#35774;&#32622;&#65292;&#25105;&#20204;&#22312;&#20960;&#30334;&#20010;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#25342;&#21462;&#25918;&#32622;&#20219;&#21153;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DeL-TaCo&#65288;&#32852;&#21512;&#28436;&#31034; - &#35821;&#35328;&#20219;&#21153;&#35843;&#33410;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#20154;&#31574;&#30053;&#35843;&#33410;&#20026;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;&#20219;&#21153;&#23884;&#20837;&#65306;&#35270;&#35273;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#36890;&#36807;&#20801;&#35768;&#36825;&#20004;&#31181;&#27169;&#24335;&#22312;&#26032;&#20219;&#21153;&#35268;&#33539;&#26102;&#30456;&#20114;&#28040;&#38500;&#27495;&#20041;&#21644;&#28548;&#28165;&#24444;&#27492;&#65292;DeL-TaCo&#65288;1&#65289;&#22823;&#22823;&#38477;&#20302;&#20102;&#25351;&#23450;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#25945;&#24072;&#24037;&#20316;&#37327;&#65292;&#24182;&#19988;&#65288;2&#65289;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demonstrations and natural language instructions are two common ways to specify and teach robots novel tasks. However, for many complex tasks, a demonstration or language instruction alone contains ambiguities, preventing tasks from being specified clearly. In such cases, a combination of both a demonstration and an instruction more concisely and effectively conveys the task to the robot than either modality alone. To instantiate this problem setting, we train a single multi-task policy on a few hundred challenging robotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task Conditioning), a method for conditioning a robotic policy on task embeddings comprised of two components: a visual demonstration and a language instruction. By allowing these two modalities to mutually disambiguate and clarify each other during novel task specification, DeL-TaCo (1) substantially decreases the teacher effort needed to specify a new task and (2) achieves better generalization performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28082;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#23545;&#20302;&#36136;&#37327;&#26197;&#30340;SZ&#36890;&#37327;&#36136;&#37327;&#65288;$Y$-$M$&#65289;&#20851;&#31995;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$&#21487;&#20197;&#20351;&#35813;&#20851;&#31995;&#26174;&#33879;&#33258;&#30456;&#20284;&#65292;&#36825;&#23545;&#20110;&#20302;&#36136;&#37327;&#26143;&#22242;&#21644;&#26143;&#31995;&#32676;&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#27874;&#38271;&#36136;&#37327;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2209.02075</link><description>&lt;p&gt;
&#22312;&#20302;&#36136;&#37327;&#26197;&#30340;&#24773;&#20917;&#19979;&#65292;&#31526;&#21495;&#22238;&#24402;&#21644;&#24378;&#32422;&#26463;&#23545;SZ&#36890;&#37327;&#36136;&#37327;&#65288;$Y$-$M$&#65289;&#20851;&#31995;&#30340;&#25913;&#36827;&#65306;&#23545;&#37325;&#23376;&#21453;&#39304;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The SZ flux-mass ($Y$-$M$) relation at low halo masses: improvements with symbolic regression and strong constraints on baryonic feedback. (arXiv:2209.02075v2 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28082;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#23545;&#20302;&#36136;&#37327;&#26197;&#30340;SZ&#36890;&#37327;&#36136;&#37327;&#65288;$Y$-$M$&#65289;&#20851;&#31995;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$&#21487;&#20197;&#20351;&#35813;&#20851;&#31995;&#26174;&#33879;&#33258;&#30456;&#20284;&#65292;&#36825;&#23545;&#20110;&#20302;&#36136;&#37327;&#26143;&#22242;&#21644;&#26143;&#31995;&#32676;&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#27874;&#38271;&#36136;&#37327;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#27963;&#21160;&#26143;&#31995;&#26680;&#21644;&#36229;&#26032;&#26143;&#30340;&#21453;&#39304;&#20250;&#24433;&#21709;CMB&#35843;&#26597;&#20013;&#26197;&#30340;&#38598;&#25104;SZ&#36890;&#37327;&#65288;$Y_\mathrm{SZ}$&#65289;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#24182;&#23548;&#33268;&#20854;&#19982;&#26197;&#36136;&#37327;&#65288;$Y_\mathrm{SZ}-M$&#65289;&#30340;&#20851;&#31995;&#20559;&#31163;&#32500;&#37324;&#23450;&#29702;&#30340;&#33258;&#30456;&#20284;&#24130;&#24459;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#24191;&#27867;&#21453;&#39304;&#26041;&#26696;&#21464;&#21270;&#30340;&#28082;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#22871;&#25151;CAMELS&#23545;&#36825;&#31181;&#20559;&#31163;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65288;&#38543;&#26426;&#26862;&#26519;&#21644;&#31526;&#21495;&#22238;&#24402;&#65289;&#30340;&#32452;&#21512;&#26469;&#25628;&#32034;$Y-M$&#20851;&#31995;&#30340;&#31867;&#27604;&#29289;&#65292;&#36825;&#20123;&#31867;&#27604;&#29289;&#23545;&#20302;&#36136;&#37327;&#65288;$M \lesssim 10 ^ {14} \&#65292;h ^ {-1} \&#65292;M_\odot$&#65289;&#30340;&#21453;&#39304;&#36807;&#31243;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20851;&#31995;&#20013;&#31616;&#21333;&#22320;&#23558;$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$&#21487;&#20351;&#20854;&#26174;&#33879;&#33258;&#30456;&#20284;&#65292;&#36825;&#21487;&#20197;&#29992;&#20316;&#20302;&#36136;&#37327;&#26143;&#22242;&#21644;&#26143;&#31995;&#32676;&#30340;&#31283;&#20581;&#22810;&#27874;&#38271;&#36136;&#37327;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#36824;&#21487;&#20197;&#26222;&#36941;&#29992;&#20110;&#25913;&#36827;&#20854;&#20182;&#22825;&#20307;&#29289;&#29702;&#30340;&#26377;&#25928;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feedback from active galactic nuclei (AGN) and supernovae can affect measurements of integrated SZ flux of halos ($Y_\mathrm{SZ}$) from CMB surveys, and cause its relation with the halo mass ($Y_\mathrm{SZ}-M$) to deviate from the self-similar power-law prediction of the virial theorem. We perform a comprehensive study of such deviations using CAMELS, a suite of hydrodynamic simulations with extensive variations in feedback prescriptions. We use a combination of two machine learning tools (random forest and symbolic regression) to search for analogues of the $Y-M$ relation which are more robust to feedback processes for low masses ($M\lesssim 10^{14}\, h^{-1} \, M_\odot$); we find that simply replacing $Y\rightarrow Y(1+M_*/M_\mathrm{gas})$ in the relation makes it remarkably self-similar. This could serve as a robust multiwavelength mass proxy for low-mass clusters and galaxy groups. Our methodology can also be generally useful to improve the domain of validity of other astrophysical 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#24314;&#31435;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#24110;&#21161;&#24314;&#31435;&#26356;&#21487;&#35299;&#37322;&#30340;&#34507;&#30333;&#36136;LM&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#21457;&#29616;&#30456;&#20851;&#39046;&#22495;&#29305;&#23450;&#30340;&#35268;&#21017;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#20110;&#35268;&#21017;&#30340;&#29983;&#29289;&#27835;&#30103;&#33647;&#29289;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2207.00982</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#24314;&#31435;&#29983;&#29289;&#21487;&#38752;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Linguistically inspired roadmap for building biologically reliable protein language models. (arXiv:2207.00982v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00982
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#24314;&#31435;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#24110;&#21161;&#24314;&#31435;&#26356;&#21487;&#35299;&#37322;&#30340;&#34507;&#30333;&#36136;LM&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#21457;&#29616;&#30456;&#20851;&#39046;&#22495;&#29305;&#23450;&#30340;&#35268;&#21017;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#20110;&#35268;&#21017;&#30340;&#29983;&#29289;&#27835;&#30103;&#33647;&#29289;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#20197;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22823;&#22810;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#22240;&#27492;&#38590;&#20197;&#35299;&#37322;&#65292;&#30446;&#21069;&#30340;&#34507;&#30333;&#36136;LM&#26041;&#27861;&#24182;&#27809;&#26377;&#23545;&#24207;&#21015;-&#21151;&#33021;&#26144;&#23556;&#30340;&#22522;&#26412;&#29702;&#35299;&#20316;&#20986;&#36129;&#29486;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29983;&#29289;&#27835;&#30103;&#33647;&#29289;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20174;&#35821;&#35328;&#23398;&#20013;&#24471;&#20986;&#30340;&#25351;&#23548;&#21487;&#20197;&#24110;&#21161;&#24314;&#31435;&#26356;&#21487;&#35299;&#37322;&#30340;&#34507;&#30333;&#36136;LM&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#23398;&#20064;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35268;&#21017;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#19981;&#21516;&#65292;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#38656;&#35201;&#27604;&#33258;&#28982;&#35821;&#35328;LM&#25552;&#20379;&#26356;&#22810;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25972;&#21512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#34507;&#30333;&#36136;LM&#27969;&#31243;&#36873;&#25321;&#30340;&#36335;&#32447;&#22270;&#65292;&#28041;&#21450;&#22521;&#35757;&#25968;&#25454;&#12289;&#26631;&#35760;&#21270;&#12289;&#26631;&#35760;&#23884;&#20837;&#12289;&#24207;&#21015;&#23884;&#20837;&#21644;&#27169;&#22411;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural-network-based language models (LMs) are increasingly applied to large-scale protein sequence data to predict protein function. However, being largely black-box models and thus challenging to interpret, current protein LM approaches do not contribute to a fundamental understanding of sequence-function mappings, hindering rule-based biotherapeutic drug development. We argue that guidance drawn from linguistics, a field specialized in analytical rule extraction from natural language data, can aid with building more interpretable protein LMs that are more likely to learn relevant domain-specific rules. Differences between protein sequence data and linguistic sequence data require the integration of more domain-specific knowledge in protein LMs compared to natural language LMs. Here, we provide a linguistics-based roadmap for protein LM pipeline choices with regard to training data, tokenization, token embedding, sequence embedding, and model interpretation. Incorporating lingui
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#24635;&#32467;&#20102;&#38024;&#23545;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#26368;&#26032;&#30340;&#25299;&#25169;&#35782;&#21035;&#21644;&#26816;&#27979;&#26041;&#26696;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20811;&#26381;&#20998;&#24067;&#24335;&#30005;&#32593;&#20013;&#27979;&#37327;&#35774;&#22791;&#26377;&#38480;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21069;&#39304;&#30340;&#21151;&#29575;&#27969;&#29289;&#29702;&#26041;&#31243;&#21644;&#32467;&#26500;&#29305;&#24615;&#26469;&#22686;&#24378;&#25299;&#25169;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2206.10837</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#24067;&#24335;&#30005;&#32593;&#25299;&#25169;&#32467;&#26500;&#65306;&#19968;&#20221;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Distribution Grid Topologies: A Tutorial. (arXiv:2206.10837v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#24635;&#32467;&#20102;&#38024;&#23545;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#26368;&#26032;&#30340;&#25299;&#25169;&#35782;&#21035;&#21644;&#26816;&#27979;&#26041;&#26696;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20811;&#26381;&#20998;&#24067;&#24335;&#30005;&#32593;&#20013;&#27979;&#37327;&#35774;&#22791;&#26377;&#38480;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21069;&#39304;&#30340;&#21151;&#29575;&#27969;&#29289;&#29702;&#26041;&#31243;&#21644;&#32467;&#26500;&#29305;&#24615;&#26469;&#22686;&#24378;&#25299;&#25169;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#39304;&#32447;&#25299;&#25169;&#32467;&#26500;&#23545;&#20110;&#25512;&#36827;&#26234;&#33021;&#36164;&#28304;&#22312;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#20013;&#30340;&#36816;&#29992;&#21644;&#24773;&#22659;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25945;&#31243;&#24635;&#32467;&#12289;&#23545;&#27604;&#21644;&#24314;&#31435;&#26377;&#29992;&#30340;&#38142;&#25509;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#25552;&#20986;&#30340;&#25299;&#25169;&#35782;&#21035;&#21644;&#26816;&#27979;&#26041;&#26696;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#37325;&#28857;&#26159;&#24378;&#35843;&#20811;&#26381;&#20998;&#24067;&#24335;&#30005;&#32593;&#20013;&#27979;&#37327;&#35774;&#22791;&#26377;&#38480;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21069;&#39304;&#30340;&#21151;&#29575;&#27969;&#29289;&#29702;&#26041;&#31243;&#21644;&#32467;&#26500;&#29305;&#24615;&#26469;&#22686;&#24378;&#25299;&#25169;&#20272;&#35745;&#12290;&#21487;&#20197;&#36890;&#36807;&#20256;&#32479;&#30340;&#34987;&#21160;&#26041;&#24335;&#25910;&#38598;&#30456;&#37327;&#27979;&#37327;&#21333;&#20803;&#25110;&#26234;&#33021;&#30005;&#34920;&#30340;&#30005;&#32593;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#22312;&#28608;&#27963;&#30005;&#32593;&#36164;&#28304;&#24182;&#27979;&#37327;&#39304;&#32447;&#30005;&#21387;&#21709;&#24212;&#26102;&#20027;&#21160;&#25910;&#38598;&#12290;&#22312;&#19981;&#21516;&#30340;&#35745;&#37327;&#35774;&#22791;&#25918;&#32622;&#24773;&#20917;&#19979;&#65292;&#22238;&#39038;&#20102;&#39304;&#32447;&#35782;&#21035;&#21644;&#26816;&#27979;&#30340;&#20998;&#26512;&#24615;&#36136;&#12290;&#36825;&#26679;&#30340;&#25299;&#25169;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#25110;&#36817;&#20284;&#22320;&#36798;&#21040;&#39044;&#26399;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unveiling feeder topologies from data is of paramount importance to advance situational awareness and proper utilization of smart resources in power distribution grids. This tutorial summarizes, contrasts, and establishes useful links between recent works on topology identification and detection schemes that have been proposed for power distribution grids. The primary focus is to highlight methods that overcome the limited availability of measurement devices in distribution grids, while enhancing topology estimates using conservation laws of power-flow physics and structural properties of feeders. Grid data from phasor measurement units or smart meters can be collected either passively in the traditional way, or actively, upon actuating grid resources and measuring the feeder's voltage response. Analytical claims on feeder identifiability and detectability are reviewed under disparate meter placement scenarios. Such topology learning claims can be attained exactly or approximately so v
&lt;/p&gt;</description></item><item><title>LogGENE&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#65292;&#20174;&#32780;&#20026;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#24615;&#24378;&#30340;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.09333</link><description>&lt;p&gt;
LogGENE: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#21307;&#30103;&#25512;&#29702;&#20219;&#21153;&#30340;&#24179;&#28369;&#26816;&#26597;&#25439;&#22833;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks. (arXiv:2206.09333v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09333
&lt;/p&gt;
&lt;p&gt;
LogGENE&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#65292;&#20174;&#32780;&#20026;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#24615;&#24378;&#30340;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#25366;&#25496;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#20174;&#20013;&#33719;&#24471;&#26657;&#20934;&#30340;&#39044;&#27979;&#20855;&#26377;&#21363;&#26102;&#30456;&#20851;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#34920;&#36798;&#31561;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#19982;&#20856;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25512;&#26029;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#26469;&#39044;&#27979;&#32473;&#23450;&#19968;&#32452;&#22522;&#22240;&#34920;&#36798;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#12290;&#26465;&#20214;&#20998;&#20301;&#25968;&#38500;&#20102;&#26377;&#21161;&#20110;&#25552;&#20379;&#39044;&#27979;&#30340;&#20016;&#23500;&#35299;&#37322;&#22806;&#65292;&#36824;&#33021;&#22815;&#25269;&#25239;&#27979;&#37327;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#20013;&#29305;&#21035;&#37325;&#35201;&#65292;&#36825;&#26159;&#19968;&#20010;&#27491;&#22312;&#24341;&#39046;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#38774;&#21521;&#33647;&#29289;&#35774;&#35745;&#21644;&#20256;&#36882;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#39537;&#21160;&#20272;&#35745;&#36807;&#31243;&#30340;&#26816;&#26597;&#25439;&#22833;&#65292;&#22312;&#20998;&#20301;&#25968;&#22238;&#24402;&#20013;&#24182;&#26080;&#19981;&#21516;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining large datasets and obtaining calibrated predictions from tem is of immediate relevance and utility in reliable deep learning. In our work, we develop methods for Deep neural networks based inferences in such datasets like the Gene Expression. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of housekeeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. Our technique is particularly consequential in High-throughput Genomics, an area which is ushering a new era in personalized health care, and targeted drug design and delivery. However, check loss, used in quantile regression to drive the estimation process is not diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#19987;&#23478;&#25968;&#25454;&#20013;&#23398;&#20064;&#36719;&#32422;&#26463;&#65292;&#24674;&#22797;&#27599;&#20010;&#21608;&#26399;&#20869;&#20195;&#29702;&#24179;&#22343;&#28385;&#36275;&#30340;&#32047;&#31215;&#32422;&#26463;&#12290;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#32422;&#26463;&#20989;&#25968;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.01311</link><description>&lt;p&gt;
&#20174;&#32422;&#26463;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#36719;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Learning Soft Constraints From Constrained Expert Demonstrations. (arXiv:2206.01311v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#19987;&#23478;&#25968;&#25454;&#20013;&#23398;&#20064;&#36719;&#32422;&#26463;&#65292;&#24674;&#22797;&#27599;&#20010;&#21608;&#26399;&#20869;&#20195;&#29702;&#24179;&#22343;&#28385;&#36275;&#30340;&#32047;&#31215;&#32422;&#26463;&#12290;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#32422;&#26463;&#20989;&#25968;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#19987;&#23478;&#25968;&#25454;&#26159;&#30001;&#20248;&#21270;&#26576;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#20195;&#29702;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#21487;&#33021;&#20250;&#20248;&#21270;&#21463;&#26576;&#20123;&#38480;&#21046;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#20013;&#36825;&#20123;&#38480;&#21046;&#24341;&#23548;&#20195;&#29702;&#34892;&#20026;&#30340;&#34920;&#36798;&#21487;&#33021;&#26356;&#20026;&#22256;&#38590;&#12290;&#25105;&#20204;&#32771;&#34385;&#22870;&#21169;&#20989;&#25968;&#24050;&#30693;&#65292;&#20294;&#32422;&#26463;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20196;&#20195;&#29702;&#25968;&#25454;&#28385;&#36275;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#20197;IRL&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35843;&#25972;&#32422;&#26463;&#20989;&#25968;&#65292;&#30452;&#21040;&#20195;&#29702;&#34892;&#20026;&#19982;&#19987;&#23478;&#34892;&#20026;&#21305;&#37197;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#24674;&#22797;&#27599;&#20010;&#21608;&#26399;&#20869;&#20195;&#29702;&#24179;&#22343;&#28385;&#36275;&#30340;&#32047;&#31215;&#36719;&#32422;&#26463;&#65292;&#32780;&#20808;&#21069;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#24674;&#22797;&#30828;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#29615;&#22659;&#12289;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#39550;&#39542;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#22870;&#21169;&#27745;&#26579;&#26694;&#26550;&#23637;&#31034;&#20102;&#29616;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20869;&#22312;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25915;&#20987;&#26041;&#24335;&#65292;&#25104;&#21151;&#27745;&#26579;&#22810;&#31181;&#26368;&#20808;&#36827;DRL&#31639;&#27861;&#30340;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#32473;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2205.14842</link><description>&lt;p&gt;
&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39640;&#25928;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning. (arXiv:2205.14842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#22870;&#21169;&#27745;&#26579;&#26694;&#26550;&#23637;&#31034;&#20102;&#29616;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20869;&#22312;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25915;&#20987;&#26041;&#24335;&#65292;&#25104;&#21151;&#27745;&#26579;&#22810;&#31181;&#26368;&#20808;&#36827;DRL&#31639;&#27861;&#30340;&#26234;&#33021;&#20307;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#32473;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#23545;&#26234;&#33021;&#20307;&#20351;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#21644;&#29615;&#22659;&#21160;&#24577;&#19968;&#26080;&#25152;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#31216;&#20026;&#23545;&#25239;MDP&#25915;&#20987;&#30340;&#36890;&#29992;&#40657;&#30418;&#22870;&#21169;&#27745;&#26579;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#29616;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20869;&#22312;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#23454;&#20363;&#21270;&#20026;&#20004;&#31181;&#26032;&#25915;&#20987;&#65292;&#21482;&#30772;&#22351;&#20102;&#24635;&#35757;&#32451;&#26102;&#38388;&#27493;&#25968;&#30340;&#23569;&#37327;&#22870;&#21169;&#65292;&#24182;&#20351;&#20195;&#29702;&#23398;&#20064;&#20302;&#25928;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#25915;&#20987;&#25928;&#29575;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#33021;&#26377;&#25928;&#22320;&#27745;&#26579;&#20351;&#29992;&#22810;&#31181;&#26368;&#20808;&#36827;DRL&#31639;&#27861;&#65288;&#22914;DQN&#12289;PPO&#12289;SAC&#31561;&#65289;&#26469;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#65292;&#22312;&#22810;&#20010;&#21463;&#27426;&#36814;&#30340;&#32463;&#20856;&#25511;&#21046;&#21644;MuJoCo&#29615;&#22659;&#20013;&#20855;&#26377;&#36739;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reward poisoning attacks on online deep reinforcement learning (DRL), where the attacker is oblivious to the learning algorithm used by the agent and the dynamics of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general, black-box reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct two new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. We provide a theoretical analysis of the efficiency of our attack and perform an extensive empirical evaluation. Our results show that our attacks efficiently poison agents learning in several popular classical control and MuJoCo environments with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;OFormer&#65292;&#23427;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.13671</link><description>&lt;p&gt;
&#29992;Transformer&#36827;&#34892;&#20559;&#24494;&#20998;&#26041;&#31243;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;OFormer&#65292;&#23427;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#35299;&#31639;&#23376;&#23398;&#20064;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#36817;&#20284;&#22522;&#30784;&#35299;&#12290;&#35299;&#31639;&#23376;&#36890;&#24120;&#30001;&#22522;&#20110;&#38382;&#39064;&#29305;&#23450;&#24402;&#32435;&#20559;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#12290;&#20363;&#22914;&#65292;&#21367;&#31215;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#20102;&#20989;&#25968;&#20540;&#34987;&#37319;&#26679;&#30340;&#26412;&#22320;&#32593;&#26684;&#32467;&#26500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#26469;&#38544;&#24335;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#21450;&#20219;&#24847;&#26597;&#35810;&#20301;&#32622;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Operator Transformer (OFormer)&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#33258;&#27880;&#24847;&#12289;&#20132;&#21449;&#27880;&#24847;&#21644;&#19968;&#32452;&#36880;&#28857;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#20043;&#19978;&#65292;&#22240;&#27492;&#22312;&#36755;&#20837;&#20989;&#25968;&#25110;&#26597;&#35810;&#20301;&#32622;&#30340;&#37319;&#26679;&#27169;&#24335;&#19978;&#20570;&#20986;&#20102;&#24456;&#23569;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20808;&#21069;&#22522;&#20110;&#21367;&#31215;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#36873;&#25321;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#35774;&#35745;&#27169;&#24335;&#30340;&#20915;&#31574;&#27169;&#22411;&#65292;&#26144;&#23556;&#21151;&#33021;&#38656;&#27714;&#21644;&#38750;&#21151;&#33021;&#38656;&#27714;&#21040;&#19968;&#32452;&#27169;&#24335;&#19978;&#65292;&#20197;&#21327;&#21161;&#23545;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#26377;&#38480;&#30340;&#35774;&#35745;&#24072;&#21644;&#26550;&#26500;&#24072;&#12290;</title><link>http://arxiv.org/abs/2204.13291</link><description>&lt;p&gt;
&#36873;&#25321;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#27169;&#24335;&#30340;&#20915;&#31574;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Models for Selecting Federated Learning Architecture Patterns. (arXiv:2204.13291v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#36873;&#25321;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#35774;&#35745;&#27169;&#24335;&#30340;&#20915;&#31574;&#27169;&#22411;&#65292;&#26144;&#23556;&#21151;&#33021;&#38656;&#27714;&#21644;&#38750;&#21151;&#33021;&#38656;&#27714;&#21040;&#19968;&#32452;&#27169;&#24335;&#19978;&#65292;&#20197;&#21327;&#21161;&#23545;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#26377;&#38480;&#30340;&#35774;&#35745;&#24072;&#21644;&#26550;&#26500;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#39269;&#39295;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36805;&#36895;&#21457;&#23637;&#12290;&#20316;&#20026;&#19968;&#20010;&#24191;&#27867;&#20998;&#24067;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#21508;&#31181;&#31995;&#32479;&#35774;&#35745;&#24605;&#32771;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35774;&#35745;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#30740;&#31350;&#20154;&#21592;&#20171;&#32461;&#20102;&#22810;&#31181;&#27169;&#24335;&#21644;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#31995;&#32479;&#35774;&#35745;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#20351;&#35774;&#35745;&#24072;&#23545;&#20309;&#26102;&#37319;&#29992;&#21738;&#31181;&#27169;&#24335;&#24863;&#21040;&#22256;&#24785;&#12290;&#26412;&#25991;&#22312;&#23545;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#36873;&#25321;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#35774;&#35745;&#27169;&#24335;&#30340;&#20915;&#31574;&#27169;&#22411;&#65292;&#20197;&#21327;&#21161;&#23545;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#26377;&#38480;&#30340;&#35774;&#35745;&#24072;&#21644;&#26550;&#26500;&#24072;&#12290;&#27599;&#20010;&#20915;&#31574;&#27169;&#22411;&#23558;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21151;&#33021;&#38656;&#27714;&#21644;&#38750;&#21151;&#33021;&#38656;&#27714;&#26144;&#23556;&#21040;&#19968;&#32452;&#27169;&#24335;&#19978;&#12290;&#25105;&#20204;&#36824;&#28548;&#28165;&#20102;&#36825;&#20123;&#27169;&#24335;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#24335;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated machine learning is growing fast in academia and industries as a solution to solve data hungriness and privacy issues in machine learning. Being a widely distributed system, federated machine learning requires various system design thinking. To better design a federated machine learning system, researchers have introduced multiple patterns and tactics that cover various system design aspects. However, the multitude of patterns leaves the designers confused about when and which pattern to adopt. In this paper, we present a set of decision models for the selection of patterns for federated machine learning architecture design based on a systematic literature review on federated machine learning, to assist designers and architects who have limited knowledge of federated machine learning. Each decision model maps functional and non-functional requirements of federated machine learning systems to a set of patterns. We also clarify the drawbacks of the patterns. We evaluated the de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TC-GNN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;GPU&#24352;&#37327;&#26680;&#24515;&#21333;&#20803;&#65288;TCUs&#65289;&#30340;GNN&#21152;&#36895;&#26694;&#26550;&#12290;&#37319;&#29992;&#31232;&#30095;&#22270;&#32763;&#35793;&#25216;&#26415;&#26469;&#21327;&#35843;&#8220;&#31232;&#30095;&#8221;GNN&#35745;&#31639;&#19982;&#39640;&#24615;&#33021;&#30340;&#8220;&#23494;&#38598;&#8221;TCUs&#65292;&#23454;&#29616;&#20102;GNN&#35745;&#31639;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2112.02052</link><description>&lt;p&gt;
TC-GNN&#65306;&#22312;GPU&#19978;&#36830;&#25509;&#31232;&#30095;GNN&#35745;&#31639;&#19982;&#23494;&#38598;Tensor Cores&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs. (arXiv:2112.02052v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TC-GNN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;GPU&#24352;&#37327;&#26680;&#24515;&#21333;&#20803;&#65288;TCUs&#65289;&#30340;GNN&#21152;&#36895;&#26694;&#26550;&#12290;&#37319;&#29992;&#31232;&#30095;&#22270;&#32763;&#35793;&#25216;&#26415;&#26469;&#21327;&#35843;&#8220;&#31232;&#30095;&#8221;GNN&#35745;&#31639;&#19982;&#39640;&#24615;&#33021;&#30340;&#8220;&#23494;&#38598;&#8221;TCUs&#65292;&#23454;&#29616;&#20102;GNN&#35745;&#31639;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20316;&#20026;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#39592;&#24178;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#65289;&#30340;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#30340;&#22522;&#20110;&#22270;&#30340;&#25805;&#20316;&#65292;GNN&#30340;&#24615;&#33021;&#36890;&#24120;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TC-GNN&#65292;&#22522;&#20110;GPU&#24352;&#37327;&#26680;&#24515;&#21333;&#20803;&#65288;TCUs&#65289;&#30340;&#31532;&#19968;&#20010;GNN&#21152;&#36895;&#26694;&#26550;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#8220;&#31232;&#30095;&#8221;GNN&#35745;&#31639;&#19982;&#39640;&#24615;&#33021;&#30340;&#8220;&#23494;&#38598;&#8221;TCUs&#21327;&#35843;&#19968;&#33268;&#65292;&#23454;&#29616;GNN&#35745;&#31639;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#20027;&#27969;GNN&#35745;&#31639;&#26694;&#26550;&#20013;&#30340;&#31232;&#30095;&#25805;&#20316;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#22270;&#32763;&#35793;&#25216;&#26415;&#65292;&#20197;&#20415;TCU&#22788;&#29702;&#31232;&#30095;&#30340;GNN&#24037;&#20316;&#36127;&#36733;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;CUDA&#26680;&#24515;&#21644;TCU&#21327;&#20316;&#35774;&#35745;&#65292;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;TC-GNN&#19982;PyTorch&#26694;&#26550;&#38598;&#25104;&#65292;&#20197;&#23454;&#29616;&#39640;&#21487;&#32534;&#31243;&#24615;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;DGL&#26694;&#26550;&#65292;&#24179;&#22343;&#21152;&#36895;&#20102;1.70&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph neural networks (GNNs), as the backbone of graph-based machine learning, demonstrate great success in various domains (e.g., e-commerce). However, the performance of GNNs is usually unsatisfactory due to the highly sparse and irregular graph-based operations. To this end, we propose TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). The core idea is to reconcile the "Sparse" GNN computation with the high-performance "Dense" TCUs. Specifically, we conduct an in-depth analysis of the sparse operations in mainstream GNN computing frameworks. We introduce a novel sparse graph translation technique to facilitate TCU processing of the sparse GNN workload. We implement an effective CUDA core and TCU collaboration design to fully utilize GPU resources. We integrate TC-GNN with the PyTorch framework for high programmability. Rigorous experiments show an average of 1.70X speedup over the state-of-the-art DGL framework across various models and dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20307;&#32032;&#30340;&#19977;&#32500;&#29289;&#20307;&#20998;&#31867;&#30340;&#24555;&#36895;&#28151;&#21512;&#32423;&#32852;&#32593;&#32476;&#65292;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#20998;&#21035;&#22788;&#29702;&#26131;&#12289;&#20013;&#21644;&#38590;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#27599;&#20010;&#20307;&#32032;&#36171;&#20104;&#26377;&#31526;&#21495;&#30340;&#36317;&#31163;&#20540;&#26469;&#25552;&#39640;&#31934;&#24230;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#24179;&#22343;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2011.04522</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20307;&#32032;&#30340;&#19977;&#32500;&#29289;&#20307;&#20998;&#31867;&#30340;&#24555;&#36895;&#28151;&#21512;&#32423;&#32852;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Fast Hybrid Cascade Network for Voxel-based 3D Object Classification. (arXiv:2011.04522v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20307;&#32032;&#30340;&#19977;&#32500;&#29289;&#20307;&#20998;&#31867;&#30340;&#24555;&#36895;&#28151;&#21512;&#32423;&#32852;&#32593;&#32476;&#65292;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#20998;&#21035;&#22788;&#29702;&#26131;&#12289;&#20013;&#21644;&#38590;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#27599;&#20010;&#20307;&#32032;&#36171;&#20104;&#26377;&#31526;&#21495;&#30340;&#36317;&#31163;&#20540;&#26469;&#25552;&#39640;&#31934;&#24230;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#24179;&#22343;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20307;&#32032;&#30340;&#19977;&#32500;&#29289;&#20307;&#20998;&#31867;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#23558;&#32463;&#20856;&#30340;&#20108;&#32500;&#21367;&#31215;&#36716;&#25442;&#20026;&#19977;&#32500;&#24418;&#24335;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#20855;&#26377;&#20108;&#36827;&#21046;&#20307;&#32032;&#34920;&#31034;&#30340;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20108;&#36827;&#21046;&#20307;&#32032;&#34920;&#31034;&#23545;&#20110;&#19977;&#32500;&#21367;&#31215;&#26469;&#35828;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20307;&#32032;&#30340;&#19977;&#32500;&#29289;&#20307;&#20998;&#31867;&#30340;&#28151;&#21512;&#32423;&#32852;&#20307;&#31995;&#32467;&#26500;&#12290;&#23427;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21253;&#25324;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#23618;&#65292;&#20998;&#21035;&#22788;&#29702;&#26131;&#12289;&#20013;&#21644;&#38590;&#30340;&#19977;&#32500;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36895;&#24230;&#26041;&#38754;&#37117;&#33021;&#22815;&#36798;&#21040;&#24179;&#34913;&#12290;&#36890;&#36807;&#32473;&#27599;&#20010;&#20307;&#32032;&#36171;&#20104;&#19968;&#20010;&#26377;&#31526;&#21495;&#30340;&#36317;&#31163;&#20540;&#65292;&#21487;&#20197;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#31934;&#24230;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28857;&#20113;&#21644;&#22522;&#20110;&#20307;&#32032;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24179;&#22343;&#25512;&#29702;&#26102;&#38388;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voxel-based 3D object classification has been thoroughly studied in recent years. Most previous methods convert the classic 2D convolution into a 3D form that will be further applied to objects with binary voxel representation for classification. However, the binary voxel representation is not very effective for 3D convolution in many cases. In this paper, we propose a hybrid cascade architecture for voxel-based 3D object classification. It consists of three stages composed of fully connected and convolutional layers, dealing with easy, moderate, and hard 3D models respectively. Both accuracy and speed can be balanced in our proposed method. By giving each voxel a signed distance value, an obvious gain regarding the accuracy can be observed. Besides, the mean inference time can be speeded up hugely compared with the state-of-the-art point cloud and voxel based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230; CNN &#21644;&#20223;&#29983;&#20915;&#31574;&#34701;&#21512;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#29366;&#24577;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545; EEG &#21644;&#21508;&#20010;&#22806;&#21608;&#29983;&#29702;&#20449;&#21495;&#20998;&#21035;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/1911.12918</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23610;&#24230;CNN&#21644;&#20223;&#29983;&#20915;&#31574;&#34701;&#21512;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#29366;&#24577;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multimodal Affective States Recognition Based on Multiscale CNNs and Biologically Inspired Decision Fusion Model. (arXiv:1911.12918v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.12918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230; CNN &#21644;&#20223;&#29983;&#20915;&#31574;&#34701;&#21512;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#29366;&#24577;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545; EEG &#21644;&#21508;&#20010;&#22806;&#21608;&#29983;&#29702;&#20449;&#21495;&#20998;&#21035;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21333;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#20449;&#21495;&#25110;&#22806;&#21608;&#29983;&#29702;&#20449;&#21495;&#65289;&#30340;&#24773;&#32490;&#29366;&#24577;&#35782;&#21035;&#27169;&#22411;&#24050;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#30340;&#24773;&#24863;&#29366;&#24577;&#35782;&#21035;&#26041;&#27861;&#23578;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;Multiscale CNNs&#65289;&#21644;&#20223;&#29983;&#20915;&#31574;&#34701;&#21512;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#29366;&#24577;&#35782;&#21035;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21407;&#22987;&#20449;&#21495;&#34987;&#19982;&#22522;&#32447;&#20449;&#21495;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#28982;&#21518;&#65292;Multiscale CNNs &#20013;&#30340;&#39640;&#23610;&#24230;CNN&#21644;&#20302;&#23610;&#24230;CNN&#20998;&#21035;&#29992;&#20110; EEG &#21644;&#27599;&#20010;&#22806;&#21608;&#29983;&#29702;&#20449;&#21495;&#30340;&#24773;&#24863;&#29366;&#24577;&#36755;&#20986;&#27010;&#29575;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#34701;&#21512;&#27169;&#22411;&#36890;&#36807;&#21508;&#31181;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#21644;&#26469;&#33258; Multiscale CNNs &#30340;&#20998;&#31867;&#27010;&#29575;&#35745;&#31639;&#21333;&#27169;&#24577;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26681;&#25454;&#20854;&#20013;&#26356;&#21487;&#38752;&#30340;&#20449;&#21495;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an encouraging progress in the affective states recognition models based on the single-modality signals as electroencephalogram (EEG) signals or peripheral physiological signals in recent years. However, multimodal physiological signals-based affective states recognition methods have not been thoroughly exploited yet. Here we propose Multiscale Convolutional Neural Networks (Multiscale CNNs) and a biologically inspired decision fusion model for multimodal affective states recognition. Firstly, the raw signals are pre-processed with baseline signals. Then, the High Scale CNN and Low Scale CNN in Multiscale CNNs are utilized to predict the probability of affective states output for EEG and each peripheral physiological signal respectively. Finally, the fusion model calculates the reliability of each single-modality signals by the Euclidean distance between various class labels and the classification probability from Multiscale CNNs, and the decision is made by the more rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#35268;&#33539;&#31354;&#38388;&#35777;&#26126;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#20989;&#25968;&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#26102;&#26159;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#20351;&#29992;&#28857;&#32447;&#24615;&#36716;&#25442;&#30340;&#26041;&#27861;&#24314;&#31435;&#21407;&#22987;NN&#27169;&#22411;&#31354;&#38388;&#21644;&#35268;&#33539;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#21482;&#35201;&#24046;&#24322;&#30697;&#38453;&#20445;&#25345;&#23436;&#25972;&#31209;&#65292;&#23601;&#19968;&#23450;&#33021;&#25910;&#25947;&#21040;&#38646;&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22823;&#35268;&#27169;NN&#20855;&#26377;&#22855;&#24322;&#30340;&#24046;&#24322;&#30697;&#38453;&#30340;&#27010;&#29575;&#38750;&#24120;&#23567;&#12290;</title><link>http://arxiv.org/abs/1903.02140</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20026;&#20160;&#20040;&#34892;&#20026;&#31867;&#20284;&#20110;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization. (arXiv:1903.02140v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.02140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#35268;&#33539;&#31354;&#38388;&#35777;&#26126;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#20989;&#25968;&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#26102;&#26159;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#20351;&#29992;&#28857;&#32447;&#24615;&#36716;&#25442;&#30340;&#26041;&#27861;&#24314;&#31435;&#21407;&#22987;NN&#27169;&#22411;&#31354;&#38388;&#21644;&#35268;&#33539;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#21482;&#35201;&#24046;&#24322;&#30697;&#38453;&#20445;&#25345;&#23436;&#25972;&#31209;&#65292;&#23601;&#19968;&#23450;&#33021;&#25910;&#25947;&#21040;&#38646;&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22823;&#35268;&#27169;NN&#20855;&#26377;&#22855;&#24322;&#30340;&#24046;&#24322;&#30697;&#38453;&#30340;&#27010;&#29575;&#38750;&#24120;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#29702;&#35770;&#24037;&#20316;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#31616;&#21333;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#35299;&#20915;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#22914;&#27492;&#25104;&#21151;&#12290;&#22312;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35268;&#33539;&#31354;&#38388;&#30340;&#25968;&#23398;&#24037;&#20855;&#20043;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35268;&#33539;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;&#23398;&#20064;NN&#30446;&#26631;&#20989;&#25968;&#26159;&#20984;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#21407;&#22987;NN&#27169;&#22411;&#31354;&#38388;&#21644;&#35268;&#33539;&#31354;&#38388;&#20043;&#38388;&#30340;&#26799;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#36890;&#36807;&#25152;&#35859;&#30340;&#24046;&#24322;&#30697;&#38453;&#34920;&#31034;&#30340;&#36880;&#28857;&#32447;&#24615;&#21464;&#25442;&#30456;&#20851;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#65292;&#22914;&#26524;&#24046;&#24322;&#30697;&#38453;&#20445;&#25345;&#23436;&#25972;&#31209;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19968;&#23450;&#20250;&#25910;&#25947;&#21040;&#38646;&#25439;&#22833;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22914;&#26524;&#36825;&#20010;&#23436;&#25972;&#31209;&#26465;&#20214;&#25104;&#31435;&#65292;&#22312;NN&#30340;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#19982;&#27491;&#24120;&#30340;&#20984;&#20248;&#21270;&#30456;&#21516;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22823;&#35268;&#27169;NN&#20855;&#26377;&#22855;&#24322;&#30340;&#24046;&#24322;&#30697;&#38453;&#30340;&#27010;&#29575;&#38750;&#24120;&#23567;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#36229;&#21442;&#25968;&#21270;&#30340;NN&#26159;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are 
&lt;/p&gt;</description></item></channel></rss>