<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11256</link><description>&lt;p&gt;
&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#31867;&#20284;&#20110;Gromov-Wassertein&#30340;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space. (arXiv:2310.11256v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#19978;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#12290;&#31532;&#19968;&#31181;&#36317;&#31163;&#26159;&#22312;&#39640;&#26031;&#27979;&#24230;&#31354;&#38388;&#19978;&#20004;&#20010;&#31163;&#25955;&#20998;&#24067;&#30340;Gromov-Wasserstein&#36317;&#31163;&#12290;&#35813;&#36317;&#31163;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#26367;&#20195;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#19981;&#33021;&#30452;&#25509;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#36816;&#36755;&#26041;&#26696;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#36825;&#26679;&#30340;&#36816;&#36755;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21478;&#19968;&#31181;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#36317;&#31163;&#19982;Gromov-Wasserstein&#23494;&#20999;&#30456;&#20851;&#12290;&#24403;&#23558;&#20801;&#35768;&#30340;&#36816;&#36755;&#32806;&#21512;&#38480;&#21046;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#36825;&#23450;&#20041;&#20102;&#21478;&#19968;&#31181;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#21478;&#19968;&#31181;&#26367;&#20195;&#65292;&#24182;&#20801;&#35768;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11244</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#21028;&#26029;&#20004;&#20010;&#23454;&#20307;&#25551;&#36848;&#26159;&#21542;&#25351;&#30340;&#26159;&#21516;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23454;&#20307;&#21305;&#37197;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#20063;&#26159;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#23558;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#20135;&#21697;&#21305;&#37197;&#36215;&#26469;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914;BERT&#25110;RoBERTa&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;ii&#65289;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#20307;&#19981;&#22815;&#20581;&#22766;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22522;&#20110;PLMs&#30340;&#21305;&#37197;&#22120;&#30340;&#22791;&#36873;&#26041;&#26696;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#23545;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT3.5&#21644;GPT4&#65292;&#20197;&#21450;&#22522;&#20110;Llama2&#30340;&#24320;&#28304;LLMs&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
&lt;/p&gt;</description></item><item><title>&#26412;&#35838;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#21160;&#24577;&#36755;&#36816;&#27979;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35762;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#36825;&#20123;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#27491;&#21453;&#39304;&#24490;&#29615;&#25913;&#36827;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.11232</link><description>&lt;p&gt;
&#23398;&#20064;&#26356;&#22909;&#30340;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Sample Better. (arXiv:2310.11232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35838;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#21160;&#24577;&#36755;&#36816;&#27979;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35762;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#36825;&#20123;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#27491;&#21453;&#39304;&#24490;&#29615;&#25913;&#36827;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#35762;&#20041;&#20171;&#32461;&#20102;&#22522;&#20110;&#21160;&#24577;&#36755;&#36816;&#27979;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#31616;&#21333;&#22522;&#30784;&#27979;&#24230;&#30340;&#26679;&#26412;&#34987;&#26144;&#23556;&#21040;&#24863;&#20852;&#36259;&#30446;&#26631;&#27979;&#24230;&#30340;&#26679;&#26412;&#12290;&#29305;&#21035;&#24378;&#35843;&#36825;&#20123;&#26041;&#27861;&#22312;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35762;&#20041;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;MC&#37319;&#26679;&#29983;&#25104;&#30340;&#25968;&#25454;&#21464;&#20998;&#23398;&#20064;&#36825;&#20123;&#26144;&#23556;&#65292;&#24182;&#22914;&#20309;&#21033;&#29992;&#23427;&#20204;&#36890;&#36807;&#27491;&#21453;&#39304;&#24490;&#29615;&#25913;&#36827;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
These lecture notes provide an introduction to recent advances in generative modeling methods based on the dynamical transportation of measures, by means of which samples from a simple base measure are mapped to samples from a target measure of interest. Special emphasis is put on the applications of these methods to Monte-Carlo (MC) sampling techniques, such as importance sampling and Markov Chain Monte-Carlo (MCMC) schemes. In this context, it is shown how the maps can be learned variationally using data generated by MC sampling, and how they can in turn be used to improve such sampling in a positive feedback loop.
&lt;/p&gt;</description></item><item><title>Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11230</link><description>&lt;p&gt;
Zipformer&#65306;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Zipformer: A faster and better encoder for automatic speech recognition. (arXiv:2310.11230v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11230
&lt;/p&gt;
&lt;p&gt;
Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#26368;&#27969;&#34892;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#23427;&#22312;&#21464;&#25442;&#22120;&#20013;&#21152;&#20837;&#20102;&#21367;&#31215;&#27169;&#22359;&#20197;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#21464;&#25442;&#22120;&#8212;&#8212;Zipformer&#12290;&#24314;&#27169;&#25913;&#21464;&#21253;&#25324;&#65306;1&#65289;&#31867;&#20284;U-Net&#30340;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#20013;&#38388;&#22534;&#26632;&#22312;&#36739;&#20302;&#30340;&#24103;&#29575;&#19979;&#36816;&#34892;&#65307;2&#65289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#65292;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#27169;&#22359;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#22797;&#20351;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#39640;&#25928;&#29575;&#65307;3&#65289;&#19968;&#31181;&#25913;&#36827;&#30340;LayerNorm&#24418;&#24335;&#65292;&#31216;&#20026;BiasNorm&#65292;&#20801;&#35768;&#25105;&#20204;&#20445;&#30041;&#19968;&#20123;&#38271;&#24230;&#20449;&#24687;&#65307;4&#65289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;SwooshR&#21644;SwooshL&#30340;&#24615;&#33021;&#20248;&#20110;Swish&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;ScaledAdam&#65292;&#23427;&#36890;&#36807;&#24403;&#21069;&#24352;&#37327;&#30340;&#35268;&#27169;&#26469;&#32553;&#25918;&#26356;&#26032;&#65292;&#20197;&#20445;&#25345;&#30456;&#23545;&#21464;&#21270;&#22823;&#33268;&#30456;&#21516;&#65292;&#24182;&#26126;&#30830;&#23398;&#20064;&#21442;&#25968;&#35268;&#27169;&#12290;&#19982;Adam&#30456;&#27604;&#65292;&#23427;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and Wenet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11211</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#24182;&#23454;&#29616;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#28041;&#21450;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#28145;&#20837;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#8212;&#8212;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#8212;&#8212;&#20026;&#20363;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#23450;&#20041;&#21644;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20195;&#29702;-&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#36825;&#20010;"&#24046;&#36317;"&#30452;&#25509;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#26159;&#21542;&#36866;&#21512;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#36825;&#20010;"&#24046;&#36317;"&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#30340;&#20852;&#36259;&#65292;&#34920;&#26126;&#26080;&#38480;&#21046;&#30340;&#20195;&#29702;&#20989;&#25968;&#23558;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#36828;&#31163;&#30340;&#28857;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11207</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33258;&#25105;&#35299;&#37322;&#65311;LLM&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. (arXiv:2310.11207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#31867;&#23545;&#35805;&#20013;&#36827;&#34892;&#25351;&#23548;&#65292;&#20197;&#20135;&#29983;&#8220;&#26377;&#24110;&#21161;&#8221;&#30340;&#22238;&#31572;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#22312;&#22238;&#31572;&#20013;&#25552;&#20379;&#35299;&#37322;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#33258;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#22312;&#20998;&#26512;&#30005;&#24433;&#35780;&#35770;&#30340;&#24773;&#24863;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#36755;&#20986;&#24773;&#24863;&#30340;&#31215;&#26497;&#24615;&#65292;&#24182;&#21015;&#20986;&#35780;&#35770;&#20013;&#24102;&#26377;&#24773;&#24863;&#30340;&#35789;&#35821;&#65288;&#22914;&#8220;fantastic&#8221;&#21644;&#8220;memorable&#8221;&#65289;&#20316;&#20026;&#35299;&#37322;&#12290;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#35299;&#37322;&#26377;&#22810;&#22909;&#65311;&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#30340;&#20219;&#21153;&#20013;&#23545;&#27492;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21518;&#32773;&#26159;&#21487;&#35299;&#37322;&#24615;&#25991;&#29486;&#20013;&#26368;&#24120;&#30740;&#31350;&#30340;&#35774;&#32622;&#65288;&#38024;&#23545;ChatGPT&#20043;&#21069;&#30340;&#27169;&#22411;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#33258;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35757;&#32451;&#38543;&#26426;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20445;&#25252;&#27599;&#20010;&#33410;&#28857;&#30340;&#38544;&#31169;&#24182;&#19988;&#20855;&#26377;&#25968;&#20540;&#19978;&#38750;&#31354;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#25968;&#25454;&#20849;&#20139;&#21644;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11203</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Nonvacuous Generalisation Bounds. (arXiv:2310.11203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11203
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35757;&#32451;&#38543;&#26426;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20445;&#25252;&#27599;&#20010;&#33410;&#28857;&#30340;&#38544;&#31169;&#24182;&#19988;&#20855;&#26377;&#25968;&#20540;&#19978;&#38750;&#31354;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#25968;&#25454;&#20849;&#20139;&#21644;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#39044;&#27979;&#22120;&#65292;&#22312;&#36825;&#31181;&#31574;&#30053;&#20013;&#65292;&#32593;&#32476;&#30340;&#27599;&#20010;&#33410;&#28857;&#36890;&#36807;&#21457;&#24067;&#26412;&#22320;&#39044;&#27979;&#22120;&#20294;&#23545;&#20854;&#20182;&#33410;&#28857;&#20445;&#23494;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26041;&#24335;&#26469;&#20445;&#25252;&#20854;&#38544;&#31169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#30340;&#38543;&#26426;&#39044;&#27979;&#22120;&#65292;&#23427;&#22312;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#30340;&#24847;&#20041;&#19978;&#32487;&#25215;&#20102;&#26412;&#22320;&#31169;&#26377;&#39044;&#27979;&#22120;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21516;&#27493;&#24773;&#20917;&#65292;&#21363;&#25152;&#26377;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#65288;&#20174;&#27867;&#21270;&#30028;&#38480;&#23548;&#20986;&#65289;&#65292;&#20197;&#21450;&#24322;&#27493;&#24773;&#20917;&#65292;&#21363;&#27599;&#20010;&#33410;&#28857;&#21487;&#20197;&#26377;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19982;&#23558;&#25152;&#26377;&#25968;&#25454;&#38598;&#20849;&#20139;&#32473;&#25152;&#26377;&#33410;&#28857;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#39044;&#27979;&#22120;&#25903;&#25345;&#30528;&#22312;&#20445;&#25252;&#27599;&#20010;&#33410;&#28857;&#38544;&#31169;&#30340;&#21516;&#26102;&#20855;&#26377;&#25968;&#20540;&#19978;&#38750;&#31354;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#26126;&#30830;&#22320;&#35745;&#31639;&#20102;&#39044;&#27979;&#24615;&#33021;&#30340;&#22686;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel strategy to train randomised predictors in federated learning, where each node of the network aims at preserving its privacy by releasing a local predictor but keeping secret its training dataset with respect to the other nodes. We then build a global randomised predictor which inherits the properties of the local private predictors in the sense of a PAC-Bayesian generalisation bound. We consider the synchronous case where all nodes share the same training objective (derived from a generalisation bound), and the asynchronous case where each node may have its own personalised training objective. We show through a series of numerical experiments that our approach achieves a comparable predictive performance to that of the batch approach where all datasets are shared across nodes. Moreover the predictors are supported by numerically nonvacuous generalisation bounds while preserving privacy for each node. We explicitly compute the increment on predictive performance an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11198</link><description>&lt;p&gt;
EEG&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#65306;&#19968;&#31181;&#19982;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30456;&#27604;&#36739;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms. (arXiv:2310.11198v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#22312;&#22823;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#24212;&#29992;&#21508;&#31181;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#29992;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#30340;&#24378;&#22823;&#28436;&#36827;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#36825;&#20123;&#26426;&#21046;&#25972;&#21512;&#21040;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#31995;&#32479;&#22320;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36731;&#37327;&#32423;&#30340;&#22522;&#20934;&#26550;&#26500;&#65292;&#26088;&#22312;&#26080;&#32541;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#21482;&#30740;&#31350;&#19968;&#20010;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#24120;&#26500;&#24314;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#12289;&#26377;&#26102;&#23884;&#22871;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#30456;&#21516;&#24773;&#20917;&#19979;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#26131;&#20110;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20197;&#21450;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#30740;&#31350;&#21644;&#25512;&#36827;BCI&#20013;&#30340;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;EXP3&#31639;&#27861;MUD-EXP3&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#20272;&#35745;&#22120;&#65292;&#22312;&#27599;&#20010;&#22238;&#21512;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#24050;&#30693;&#32456;&#27490;&#22238;&#21512;&#32034;&#24341;$T$&#12289;&#29992;&#25143;&#25968;&#37327;$M$&#12289;&#33218;&#25968;&#37327;$N$&#21644;&#24310;&#36831;&#19978;&#30028;$d_{max}$&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$&#12290;&#23545;&#20110;&#26410;&#30693;$T$&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11188</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;EXP3&#31639;&#27861;&#21450;&#20854;&#22312;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback. (arXiv:2310.11188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;EXP3&#31639;&#27861;MUD-EXP3&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#20272;&#35745;&#22120;&#65292;&#22312;&#27599;&#20010;&#22238;&#21512;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#24050;&#30693;&#32456;&#27490;&#22238;&#21512;&#32034;&#24341;$T$&#12289;&#29992;&#25143;&#25968;&#37327;$M$&#12289;&#33218;&#25968;&#37327;$N$&#21644;&#24310;&#36831;&#19978;&#30028;$d_{max}$&#30340;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#20026;$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$&#12290;&#23545;&#20110;&#26410;&#30693;$T$&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#24310;&#36831;&#21453;&#39304;&#32467;&#26524;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#65292;&#24182;&#19988;&#23545;&#20869;&#37096;&#20998;&#24067;&#27809;&#26377;&#38480;&#21046;&#12290;&#24403;&#29609;&#23478;&#36873;&#25321;&#19968;&#20010;&#33218;&#26102;&#65292;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#30340;&#21453;&#39304;&#21487;&#33021;&#19981;&#20250;&#31435;&#21363;&#25509;&#25910;&#21040;&#65292;&#32780;&#26159;&#22312;&#19968;&#20010;&#26410;&#30693;&#30340;&#26102;&#38388;&#24310;&#36831;&#20043;&#21518;&#25165;&#33021;&#25509;&#25910;&#21040;&#12290;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#65292;&#19981;&#21516;&#29992;&#25143;&#30340;&#21453;&#39304;&#24310;&#36831;&#27809;&#26377;&#28508;&#22312;&#30340;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#29992;&#25143;&#24310;&#36831;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;MUD-EXP3&#30340;&#25913;&#36827;EXP3&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#20272;&#35745;&#22120;&#22312;&#27599;&#20010;&#22238;&#21512;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#24050;&#30693;&#32456;&#27490;&#22238;&#21512;&#32034;&#24341;$T$&#12289;&#29992;&#25143;&#25968;&#37327;$M$&#12289;&#33218;&#25968;&#37327;$N$&#21644;&#24310;&#36831;&#19978;&#30028;$d_{max}$&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$&#30340;&#36951;&#25022;&#20540;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26410;&#30693;$T$&#30340;&#26356;&#24120;&#35265;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;t-SGNE&#30340;&#26032;&#39062;&#38477;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21487;&#35270;&#21270;&#22823;&#35268;&#27169;&#22270;&#34920;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#34920;&#30340;&#37051;&#22495;&#32467;&#26500;&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#24182;&#20351;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#38477;&#20302;&#20026;&#32447;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;t-SGNE&#30340;&#22270;&#34920;&#23884;&#20837;&#31639;&#27861;SPLEE&#12290;&#36890;&#36807;SPLEE&#33719;&#24471;&#39640;&#32500;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;t-SGNE&#38477;&#32500;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11186</link><description>&lt;p&gt;
&#39640;&#25928;&#21487;&#35270;&#21270;&#22823;&#35268;&#27169;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
Efficiently Visualizing Large Graphs. (arXiv:2310.11186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;t-SGNE&#30340;&#26032;&#39062;&#38477;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21487;&#35270;&#21270;&#22823;&#35268;&#27169;&#22270;&#34920;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#34920;&#30340;&#37051;&#22495;&#32467;&#26500;&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#24182;&#20351;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#38477;&#20302;&#20026;&#32447;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;t-SGNE&#30340;&#22270;&#34920;&#23884;&#20837;&#31639;&#27861;SPLEE&#12290;&#36890;&#36807;SPLEE&#33719;&#24471;&#39640;&#32500;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;t-SGNE&#38477;&#32500;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#22270;&#34920;&#21487;&#35270;&#21270;&#26041;&#27861;&#30001;&#20110;&#24615;&#33021;&#38382;&#39064;&#20165;&#36866;&#29992;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#22270;&#34920;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#34920;&#21487;&#35270;&#21270;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#31216;&#20026;t&#20998;&#24067;&#38543;&#26426;&#22270;&#34920;&#37051;&#22495;&#23884;&#20837;&#65288;t-SGNE&#65289;&#12290;t-SGNE&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22270;&#34920;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#21487;&#35270;&#21270;&#12290;&#20316;&#20026;&#26631;&#20934;t-SNE&#26041;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;t-SGNE&#36991;&#20813;&#20102;&#32791;&#26102;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#35745;&#31639;&#12290;&#30456;&#21453;&#65292;&#23427;&#21033;&#29992;&#22270;&#34920;&#30340;&#37051;&#22495;&#32467;&#26500;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#38477;&#20302;&#20026;&#32447;&#24615;&#65292;&#20174;&#32780;&#25903;&#25345;&#26356;&#22823;&#30340;&#22270;&#34920;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36866;&#24212;t-SGNE&#65292;&#25105;&#20204;&#23558;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#19982;&#22270;&#34920;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#22270;&#34920;&#23884;&#20837;&#31639;&#27861;&#26368;&#30701;&#36335;&#24452;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#23884;&#20837;&#65288;SPLEE&#65289;&#12290;&#36890;&#36807;SPLEE&#33719;&#24471;&#22823;&#35268;&#27169;&#22270;&#34920;&#30340;&#39640;&#32500;&#23884;&#20837;&#65292;&#28982;&#21518;&#20351;&#29992;t-SGNE&#23558;&#20854;&#38477;&#32500;&#20197;&#36827;&#34892;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#21487;&#35270;&#21270;&#20855;&#26377;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing graph visualization methods based on dimension reduction are limited to relatively small graphs due to performance issues. In this work, we propose a novel dimension reduction method for graph visualization, called t-Distributed Stochastic Graph Neighbor Embedding (t-SGNE). t-SGNE is specifically designed to visualize cluster structures in the graph. As a variant of the standard t-SNE method, t-SGNE avoids the time-consuming computations of pairwise similarity. Instead, it uses the neighbor structures of the graph to reduce the time complexity from quadratic to linear, thus supporting larger graphs. In addition, to suit t-SGNE, we combined Laplacian Eigenmaps with the shortest path algorithm in graphs to form the graph embedding algorithm ShortestPath Laplacian Eigenmaps Embedding (SPLEE). Performing SPLEE to obtain a high-dimensional embedding of the large-scale graph and then using t-SGNE to reduce its dimension for visualization, we are able to visualize graphs with up
&lt;/p&gt;</description></item><item><title>MST-GAT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11169</link><description>&lt;p&gt;
MST-GAT: &#22522;&#20110;&#22810;&#27169;&#24577;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection. (arXiv:2310.11169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11169
&lt;/p&gt;
&lt;p&gt;
MST-GAT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#32500;&#25252;&#24037;&#20316;&#35774;&#22791;&#65288;&#20363;&#22914;&#27700;&#22788;&#29702;&#31995;&#32479;&#21644;&#33322;&#22825;&#22120;&#65289;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#35774;&#22791;&#30340;&#25968;&#25454;&#29305;&#24449;&#26159;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#27169;&#24577;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#25429;&#25417;&#21040;&#19981;&#21516;&#27169;&#24577;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#23548;&#33268;&#26356;&#22810;&#30340;&#20551;&#38452;&#24615;&#21644;&#20551;&#38451;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MST-GAT&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MST-GAT&#39318;&#20808;&#20351;&#29992;&#22810;&#27169;&#24577;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;M-GAT&#65289;&#21644;&#26102;&#22495;&#21367;&#31215;&#32593;&#32476;&#26469;&#25429;&#25417;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;M-GAT&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#20004;&#20010;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;&#21363;&#27169;&#24577;&#20869;&#21644;&#27169;&#24577;&#38388;&#27880;&#24847;&#21147;&#65289;&#26469;&#26126;&#30830;&#22320;&#24314;&#27169;&#27169;&#24577;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;MST-GAT&#20248;&#21270;&#20102;&#37325;&#24314;&#35823;&#24046;&#21644;&#38388;&#38548;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal time series (MTS) anomaly detection is crucial for maintaining the safety and stability of working devices (e.g., water treatment system and spacecraft), whose data are characterized by multivariate time series with diverse modalities. Although recent deep learning methods show great potential in anomaly detection, they do not explicitly capture spatial-temporal relationships between univariate time series of different modalities, resulting in more false negatives and false positives. In this paper, we propose a multimodal spatial-temporal graph attention network (MST-GAT) to tackle this problem. MST-GAT first employs a multimodal graph attention network (M-GAT) and a temporal convolution network to capture the spatial-temporal correlation in multimodal time series. Specifically, M-GAT uses a multi-head attention module and two relational attention modules (i.e., intra- and inter-modal attention) to model modal correlations explicitly. Furthermore, MST-GAT optimizes the reco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#20026;&#21442;&#19982;&#30340;&#33258;&#21160;&#21644;&#24358;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#20849;&#21516;&#21019;&#24314;&#21644;&#22768;&#27880;&#37322;&#65292;&#23454;&#29616;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.11165</link><description>&lt;p&gt;
Serenade: &#20154;&#20026;&#21442;&#19982;&#30340;&#33258;&#21160;&#21644;&#24358;&#20272;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Serenade: A Model for Human-in-the-loop Automatic Chord Estimation. (arXiv:2310.11165v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11165
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#20026;&#21442;&#19982;&#30340;&#33258;&#21160;&#21644;&#24358;&#20272;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#20849;&#21516;&#21019;&#24314;&#21644;&#22768;&#27880;&#37322;&#65292;&#23454;&#29616;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#21644;&#22768;&#20998;&#26512;&#23545;&#20110;&#33258;&#21160;&#30340;&#20998;&#21106;&#12289;&#35821;&#26009;&#24211;&#20998;&#26512;&#21644;&#33258;&#21160;&#21644;&#24358;&#26631;&#31614;&#20272;&#35745;&#31561;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#38899;&#20048;&#21644;&#22768;&#30340;&#27169;&#31946;&#24615;&#23548;&#33268;&#20102;&#26377;&#38480;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#26174;&#38706;&#20986;&#20102;&#30446;&#21069;&#36890;&#29992;&#25351;&#26631;&#22914;&#20934;&#30830;&#24615;&#23384;&#22312;&#30340;&#29942;&#39048;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38382;&#39064;&#35201;&#20040;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#36807;&#21019;&#24314;&#22810;&#25968;&#35268;&#21017;&#27880;&#37322;&#36827;&#34892;&#35299;&#20915;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#23398;&#20064;&#36719;&#30446;&#26631;&#36827;&#34892;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#20849;&#21516;&#20026;&#38899;&#39057;&#36712;&#36857;&#21019;&#24314;&#21644;&#22768;&#27880;&#37322;&#12290;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#22768;&#39044;&#27979;&#21518;&#65292;&#20154;&#31867;&#31232;&#30095;&#22320;&#26631;&#27880;&#27169;&#22411;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#37096;&#20998;&#65292;&#28982;&#21518;&#27169;&#22411;&#26681;&#25454;&#20154;&#31867;&#30340;&#25351;&#23548;&#35843;&#25972;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#27969;&#34892;&#38899;&#20048;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#20154;&#20026;&#21442;&#19982;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#21644;&#22768;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational harmony analysis is important for MIR tasks such as automatic segmentation, corpus analysis and automatic chord label estimation. However, recent research into the ambiguous nature of musical harmony, causing limited inter-rater agreement, has made apparent that there is a glass ceiling for common metrics such as accuracy. Commonly, these issues are addressed either in the training data itself by creating majority-rule annotations or during the training phase by learning soft targets. We propose a novel alternative approach in which a human and an autoregressive model together co-create a harmonic annotation for an audio track. After automatically generating harmony predictions, a human sparsely annotates parts with low model confidence and the model then adjusts its predictions following human guidance. We evaluate our model on a dataset of popular music and we show that, with this human-in-the-loop approach, harmonic analysis performance improves over a model-only appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#25955;&#32852;&#24819;&#20219;&#21153;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20808;&#36827;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36229;&#36807;&#20102;&#22823;&#22810;&#25968;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.11158</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#21147;&#65306;&#27169;&#22411;&#33021;&#21542;&#20135;&#29983;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65311;
&lt;/p&gt;
&lt;p&gt;
Probing the Creativity of Large Language Models: Can models produce divergent semantic association?. (arXiv:2310.11158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#25955;&#32852;&#24819;&#20219;&#21153;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20808;&#36827;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36229;&#36807;&#20102;&#22823;&#22810;&#25968;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#35328;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#36827;&#19968;&#27493;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#20869;&#23481;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#35748;&#30693;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24605;&#32500;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#25955;&#32852;&#24819;&#20219;&#21153;&#65288;DAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23458;&#35266;&#34913;&#37327;&#21019;&#36896;&#21147;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19981;&#30456;&#20851;&#30340;&#21333;&#35789;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65306;&#65288;1&#65289;&#22312;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#31574;&#30053;&#26102;&#65292;GPT-4&#36229;&#36807;&#20102;96&#65285;&#30340;&#20154;&#31867;&#65292;&#32780;GPT-3.5-turbo&#36229;&#36807;&#20102;&#24179;&#22343;&#27700;&#24179;&#65307;&#65288;2&#65289;&#23545;&#20110;&#27169;&#22411;&#32780;&#35328;&#65292;&#38543;&#26426;&#25277;&#26679;&#21644;&#28201;&#24230;&#35843;&#33410;&#26159;&#33719;&#24471;&#26356;&#39640;DAT&#20998;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#22312;&#21019;&#36896;&#21147;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36825;&#26159;&#21019;&#36896;&#21147;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24503;&#22269;&#23460;&#20869;&#27681;&#27668;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#30340;&#24503;&#22269;&#39640;&#20998;&#36776;&#29575;&#23460;&#20869;&#27681;&#27668;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model. (arXiv:2310.11143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24503;&#22269;&#23460;&#20869;&#27681;&#27668;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#27681;&#27668;&#26159;&#19968;&#31181;&#33268;&#30284;&#30340;&#25918;&#23556;&#24615;&#27668;&#20307;&#65292;&#21487;&#20197;&#22312;&#23460;&#20869;&#31215;&#32047;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#23460;&#20869;&#27681;&#26292;&#38706;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#27979;&#37327;&#27963;&#21160;&#20272;&#35745;&#24471;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29305;&#24449;&#24448;&#24448;&#19982;&#20154;&#21475;&#29305;&#24449;&#19981;&#21516;&#65292;&#36825;&#26159;&#30001;&#20110;&#35768;&#22810;&#30456;&#20851;&#22240;&#32032;&#65292;&#22914;&#22320;&#36136;&#28304;&#27681;&#27668;&#30340;&#21487;&#29992;&#24615;&#25110;&#27004;&#23618;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#26679;&#26412;&#22823;&#23567;&#36890;&#24120;&#19981;&#20801;&#35768;&#20197;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#36827;&#34892;&#26292;&#38706;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#32431;&#25968;&#25454;&#26041;&#27861;&#26356;&#21152;&#29616;&#23454;&#22320;&#20272;&#35745;&#23460;&#20869;&#27681;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#24314;&#27169;&#26041;&#27861;&#65306;1&#65289;&#24212;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65292;&#20351;&#29992;&#29615;&#22659;&#21644;&#24314;&#31569;&#25968;&#25454;&#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#65292;&#20272;&#35745;&#20102;&#24503;&#22269;&#27599;&#20010;&#20303;&#23429;&#27004;&#30340;&#27599;&#20010;&#27004;&#23618;&#30340;&#23460;&#20869;&#27681;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#65307;2&#65289;&#20351;&#29992;&#27010;&#29575;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#25216;&#26415;&#20351;&#23427;&#20204;&#32452;&#21512;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and
&lt;/p&gt;</description></item><item><title>BayesDiff&#25552;&#20986;&#20102;&#19968;&#31181;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#36845;&#20195;&#21407;&#21017;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20272;&#35745;&#65292;&#24182;&#21487;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#36807;&#28388;&#20302;&#36136;&#37327;&#22270;&#20687;&#65292;&#22686;&#24378;&#25104;&#21151;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#32416;&#27491;&#22833;&#36133;&#29983;&#25104;&#32467;&#26524;&#20013;&#30340;&#20266;&#24433;&#12290;</title><link>http://arxiv.org/abs/2310.11142</link><description>&lt;p&gt;
BayesDiff: &#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20272;&#35745;&#25193;&#25955;&#20013;&#30340;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference. (arXiv:2310.11142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11142
&lt;/p&gt;
&lt;p&gt;
BayesDiff&#25552;&#20986;&#20102;&#19968;&#31181;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#36845;&#20195;&#21407;&#21017;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20272;&#35745;&#65292;&#24182;&#21487;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#36807;&#28388;&#20302;&#36136;&#37327;&#22270;&#20687;&#65292;&#22686;&#24378;&#25104;&#21151;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#32416;&#27491;&#22833;&#36133;&#29983;&#25104;&#32467;&#26524;&#20013;&#30340;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#36136;&#37327;&#36739;&#20302;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#26679;&#26412;&#24230;&#37327;&#65292;&#23545;&#20854;&#36827;&#34892;&#37492;&#21035;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BayesDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#36845;&#20195;&#21407;&#21017;&#26469;&#25551;&#36848;&#25193;&#25955;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#26368;&#21518;&#19968;&#23618;&#30340;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#20272;&#35745;&#30340;&#20687;&#32032;&#32423;&#19981;&#30830;&#23450;&#24615;&#19981;&#20165;&#21487;&#20197;&#32858;&#21512;&#25104;&#26679;&#26412;&#32423;&#24230;&#37327;&#65292;&#20197;&#36807;&#28388;&#20986;&#36136;&#37327;&#36739;&#20302;&#30340;&#22270;&#20687;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#22312;&#25991;&#26412;&#36716;&#22270;&#20687;&#20219;&#21153;&#20013;&#22686;&#24378;&#25104;&#21151;&#30340;&#29983;&#25104;&#32467;&#26524;&#24182;&#32416;&#27491;&#22833;&#36133;&#30340;&#29983;&#25104;&#32467;&#26524;&#20013;&#30340;&#20266;&#24433;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BayesDiff&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TEEN&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#26356;&#22810;&#26679;&#21270;&#30340;&#36712;&#36857;&#26469;&#25552;&#39640;&#39044;&#26399;&#22238;&#25253;&#30340;&#21516;&#26102;&#22686;&#24378;&#38598;&#21512;&#31574;&#30053;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11138</link><description>&lt;p&gt;
&#20445;&#25345;&#21508;&#31181;&#36712;&#36857;&#65306;&#20419;&#36827;&#36830;&#32493;&#25511;&#21046;&#20013;&#25506;&#32034;&#38598;&#21512;&#31574;&#30053;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control. (arXiv:2310.11138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TEEN&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#26356;&#22810;&#26679;&#21270;&#30340;&#36712;&#36857;&#26469;&#25552;&#39640;&#39044;&#26399;&#22238;&#25253;&#30340;&#21516;&#26102;&#22686;&#24378;&#38598;&#21512;&#31574;&#30053;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#19982;&#38598;&#21512;&#26041;&#27861;&#30340;&#32467;&#21512;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#19978;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#19968;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#22810;&#27169;&#22411;&#30340;&#21033;&#29992;&#65292;&#23427;&#22686;&#24378;&#20102;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#21644;&#20540;&#20989;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#29616;&#26377;&#38598;&#21512;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#35777;&#25104;&#21151;&#30340;&#20998;&#26512;&#36824;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#26032;&#20998;&#26512;&#25581;&#31034;&#20102;&#20043;&#21069;&#30340;&#38598;&#21512;DRL&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21487;&#33021;&#21463;&#21040;&#19981;&#22815;&#22810;&#26679;&#21270;&#30340;&#23376;&#31574;&#30053;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Trajectories-awarE Ensemble exploratioN (TEEN)&#12290;TEEN&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25552;&#39640;&#39044;&#26399;&#22238;&#25253;&#30340;&#21516;&#26102;&#20419;&#36827;&#26356;&#22810;&#26679;&#21270;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;TEEN&#19981;&#20165;&#22686;&#24378;&#20102;&#38598;&#21512;&#31574;&#30053;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \textbf{T}rajectories-awar\textbf{E} \textbf{E}nsemble exploratio\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#38750;&#21442;&#25968;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;k&#26368;&#36817;&#37051;&#30340;CMI&#20272;&#35745;&#22120;&#21644;&#26412;&#22320;&#32622;&#25442;&#26041;&#26696;&#65292;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#21253;&#21547;&#25968;&#20540;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.11132</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#30340;&#28151;&#21512;&#36830;&#32493;-&#20998;&#31867;&#21464;&#37327;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65306;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#25968;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation. (arXiv:2310.11132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#38750;&#21442;&#25968;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;k&#26368;&#36817;&#37051;&#30340;CMI&#20272;&#35745;&#22120;&#21644;&#26412;&#22320;&#32622;&#25442;&#26041;&#26696;&#65292;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#21253;&#21547;&#25968;&#20540;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;CIT&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#21464;&#37327;&#36873;&#25321;&#65292;&#20063;&#26159;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22823;&#22810;&#25968;&#24403;&#19979;&#30340;CIT&#26041;&#27861;&#20551;&#35774;&#25152;&#26377;&#21464;&#37327;&#37117;&#26159;&#25968;&#20540;&#25110;&#25152;&#26377;&#21464;&#37327;&#37117;&#26159;&#20998;&#31867;&#30340;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#28041;&#21450;&#21253;&#21547;&#25968;&#20540;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#12290;&#38750;&#21442;&#25968;&#30340;CIT&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#20272;&#35745;&#22120;&#32467;&#21512;&#26412;&#22320;&#32622;&#25442;&#26041;&#26696;&#36827;&#34892;&#12290;&#26368;&#36817;&#65292;&#24050;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#30340;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;CMI&#20272;&#35745;&#22120;&#12290;&#19982;&#20219;&#20309;k-NN&#26041;&#27861;&#19968;&#26679;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#20381;&#36182;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#23450;&#20041;&#12290;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#23545;&#20998;&#31867;&#21464;&#37327;&#36827;&#34892;one-hot&#32534;&#30721;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#20174;&#32780;&#23558;&#20998;&#31867;&#21464;&#37327;&#35270;&#20026;&#31163;&#25955;&#25968;&#20540;&#21464;&#37327;&#65292;&#32780;&#21478;&#19968;&#31181;&#26041;&#27861;&#21017;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#21464;&#37327;&#20316;&#20026;&#26465;&#20214;&#26469;&#36890;&#36807;&#29109;&#26469;&#34920;&#31034;CMI&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these 
&lt;/p&gt;</description></item><item><title>FROST&#26159;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23545;ML&#31649;&#36947;&#33021;&#37327;&#28040;&#32791;&#36827;&#34892;&#20998;&#26512;&#24182;&#20248;&#21270;&#30828;&#20214;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#20934;&#30830;&#24615;&#25110;&#24341;&#20837;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;26.4%&#30340;&#33021;&#37327;&#33410;&#32422;&#12290;</title><link>http://arxiv.org/abs/2310.11131</link><description>&lt;p&gt;
FROST:&#38754;&#21521;&#33021;&#25928;AI-on-5G&#24179;&#21488;&#30340;GPU&#21151;&#32791;&#35843;&#25972;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FROST: Towards Energy-efficient AI-on-5G Platforms -- A GPU Power Capping Evaluation. (arXiv:2310.11131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11131
&lt;/p&gt;
&lt;p&gt;
FROST&#26159;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23545;ML&#31649;&#36947;&#33021;&#37327;&#28040;&#32791;&#36827;&#34892;&#20998;&#26512;&#24182;&#20248;&#21270;&#30828;&#20214;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#20934;&#30830;&#24615;&#25110;&#24341;&#20837;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;26.4%&#30340;&#33021;&#37327;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#26159;&#19968;&#20010;&#36805;&#36895;&#22686;&#38271;&#30340;&#24066;&#22330;&#65292;&#22312;&#26410;&#26469;&#20960;&#24180;&#26377;&#30528;&#39044;&#35745;&#30340;&#22686;&#38271;&#12290;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#23545;&#32593;&#32476;&#30340;&#35774;&#22791;&#25903;&#20986;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#28040;&#32791;&#20102;&#24635;&#33021;&#32791;&#30340;73%&#12290;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#20248;&#21270;&#30340;&#29702;&#24819;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26679;&#30340;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;ML&#30340;&#33021;&#32791;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#20986;FROST&#28789;&#27963;&#37325;&#26500;&#19982;&#22312;&#32447;&#31995;&#32479;&#35843;&#20248;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#31526;&#21512;O-RAN&#35268;&#33539;&#21644;&#21407;&#21017;&#30340;&#33021;&#37327;&#24863;&#30693;ML&#31649;&#36947;&#35299;&#20915;&#26041;&#26696;&#12290;FROST&#33021;&#22815;&#23545;ML&#31649;&#36947;&#30340;&#33021;&#37327;&#28040;&#32791;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#30456;&#24212;&#22320;&#20248;&#21270;&#30828;&#20214;&#65292;&#20174;&#32780;&#38480;&#21046;&#21151;&#32791;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FROST&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;26.4%&#30340;&#33021;&#37327;&#33410;&#32422;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25110;&#24341;&#20837;&#26174;&#33879;&#30340;&#26102;&#38388;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Open Radio Access Network (O-RAN) is a burgeoning market with projected growth in the upcoming years. RAN has the highest CAPEX impact on the network and, most importantly, consumes 73% of its total energy. That makes it an ideal target for optimisation through the integration of Machine Learning (ML). However, the energy consumption of ML is frequently overlooked in such ecosystems. Our work addresses this critical aspect by presenting FROST Flexible Reconfiguration method with Online System Tuning - a solution for energy-aware ML pipelines that adhere to O-RAN's specifications and principles. FROST is capable of profiling the energy consumption of an ML pipeline and optimising the hardware accordingly, thereby limiting the power draw. Our findings indicate that FROST can achieve energy savings of up to 26.4% without compromising the model's accuracy or introducing significant time delays.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#25299;&#25169;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#34913;&#37327;&#32593;&#32476;&#23545;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#27604;&#27973;&#23618;&#32593;&#32476;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#25299;&#25169;&#31616;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11130</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Topological Expressivity of ReLU Neural Networks. (arXiv:2310.11130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#25299;&#25169;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#34913;&#37327;&#32593;&#32476;&#23545;&#25968;&#25454;&#25299;&#25169;&#32467;&#26500;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#27604;&#27973;&#23618;&#32593;&#32476;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#25299;&#25169;&#31616;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#25299;&#25169;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25913;&#21464;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;&#19968;&#20010;&#25299;&#25169;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#19968;&#20010;&#25299;&#25169;&#31616;&#21333;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25299;&#25169;&#31616;&#21270;&#21487;&#20197;&#29992;Betti&#25968;&#26469;&#34913;&#37327;&#65292;Betti&#25968;&#26159;&#25299;&#25169;&#31354;&#38388;&#30340;&#20195;&#25968;&#19981;&#21464;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#34913;&#37327;&#25351;&#26631;&#26469;&#30830;&#23450;&#32473;&#23450;&#26550;&#26500;&#19979;ReLU&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#30340;&#25299;&#25169;&#31616;&#21270;&#30340;&#19978;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#25581;&#31034;ReLU&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#25968;&#25454;&#30340;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#20026;&#28145;&#20837;&#29702;&#35299;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#29305;&#21035;&#26159;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#25299;&#25169;&#31616;&#21270;&#26041;&#38754;&#27604;&#27973;&#23618;&#32593;&#32476;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.11122</link><description>&lt;p&gt;
&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#29616;&#20195;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#36873;&#25321;&#28041;&#21450;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#20998;&#24067;&#30340;&#35268;&#33539;&#12289;&#21518;&#39564;&#36924;&#36817;&#22120;&#21644;&#25968;&#25454;&#12290;&#27599;&#20010;&#36873;&#25321;&#37117;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#26029;&#21644;&#21518;&#32493;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;&#25935;&#24863;&#24615;&#20998;&#26512;&#25972;&#21512;&#21040;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#65288;ABI&#65292;&#21363;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#25512;&#26029;&#65289;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32534;&#30721;&#26367;&#20195;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25512;&#26029;&#26469;&#35780;&#20272;&#23545;&#21508;&#31181;&#25968;&#25454;&#25200;&#21160;&#25110;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#20912;&#20923;&#20999;&#29255;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#22270;&#20687;&#32454;&#33410;&#65292;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.11112</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20445;&#25345;&#32452;&#32455;&#32467;&#26500;&#30340;&#32452;&#32455;&#20912;&#20923;&#20999;&#29255;&#30340;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Super resolution of histopathological frozen sections via deep learning preserving tissue structure. (arXiv:2310.11112v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#20912;&#20923;&#20999;&#29255;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#22270;&#20687;&#32454;&#33410;&#65292;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19982;&#20934;&#22791;&#27704;&#20037;&#24615;&#20999;&#29255;&#30456;&#27604;&#65292;&#21046;&#22791;&#20912;&#20923;&#20999;&#29255;&#30340;&#36807;&#31243;&#26356;&#24555;&#65292;&#21487;&#20197;&#22312;&#25163;&#26415;&#26399;&#38388;&#36827;&#34892;&#65292;&#26679;&#26412;&#25195;&#25551;&#26102;&#38388;&#24212;&#24471;&#21040;&#20248;&#21270;&#12290;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#21487;&#20197;&#22312;&#20302;&#25918;&#22823;&#20493;&#25968;&#19979;&#23545;&#26679;&#26412;&#36827;&#34892;&#25104;&#20687;&#65292;&#20174;&#32780;&#33410;&#30465;&#25195;&#25551;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#32455;&#30149;&#29702;&#20912;&#20923;&#20999;&#29255;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#33719;&#24471;&#26356;&#22909;&#30340;&#30072;&#21464;&#24230;&#37327;&#65292;&#32780;&#19981;&#26159;&#36861;&#27714;&#21487;&#33021;&#20250;&#25439;&#23475;&#20851;&#38190;&#35786;&#26029;&#20449;&#24687;&#30340;&#36924;&#30495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#25554;&#20540;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#30041;&#20851;&#38190;&#22270;&#20687;&#32454;&#33410;&#65292;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#26356;&#39640;&#30340;&#26435;&#37325;&#20998;&#37197;&#32473;&#37325;&#24314;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Histopathology plays a pivotal role in medical diagnostics. In contrast to preparing permanent sections for histopathology, a time-consuming process, preparing frozen sections is significantly faster and can be performed during surgery, where the sample scanning time should be optimized. Super-resolution techniques allow imaging the sample in lower magnification and sparing scanning time. In this paper, we present a new approach to super resolution for histopathological frozen sections, with focus on achieving better distortion measures, rather than pursuing photorealistic images that may compromise critical diagnostic information. Our deep-learning architecture focuses on learning the error between interpolated images and real images, thereby it generates high-resolution images while preserving critical image details, reducing the risk of diagnostic misinterpretation. This is done by leveraging the loss functions in the frequency domain, assigning higher weights to the reconstruction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#21482;&#26377;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#20123;&#26368;&#23567;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#31934;&#30830;&#30340;LDA&#25237;&#24433;&#21521;&#37327;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26368;&#23567;&#20449;&#24687;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;MILDA&#65289;&#27169;&#22411;&#19982;&#26377;&#30417;&#30563;&#30340;LDA&#27169;&#22411;&#30340;&#24615;&#33021;&#25509;&#36817;&#12290;</title><link>http://arxiv.org/abs/2310.11110</link><description>&lt;p&gt;
&#26368;&#23567;&#20449;&#24687;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;LDA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data. (arXiv:2310.11110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#21482;&#26377;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#20123;&#26368;&#23567;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#31934;&#30830;&#30340;LDA&#25237;&#24433;&#21521;&#37327;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26368;&#23567;&#20449;&#24687;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;MILDA&#65289;&#27169;&#22411;&#19982;&#26377;&#30417;&#30563;&#30340;LDA&#27169;&#22411;&#30340;&#24615;&#33021;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#26159;&#26368;&#21476;&#32769;&#19988;&#26368;&#27969;&#34892;&#30340;&#32447;&#24615;&#26041;&#27861;&#20043;&#19968;&#65292;&#29992;&#20110;&#26377;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22914;&#26524;&#26377;&#19968;&#20123;&#26368;&#23567;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#37027;&#20040;&#21487;&#20197;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#35745;&#31639;&#20986;LDA&#27169;&#22411;&#30340;&#31934;&#30830;&#25237;&#24433;&#21521;&#37327;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21482;&#38656;&#35201;&#20197;&#19979;&#19977;&#20010;&#20449;&#24687;&#20013;&#30340;&#20219;&#24847;&#19968;&#20010;&#21363;&#21487;&#35745;&#31639;LDA&#25237;&#24433;&#21521;&#37327;&#65292;&#22914;&#26524;&#21482;&#26377;&#26410;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#65306;&#65288;1&#65289;&#20004;&#20010;&#31867;&#21035;&#20013;&#20219;&#24847;&#19968;&#20010;&#30340;&#31867;&#21035;&#24179;&#22343;&#20540;&#65292;&#65288;2&#65289;&#20004;&#20010;&#31867;&#21035;&#24179;&#22343;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#65288;&#32463;&#36807;&#32553;&#25918;&#65289;&#65292;&#25110;&#32773;&#65288;3&#65289;&#31867;&#21035;&#21327;&#26041;&#24046;&#30697;&#38453;&#65288;&#32463;&#36807;&#32553;&#25918;&#65289;&#12290;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#35777;&#26126;&#36825;&#31181;&#26368;&#23567;&#20449;&#24687;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;MILDA&#65289;&#27169;&#22411;&#19982;&#26377;&#30417;&#30563;&#30340;LDA&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#25509;&#36817;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MILDA&#25237;&#24433;&#21521;&#37327;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#35745;&#31639;&#20986;&#26469;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#19982;LDA&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Discriminant Analysis (LDA) is one of the oldest and most popular linear methods for supervised classification problems. In this paper, we demonstrate that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. More precisely, we show that only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available: (1) the class average of one of the two classes, (2) the difference between both class averages (up to a scaling), or (3) the class covariance matrices (up to a scaling). These theoretical results are validated in numerical experiments, demonstrating that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model. Furthermore, we show that the MILDA projection vector can be computed in a closed form with a computational cost comparable to LD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLUs&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#30830;&#24615;&#39564;&#35777;&#35745;&#31639;&#19978;&#30028;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11104</link><description>&lt;p&gt;
ReLU-FNNs&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#65306;&#20351;&#29992;&#31934;&#30830;&#24615;&#39564;&#35777;&#35745;&#31639;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification. (arXiv:2310.11104v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLUs&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#30830;&#24615;&#39564;&#35777;&#35745;&#31639;&#19978;&#30028;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLUs&#65289;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#12290;&#23545;&#20110;&#30446;&#26631;&#36755;&#20837;&#65292;FNN&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#26159;&#34913;&#37327;&#20854;&#21487;&#38752;&#24615;&#30340;&#21512;&#29702;&#25351;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;&#25429;&#25417;ReLUs&#34892;&#20026;&#30340;&#20056;&#27861;&#22120;&#30340;&#26631;&#20934;&#36807;&#31243;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#26412;&#22320;Lipschitz&#24120;&#25968;&#30340;&#19978;&#30028;&#35745;&#31639;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#65288;SDP&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#20849;&#27491;&#20056;&#27861;&#22120;&#26469;&#20934;&#30830;&#25429;&#25417;ReLU&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#20110;&#19978;&#30028;&#35745;&#31639;&#30340;SDP&#30340;&#23545;&#20598;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24471;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#27979;&#35797;&#26469;&#30830;&#23450;&#35745;&#31639;&#19978;&#30028;&#30340;&#31934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#20010;ReLU&#30340;&#23454;&#38469;FNNs&#65292;&#36825;&#20123;SDP&#26159;&#26080;&#27861;&#35299;&#20915;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#20943;&#24207;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#36755;&#20837;&#36755;&#20986;&#23646;&#24615;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#25311;&#21512;&#30340;&#24471;&#20998;&#65292;&#35813;&#24471;&#20998;&#36890;&#36807;&#30417;&#27979;&#28145;&#24230;&#27169;&#22411;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#30340;&#36951;&#24536;&#36895;&#29575;&#26469;&#34913;&#37327;&#12290;&#23454;&#35777;&#32467;&#26524;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#22312;&#25968;&#25454;&#31354;&#38388;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#27867;&#21270;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#26377;&#21161;&#20110;&#28548;&#28165;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#25311;&#21512;&#30340;&#22256;&#24785;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.11094</link><description>&lt;p&gt;
&#37325;&#23398;&#24050;&#36951;&#24536;&#30693;&#35782;&#65306;&#20851;&#20110;&#36951;&#24536;&#65292;&#36807;&#25311;&#21512;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs. (arXiv:2310.11094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#25311;&#21512;&#30340;&#24471;&#20998;&#65292;&#35813;&#24471;&#20998;&#36890;&#36807;&#30417;&#27979;&#28145;&#24230;&#27169;&#22411;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#30340;&#36951;&#24536;&#36895;&#29575;&#26469;&#34913;&#37327;&#12290;&#23454;&#35777;&#32467;&#26524;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#22312;&#25968;&#25454;&#31354;&#38388;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#27867;&#21270;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#26377;&#21161;&#20110;&#28548;&#28165;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#25311;&#21512;&#30340;&#22256;&#24785;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#25311;&#21512;&#30340;&#19981;&#32463;&#24120;&#21457;&#29983;&#20196;&#20154;&#22256;&#24785;&#12290;&#29702;&#35770;&#39044;&#27979;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#22823;&#65292;&#23427;&#20204;&#26368;&#32456;&#24212;&#35813;&#21464;&#24471;&#36807;&#24230;&#36866;&#24212;&#26576;&#20010;&#29305;&#23450;&#30340;&#35757;&#32451;&#38598;&#65292;&#20174;&#32780;&#23548;&#33268;&#27867;&#21270;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#23454;&#35777;&#32467;&#26524;&#20013;&#65292;&#22686;&#21152;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#25110;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#20960;&#20046;&#20174;&#19981;&#25439;&#23475;&#27867;&#21270;&#12290;&#36825;&#26159;&#22240;&#20026;&#25105;&#20204;&#34913;&#37327;&#36807;&#25311;&#21512;&#30340;&#26041;&#24335;&#22826;&#36807;&#26377;&#38480;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#25311;&#21512;&#31243;&#24230;&#30340;&#24471;&#20998;&#65292;&#35813;&#24471;&#20998;&#30417;&#27979;&#28145;&#24230;&#27169;&#22411;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#30340;&#36951;&#24536;&#36895;&#29575;&#12290;&#36825;&#20010;&#20998;&#25968;&#34920;&#26126;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#27867;&#21270;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#22312;&#25968;&#25454;&#31354;&#38388;&#30340;&#26576;&#20123;&#21306;&#22495;&#20013;&#65292;&#27867;&#21270;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#24403;&#29992;&#36825;&#31181;&#26041;&#24335;&#27979;&#37327;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#25311;&#21512;&#21487;&#20197;&#22312;&#39564;&#35777;&#31934;&#24230;&#38477;&#20302;&#21644;&#19981;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#21457;&#29983;&#65292;&#24182;&#19988;&#21487;&#33021;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#24120;&#35265;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21487;&#33021;&#26377;&#21161;&#20110;&#28548;&#28165;&#21069;&#36848;&#30340;&#22256;&#24785;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infrequent occurrence of overfit in deep neural networks is perplexing. On the one hand, theory predicts that as models get larger they should eventually become too specialized for a specific training set, with ensuing decrease in generalization. In contrast, empirical results in image classification indicate that increasing the training time of deep models or using bigger models almost never hurts generalization. Is it because the way we measure overfit is too limited? Here, we introduce a novel score for quantifying overfit, which monitors the forgetting rate of deep models on validation data. Presumably, this score indicates that even while generalization improves overall, there are certain regions of the data space where it deteriorates. When thus measured, we show that overfit can occur with and without a decrease in validation accuracy, and may be more common than previously appreciated. This observation may help to clarify the aforementioned confusing picture. We use our obs
&lt;/p&gt;</description></item><item><title>SODA&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#36866;&#37197;&#22120;&#26469;&#25552;&#39640;&#27979;&#35797;&#25968;&#25454;&#36866;&#24212;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30001;&#20110;&#25968;&#25454;&#36866;&#37197;&#22120;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#29305;&#24449;&#25439;&#22351;&#24102;&#26469;&#30340;&#38480;&#21046;&#24615;&#25913;&#36827;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11093</link><description>&lt;p&gt;
SODA: &#35757;&#32451;&#24378;&#40065;&#26834;&#24615;&#30340;&#27979;&#35797;&#25968;&#25454;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
SODA: Robust Training of Test-Time Data Adaptors. (arXiv:2310.11093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11093
&lt;/p&gt;
&lt;p&gt;
SODA&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#36866;&#37197;&#22120;&#26469;&#25552;&#39640;&#27979;&#35797;&#25968;&#25454;&#36866;&#24212;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30001;&#20110;&#25968;&#25454;&#36866;&#37197;&#22120;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#29305;&#24449;&#25439;&#22351;&#24102;&#26469;&#30340;&#38480;&#21046;&#24615;&#25913;&#36827;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36866;&#24212;&#24615;&#27169;&#22411;&#21487;&#20197;&#20943;&#36731;&#30001;&#20110;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;&#26080;&#27861;&#35775;&#38382;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#65288;ZOO&#65289;&#26469;&#35757;&#32451;&#25968;&#25454;&#36866;&#37197;&#22120;&#65292;&#20197;&#23558;&#27979;&#35797;&#25968;&#25454;&#35843;&#25972;&#21040;&#36866;&#21512;&#24050;&#37096;&#32626;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;ZOO&#35757;&#32451;&#30340;&#25968;&#25454;&#36866;&#37197;&#22120;&#36890;&#24120;&#24102;&#26469;&#20102;&#26377;&#38480;&#30340;&#25913;&#36827;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36866;&#37197;&#22120;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#29305;&#24449;&#30340;&#25439;&#22351;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#36866;&#24212;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;ZOO&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38382;&#39064;&#30452;&#25509;&#26469;&#28304;&#20110;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#36866;&#37197;&#22120;&#30340;&#26799;&#24230;&#30340;&#19981;&#21487;&#38752;&#20272;&#35745;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20998;&#37197;&#32473;&#27979;&#35797;&#25968;&#25454;&#30340;&#20266;&#26631;&#31614;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20266;&#26631;&#31614;&#31283;&#20581;&#30340;&#25968;&#25454;&#36866;&#24212;&#65288;SODA&#65289;&#26469;&#25913;&#36827;&#25968;&#25454;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SODA&#21033;&#29992;&#39640;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#21487;&#38752;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting models deployed to test distributions can mitigate the performance degradation caused by distribution shifts. However, privacy concerns may render model parameters inaccessible. One promising approach involves utilizing zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. Nevertheless, the data adaptor trained with ZOO typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor. To address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data. Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation. Specifically, SODA leverages high-confidence predicted labels as reli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#39046;&#22495;&#19981;&#21464;&#30340;&#29992;&#25143;&#20852;&#36259;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#21644;&#27880;&#20837;&#19990;&#30028;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2310.11088</link><description>&lt;p&gt;
MeKB-Rec&#65306;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation. (arXiv:2310.11088v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#39046;&#22495;&#19981;&#21464;&#30340;&#29992;&#25143;&#20852;&#36259;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#21644;&#27880;&#20837;&#19990;&#30028;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22914;&#20309;&#38024;&#23545;&#26032;&#29992;&#25143;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#33616;&#65292;&#21363;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#19981;&#21464;&#30340;&#20852;&#36259;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#26032;&#22411;&#36328;&#39046;&#22495;&#25512;&#33616;&#33539;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#29992;&#25143;&#21644;&#23454;&#20307;&#36827;&#34892;&#20851;&#32852;&#65292;&#26500;&#24314;&#20102;&#29992;&#25143;&#20852;&#36259;&#30340;PKG&#65292;&#21363;MeKB&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#20102;MeKB&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;&#12290;&#20026;&#20102;&#39640;&#25928;&#21033;&#29992;CDR&#20013;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;MeKB-Rec&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#19990;&#30028;&#30693;&#35782;&#27880;&#20837;&#21040;&#23545;&#29992;&#25143;&#20852;&#36259;&#30340;&#29702;&#35299;&#20013;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#20102;&#35821;&#20041;&#26144;&#23556;&#65292;&#28040;&#38500;&#20102;&#23545;&#39046;&#22495;&#20869;&#29992;&#25143;&#34892;&#20026;&#30340;&#35201;&#27714;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing challenge in modern recommender systems to effectively make recommendations for new users, namely the cold-start problem. Cross-Domain Recommendation (CDR) has been proposed to address this challenge, but current ways to represent users' interests across systems are still severely limited. We introduce Personal Knowledge Graph (PKG) as a domain-invariant interest representation, and propose a novel CDR paradigm named MeKB-Rec. We first link users and entities in a knowledge base to construct a PKG of users' interests, named MeKB. Then we learn a semantic representation of MeKB for the cross-domain recommendation. To efficiently utilize limited training data in CDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into understanding users' interests. Beyond most existing systems, our approach builds a semantic mapping across domains which breaks the requirement for in-domain user behaviors, enabling zero-shot recommendations for new users in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21517;&#20026;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;FPbiLSTM&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#36827;&#34892;&#20132;&#36890;&#26041;&#24335;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#20256;&#24863;&#22120;&#25968;&#30446;&#21644;&#22788;&#29702;&#38656;&#27714;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24314;&#27169;&#36807;&#31243;&#65292;&#21516;&#26102;&#20860;&#39038;&#32467;&#26524;&#36136;&#37327;&#12290;&#36890;&#36807;&#25193;&#23637;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#20132;&#36890;&#26041;&#24335;&#20013;&#30340;&#26102;&#38388;&#31227;&#21160;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.11087</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#36827;&#34892;&#20132;&#36890;&#26041;&#24335;&#26816;&#27979;&#30340;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection. (arXiv:2310.11087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21517;&#20026;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;FPbiLSTM&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#36827;&#34892;&#20132;&#36890;&#26041;&#24335;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#20256;&#24863;&#22120;&#25968;&#30446;&#21644;&#22788;&#29702;&#38656;&#27714;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24314;&#27169;&#36807;&#31243;&#65292;&#21516;&#26102;&#20860;&#39038;&#32467;&#26524;&#36136;&#37327;&#12290;&#36890;&#36807;&#25193;&#23637;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#20132;&#36890;&#26041;&#24335;&#20013;&#30340;&#26102;&#38388;&#31227;&#21160;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#30340;&#24191;&#27867;&#21033;&#29992;&#20026;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#21487;&#29992;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#20256;&#24863;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#21033;&#20110;&#26816;&#27979;&#20132;&#36890;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#20174;&#26234;&#33021;&#25163;&#26426;&#25910;&#38598;&#30340;&#23569;&#37327;&#20256;&#24863;&#25968;&#25454;&#65292;&#23454;&#29616;&#23545;&#24120;&#35265;&#26085;&#24120;&#20986;&#34892;&#27963;&#21160;&#30340;&#20934;&#30830;&#27169;&#24335;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;FPbiLSTM&#65289;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#20943;&#23569;&#25152;&#38656;&#30340;&#20256;&#24863;&#22120;&#25968;&#30446;&#21644;&#22788;&#29702;&#38656;&#27714;&#65292;&#20174;&#32780;&#22312;&#19981;&#29306;&#29298;&#32467;&#26524;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;FPbiLSTM&#22312;&#29616;&#26377;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#27973;&#23618;&#20016;&#23500;&#24615;&#21644;&#28145;&#23618;&#29305;&#24449;&#30340;&#38887;&#24615;&#65292;&#25429;&#25417;&#21508;&#31181;&#20132;&#36890;&#26041;&#24335;&#20013;&#30340;&#26102;&#38388;&#31227;&#21160;&#27169;&#24335;&#12290;&#23427;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread utilization of smartphones has provided extensive availability to Inertial Measurement Units, providing a wide range of sensory data that can be advantageous for the detection of transportation modes. The objective of this study is to propose a novel end-to-end approach to effectively explore a reduced amount of sensory data collected from a smartphone to achieve accurate mode detection in common daily traveling activities. Our approach, called Feature Pyramid biLSTM (FPbiLSTM), is characterized by its ability to reduce the number of sensors required and processing demands, resulting in a more efficient modeling process without sacrificing the quality of the outcomes than the other current models. FPbiLSTM extends an existing CNN biLSTM model with the Feature Pyramid Network, leveraging the advantages of both shallow layer richness and deeper layer feature resilience for capturing temporal moving patterns in various transportation modes. It exhibits an excellent performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;CSG&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#21270;&#35757;&#32451;&#26041;&#27861;&#21644;&#36731;&#37327;&#32423;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25353;&#38590;&#26131;&#31243;&#24230;&#20248;&#21270;&#26679;&#26412;&#23637;&#31034;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11083</link><description>&lt;p&gt;
CSG: &#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CSG: Curriculum Representation Learning for Signed Graph. (arXiv:2310.11083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;CSG&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#21270;&#35757;&#32451;&#26041;&#27861;&#21644;&#36731;&#37327;&#32423;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25353;&#38590;&#26131;&#31243;&#24230;&#20248;&#21270;&#26679;&#26412;&#23637;&#31034;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#31526;&#21495;&#22270;&#23545;&#20110;&#24314;&#27169;&#20855;&#26377;&#27491;&#36127;&#36830;&#25509;&#30340;&#22797;&#26434;&#20851;&#31995;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#20854;&#20998;&#26512;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20043;&#21069;&#65292;&#27809;&#26377;&#38024;&#23545;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#19988;&#20256;&#32479;&#30340;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#27809;&#26377;&#35299;&#20915;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#23398;&#20064;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#20174;&#26131;&#21040;&#38590;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#23398;&#20064;&#12290;&#20026;&#20102;&#34913;&#37327;&#23398;&#20064;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;CSG&#65289;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22312;&#38142;&#25509;&#31526;&#21495;&#39044;&#27979;&#65288;AUC&#65289;&#26041;&#38754;&#23558;SGNN&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;23.7&#65285;&#65292;&#24182;&#19988;&#22312;AUC&#30340;&#26631;&#20934;&#24046;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#65292;&#26368;&#22810;&#20943;&#23569;&#20102;8.4&#12290;
&lt;/p&gt;
&lt;p&gt;
Signed graphs are valuable for modeling complex relationships with positive and negative connections, and Signed Graph Neural Networks (SGNNs) have become crucial tools for their analysis. However, prior to our work, no specific training plan existed for SGNNs, and the conventional random sampling approach did not address varying learning difficulties within the graph's structure. We proposed a curriculum-based training approach, where samples progress from easy to complex, inspired by human learning. To measure learning difficulty, we introduced a lightweight mechanism and created the Curriculum representation learning framework for Signed Graphs (CSG). This framework optimizes the order in which samples are presented to the SGNN model. Empirical validation across six real-world datasets showed impressive results, enhancing SGNN model accuracy by up to 23.7% in link sign prediction (AUC) and significantly improving stability with an up to 8.4 reduction in the standard deviation of AUC
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#21512;&#25104;&#33268;&#27515;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#27973;&#23618;&#22810;&#35270;&#22270;GNN&#21644;&#26631;&#20934;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;SL&#39044;&#27979;&#20013;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#30340;&#38750;SL&#22522;&#22240;&#20851;&#31995;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11082</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#21512;&#25104;&#33268;&#27515;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction. (arXiv:2310.11082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11082
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#21512;&#25104;&#33268;&#27515;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#27973;&#23618;&#22810;&#35270;&#22270;GNN&#21644;&#26631;&#20934;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;SL&#39044;&#27979;&#20013;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#30340;&#38750;SL&#22522;&#22240;&#20851;&#31995;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#33268;&#27515;&#65288;SL&#65289;&#39044;&#27979;&#26159;&#29992;&#20110;&#35782;&#21035;&#20004;&#20010;&#22522;&#22240;&#30340;&#20849;&#31361;&#21464;&#26159;&#21542;&#23548;&#33268;&#32454;&#32990;&#27515;&#20129;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#31574;&#30053;&#26159;&#23558;SL&#39044;&#27979;&#25277;&#35937;&#20026;&#22312;SL&#25968;&#25454;&#20013;&#30340;&#22522;&#22240;&#33410;&#28857;&#19978;&#30340;&#36793;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;GNNs&#23384;&#22312;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#30340;&#38750;SL&#22522;&#22240;&#20851;&#31995;&#20449;&#24687;&#26469;&#20419;&#36827;SL&#39044;&#27979;&#38754;&#20020;&#30528;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;SL&#39044;&#27979;&#65288;MSGT-SL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27973;&#23618;&#22810;&#35270;&#22270;GNN&#26469;&#20174;SL&#21644;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#33719;&#21462;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32534;&#30721;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#22522;&#22240;&#29305;&#24449;&#36755;&#20837;&#21040;&#26631;&#20934;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#20197;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20174;SL&#25968;&#25454;&#20013;&#30340;&#25209;&#37327;&#22522;&#22240;&#24320;&#22987;&#65292;&#37319;&#29992;&#24182;&#34892;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic lethality (SL) prediction is used to identify if the co-mutation of two genes results in cell death. The prevalent strategy is to abstract SL prediction as an edge classification task on gene nodes within SL data and achieve it through graph neural networks (GNNs). However, GNNs suffer from limitations in their message passing mechanisms, including over-smoothing and over-squashing issues. Moreover, harnessing the information of non-SL gene relationships within large-scale multi-omics data to facilitate SL prediction poses a non-trivial challenge. To tackle these issues, we propose a new multi-omics sampling-based graph transformer for SL prediction (MSGT-SL). Concretely, we introduce a shallow multi-view GNN to acquire local structural patterns from both SL and multi-omics data. Further, we input gene features that encode multi-view information into the standard self-attention to capture long-range dependencies. Notably, starting with batch genes from SL data, we adopt paral
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23454;&#35777;&#21457;&#29616;&#36807;&#25311;&#21512;&#26102;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#22522;&#20110;&#27492;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#20855;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11077</link><description>&lt;p&gt;
&#22242;&#32467;&#19968;&#33268;&#65306;&#21033;&#29992;&#20998;&#26102;&#19968;&#33268;&#24615;&#38598;&#21512;&#26469;&#23545;&#25239;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit. (arXiv:2310.11077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23454;&#35777;&#21457;&#29616;&#36807;&#25311;&#21512;&#26102;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#22522;&#20110;&#27492;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#20855;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25311;&#21512;&#21407;&#22987;&#22270;&#20687;&#19978;&#23450;&#20041;&#30340;&#38750;&#24120;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#36825;&#31181;&#24378;&#22823;&#23398;&#20064;&#22120;&#30340;&#32570;&#28857;&#26159;&#36807;&#25311;&#21512;&#35757;&#32451;&#38598;&#30340;&#21361;&#38505;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#36890;&#24120;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#35757;&#32451;&#30340;&#8220;&#25552;&#21069;&#20572;&#27490;&#8221;&#26469;&#36991;&#20813;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#23427;&#23545;&#25239;&#36807;&#25311;&#21512;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#24403;&#21457;&#29983;&#36807;&#25311;&#21512;&#26102;&#65292;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#36825;&#19968;&#28857;&#24050;&#22312;&#24120;&#29992;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#23454;&#35777;&#12290;&#22312;&#36825;&#20123;&#32467;&#26524;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#25239;&#36807;&#25311;&#21512;&#65292;&#20854;&#20013;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#20855;&#19968;&#33268;&#24615;&#30340;&#39044;&#27979;&#32467;&#26524;&#30830;&#23450;&#30340;&#12290;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24120;&#35268;&#38598;&#25104;&#36973;&#21463;&#36807;&#25311;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have become the method of choice for solving many image classification tasks, largely because they can fit very complex functions defined over raw images. The downside of such powerful learners is the danger of overfitting the training set, leading to poor generalization, which is usually avoided by regularization and "early stopping" of the training. In this paper, we propose a new deep network ensemble classifier that is very effective against overfit. We begin with the theoretical analysis of a regression model, whose predictions - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method designed to combat overfit, where the prediction is determined by the most consensual prediction throughout the training. On multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22270;&#23884;&#20837;&#26694;&#26550;&#65288;LDP-GE&#65289;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;LDP&#26426;&#21046;&#26469;&#20445;&#25252;&#33410;&#28857;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;&#20010;&#24615;&#21270;PageRank&#20316;&#20026;&#36817;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LDP-GE&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25240;&#34935;&#25928;&#26524;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11060</link><description>&lt;p&gt;
&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Graph Embedding. (arXiv:2310.11060v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22270;&#23884;&#20837;&#26694;&#26550;&#65288;LDP-GE&#65289;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;LDP&#26426;&#21046;&#26469;&#20445;&#25252;&#33410;&#28857;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;&#20010;&#24615;&#21270;PageRank&#20316;&#20026;&#36817;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LDP-GE&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25240;&#34935;&#25928;&#26524;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23884;&#20837;&#34987;&#35777;&#26126;&#26159;&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#28508;&#22312;&#34920;&#31034;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20294;&#22312;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#30340;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#24341;&#21457;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#21457;&#33021;&#28385;&#36275;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#22270;&#23884;&#20837;&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#20445;&#25252;&#22270;&#23884;&#20837;&#26694;&#26550;LDP-GE&#65292;&#29992;&#20110;&#20445;&#25252;&#33410;&#28857;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;LDP&#26426;&#21046;&#26469;&#28151;&#28102;&#33410;&#28857;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20010;&#24615;&#21270;PageRank&#20316;&#20026;&#36817;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;LDP-GE&#26694;&#26550;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25928;&#29992;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;LDP-GE&#22312;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph embedding has been demonstrated to be a powerful tool for learning latent representations for nodes in a graph. However, despite its superior performance in various graph-based machine learning tasks, learning over graphs can raise significant privacy concerns when graph data involves sensitive information. To address this, in this paper, we investigate the problem of developing graph embedding algorithms that satisfy local differential privacy (LDP). We propose LDP-GE, a novel privacy-preserving graph embedding framework, to protect the privacy of node data. Specifically, we propose an LDP mechanism to obfuscate node data and adopt personalized PageRank as the proximity measure to learn node representations. Then, we theoretically analyze the privacy guarantees and utility of the LDP-GE framework. Extensive experiments conducted over several real-world graph datasets demonstrate that LDP-GE achieves favorable privacy-utility trade-offs and significantly outperforms existing appr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#36873;&#25321;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#65292;&#24182;&#21033;&#29992;&#20256;&#36755;&#29109;&#26469;&#20272;&#35745;&#20449;&#24687;&#30340;&#22240;&#26524;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.11059</link><description>&lt;p&gt;
&#36890;&#36807;&#20256;&#36755;&#29109;&#36827;&#34892;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Selection via Transfer Entropy. (arXiv:2310.11059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#36873;&#25321;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#65292;&#24182;&#21033;&#29992;&#20256;&#36755;&#29109;&#26469;&#20272;&#35745;&#20449;&#24687;&#30340;&#22240;&#26524;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#32463;&#24120;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19981;&#20339;&#65292;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#29305;&#24449;&#36873;&#25321;&#26159;&#36873;&#25321;&#30456;&#20851;&#19988;&#38750;&#20887;&#20313;&#29305;&#24449;&#23376;&#38598;&#30340;&#36807;&#31243;&#65292;&#22240;&#27492;&#26159;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#19981;&#26816;&#26597;&#25152;&#36873;&#29305;&#24449;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#22240;&#26524;&#21457;&#29616;&#26088;&#22312;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#35782;&#21035;&#29305;&#24449;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#36873;&#25321;&#21644;&#22240;&#26524;&#21457;&#29616;&#20132;&#21449;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#21069;&#21521;&#21644;&#21518;&#21521;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#20256;&#36755;&#29109;&#26469;&#20272;&#35745;&#20449;&#24687;&#30340;&#22240;&#26524;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are designed to capture complex relationships between features. In this context, the high dimensionality of data often results in poor model performance, with the risk of overfitting. Feature selection, the process of selecting a subset of relevant and non-redundant features, is, therefore, an essential step to mitigate these issues. However, classical feature selection approaches do not inspect the causal relationship between selected features and target, which can lead to misleading results in real-world applications. Causal discovery, instead, aims to identify causal relationships between features with observational data. In this paper, we propose a novel methodology at the intersection between feature selection and causal discovery, focusing on time series. We introduce a new causal feature selection approach that relies on the forward and backward feature selection procedures and leverages transfer entropy to estimate the causal flow of information from
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2310.11049</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;6&#20013;&#30340;&#38750;&#32435;&#20219;&#21153;:&#27861;&#24459;&#35780;&#20272;&#26041;&#27861;&#35770;&#12290;(arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation. (arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#20027;&#35201;&#38598;&#20013;&#22312;&#19977;&#20010;&#23376;&#20219;&#21153;&#19978;&#65306;&#20219;&#21153;B&#30340;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(L-NER)&#65292;&#20219;&#21153;C1&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;(LJP)&#21644;&#20219;&#21153;C2&#30340;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;(CJPE)&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#20219;&#21153;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#24182;&#35814;&#32454;&#21576;&#29616;&#20102;&#32467;&#26524;&#65292;&#21253;&#25324;&#25968;&#25454;&#32479;&#35745;&#21644;&#26041;&#27861;&#35770;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20687;&#26412;&#30740;&#31350;&#20013;&#25152;&#28041;&#21450;&#30340;&#27861;&#24459;&#20219;&#21153;&#27491;&#22312;&#22240;&#33258;&#21160;&#21270;&#27861;&#24459;&#20998;&#26512;&#21644;&#25903;&#25345;&#30340;&#38656;&#27714;&#22686;&#21152;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#25490;&#34892;&#27036;&#19978;&#25253;&#21578;&#30340;&#20219;&#21153;B&#12289;&#20219;&#21153;C1&#21644;&#20219;&#21153;C2&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;15th&#12289;11th&#21644;1st&#30340;&#31454;&#20105;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.11046</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#30340;&#24555;&#36895;&#22270;&#32467;&#26500;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Fast Graph Condensation with Structure-based Neural Tangent Kernel. (arXiv:2310.11046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#26102;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21452;&#23618;&#20248;&#21270;&#26550;&#26500;&#26469;&#21387;&#32553;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21516;&#26679;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#25913;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#22312;&#21452;&#23618;&#20248;&#21270;&#30340;&#20869;&#24490;&#29615;&#20013;&#36845;&#20195;&#35757;&#32451;GNN&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65288;GC-SNTK&#65289;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#65288;SNTK&#65289;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#21270;&#30340;&#20302;&#31209;&#21644;&#20302;&#31934;&#24230;&#22240;&#24335;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30697;&#38453;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#23384;&#20648;&#21644;&#22788;&#29702;&#22823;&#22411;&#30697;&#38453;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.11028</link><description>&lt;p&gt;
&#30697;&#38453;&#21387;&#32553;&#36890;&#36807;&#38543;&#26426;&#20302;&#31209;&#20302;&#31934;&#24230;&#22240;&#24335;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Matrix Compression via Randomized Low Rank and Low Precision Factorization. (arXiv:2310.11028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11028
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21270;&#30340;&#20302;&#31209;&#21644;&#20302;&#31934;&#24230;&#22240;&#24335;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30697;&#38453;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#23384;&#20648;&#21644;&#22788;&#29702;&#22823;&#22411;&#30697;&#38453;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#32452;&#32455;&#21644;&#25805;&#20316;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30697;&#38453;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#20803;&#32032;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#23384;&#20648;&#21644;&#22788;&#29702;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#20351;&#29992;&#35201;&#27714;&#24456;&#39640;&#12290;&#34429;&#28982;&#36825;&#20123;&#30697;&#38453;&#38750;&#24120;&#22823;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#36817;&#20284;&#20302;&#31209;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#26469;&#33719;&#24471;&#20219;&#20309;&#30697;&#38453; $\mathbf{A}$ &#30340;&#20302;&#31209;&#20998;&#35299;&#65292;&#21363; $\mathbf{A} \approx \mathbf{L}\mathbf{R}$&#65292;&#20854;&#20013; $\mathbf{L}$ &#21644; $\mathbf{R}$ &#26159;&#20302;&#31209;&#22240;&#23376;&#12290;$\mathbf{L}$ &#21644; $\mathbf{R}$ &#20013;&#30340;&#20803;&#32032;&#24635;&#25968;&#21487;&#20197;&#26174;&#33879;&#23569;&#20110; $\mathbf{A}$ &#20013;&#30340;&#20803;&#32032;&#24635;&#25968;&#12290;&#27492;&#22806;&#65292;$\mathbf{L}$ &#21644; $\mathbf{R}$ &#30340;&#26465;&#30446;&#34987;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#26684;&#24335; $--$ &#36890;&#36807;&#32473;&#20986;&#20302;&#31209;&#21644;&#20302;&#31934;&#24230;&#22240;&#24335;&#20998;&#35299;&#26469;&#21387;&#32553; $\mathbf{A}$&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#39318;&#20808;&#35745;&#31639; $\mathbf$
&lt;/p&gt;
&lt;p&gt;
Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix $\mathbf{A}$ as $\mathbf{A} \approx \mathbf{L}\mathbf{R}$, where $\mathbf{L}$ and $\mathbf{R}$ are the low rank factors. The total number of elements in $\mathbf{L}$ and $\mathbf{R}$ can be significantly less than that in $\mathbf{A}$. Furthermore, the entries of $\mathbf{L}$ and $\mathbf{R}$ are quantized to low precision formats $--$ compressing $\mathbf{A}$ by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of $\mathb
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#27880;&#24847;&#21147;&#30340;&#22270;&#21464;&#25442;&#22120;&#65288;SignGT&#65289;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21508;&#31181;&#39057;&#29575;&#20449;&#24687;&#65292;&#23545;&#20110;&#23398;&#20064;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#30340;&#24322;&#36136;&#22270;&#31561;&#19981;&#21516;&#30340;&#22270;&#24418;&#38750;&#24120;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.11025</link><description>&lt;p&gt;
SignGT&#65306;&#22522;&#20110;&#24102;&#31526;&#21495;&#27880;&#24847;&#21147;&#30340;&#22270;&#21464;&#25442;&#22120;&#29992;&#20110;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning. (arXiv:2310.11025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#27880;&#24847;&#21147;&#30340;&#22270;&#21464;&#25442;&#22120;&#65288;SignGT&#65289;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21508;&#31181;&#39057;&#29575;&#20449;&#24687;&#65292;&#23545;&#20110;&#23398;&#20064;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#30340;&#24322;&#36136;&#22270;&#31561;&#19981;&#21516;&#30340;&#22270;&#24418;&#38750;&#24120;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22270;&#21464;&#25442;&#22120;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#19978;&#30340;&#22270;&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;&#22270;&#21464;&#25442;&#22120;&#30340;&#26680;&#24515;&#27169;&#22359;&#65289;&#35270;&#20026;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#19978;&#30340;&#20004;&#27493;&#32858;&#21512;&#25805;&#20316;&#12290;&#30001;&#20110;&#29983;&#25104;&#27491;&#27880;&#24847;&#20540;&#30340;&#29305;&#24615;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#31561;&#21516;&#20110;&#23545;&#25152;&#26377;&#33410;&#28857;&#36827;&#34892;&#24179;&#28369;&#25805;&#20316;&#65292;&#20445;&#30041;&#20102;&#20302;&#39057;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#25429;&#25417;&#20302;&#39057;&#20449;&#24687;&#22312;&#23398;&#20064;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21253;&#21547;&#39640;&#39057;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#30340;&#24322;&#36136;&#22270;&#31561;&#19981;&#21516;&#30340;&#22270;&#24418;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#31526;&#21495;&#27880;&#24847;&#21147;&#30340;&#22270;&#21464;&#25442;&#22120;&#65288;SignGT&#65289;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#22270;&#20013;&#30340;&#21508;&#31181;&#39057;&#29575;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SignGT&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#31526;&#21495;&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SignSA&#65289;&#65292;&#26681;&#25454;&#33410;&#28857;&#23545;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29983;&#25104;&#24102;&#31526;&#21495;&#27880;&#24847;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#20860;&#23481;Transformer&#26041;&#27861;&#65288;CoFormer&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#27599;&#20010;&#20010;&#20307;&#26679;&#26412;&#30340;&#32508;&#21512;&#26102;&#24207;&#20132;&#20114;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11022</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#20860;&#23481;Transformer
&lt;/p&gt;
&lt;p&gt;
Compatible Transformer for Irregularly Sampled Multivariate Time Series. (arXiv:2310.11022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#20860;&#23481;Transformer&#26041;&#27861;&#65288;CoFormer&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#27599;&#20010;&#20010;&#20307;&#26679;&#26412;&#30340;&#32508;&#21512;&#26102;&#24207;&#20132;&#20114;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20998;&#26512;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20808;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20551;&#35774;&#26102;&#38388;&#24207;&#21015;&#30340;&#27491;&#21017;&#23376;&#37319;&#26679;&#65292;&#20854;&#20013;&#30456;&#37051;&#27979;&#37327;&#20043;&#38388;&#30340;&#38388;&#38548;&#21644;&#26679;&#26412;&#25968;&#37327;&#20445;&#25345;&#19981;&#21464;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#24178;&#39044;&#65292;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#27491;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#19981;&#35268;&#21017;&#24615;&#65292;&#22240;&#20026;&#22312;&#26102;&#38388;&#21644;&#21464;&#37327;&#32500;&#24230;&#19978;&#23384;&#22312;&#38169;&#20301;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20860;&#23481;Transformer&#65288;CoFormer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#20026;&#27599;&#20010;&#20010;&#20307;&#26679;&#26412;&#23454;&#29616;&#20840;&#38754;&#30340;&#26102;&#24207;&#20132;&#20114;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;CoFormer&#20013;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#26679;&#26412;&#35270;&#20026;&#19968;&#20010;&#29420;&#29305;&#30340;&#21464;&#37327;-&#26102;&#38388;&#28857;&#65292;&#24182;&#21033;&#29992;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#22522;&#20110;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#30456;&#37051;&#28857;&#30340;&#36880;&#26679;&#26412;&#26102;&#24207;/&#20132;&#20114;&#29305;&#24449;&#12290;&#20511;&#21161;CoFormer&#20316;&#20026;&#26680;&#24515;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#19981;&#35268;&#21017;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
To analyze multivariate time series, most previous methods assume regular subsampling of time series, where the interval between adjacent measurements and the number of samples remain unchanged. Practically, data collection systems could produce irregularly sampled time series due to sensor failures and interventions. However, existing methods designed for regularly sampled multivariate time series cannot directly handle irregularity owing to misalignment along both temporal and variate dimensions. To fill this gap, we propose Compatible Transformer (CoFormer), a transformer-based encoder to achieve comprehensive temporal-interaction feature learning for each individual sample in irregular multivariate time series. In CoFormer, we view each sample as a unique variate-time point and leverage intra-variate/inter-variate attentions to learn sample-wise temporal/interaction features based on intra-variate/inter-variate neighbors. With CoFormer as the core, we can analyze irregularly sample
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#32852;&#37030;&#36172;&#21338;&#26426;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#22312;&#23436;&#20840;&#24322;&#27493;&#29615;&#22659;&#19979;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#39640;&#25928;&#36890;&#20449;&#25104;&#26412;&#30340;&#32852;&#37030;&#24322;&#27493;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11015</link><description>&lt;p&gt;
&#24322;&#27493;&#32852;&#37030;&#36172;&#21338;&#26426;&#20013;&#30340;&#32431;&#25506;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration in Asynchronous Federated Bandits. (arXiv:2310.11015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#32852;&#37030;&#36172;&#21338;&#26426;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#22312;&#23436;&#20840;&#24322;&#27493;&#29615;&#22659;&#19979;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#39640;&#25928;&#36890;&#20449;&#25104;&#26412;&#30340;&#32852;&#37030;&#24322;&#27493;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#65292;&#20854;&#20013;M&#20010;&#20195;&#29702;&#36890;&#36807;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#21512;&#20316;&#22320;&#30830;&#23450;&#26368;&#20339;&#25277;&#33218;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#24310;&#36831;&#21644;&#20195;&#29702;&#19981;&#21487;&#29992;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030;&#24322;&#27493;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#22266;&#23450;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#23436;&#20840;&#24322;&#27493;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#39640;&#25928;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#20174;&#32463;&#39564;&#19978;&#38416;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#21147;&#21644;&#36890;&#20449;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the federated pure exploration problem of multi-armed bandits and linear bandits, where $M$ agents cooperatively identify the best arm via communicating with the central server. To enhance the robustness against latency and unavailability of agents that are common in practice, we propose the first federated asynchronous multi-armed bandit and linear bandit algorithms for pure exploration with fixed confidence. Our theoretical analysis shows the proposed algorithms achieve near-optimal sample complexities and efficient communication costs in a fully asynchronous environment. Moreover, experimental results based on synthetic and real-world data empirically elucidate the effectiveness and communication cost-efficiency of the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#23398;&#39057;&#29575;&#26803;&#21644;&#21487;&#32534;&#31243;&#20809;&#35760;&#24518;&#30340;&#39640;&#20809;&#35889;&#20869;&#23384;&#35745;&#31639;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#31354;&#38388;&#22797;&#29992;&#21644;&#39057;&#29575;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#24182;&#34892;&#24615;&#12289;&#21487;&#32534;&#31243;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11014</link><description>&lt;p&gt;
&#22522;&#20110;&#20809;&#23398;&#39057;&#29575;&#26803;&#21644;&#21487;&#32534;&#31243;&#20809;&#35760;&#24518;&#30340;&#39640;&#20809;&#35889;&#20869;&#23384;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral In-Memory Computing with Optical Frequency Combs and Programmable Optical Memories. (arXiv:2310.11014v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#23398;&#39057;&#29575;&#26803;&#21644;&#21487;&#32534;&#31243;&#20809;&#35760;&#24518;&#30340;&#39640;&#20809;&#35889;&#20869;&#23384;&#35745;&#31639;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#31354;&#38388;&#22797;&#29992;&#21644;&#39057;&#29575;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#24182;&#34892;&#24615;&#12289;&#21487;&#32534;&#31243;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23545;&#24191;&#27867;&#30340;&#30697;&#38453;-&#21521;&#37327;&#20056;&#27861;&#36816;&#31639;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#36827;&#32780;&#23545;&#20256;&#32479;&#30340;&#20911;&#183;&#35834;&#20234;&#26364;&#35745;&#31639;&#26550;&#26500;&#30340;&#33021;&#21147;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#26367;&#20195;&#26041;&#26696;&#65292;&#22914;&#20869;&#23384;&#35745;&#31639;&#31995;&#32479;&#65292;&#20197;&#24320;&#21457;&#26356;&#24555;&#36895;&#12289;&#26356;&#33021;&#32791;&#26377;&#25928;&#30340;&#30828;&#20214;&#12290;&#23588;&#20854;&#26159;&#22522;&#20110;&#20809;&#23398;&#30340;&#35745;&#31639;&#31995;&#32479;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#23427;&#26377;&#26395;&#20197;&#26356;&#33021;&#32791;&#26377;&#25928;&#30340;&#26041;&#24335;&#22788;&#29702;&#30697;&#38453;-&#21521;&#37327;&#20056;&#27861;&#12290;&#23613;&#31649;&#21021;&#27493;&#32467;&#26524;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;&#24320;&#21457;&#20986;&#39640;&#24230;&#24182;&#34892;&#12289;&#21487;&#32534;&#31243;&#19988;&#21487;&#25193;&#23637;&#30340;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#33021;&#22815;&#19982;&#30005;&#23376;&#35745;&#31639;&#30828;&#20214;&#30456;&#23218;&#32654;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#20102;&#20809;&#23398;&#39057;&#29575;&#26803;&#30340;&#31354;&#38388;&#22797;&#29992;&#21644;&#39057;&#29575;&#22797;&#29992;&#30340;&#39640;&#20809;&#35889;&#20869;&#23384;&#35745;&#31639;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#31354;&#38388;&#20809;&#35843;&#21046;&#22120;&#20316;&#20026;&#21487;&#32534;&#31243;&#20809;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in machine learning across numerous industries have amplified the demand for extensive matrix-vector multiplication operations, thereby challenging the capacities of traditional von Neumann computing architectures. To address this, researchers are currently exploring alternatives such as in-memory computing systems to develop faster and more energy-efficient hardware. In particular, there is renewed interest in computing systems based on optics, which could potentially handle matrix-vector multiplication in a more energy-efficient way. Despite promising initial results, developing a highly parallel, programmable, and scalable optical computing system capable of rivaling electronic computing hardware still remains elusive. In this context, we propose a hyperspectral in-memory computing architecture that integrates space multiplexing with frequency multiplexing of optical frequency combs and uses spatial light modulators as a programmable optical memory, thereby bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11011</link><description>&lt;p&gt;
&#20174;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#34920;&#31034;&#21040;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#65306;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling. (arXiv:2310.11011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#23494;&#24230;&#20272;&#35745;&#21644;&#20174;&#26377;&#38480;&#26679;&#26412;&#20013;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#26681;&#26412;&#24615;&#30340;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#24046;&#21170;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#23558;&#22240;&#26524;&#29702;&#35770;&#34701;&#20837;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#20013;&#12290;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#25551;&#36848;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#23545;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#21644;&#26426;&#21046;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#22320;&#32467;&#21512;&#12290;&#22240;&#26524;&#27169;&#22411;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#20010;&#26377;&#30410;&#30340;&#23646;&#24615;&#65292;&#22914;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual ge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.11009</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#36335;&#39044;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#32463;&#20856;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#21551;&#21457;&#24335;&#24230;&#37327;&#34987;&#36873;&#25321;&#20026;&#22312;&#19982;&#38142;&#36335;&#24418;&#25104;&#30456;&#20851;&#30340;&#22522;&#26412;&#22240;&#32032;&#19978;&#19982;&#20043;&#30456;&#20851;&#33391;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#19968;&#31867;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#20248;&#21183;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#20197;&#21450;&#25429;&#25417;&#20505;&#36873;&#38142;&#36335;&#20013;&#33410;&#28857;&#20043;&#38388;&#20851;&#31995;&#30340;&#8220;&#23545;&#21521;&#32534;&#30721;&#8221;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#24050;&#32463;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#21521;&#32534;&#30721;&#24448;&#24448;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#22522;&#26412;&#22240;&#32032;&#26469;&#20998;&#31867;&#25152;&#26377;&#38142;&#36335;&#12290;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#22914;&#20309;&#27491;&#30830;&#20998;&#31867;&#21487;&#33021;&#30001;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#21508;&#31181;&#19981;&#21516;&#38142;&#36335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#32416;&#38169;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#26131;&#20986;&#38169;&#21333;&#35789;&#30340;&#20998;&#25968;&#24182;&#23558;&#20854;&#29992;&#20316;&#20808;&#39564;&#20998;&#24067;&#26469;&#25351;&#23548;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32416;&#38169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#26377;&#25928;&#65292;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11003</link><description>&lt;p&gt;
&#20026;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#32416;&#38169;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Correction Focused Language Model Training for Speech Recognition. (arXiv:2310.11003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#32416;&#38169;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#26131;&#20986;&#38169;&#21333;&#35789;&#30340;&#20998;&#25968;&#24182;&#23558;&#20854;&#29992;&#20316;&#20808;&#39564;&#20998;&#24067;&#26469;&#25351;&#23548;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32416;&#38169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#26377;&#25928;&#65292;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#24120;&#34987;&#29992;&#20110;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#12290;&#20256;&#32479;&#30340;LM&#35757;&#32451;&#26041;&#24335;&#23558;&#35821;&#26009;&#24211;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#24179;&#31561;&#23545;&#24453;&#65292;&#23548;&#33268;ASR&#24615;&#33021;&#30340;&#25913;&#36827;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32416;&#38169;&#22411;LM&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#20808;&#22788;&#29702;ASR&#26131;&#20986;&#38169;&#30340;&#21333;&#35789;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#21333;&#35789;&#32423;ASR&#26131;&#20986;&#38169;&#20998;&#25968;&#65292;&#34920;&#31034;ASR&#38169;&#35823;&#35782;&#21035;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23558;&#20854;&#24418;&#25104;&#19968;&#20010;&#20808;&#39564;&#21333;&#35789;&#20998;&#24067;&#20197;&#25351;&#23548;LM&#35757;&#32451;&#12290;&#20026;&#20102;&#22312;&#20165;&#26377;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32416;&#38169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#26131;&#20986;&#38169;&#20998;&#25968;&#39044;&#27979;&#22120;&#21644;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#24182;&#36827;&#34892;&#22810;&#20219;&#21153;&#24494;&#35843;&#12290;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;LM&#30456;&#27604;&#65292;&#32416;&#38169;&#22411;&#35757;&#32451;&#22312;&#36275;&#22815;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#36798;&#21040;&#30456;&#23545;5.5%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#31354;&#38388;&#23616;&#37096;&#22825;&#27668;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#20301;&#32622;&#21644;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#21019;&#24314;&#39640;&#20998;&#36776;&#29575;&#30340;&#22825;&#27668;&#27169;&#22411;&#65292;&#24182;&#30417;&#27979;&#22825;&#27668;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#24182;&#23454;&#26102;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#19968;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2310.11001</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#31354;&#38388;&#23616;&#37096;&#22825;&#27668;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques. (arXiv:2310.11001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#31354;&#38388;&#23616;&#37096;&#22825;&#27668;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#20301;&#32622;&#21644;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#21019;&#24314;&#39640;&#20998;&#36776;&#29575;&#30340;&#22825;&#27668;&#27169;&#22411;&#65292;&#24182;&#30417;&#27979;&#22825;&#27668;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#24182;&#23454;&#26102;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#19968;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#31354;&#38388;&#23616;&#37096;&#22825;&#27668;&#39044;&#27979;&#23545;&#20110;&#20892;&#19994;&#21040;&#28798;&#23475;&#31649;&#29702;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#32593;&#32476;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#32467;&#21512;&#31354;&#38388;&#23616;&#37096;&#22825;&#27668;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#31354;&#38388;&#20998;&#24067;&#20294;&#30456;&#23545;&#25509;&#36817;&#30340;&#20301;&#32622;&#21644;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#21019;&#24314;&#39640;&#20998;&#36776;&#29575;&#30340;&#22825;&#27668;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#30701;&#26399;&#12289;&#23616;&#37096;&#30340;&#22825;&#27668;&#26465;&#20214;&#65292;&#22914;&#28201;&#24230;&#12289;&#21387;&#21147;&#21644;&#28287;&#24230;&#12290;&#36890;&#36807;&#30417;&#27979;&#36825;&#20123;&#20301;&#32622;&#30340;&#22825;&#27668;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#24182;&#23454;&#26102;&#26377;&#25928;&#22320;&#26816;&#27979;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#30340;&#22825;&#27668;&#27169;&#24335;&#65292;&#25552;&#20379;&#21450;&#26102;&#30340;&#35686;&#25253;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and timely hyperlocal weather predictions are essential for various applications, ranging from agriculture to disaster management. In this paper, we propose a novel approach that combines hyperlocal weather prediction and anomaly detection using IoT sensor networks and advanced machine learning techniques. Our approach leverages data from multiple spatially-distributed yet relatively close locations and IoT sensors to create high-resolution weather models capable of predicting short-term, localized weather conditions such as temperature, pressure, and humidity. By monitoring changes in weather parameters across these locations, our system is able to enhance the spatial resolution of predictions and effectively detect anomalies in real-time. Additionally, our system employs unsupervised learning algorithms to identify unusual weather patterns, providing timely alerts. Our findings indicate that this system has the potential to enhance decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.10998</link><description>&lt;p&gt;
&#20351;&#29992;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation. (arXiv:2310.10998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22270;&#30340;&#35268;&#27169;&#20351;&#24471;GNNs&#30340;&#23454;&#26102;&#25512;&#35770;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#21487;&#25193;&#23637;GNNs&#21033;&#29992;&#32447;&#24615;&#20256;&#25773;&#23545;&#29305;&#24449;&#36827;&#34892;&#39044;&#22788;&#29702;&#24182;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#35770;&#36807;&#31243;&#65292;&#20294;&#22312;&#23545;&#26410;&#30693;&#33410;&#28857;&#36827;&#34892;&#25512;&#35770;&#26102;&#20173;&#28982;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#38656;&#35201;&#24050;&#30693;&#19988;&#22266;&#23450;&#30340;&#22270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#36825;&#31181;&#24402;&#32435;&#35774;&#32622;&#19979;&#30340;&#21487;&#25193;&#23637;GNNs&#25512;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#34917;&#20607;&#25439;&#22833;&#30340;&#31934;&#24230;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20607;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20801;&#35768;&#20256;&#25773;&#30340;&#23618;&#25968;&#36229;&#36807;&#25152;&#36873;&#25321;&#30340;&#28145;&#24230;&#65292;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23398;&#26415;&#12289;&#20154;&#21475;&#32479;&#35745;&#12289;&#31038;&#20250;&#32463;&#27982;&#21644;&#23439;&#35266;&#32463;&#27982;&#31561;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#22823;&#23398;&#36749;&#23398;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#23398;&#26415;&#25968;&#25454;&#26159;&#23545;&#27169;&#22411;&#24615;&#33021;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10987</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#23398;&#29983;&#20250;&#36749;&#23398;&#65311;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22823;&#23398;&#36864;&#23398;&#39044;&#27979;&#21644;&#30456;&#20851;&#22240;&#32032;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques. (arXiv:2310.10987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23398;&#26415;&#12289;&#20154;&#21475;&#32479;&#35745;&#12289;&#31038;&#20250;&#32463;&#27982;&#21644;&#23439;&#35266;&#32463;&#27982;&#31561;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#22823;&#23398;&#36749;&#23398;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#23398;&#26415;&#25968;&#25454;&#26159;&#23545;&#27169;&#22411;&#24615;&#33021;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27605;&#19994;&#29575;&#21644;&#36749;&#23398;&#29575;&#19968;&#30452;&#26159;&#25945;&#32946;&#26426;&#26500;&#21644;&#23398;&#29983;&#20005;&#32899;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#39640;&#36749;&#23398;&#29575;&#23545;&#20010;&#20154;&#23398;&#29983;&#21644;&#25945;&#32946;&#26426;&#26500;&#37117;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#23398;&#26415;&#12289;&#20154;&#21475;&#32479;&#35745;&#12289;&#31038;&#20250;&#32463;&#27982;&#21644;&#23439;&#35266;&#32463;&#27982;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#20102;&#22823;&#23398;&#36749;&#23398;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#30456;&#20851;&#22240;&#32032;&#20998;&#26512;&#65292;&#20197;&#20998;&#26512;&#21738;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#27605;&#19994;&#21644;&#36749;&#23398;&#29366;&#24577;&#26041;&#38754;&#30340;&#24615;&#33021;&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;&#36825;&#20123;&#29305;&#24449;&#34987;&#29992;&#20110;&#35757;&#32451;&#22235;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#20197;&#30830;&#23450;&#23398;&#29983;&#26159;&#21542;&#20250;&#27605;&#19994;&#25110;&#36749;&#23398;&#12290;&#20998;&#31867;&#22120;&#22312;&#39044;&#27979;&#36749;&#23398;&#29366;&#24577;&#26041;&#38754;&#30340;&#25972;&#20307;&#24615;&#33021;&#26174;&#31034;&#20986;&#24179;&#22343;ROC-AUC&#24471;&#20998;&#20026;0.935&#12290;&#22312;&#25490;&#38500;&#20102;&#25152;&#26377;&#19982;&#23398;&#26415;&#30456;&#20851;&#30340;&#29305;&#24449;&#21518;&#65292;&#24179;&#22343;ROC-AUC&#24471;&#20998;&#20174;0.935&#38477;&#33267;0.811&#65292;&#34920;&#26126;&#23398;&#26415;&#25968;&#25454;&#26159;&#23545;&#27169;&#22411;&#24615;&#33021;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graduation and dropout rates have always been a serious consideration for educational institutions and students. High dropout rates negatively impact both the lives of individual students and institutions. To address this problem, this study examined university dropout prediction using academic, demographic, socioeconomic, and macroeconomic data types. Additionally, we performed associated factor analysis to analyze which type of data would be most influential on the performance of machine learning models in predicting graduation and dropout status. These features were used to train four binary classifiers to determine if students would graduate or drop out. The overall performance of the classifiers in predicting dropout status had an average ROC-AUC score of 0.935. The data type most influential to the model performance was found to be academic data, with the average ROC-AUC score dropping from 0.935 to 0.811 when excluding all academic-related features from the data set. Preliminary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20272;&#35745;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#35797;&#22270;&#24357;&#21512;&#29616;&#26377;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#20013;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#33267;&#20219;&#24847;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#20849;&#36717;&#21464;&#25442;&#28388;&#27874;&#22120; (CTF)&#65292;&#24182;&#25552;&#20986;&#20102;&#20854;&#38598;&#21512;&#36817;&#20284;&#29256;&#26412; (ECTF)&#12290;</title><link>http://arxiv.org/abs/2310.10976</link><description>&lt;p&gt;
&#31934;&#30830;&#38750;&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Exact nonlinear state estimation. (arXiv:2310.10976v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20272;&#35745;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#35797;&#22270;&#24357;&#21512;&#29616;&#26377;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#20013;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#33267;&#20219;&#24847;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#20849;&#36717;&#21464;&#25442;&#28388;&#27874;&#22120; (CTF)&#65292;&#24182;&#25552;&#20986;&#20102;&#20854;&#38598;&#21512;&#36817;&#20284;&#29256;&#26412; (ECTF)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#22823;&#22810;&#25968;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22522;&#20110;&#39640;&#26031;&#20551;&#35774;&#12290;&#23613;&#31649;&#36825;&#20123;&#20551;&#35774;&#26041;&#20415;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#20250;&#23548;&#33268;&#20998;&#26512;&#20559;&#24046;&#21644;&#21518;&#32493;&#39044;&#27979;&#24694;&#21270;&#12290;&#38750;&#21442;&#25968;&#12289;&#22522;&#20110;&#31890;&#23376;&#30340;&#25968;&#25454;&#21516;&#21270;&#31639;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#22312;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#20173;&#38754;&#20020;&#25805;&#20316;&#19978;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20511;&#37492;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35797;&#22270;&#24357;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#20013;&#29616;&#26377;&#24046;&#36317;&#30340;&#26032;&#30340;&#38750;&#32447;&#24615;&#20272;&#35745;&#29702;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20849;&#36717;&#21464;&#25442;&#28388;&#27874;&#22120; (CTF)&#65292;&#24182;&#26174;&#31034;&#20854;&#33021;&#22815;&#25512;&#24191;&#33267;&#20219;&#24847;&#38750;&#39640;&#26031;&#20998;&#24067;&#12290;&#26032;&#30340;&#28388;&#27874;&#22120;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#20363;&#22914;&#33021;&#22815;&#20445;&#30041;&#20808;&#21069;&#29366;&#24577;&#20013;&#30340;&#32479;&#35745;&#20851;&#31995;&#24182;&#25910;&#25947;&#33267;&#39640;&#31934;&#24230;&#30340;&#35266;&#27979;&#20540;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#26032;&#29702;&#35770;&#30340;&#19968;&#20010;&#38598;&#21512;&#36817;&#20284; (ECTF)&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20803;&#23398;&#20064;&#22522;&#20934;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#25110;&#19982;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.10971</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Meta-Learning. (arXiv:2310.10971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20803;&#23398;&#20064;&#22522;&#20934;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#25110;&#19982;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26080;&#38656;&#24494;&#35843;&#23601;&#33021;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#26816;&#27979;&#26032;&#23545;&#35937;&#30340;&#35270;&#35273;&#27169;&#22411;&#23578;&#26410;&#33021;&#22815;&#22797;&#21046;&#36825;&#31181;&#33021;&#21147;&#65292;&#32780;&#26159;&#34920;&#29616;&#31967;&#31957;&#25110;&#38656;&#35201;&#23545;&#31867;&#20284;&#23545;&#35937;&#36827;&#34892;&#20803;&#35757;&#32451;&#21644;/&#25110;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#32780;&#26080;&#38656;&#24494;&#35843;&#26469;&#27169;&#20223;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#31867;&#20284;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23558;&#20803;&#23398;&#20064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#24050;&#30693;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857;&#21644;&#26410;&#30693;&#26631;&#31614;&#30340;&#27979;&#35797;&#25968;&#25454;&#28857;&#19978;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#22312;11&#20010;&#20803;&#23398;&#20064;&#22522;&#20934;&#20013;&#30340;8&#20010;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861; - &#26080;&#38656;&#20803;&#35757;&#32451;&#25110;&#24494;&#35843; - &#36229;&#36807;&#25110;&#19982;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#32463;&#36807;&#20803;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;P&gt;M&gt;F&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P&gt;M&gt;F, which is meta-trained on these benchmarks.
&lt;/p&gt;</description></item><item><title>SD-PINN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;&#31354;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#25968;&#65292;&#26080;&#38656;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#31354;&#38388;&#21464;&#21270;&#20302;&#31209;&#20551;&#35774;&#24674;&#22797;&#27809;&#26377;&#21487;&#29992;&#27979;&#37327;&#25968;&#25454;&#30340;&#20301;&#32622;&#30340;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.10970</link><description>&lt;p&gt;
SD-PINN: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31354;&#38388;&#30456;&#20851;&#20559;&#24494;&#20998;&#26041;&#31243;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery. (arXiv:2310.10970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10970
&lt;/p&gt;
&lt;p&gt;
SD-PINN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#24674;&#22797;&#31354;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#25968;&#65292;&#26080;&#38656;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#31354;&#38388;&#21464;&#21270;&#20302;&#31209;&#20551;&#35774;&#24674;&#22797;&#27809;&#26377;&#21487;&#29992;&#27979;&#37327;&#25968;&#25454;&#30340;&#20301;&#32622;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#33021;&#22815;&#30452;&#25509;&#20174;&#29289;&#29702;&#27979;&#37327;&#20013;&#24674;&#22797;&#22312;&#25972;&#20010;&#31354;&#38388;&#22495;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#31995;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;&#65288;SD-PINN&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#24674;&#22797;&#31354;&#38388;&#30456;&#20851;&#30340;PDE&#31995;&#25968;&#65292;&#28040;&#38500;&#20102;&#23545;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#29702;&#19987;&#19994;&#30693;&#35782;&#30340;&#35201;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30001;&#20110;&#21152;&#20837;&#20102;&#29289;&#29702;&#32422;&#26463;&#32780;&#23545;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#23427;&#36824;&#33021;&#22815;&#23558;PDE&#31995;&#25968;&#30340;&#31354;&#38388;&#21464;&#21270;&#20302;&#31209;&#20551;&#35774;&#32435;&#20837;&#32771;&#34385;&#65292;&#20174;&#32780;&#24674;&#22797;&#27809;&#26377;&#21487;&#29992;&#27979;&#37327;&#25968;&#25454;&#30340;&#20301;&#32622;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physics-informed neural network (PINN) is capable of recovering partial differential equation (PDE) coefficients that remain constant throughout the spatial domain directly from physical measurements. In this work, we propose a spatially dependent physics-informed neural network (SD-PINN), which enables the recovery of coefficients in spatially-dependent PDEs using a single neural network, eliminating the requirement for domain-specific physical expertise. The proposed method exhibits robustness to noise owing to the incorporation of physical constraints. It can also incorporate the low-rank assumption of the spatial variation for the PDE coefficients to recover the coefficients at locations without available measurements.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24310;&#36831;&#30340;&#26080;&#36870;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27714;&#35299;&#32477;&#23545;&#20540;&#26041;&#31243;&#65292;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#24615;&#21644;&#35299;&#20915;&#19968;&#31867;&#29305;&#27530;AVE&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10965</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#27714;&#35299;&#32477;&#23545;&#20540;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
The neural network models with delays for solving absolute value equations. (arXiv:2310.10965v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24310;&#36831;&#30340;&#26080;&#36870;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27714;&#35299;&#32477;&#23545;&#20540;&#26041;&#31243;&#65292;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#24615;&#21644;&#35299;&#20915;&#19968;&#31867;&#29305;&#27530;AVE&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28151;&#21512;&#24310;&#36831;&#30340;&#26080;&#36870;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27714;&#35299;&#32477;&#23545;&#20540;&#26041;&#31243;(AVE) Ax -|x| - b = 0&#65292;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#31181;&#20855;&#26377;&#31163;&#25955;&#24310;&#36831;&#30340;&#26080;&#36870;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#29305;&#20363;&#12290;&#36890;&#36807;&#20351;&#29992;Lyapunov-Krasovskii&#29702;&#35770;&#21644;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#65288;LMI&#65289;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25152;&#24320;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;AVE&#30340;&#35299;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#24615;&#12290;&#19982;&#29616;&#26377;&#29992;&#20110;&#27714;&#35299;AVE&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#35299;&#20915;&#19968;&#31867;&#20855;&#26377;&#8741;A^ -1 &#8741;&gt;1- &#30340;AVE&#30340;&#33021;&#21147;&#12290;&#32473;&#20986;&#20102;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20004;&#20010;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An inverse-free neural network model with mixed delays is proposed for solving the absolute value equation (AVE) $Ax -|x| - b =0$, which includes an inverse-free neural network model with discrete delay as a special case. By using the Lyapunov-Krasovskii theory and the linear matrix inequality (LMI) method, the developed neural network models are proved to be exponentially convergent to the solution of the AVE. Compared with the existing neural network models for solving the AVE, the proposed models feature the ability of solving a class of AVE with $\|A^{-1}\|&gt;1$. Numerical simulations are given to show the effectiveness of the two delayed neural network models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10958</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction. (arXiv:2310.10958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#26377;&#25928;&#30340;DNN&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;DNN&#35757;&#32451;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;DNN&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36981;&#24490;&#26576;&#31181;&#35268;&#24459;&#30340;&#35266;&#23519;&#65292;&#21457;&#29616;&#20102;&#21442;&#25968;&#39044;&#27979;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;DNN&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#32423;&#12289;&#30828;&#20214;&#38480;&#21046;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23545;&#22122;&#22768;&#23481;&#24525;&#24230;&#30340;&#29305;&#24615;&#65292;&#37319;&#29992;&#21442;&#25968;&#32447;&#24615;&#39044;&#27979;&#65288;PLP&#65289;&#26041;&#27861;&#26469;&#36827;&#34892;DNN&#21442;&#25968;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#22312;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#39592;&#26550;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#19982;&#27491;&#24120;&#30340;&#35757;&#32451;&#26041;&#24335;&#30456;&#27604;&#65292;&#36890;&#36807;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have achieved remarkable success in various fields, including computer vision and natural language processing. However, training an effective DNN model still poses challenges. This paper aims to propose a method to optimize the training effectiveness of DNN, with the goal of improving model performance. Firstly, based on the observation that the DNN parameters change in certain laws during training process, the potential of parameter prediction for improving model training efficiency and performance is discovered. Secondly, considering the magnitude of DNN model parameters, hardware limitations and characteristics of Stochastic Gradient Descent (SGD) for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to perform DNN parameter prediction. Finally, validations are carried out on some representative backbones. Experiment results show that compare to the normal training ways, under the same training conditions and epochs, by employing propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#25928;&#26524;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#38750;&#30456;&#20851;&#32500;&#24230;&#20135;&#29983;"&#28322;&#20986;"&#25928;&#24212;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25968;&#25454;&#38598;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.10955</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#25928;&#26524;&#30340;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A State-Vector Framework for Dataset Effects. (arXiv:2310.10955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#25928;&#26524;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#38750;&#30456;&#20851;&#32500;&#24230;&#20135;&#29983;"&#28322;&#20986;"&#25928;&#24212;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25968;&#25454;&#38598;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#31995;&#32479;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#20197;&#20415;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#23558;&#29702;&#24819;&#21270;&#25506;&#27979;&#27979;&#35797;&#32467;&#26524;&#20316;&#20026;&#21521;&#37327;&#31354;&#38388;&#30340;&#22522;&#30784;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#29420;&#31435;&#21644;&#20114;&#21160;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#25928;&#26524;&#26159;&#29305;&#24449;&#24615;&#30340;&#65292;&#24182;&#19988;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#20123;"&#28322;&#20986;"&#25928;&#24212;&#65306;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#22312;&#30475;&#20284;&#19982;&#39044;&#26399;&#20219;&#21153;&#26080;&#20851;&#30340;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#20026;&#31995;&#32479;&#22320;&#29702;&#35299;&#25968;&#25454;&#38598;&#25928;&#26524;&#65292;&#36825;&#26159;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and interacting datasets. We show that the significant effects of some commonly-used language understanding datasets are characteristic and are concentrated on a few linguistic dimensions. Additionally, we observe some ``spill-over'' effects: the datasets could impact the models along dimensions that may seem unrelated to the intended tasks. Our state-vector framework paves the way for a systematic understanding of the dataset effects, a crucial component in responsible and robust model development.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#22270;&#30028;&#38480;&#30340;&#35757;&#32451;&#22823;&#22411;&#36755;&#20837;&#22270;&#30340;&#37319;&#26679;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23567;&#26679;&#26412;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#19982;&#25972;&#20010;&#22270;&#35757;&#32451;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#36825;&#20026;&#20351;&#29992;&#37319;&#26679;&#35757;&#32451;GNN&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12289;&#36229;&#21442;&#25968;&#21644;&#37319;&#26679;&#31639;&#27861;&#26041;&#38754;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.10953</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#22270;&#30028;&#38480;&#30340;&#37319;&#26679;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Local Graph Limits Perspective on Sampling-Based GNNs. (arXiv:2310.10953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#22270;&#30028;&#38480;&#30340;&#35757;&#32451;&#22823;&#22411;&#36755;&#20837;&#22270;&#30340;&#37319;&#26679;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23567;&#26679;&#26412;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#19982;&#25972;&#20010;&#22270;&#35757;&#32451;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#36825;&#20026;&#20351;&#29992;&#37319;&#26679;&#35757;&#32451;GNN&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12289;&#36229;&#21442;&#25968;&#21644;&#37319;&#26679;&#31639;&#27861;&#26041;&#38754;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#36755;&#20837;&#22270;&#20013;&#30340;&#23567;&#22411;&#22266;&#23450;&#22823;&#23567;&#30340;&#37319;&#26679;&#23376;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;GNN&#65292;&#22914;GraphSAGE&#21644;FastGCN&#12290;&#20511;&#21161;&#22270;&#30340;&#23616;&#37096;&#30028;&#38480;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#36755;&#20837;&#22270;&#30340;&#23567;&#26679;&#26412;&#36827;&#34892;&#37319;&#26679;&#35757;&#32451;&#30340;&#21442;&#25968;&#19982;&#22312;&#25972;&#20010;&#22270;&#19978;&#35757;&#32451;&#30456;&#21516;&#32467;&#26500;&#30340;&#21442;&#25968;&#22312;&#949;-&#37051;&#22495;&#20869;&#12290;&#25105;&#20204;&#20197;&#949;&#30340;&#20989;&#25968;&#25512;&#23548;&#20986;&#26679;&#26412;&#25968;&#37327;&#12289;&#22270;&#30340;&#22823;&#23567;&#21644;&#35757;&#32451;&#27493;&#39588;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35757;&#32451;GNN&#26102;&#20351;&#29992;&#37319;&#26679;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#23427;&#20204;&#36824;&#26263;&#31034;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#30340;&#23567;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#19994;&#32773;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35782;&#21035;&#21644;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12289;&#36229;&#21442;&#25968;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework for training Graph Neural Networks (GNNs) on large input graphs via training on small, fixed-size sampled subgraphs. This framework is applicable to a wide range of models, including popular sampling-based GNNs, such as GraphSAGE and FastGCN. Leveraging the theory of graph local limits, we prove that, under mild assumptions, parameters learned from training sampling-based GNNs on small samples of a large input graph are within an $\epsilon$-neighborhood of the outcome of training the same architecture on the whole graph. We derive bounds on the number of samples, the size of the graph, and the training steps required as a function of $\epsilon$. Our results give a novel theoretical understanding for using sampling in training GNNs. They also suggest that by training GNNs on small samples of the input graph, practitioners can identify and select the best models, hyperparameters, and sampling algorithms more efficiently. We empirically illustrate our re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#30001;&#38750;&#36127;&#38646;&#33192;&#32960;&#36830;&#32493;&#36793;&#26435;&#32452;&#25104;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#25311;&#22269;&#38469;&#36152;&#26131;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#33410;&#28857;&#20449;&#24687;&#21644;&#21160;&#24577;&#25928;&#24212;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#20110;&#31038;&#21306;&#26631;&#31614;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;&#19968;&#20010;&#39640;&#25928;&#30340;&#20004;&#27493;&#31639;&#27861;&#34987;&#24320;&#21457;&#29992;&#20110;&#20272;&#35745;&#21327;&#21464;&#25928;&#24212;&#21644;&#31038;&#21306;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.10952</link><description>&lt;p&gt;
&#38480;&#21046;&#30340;Tweedie&#38543;&#26426;&#22359;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Restricted Tweedie Stochastic Block Models. (arXiv:2310.10952v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10952
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#30001;&#38750;&#36127;&#38646;&#33192;&#32960;&#36830;&#32493;&#36793;&#26435;&#32452;&#25104;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#25311;&#22269;&#38469;&#36152;&#26131;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#33410;&#28857;&#20449;&#24687;&#21644;&#21160;&#24577;&#25928;&#24212;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#20110;&#31038;&#21306;&#26631;&#31614;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;&#19968;&#20010;&#39640;&#25928;&#30340;&#20004;&#27493;&#31639;&#27861;&#34987;&#24320;&#21457;&#29992;&#20110;&#20272;&#35745;&#21327;&#21464;&#25928;&#24212;&#21644;&#31038;&#21306;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22359;&#27169;&#22411; (SBM) &#26159;&#22312;&#32593;&#32476;&#20013;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#30340;&#24191;&#27867;&#24212;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#30001;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SBM&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#30001;&#38750;&#36127;&#30340;&#38646;&#33192;&#32960;&#36830;&#32493;&#36793;&#26435;&#32452;&#25104;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#20026;&#20102;&#27169;&#25311;&#22269;&#38469;&#36152;&#26131;&#32593;&#32476;&#65292;&#20854;&#20013;&#36793;&#26435;&#34920;&#31034;&#22269;&#23478;&#20043;&#38388;&#30340;&#36152;&#26131;&#20215;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38480;&#21046;Tweedie&#20998;&#24067;&#30340;&#21019;&#26032;SBM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#33410;&#28857;&#20449;&#24687;&#65292;&#22914;&#22269;&#23478;&#20043;&#38388;&#30340;&#22320;&#29702;&#36317;&#31163;&#65292;&#24182;&#32771;&#34385;&#20854;&#23545;&#36793;&#26435;&#30340;&#21160;&#24577;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#33410;&#28857;&#25968;&#36275;&#22815;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20272;&#35745;&#36825;&#20010;&#21327;&#21464;&#25928;&#24212;&#26102;&#65292;&#21487;&#20197;&#29420;&#31435;&#20110;&#27599;&#20010;&#33410;&#28857;&#30340;&#31038;&#21306;&#26631;&#31614;&#65292;&#22312;&#35745;&#31639;&#25105;&#20204;&#27169;&#22411;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#26102;&#12290;&#36825;&#20010;&#32467;&#26524;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#19968;&#31181;&#39640;&#25928;&#30340;&#20004;&#27493;&#31639;&#27861;&#65292;&#23558;&#21327;&#21464;&#25928;&#24212;&#30340;&#20272;&#35745;&#19982;&#31038;&#21306;&#26631;&#31614;&#30340;&#20272;&#35745;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic block model (SBM) is a widely used framework for community detection in networks, where the network structure is typically represented by an adjacency matrix. However, conventional SBMs are not directly applicable to an adjacency matrix that consists of non-negative zero-inflated continuous edge weights. To model the international trading network, where edge weights represent trading values between countries, we propose an innovative SBM based on a restricted Tweedie distribution. Additionally, we incorporate nodal information, such as the geographical distance between countries, and account for its dynamic effect on edge weights. Notably, we show that given a sufficiently large number of nodes, estimating this covariate effect becomes independent of community labels of each node when computing the maximum likelihood estimator of parameters in our model. This result enables the development of an efficient two-step algorithm that separates the estimation of covariate effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20132;&#36890;&#29702;&#35770;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#36710;&#36742;&#32534;&#38431;&#21644;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20316;&#20026;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21327;&#35843;&#65292;&#20197;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#21644;&#32531;&#35299;&#22478;&#24066;&#25317;&#22581;&#12290;</title><link>http://arxiv.org/abs/2310.10948</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#35299;&#20915;&#22478;&#24066;&#25317;&#22581;&#65306;&#22522;&#20110;&#24322;&#26500;GNN&#30340;&#21327;&#35843;&#32534;&#38431;&#21644;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control. (arXiv:2310.10948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20132;&#36890;&#29702;&#35770;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#36710;&#36742;&#32534;&#38431;&#21644;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20316;&#20026;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21327;&#35843;&#65292;&#20197;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#21644;&#32531;&#35299;&#22478;&#24066;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29420;&#31435;&#25110;&#20998;&#23618;&#26041;&#24335;&#24320;&#21457;&#20449;&#21495;&#25511;&#21046;&#21644;&#36710;&#36742;&#32534;&#38431;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#20013;&#32852;&#21512;&#25511;&#21046;&#36825;&#20004;&#32773;&#20197;&#20943;&#36731;&#20132;&#36890;&#25317;&#22581;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#20449;&#21495;&#25511;&#21046;&#21644;&#32534;&#38431;&#20043;&#38388;&#22266;&#26377;&#30340;&#29289;&#29702;&#21644;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#35843;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#22522;&#20110;&#24322;&#26500;&#22270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20132;&#36890;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;1&#65289;&#23558;&#32534;&#38431;&#21644;&#20449;&#21495;&#25511;&#21046;&#35774;&#35745;&#20026;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#20855;&#26377;&#33258;&#24049;&#30340;&#35266;&#27979;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65307;2&#65289;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35774;&#35745;&#21327;&#35843;&#65292;&#20197;&#20419;&#36827;&#21306;&#22495;&#33539;&#22260;&#20869;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26080;&#32541;&#20449;&#24687;&#20132;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;SUMO&#27169;&#25311;&#29615;&#22659;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale. We evaluate our approach through SUMO simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#30828;&#32422;&#26463;&#30340;&#24378;&#30423;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#21644;&#32047;&#31215;&#30828;&#32422;&#26463;&#36829;&#21453;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.10946</link><description>&lt;p&gt;
&#22810;&#28857;&#21453;&#39304;&#30340;&#24102;&#26377;&#30828;&#32422;&#26463;&#30340;&#24378;&#30423;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-point Feedback of Bandit Convex Optimization with Hard Constraints. (arXiv:2310.10946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#30828;&#32422;&#26463;&#30340;&#24378;&#30423;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#21644;&#32047;&#31215;&#30828;&#32422;&#26463;&#36829;&#21453;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#32422;&#26463;&#30340;&#24378;&#30423;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#26088;&#22312;&#22312;&#23616;&#37096;&#25439;&#22833;&#20989;&#25968;&#20449;&#24687;&#19979;&#29983;&#25104;&#19968;&#31995;&#21015;&#20915;&#31574;&#65292;&#20197;&#21516;&#26102;&#38477;&#20302;&#32047;&#31215;&#25439;&#22833;&#21644;&#32047;&#31215;&#32422;&#26463;&#36829;&#21453;&#12290;&#25105;&#20204;&#37319;&#29992;&#32047;&#31215;&#8220;&#30828;&#8221;&#32422;&#26463;&#36829;&#21453;&#20316;&#20026;&#32422;&#26463;&#36829;&#21453;&#30340;&#24230;&#37327;&#65292;&#20854;&#23450;&#20041;&#20026;$\sum_{t=1}^{T}\max\{g_t(\boldsymbol{x}_t), 0\}$&#12290;&#30001;&#20110;&#26368;&#22823;&#25805;&#20316;&#31526;&#30340;&#23384;&#22312;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#8220;&#38271;&#26399;&#8221;&#32422;&#26463;&#36829;&#21453;&#24230;&#37327;&#65292;&#20005;&#26684;&#21487;&#34892;&#35299;&#26080;&#27861;&#28040;&#38500;&#36829;&#21453;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36951;&#25022;&#21644;&#32047;&#31215;&#30828;&#32422;&#26463;&#36829;&#21453;&#26041;&#38754;&#37117;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#22686;&#38271;&#65292;&#20854;&#20013;&#26799;&#24230;&#20272;&#35745;&#20351;&#29992;&#20102;&#20004;&#20010;&#28857;&#30340;&#20989;&#25968;&#35780;&#20272;&#12290;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;$O(d^2T^{\max\{c,1-c\}})$&#30340;&#36951;&#25022;&#30028;&#21644;$O(d^2T^{1-\frac{c}{2}})$&#30340;&#32047;&#31215;&#30828;&#32422;&#26463;&#36829;&#21453;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies bandit convex optimization with constraints, where the learner aims to generate a sequence of decisions under partial information of loss functions such that the cumulative loss is reduced as well as the cumulative constraint violation is simultaneously reduced. We adopt the cumulative \textit{hard} constraint violation as the metric of constraint violation, which is defined by $\sum_{t=1}^{T} \max\{g_t(\boldsymbol{x}_t), 0\}$. Owing to the maximum operator, a strictly feasible solution cannot cancel out the effects of violated constraints compared to the conventional metric known as \textit{long-term} constraints violation. We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation, in which the gradient is estimated with a two-point function evaluation. Precisely, our algorithm attains $O(d^2T^{\max\{c,1-c\}})$ regret bounds and $O(d^2T^{1-\frac{c}{2}})$ cumulative hard constr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#26368;&#20248;&#25511;&#21046; (OC)&#21644;&#24378;&#21270;&#23398;&#20064; (RL)&#26041;&#27861;&#22312;&#33258;&#20027;&#26080;&#20154;&#26426;&#36187;&#36710;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#30452;&#25509;&#20248;&#21270;&#20219;&#21153;&#23618;&#38754;&#30340;&#30446;&#26631;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#30340;&#38543;&#26426;&#22240;&#32032;&#65292;&#32780;&#26368;&#20248;&#25511;&#21046;&#30340;&#20998;&#35299;&#38480;&#21046;&#20102;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.10943</link><description>&lt;p&gt;
&#22312;&#33258;&#20027;&#36187;&#36710;&#20013;&#36798;&#21040;&#26497;&#38480;: &#26368;&#20248;&#25511;&#21046;&#19982;&#24378;&#21270;&#23398;&#20064;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning. (arXiv:2310.10943v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#26368;&#20248;&#25511;&#21046; (OC)&#21644;&#24378;&#21270;&#23398;&#20064; (RL)&#26041;&#27861;&#22312;&#33258;&#20027;&#26080;&#20154;&#26426;&#36187;&#36710;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#30452;&#25509;&#20248;&#21270;&#20219;&#21153;&#23618;&#38754;&#30340;&#30446;&#26631;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#30340;&#38543;&#26426;&#22240;&#32032;&#65292;&#32780;&#26368;&#20248;&#25511;&#21046;&#30340;&#20998;&#35299;&#38480;&#21046;&#20102;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#20026;&#25935;&#25463;&#31227;&#21160;&#26426;&#22120;&#20154;&#35774;&#35745;&#25511;&#21046;&#31995;&#32479;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#33258;&#20027;&#26080;&#20154;&#26426;&#36187;&#36710;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;(RL)&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#32988;&#36807;&#26368;&#20248;&#25511;&#21046;(OC)&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#22522;&#26412;&#22240;&#32032;&#23545;RL&#30340;&#25104;&#21151;&#25110;OC&#30340;&#38480;&#21046;&#26377;&#25152;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;RL&#30456;&#23545;&#20110;OC&#30340;&#26681;&#26412;&#20248;&#21183;&#19981;&#22312;&#20110;&#20248;&#21270;&#30446;&#26631;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#26159;&#22312;&#20110;&#23427;&#20248;&#21270;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#30446;&#26631;&#12290;OC&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#20351;&#29992;&#19968;&#20010;&#26126;&#30830;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#22914;&#36712;&#36857;&#65292;&#20316;&#20026;&#25509;&#21475;&#12290;&#36825;&#31181;&#20998;&#35299;&#38480;&#21046;&#20102;&#25511;&#21046;&#22120;&#21487;&#20197;&#34920;&#36798;&#30340;&#34892;&#20026;&#33539;&#22260;&#65292;&#24403;&#38754;&#20020;&#26410;&#24314;&#27169;&#30340;&#24433;&#21709;&#26102;&#65292;&#23548;&#33268;&#25511;&#21046;&#24615;&#33021;&#36739;&#24046;&#12290;&#30456;&#21453;&#65292;RL&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;&#20219;&#21153;&#23618;&#38754;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#38543;&#26426;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain rando
&lt;/p&gt;</description></item><item><title>MASON-NLP&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;&#12290;&#20219;&#21153;1&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#19981;&#21516;&#26465;&#20214;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10941</link><description>&lt;p&gt;
MASON-NLP&#22312;eRisk 2023&#19978;&#30340;&#36129;&#29486;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts. (arXiv:2310.10941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10941
&lt;/p&gt;
&lt;p&gt;
MASON-NLP&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#26816;&#27979;&#25233;&#37057;&#30151;&#29366;&#12290;&#20219;&#21153;1&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#19981;&#21516;&#26465;&#20214;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#23545;&#20154;&#20204;&#29983;&#27963;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#30340;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#20132;&#27969;&#26041;&#24335;&#65288;&#21253;&#25324;&#21475;&#22836;&#21644;&#20070;&#38754;&#25991;&#23383;&#65289;&#26816;&#27979;&#21040;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#12290;&#29305;&#21035;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#19968;&#20010;&#21487;&#20379;&#25105;&#20204;&#26816;&#26597;&#25233;&#37057;&#30151;&#29366;&#30340;&#20016;&#23500;&#32780;&#26041;&#20415;&#30340;&#25991;&#26412;&#26469;&#28304;&#12290;&#36125;&#20811;&#25233;&#37057;&#37327;&#34920;&#65288;BDI&#65289;&#38382;&#21367;&#36890;&#24120;&#29992;&#20110;&#34913;&#37327;&#25233;&#37057;&#30151;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23427;&#26159;&#26412;&#30740;&#31350;&#20013;&#21487;&#20197;&#36741;&#21161;&#20351;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#30740;&#31350;&#33539;&#22260;&#32553;&#23567;&#21040;&#36825;&#20123;&#30151;&#29366;&#65292;&#22240;&#20026;&#27599;&#20010;BDI&#38382;&#39064;&#37117;&#19982;&#29305;&#23450;&#30340;&#25233;&#37057;&#30151;&#29366;&#30456;&#20851;&#32852;&#12290;&#38656;&#35201;&#35760;&#20303;&#65292;&#24182;&#38750;&#27599;&#20010;&#25233;&#37057;&#30151;&#24739;&#32773;&#21516;&#26102;&#34920;&#29616;&#20986;&#25152;&#26377;&#30151;&#29366;&#65292;&#32780;&#26159;&#20854;&#20013;&#30340;&#19968;&#20123;&#32508;&#21512;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#33021;&#30830;&#23450;&#19968;&#20010;&#21477;&#23376;&#25110;&#19968;&#27573;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#29305;&#23450;&#26465;&#20214;&#30456;&#20851;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;eRisk 2023&#20219;&#21153;1&#30340;&#35774;&#35745;&#30446;&#30340;&#27491;&#26159;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26465;&#20214;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a mental health disorder that has a profound impact on people's lives. Recent research suggests that signs of depression can be detected in the way individuals communicate, both through spoken words and written texts. In particular, social media posts are a rich and convenient text source that we may examine for depressive symptoms. The Beck Depression Inventory (BDI) Questionnaire, which is frequently used to gauge the severity of depression, is one instrument that can aid in this study. We can narrow our study to only those symptoms since each BDI question is linked to a particular depressive symptom. It's important to remember that not everyone with depression exhibits all symptoms at once, but rather a combination of them. Therefore, it is extremely useful to be able to determine if a sentence or a piece of user-generated content is pertinent to a certain condition. With this in mind, the eRisk 2023 Task 1 was designed to do exactly that: assess the relevance of diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31616;&#21333;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#26041;&#27861;&#35745;&#31639;&#30340;&#23569;&#37327;&#21521;&#37327;&#36827;&#34892;&#39030;&#28857;&#23884;&#20837;&#65292;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#35745;&#31639;&#65292;&#33021;&#22815;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#30495;&#23454;&#32858;&#31867;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#26356;&#24555;&#65292;&#32858;&#31867;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.10939</link><description>&lt;p&gt;
&#29702;&#35770;&#19982;&#23454;&#36341;&#20013;&#30340;&#24555;&#36895;&#31616;&#21333;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fast and Simple Spectral Clustering in Theory and Practice. (arXiv:2310.10939v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31616;&#21333;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#26041;&#27861;&#35745;&#31639;&#30340;&#23569;&#37327;&#21521;&#37327;&#36827;&#34892;&#39030;&#28857;&#23884;&#20837;&#65292;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#35745;&#31639;&#65292;&#33021;&#22815;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#30495;&#23454;&#32858;&#31867;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#26356;&#24555;&#65292;&#32858;&#31867;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#26159;&#19968;&#31181;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22270;G&#20013;&#25214;&#21040;k&#20010;&#32858;&#31867;&#12290;&#22312;&#20256;&#32479;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#20013;&#65292;&#36890;&#36807;&#35745;&#31639;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;k&#20010;&#29305;&#24449;&#21521;&#37327;&#65292;&#23558;&#22270;G&#30340;&#39030;&#28857;&#23884;&#20837;&#21040;R^k&#20013;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36825;&#20010;&#23884;&#20837;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#25903;&#37197;&#30528;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#30001;&#24130;&#26041;&#27861;&#35745;&#31639;&#30340;O(log(k))&#20010;&#21521;&#37327;&#30340;&#39030;&#28857;&#23884;&#20837;&#12290;&#39030;&#28857;&#23884;&#20837;&#30340;&#35745;&#31639;&#26102;&#38388;&#19982;&#22270;&#30340;&#22823;&#23567;&#20960;&#20046;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;&#22270;&#30340;&#33258;&#28982;&#20551;&#35774;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#26032;&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#27604;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20135;&#29983;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#20960;&#20046;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering is a popular and effective algorithm designed to find $k$ clusters in a graph $G$. In the classical spectral clustering algorithm, the vertices of $G$ are embedded into $\mathbb{R}^k$ using $k$ eigenvectors of the graph Laplacian matrix. However, computing this embedding is computationally expensive and dominates the running time of the algorithm. In this paper, we present a simple spectral clustering algorithm based on a vertex embedding with $O(\log(k))$ vectors computed by the power method. The vertex embedding is computed in nearly-linear time with respect to the size of the graph, and the algorithm provably recovers the ground truth clusters under natural assumptions on the input graph. We evaluate the new algorithm on several synthetic and real-world datasets, finding that it is significantly faster than alternative clustering algorithms, while producing results with approximately the same clustering accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#20805;&#36275;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10935</link><description>&lt;p&gt;
&#22522;&#20110;&#23478;&#24237;&#21161;&#25163;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#65306;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti. (arXiv:2310.10935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10935
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#20805;&#36275;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#21161;&#25163;&#22312;&#25105;&#20204;&#39640;&#24230;&#25216;&#26415;&#21270;&#30340;&#31038;&#20250;&#20013;&#30340;&#22320;&#20301;&#24471;&#21040;&#24041;&#22266;&#65292;&#26377;&#24517;&#35201;&#36866;&#24212;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#29615;&#22659;&#65292;&#21253;&#25324;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21475;&#35821;&#24418;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#39318;&#20010;&#29992;&#20110;&#27491;&#24335;&#23391;&#21152;&#25289;&#35821;&#12289;&#21475;&#35821;&#23391;&#21152;&#25289;&#35821;&#21644;&#38177;&#23572;&#36203;&#33922;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#22635;&#20805;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#24635;&#20849;&#21253;&#21547;10&#20010;&#21807;&#19968;&#24847;&#22270;&#30340;984&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#20805;&#36275;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22312;&#21475;&#35821;&#23391;&#21152;&#25289;&#35821;&#30340;&#24847;&#22270;&#26816;&#27979;&#20013;&#65292;GPT-3.5&#27169;&#22411;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;0.94&#30340;F1&#24471;&#20998;&#65292;&#22312;&#27133;&#20301;&#22635;&#20805;&#20013;&#36798;&#21040;&#20102;0.51&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
As voice assistants cement their place in our technologically advanced society, there remains a need to cater to the diverse linguistic landscape, including colloquial forms of low-resource languages. Our study introduces the first-ever comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples across 10 unique intents. Our analysis reveals the robustness of large language models for tackling downstream tasks with inadequate data. The GPT-3.5 model achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot filling for colloquial Bangla.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#39044;&#27979;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#25233;&#37057;&#39118;&#38505;&#65292;&#20197;&#20943;&#23569;&#35823;&#35786;&#24182;&#25913;&#21892;&#25972;&#20307;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.10928</link><description>&lt;p&gt;
&#22312;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#20419;&#36827;&#25233;&#37057;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care. (arXiv:2310.10928v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#39044;&#27979;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#25233;&#37057;&#39118;&#38505;&#65292;&#20197;&#20943;&#23569;&#35823;&#35786;&#24182;&#25913;&#21892;&#25972;&#20307;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#26159;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#25233;&#37057;&#30151;&#26159;&#24120;&#35265;&#30340;&#30149;&#30151;&#12290;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#26159;&#22823;&#22810;&#25968;&#25233;&#37057;&#30151;&#24739;&#32773;&#30340;&#39318;&#35201;&#32852;&#31995;&#28857;&#65292;&#20294;&#32422;&#26377;25%&#30340;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#21307;&#29983;&#30340;&#35786;&#26029;&#19981;&#20934;&#30830;&#12290;&#35768;&#22810;&#20854;&#20182;&#38556;&#30861;&#20063;&#22952;&#30861;&#20102;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#25233;&#37057;&#30151;&#22312;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#35823;&#35786;&#65292;&#24182;&#25913;&#21892;&#25972;&#20307;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#12290;&#30005;&#23376;&#20581;&#24247;&#21672;&#35810;&#24120;&#24120;&#20986;&#29616;&#35270;&#39057;&#38382;&#39064;&#65292;&#22914;&#36830;&#25509;&#19981;&#33391;&#25110;&#36890;&#35805;&#20013;&#26029;&#12290;&#23545;&#20110;&#21487;&#33021;&#32570;&#20047;&#31283;&#23450;&#30340;&#20114;&#32852;&#32593;&#36830;&#25509;&#30340;&#20302;&#25910;&#20837;&#24739;&#32773;&#32780;&#35328;&#65292;&#20165;&#38899;&#39057;&#30340;&#30005;&#23376;&#20581;&#24247;&#26356;&#23454;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#12290;&#30446;&#26631;&#26159;&#65306;1&#65289;&#25910;&#38598;24&#20154;&#30340;&#38899;&#39057;&#25968;&#25454;&#65288;12&#20154;&#24739;&#26377;&#25233;&#37057;&#30151;&#65292;12&#20154;&#27809;&#26377;&#24515;&#29702;&#20581;&#24247;&#25110;&#37325;&#22823;&#20581;&#24247;&#38382;&#39064;&#30340;&#35786;&#26029;&#65289;&#65307;2&#65289;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#12290;&#20351;&#29992;&#20102;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;TPOT&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Telehealth is a valuable tool for primary health care (PHC), where depression is a common condition. PHC is the first point of contact for most people with depression, but about 25% of diagnoses made by PHC physicians are inaccurate. Many other barriers also hinder depression detection and treatment in PHC. Artificial intelligence (AI) may help reduce depression misdiagnosis in PHC and improve overall diagnosis and treatment outcomes. Telehealth consultations often have video issues, such as poor connectivity or dropped calls. Audio-only telehealth is often more practical for lower-income patients who may lack stable internet connections. Thus, our study focused on using audio data to predict depression risk. The objectives were to: 1) Collect audio data from 24 people (12 with depression and 12 without mental health or major health condition diagnoses); 2) Build a machine learning model to predict depression risk. TPOT, an autoML tool, was used to select the best machine learning algo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32463;&#20856;&#21644;&#37327;&#23376;&#35745;&#31639;&#33539;&#24335;&#20013;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#25191;&#34892;&#26102;&#38388;&#36739;&#38271;&#12290;&#21516;&#26102;&#25351;&#20986;&#65292;&#22686;&#21152;&#37327;&#23376;&#35745;&#31639;&#33021;&#21147;&#21644;&#24182;&#34892;&#24230;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10910</link><description>&lt;p&gt;
&#37327;&#23376;&#26102;&#20195;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#37327;&#23376;&#19982;&#32463;&#20856;&#25903;&#25345;&#21521;&#37327;&#26426;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines. (arXiv:2310.10910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32463;&#20856;&#21644;&#37327;&#23376;&#35745;&#31639;&#33539;&#24335;&#20013;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#25191;&#34892;&#26102;&#38388;&#36739;&#38271;&#12290;&#21516;&#26102;&#25351;&#20986;&#65292;&#22686;&#21152;&#37327;&#23376;&#35745;&#31639;&#33021;&#21147;&#21644;&#24182;&#34892;&#24230;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#27604;&#32463;&#20856;&#21644;&#37327;&#23376;&#35745;&#31639;&#33539;&#24335;&#20013;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#24378;&#35843;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#25105;&#20204;&#23457;&#35270;&#32463;&#20856;SVM&#21644;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#22312;&#40482;&#23614;&#33457;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#37319;&#29992;&#22522;&#20110;Qiskit&#24211;&#30340;&#24191;&#27867;&#23454;&#39564;&#33539;&#22260;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#65292;QSVM&#21487;&#20197;&#36798;&#21040;&#19982;&#32463;&#20856;SVM&#30456;&#23218;&#32654;&#30340;&#20934;&#30830;&#29575;&#65292;&#23613;&#31649;&#25191;&#34892;&#26102;&#38388;&#30446;&#21069;&#36739;&#38271;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;&#22686;&#21152;&#37327;&#23376;&#35745;&#31639;&#33021;&#21147;&#21644;&#24182;&#34892;&#24230;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;&#37327;&#23376;&#26102;&#20195;&#19979;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#28508;&#21147;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work endeavors to juxtapose the efficacy of machine learning algorithms within classical and quantum computational paradigms. Particularly, by emphasizing on Support Vector Machines (SVM), we scrutinize the classification prowess of classical SVM and Quantum Support Vector Machines (QSVM) operational on quantum hardware over the Iris dataset. The methodology embraced encapsulates an extensive array of experiments orchestrated through the Qiskit library, alongside hyperparameter optimization. The findings unveil that in particular scenarios, QSVMs extend a level of accuracy that can vie with classical SVMs, albeit the execution times are presently protracted. Moreover, we underscore that augmenting quantum computational capacity and the magnitude of parallelism can markedly ameliorate the performance of quantum machine learning algorithms. This inquiry furnishes invaluable insights regarding the extant scenario and future potentiality of machine learning applications in the quantum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#20869;&#23384;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#20869;&#23384;&#20196;&#29260;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#24040;&#22823;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10909</link><description>&lt;p&gt;
&#24322;&#26500;&#20869;&#23384;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Heterogenous Memory Augmented Neural Networks. (arXiv:2310.10909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#20869;&#23384;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#20869;&#23384;&#20196;&#29260;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#24040;&#22823;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#21322;&#21442;&#25968;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#19982;&#38750;&#21442;&#25968;&#32452;&#20214;&#65288;&#22914;&#22806;&#37096;&#23384;&#20648;&#27169;&#22359;&#21644;&#25968;&#25454;&#26816;&#32034;&#65289;&#30456;&#32467;&#21512;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21322;&#21442;&#25968;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#21407;&#22987;&#25968;&#25454;&#28857;&#8212;&#8212;&#36825;&#31181;&#31574;&#30053;&#22312;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#21644;&#24403;&#21069;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22823;&#37327;&#26631;&#35760;&#30340;&#33021;&#21147;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#20869;&#23384;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#20869;&#23384;&#20196;&#29260;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#24040;&#22823;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#26041;&#27861;&#21487;&#20197;&#19982;&#21508;&#31181;&#39592;&#24178;&#32593;&#32476;&#65288;MLP&#12289;CNN&#12289;GNN&#21644;Transformer&#65289;&#26080;&#32541;&#32467;&#21512;&#65292;&#20197;&#25554;&#20837;&#21644;&#25773;&#25918;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20110;&#22270;&#20687;&#21644;&#22270;&#30340;&#20219;&#21153;&#19979;&#22312;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#21644;&#20998;&#24067;&#22806;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios. However, existing semi-parametric methods mostly depend on independent raw data points - this strategy is difficult to scale up due to both high computational costs and the incapacity of current attention mechanisms with a large number of tokens. In this paper, we introduce a novel heterogeneous memory augmentation approach for neural networks which, by introducing learnable memory tokens with attention mechanism, can effectively boost performance without huge computational overhead. Our general-purpose method can be seamlessly combined with various backbones (MLP, CNN, GNN, and Transformer) in a plug-and-play manner. We extensively evaluate our approach on various image and graph-based tasks under both in-distribution (ID) and OOD condi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.10908</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#27169;&#22359;&#21270;&#32467;&#26500;&#65306;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#33021;&#21542;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#22359;&#21270;&#35774;&#35745;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23637;&#31034;&#20986;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#31561;&#20248;&#28857;&#12290;&#29616;&#26377;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#8220;&#26174;&#24335;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#26159;&#39044;&#20808;&#23450;&#20041;&#30340;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#34987;&#26399;&#26395;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22312;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;Transformer&#20013;&#23384;&#22312;&#8220;&#38544;&#24335;&#8221;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#21363;&#8220;&#33258;&#21457;&#27169;&#22359;&#21270;&#8221;&#12290;&#20182;&#20204;&#34920;&#26126;&#36825;&#26679;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#22312;&#26089;&#26399;&#39044;&#35757;&#32451;&#38454;&#27573;&#23601;&#20250;&#20986;&#29616;&#65292;&#24182;&#19988;&#23436;&#20840;&#26159;&#33258;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;Transformer&#27169;&#22411;&#20173;&#28982;&#34987;&#35270;&#20026;&#21333;&#20307;&#27169;&#22411;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20854;&#27169;&#22359;&#21270;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#37492;&#20110;&#26174;&#24335;&#27169;&#22359;&#21270;&#26550;&#26500;&#30340;&#20248;&#33391;&#29305;&#24615;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (E
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#26367;&#20195;&#20027;&#21160;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10907</link><description>&lt;p&gt;
&#38024;&#23545;&#36339;&#36291;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#26367;&#20195;&#20027;&#21160;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Surrogate Active Subspaces for Jump-Discontinuous Functions. (arXiv:2310.10907v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#26367;&#20195;&#20027;&#21160;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26367;&#20195;&#24314;&#27169;&#21644;&#27963;&#36291;&#23376;&#31354;&#38388;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#24378;&#22823;&#33539;&#20363;&#12290;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#31163;&#25955;&#36755;&#20986;&#30340;Agent-Based&#27169;&#22411;&#31561;&#19981;&#36830;&#32493;&#27169;&#25311;&#22120;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24212;&#29992;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#23545;&#20110;&#36825;&#31867;&#20272;&#35745;&#22120;&#65292;&#26367;&#20195;&#35745;&#31639;&#30340;&#27963;&#36291;&#23376;&#31354;&#38388;&#21487;&#20197;&#20135;&#29983;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27963;&#36291;&#23376;&#31354;&#38388;&#26159;&#36890;&#36807;&#26799;&#24230;&#23450;&#20041;&#30340;&#65292;&#24403;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#36830;&#32493;&#27169;&#25311;&#22120;&#26102;&#65292;&#20272;&#35745;&#30340;&#26159;&#20160;&#20040;&#37327;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#36827;&#34892;&#27492;&#31867;&#20998;&#26512;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#19968;&#20123;&#30149;&#24577;&#24773;&#20917;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#27963;&#36291;&#23376;&#31354;&#38388;&#25193;&#23637;&#21040;&#19981;&#36830;&#32493;&#20989;&#25968;&#19978;&#65292;&#28548;&#28165;&#20102;&#22312;&#27492;&#31867;&#20998;&#26512;&#20013;&#23454;&#38469;&#20272;&#35745;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36824;&#23545;&#21512;&#25104;&#27979;&#35797;&#20989;&#25968;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#39640;&#26031;&#36807;&#31243;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate modeling and active subspaces have emerged as powerful paradigms in computational science and engineering. Porting such techniques to computational models in the social sciences brings into sharp relief their limitations in dealing with discontinuous simulators, such as Agent-Based Models, which have discrete outputs. Nevertheless, prior applied work has shown that surrogate estimates of active subspaces for such estimators can yield interesting results. But given that active subspaces are defined by way of gradients, it is not clear what quantity is being estimated when this methodology is applied to a discontinuous simulator. We begin this article by showing some pathologies that can arise when conducting such an analysis. This motivates an extension of active subspaces to discontinuous functions, clarifying what is actually being estimated in such analyses. We also conduct numerical experiments on synthetic test functions to compare Gaussian process estimates of active sub
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.10899</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10899
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#20960;&#20046;&#27809;&#26377;&#30693;&#35782;&#25110;&#25511;&#21046;&#33021;&#21147;&#12290;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;--&#23545;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#20559;&#22909;--&#26159;&#29702;&#35299;&#21644;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#34892;&#20026;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#26469;&#30740;&#31350;&#27169;&#22411;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#32467;&#26500;&#25110;&#31934;&#24515;&#31574;&#21010;&#30340;&#35757;&#32451;&#26041;&#24335;&#27880;&#20837;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26356;&#26426;&#26800;&#30340;&#26041;&#27861;&#65306;&#23376;&#20219;&#21153;&#24402;&#32435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#23454;&#29616;&#29305;&#23450;&#23376;&#20219;&#21153;&#30340;&#21151;&#33021;&#23376;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#27880;&#20837;&#23545;&#21033;&#29992;&#35813;&#23376;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#23376;&#20219;&#21153;&#24402;&#32435;&#28789;&#27963;&#39640;&#25928;&#65292;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#20110;&#31038;&#21306;&#26816;&#27979;&#20013;&#23454;&#29616;&#26368;&#20248;&#21010;&#20998;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#26368;&#24120;&#29992;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#19982;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.10898</link><description>&lt;p&gt;
&#23545;&#20110;&#31038;&#21306;&#26816;&#27979;&#20013;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#22312;&#36817;&#20284;&#12289;&#21551;&#21457;&#24335;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection. (arXiv:2310.10898v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#20110;&#31038;&#21306;&#26816;&#27979;&#20013;&#23454;&#29616;&#26368;&#20248;&#21010;&#20998;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#26368;&#24120;&#29992;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#19982;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#35745;&#31639;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#65288;&#27169;&#22359;&#24615;&#65289;&#22312;&#32593;&#32476;&#33410;&#28857;&#30340;&#21010;&#20998;&#19978;&#26469;&#26816;&#27979;&#31038;&#21306;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#22312;&#23454;&#29616;&#26368;&#20248;&#21010;&#20998;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;104&#20010;&#32593;&#32476;&#65292;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#21644;&#20855;&#26377;&#27169;&#22359;&#21270;&#32467;&#26500;&#30340;&#21512;&#25104;&#22270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21313;&#31181;&#36817;&#20284;&#27169;&#22359;&#21270;&#31639;&#27861;&#65292;&#23545;&#27604;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#22522;&#20934;&#32447;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#31934;&#30830;&#30340;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20840;&#23616;&#20248;&#21270;&#27169;&#22359;&#24615;&#12290;&#20998;&#26512;&#30340;&#21313;&#31181;&#31639;&#27861;&#21253;&#25324;&#20843;&#20010;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20004;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;&#21464;&#31181;&#65292;&#20197;&#21450;&#20960;&#31181;Bayan&#36817;&#20284;&#31639;&#27861;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26368;&#24120;&#29992;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#24471;&#21040;&#30340;&#21010;&#20998;&#19982;&#32593;&#32476;&#30340;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#22914;&#19979;&#25152;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection, a fundamental problem in computational sciences, finds applications in various domains. Heuristics are often employed to detect communities through maximizing an objective function, modularity, over partitions of network nodes. Our research delves into the performance of different modularity maximization algorithms in achieving optimal partitions. We use 104 networks, comprising real-world instances from diverse contexts and synthetic graphs with modular structures. We analyze ten inexact modularity-based algorithms against an exact baseline which is an exact integer programming method that globally optimizes modularity. The ten algorithms analyzed include eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm. Our analysis uncovers substantial dissimilarities between the partitions obtained by most commonly used modularity-based methods and any optimal partition of the networks, as indicate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;TCR-&#34920;&#20301;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveTCR&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#24182;&#26368;&#22823;&#21270;&#24615;&#33021;&#25552;&#21319;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39044;&#27979;&#36807;&#31243;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.10893</link><description>&lt;p&gt;
&#39640;&#25928;TCR-&#34920;&#20301;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Active Learning Framework for Cost-Effective TCR-Epitope Binding Affinity Prediction. (arXiv:2310.10893v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;TCR-&#34920;&#20301;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveTCR&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#24182;&#26368;&#22823;&#21270;&#24615;&#33021;&#25552;&#21319;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39044;&#27979;&#36807;&#31243;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
T&#32454;&#32990;&#21463;&#20307;&#65288;TCR&#65289;&#26159;&#36866;&#24212;&#24615;&#20813;&#30123;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36127;&#36131;&#36890;&#36807;&#35782;&#21035;&#23492;&#20027;&#32454;&#32990;&#34920;&#38754;&#21576;&#29616;&#30340;&#34920;&#20301;&#24207;&#21015;&#26469;&#24212;&#23545;&#23041;&#32961;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;TCR&#21644;&#34920;&#20301;&#24207;&#21015;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#21463;&#21040;&#32570;&#20047;&#22823;&#37327;&#24050;&#27880;&#37322;&#30340;TCR-&#34920;&#20301;&#23545;&#30340;&#38480;&#21046;&#12290;&#27880;&#37322;&#23427;&#20204;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#38656;&#35201;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#28287;&#23454;&#39564;&#35780;&#20272;&#12290;&#20026;&#20102;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ActiveTCR&#65292;&#19968;&#20010;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#21644;TCR-&#34920;&#20301;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;ActiveTCR&#20174;&#19968;&#23567;&#32452;&#26631;&#35760;&#30340;&#35757;&#32451;&#23545;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;&#25628;&#32034;&#20540;&#24471;&#27880;&#37322;&#30340;&#26410;&#26631;&#35760;TCR-&#34920;&#20301;&#23545;&#12290;&#23427;&#26088;&#22312;&#26368;&#22823;&#21270;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#22235;&#31181;&#26597;&#35810;&#31574;&#30053;&#19982;&#38543;&#26426;&#25277;&#26679;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;ActiveTCR&#33021;&#22815;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
T cell receptors (TCRs) are critical components of adaptive immune systems, responsible for responding to threats by recognizing epitope sequences presented on host cell surface. Computational prediction of binding affinity between TCRs and epitope sequences using machine/deep learning has attracted intense attention recently. However, its success is hindered by the lack of large collections of annotated TCR-epitope pairs. Annotating their binding affinity requires expensive and time-consuming wet-lab evaluation. To reduce annotation cost, we present ActiveTCR, a framework that incorporates active learning and TCR-epitope binding affinity prediction models. Starting with a small set of labeled training pairs, ActiveTCR iteratively searches for unlabeled TCR-epitope pairs that are ''worth'' for annotation. It aims to maximize performance gains while minimizing the cost of annotation. We compared four query strategies with a random sampling baseline and demonstrated that ActiveTCR reduce
&lt;/p&gt;</description></item><item><title>Calysto Scheme&#26159;&#19968;&#20010;&#23558;Scheme&#36716;&#25442;&#20026;Python&#30340;&#39033;&#30446;&#65292;&#25903;&#25345;&#26631;&#20934;Scheme&#21151;&#33021;&#21644;Python&#24211;&#30340;&#20114;&#25805;&#20316;&#12290;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#21644;&#25945;&#23398;&#65292;&#19988;&#24050;&#34987;&#25104;&#21151;&#25972;&#21512;&#20837;Jupyter Notebook&#12290;</title><link>http://arxiv.org/abs/2310.10886</link><description>&lt;p&gt;
Calysto Scheme&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
The Calysto Scheme Project. (arXiv:2310.10886v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10886
&lt;/p&gt;
&lt;p&gt;
Calysto Scheme&#26159;&#19968;&#20010;&#23558;Scheme&#36716;&#25442;&#20026;Python&#30340;&#39033;&#30446;&#65292;&#25903;&#25345;&#26631;&#20934;Scheme&#21151;&#33021;&#21644;Python&#24211;&#30340;&#20114;&#25805;&#20316;&#12290;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#21644;&#25945;&#23398;&#65292;&#19988;&#24050;&#34987;&#25104;&#21151;&#25972;&#21512;&#20837;Jupyter Notebook&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Calysto Scheme&#26159;&#29992;Continuation-Passing Style&#32534;&#20889;&#30340;Scheme&#65292;&#32463;&#36807;&#19968;&#31995;&#21015;&#20445;&#25345;&#27491;&#30830;&#24615;&#30340;&#31243;&#24207;&#36716;&#25442;&#21518;&#36716;&#21270;&#20026;Python&#12290;&#23427;&#25903;&#25345;&#26631;&#20934;Scheme&#21151;&#33021;&#65292;&#21253;&#25324;call/cc&#65292;&#20197;&#21450;&#35821;&#27861;&#25193;&#23637;&#12289;&#29992;&#20110;&#33258;&#21160;&#22238;&#28335;&#30340;&#38750;&#30830;&#23450;&#24615;&#25805;&#20316;&#31526;&#21644;&#35768;&#22810;&#25193;&#23637;&#65292;&#20197;&#20415;&#19982;Python&#36827;&#34892;&#20114;&#25805;&#20316;&#12290;&#30001;&#20110;&#20854;&#22522;&#20110;Python&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#29616;&#20195;Python&#24211;&#65292;&#21253;&#25324;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20854;&#20182;&#25945;&#23398;&#29615;&#22659;&#30340;&#24211;&#12290;&#23613;&#31649;Calysto Scheme&#30340;&#24320;&#21457;&#30446;&#30340;&#26159;&#20026;&#20102;&#25945;&#32946;&#65292;&#20294;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#26131;&#23433;&#35013;&#24615;&#65292;&#23427;&#34987;&#35777;&#26126;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20063;&#24456;&#26377;&#29992;&#12290;&#23427;&#24050;&#32463;&#38598;&#25104;&#21040;Jupyter Notebook&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#24182;&#22312;&#25945;&#23460;&#20013;&#29992;&#20110;&#25945;&#25480;&#32534;&#31243;&#35821;&#35328;&#20837;&#38376;&#35838;&#31243;&#65292;&#28155;&#21152;&#20102;&#19968;&#20123;&#26377;&#36259;&#19988;&#29420;&#29305;&#30340;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calysto Scheme is written in Scheme in Continuation-Passing Style, and converted through a series of correctness-preserving program transformations into Python. It has support for standard Scheme functionality, including call/cc, as well as syntactic extensions, a nondeterministic operator for automatic backtracking, and many extensions to allow Python interoperation. Because of its Python foundation, it can take advantage of modern Python libraries, including those for machine learning and other pedagogical contexts. Although Calysto Scheme was developed with educational purposes in mind, it has proven to be generally useful due to its simplicity and ease of installation. It has been integrated into the Jupyter Notebook ecosystem and used in the classroom to teach introductory Programming Languages with some interesting and unique twists.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLoad&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22635;&#20805;&#37327;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.10879</link><description>&lt;p&gt;
BLoad&#65306;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#39640;&#25928;&#39034;&#24207;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling. (arXiv:2310.10879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLoad&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22635;&#20805;&#37327;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#21644;&#25968;&#25454;&#38598;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#24320;&#21457;&#20248;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#26412;&#30333;&#30382;&#20070;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#30340;&#39640;&#25928;&#24615;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#24207;&#21015;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#20013;&#23454;&#29616;&#39640;&#25928;&#22788;&#29702;&#65292;&#21516;&#26102;&#36824;&#33021;&#26368;&#23567;&#21270;&#39069;&#22806;&#24320;&#38144;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#26041;&#26696;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#22635;&#20805;&#37327;&#20943;&#23569;&#36229;&#36807;100&#20493;&#65292;&#21516;&#26102;&#19981;&#21024;&#38500;&#20219;&#20309;&#24103;&#65292;&#20174;&#32780;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24635;&#20307;&#19978;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing complexity of modern deep neural network models and the expanding sizes of datasets necessitate the development of optimized and scalable training methods. In this white paper, we addressed the challenge of efficiently training neural network models using sequences of varying sizes. To address this challenge, we propose a novel training scheme that enables efficient distributed data-parallel training on sequences of different sizes with minimal overhead. By using this scheme we were able to reduce the padding amount by more than 100$x$ while not deleting a single frame, resulting in an overall increased performance on both training time and Recall in our experiments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#28378;&#21160;&#23637;&#24320;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#36830;&#25509;&#21644;&#33258;&#21160;&#21270;&#36710;&#36742;&#30340;&#29983;&#24577;&#39550;&#39542;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10878</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28378;&#21160;&#23637;&#24320;&#26041;&#27861;&#25511;&#21046;&#36830;&#25509;&#21644;&#33258;&#21160;&#21270;&#36710;&#36742;&#30340;&#29983;&#24577;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout. (arXiv:2310.10878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10878
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#28378;&#21160;&#23637;&#24320;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#36830;&#25509;&#21644;&#33258;&#21160;&#21270;&#36710;&#36742;&#30340;&#29983;&#24577;&#39550;&#39542;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36890;&#36807;&#20351;&#29992;&#36710;&#21040;&#19968;&#20999;&#20449;&#24687;&#26469;&#20248;&#21270;&#36710;&#36742;&#36895;&#24230;&#21644;&#21160;&#21147;&#20256;&#21160;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#26368;&#23567;&#21270;&#33021;&#28304;&#28040;&#32791;&#30340;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#29983;&#24577;&#39550;&#39542;&#38382;&#39064;&#65292;&#36890;&#24120;&#23384;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#36739;&#39640;&#30340;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#22312;&#32447;&#23454;&#26045;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22810;&#35270;&#35282;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20840;&#31243;&#20215;&#20540;&#20989;&#25968;&#20197;&#35299;&#20915;&#36335;&#24452;&#20449;&#24687;&#30340;&#21464;&#24322;&#24615;&#65292;&#28982;&#21518;&#29992;&#20110;&#36817;&#20284;&#28378;&#21160;&#35270;&#30028;&#20248;&#21270;&#20013;&#30340;&#32456;&#31471;&#25104;&#26412;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#36335;&#32447;&#27169;&#25311;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#19982;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#38543;&#26426;&#20248;&#21270;&#35299;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#35757;&#32451;&#33539;&#24335;&#21644;&#21487;&#24573;&#30053;&#30340;&#36710;&#36733;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connected and autonomous vehicles have the potential to minimize energy consumption by optimizing the vehicle velocity and powertrain dynamics with Vehicle-to-Everything info en route. Existing deterministic and stochastic methods created to solve the eco-driving problem generally suffer from high computational and memory requirements, which makes online implementation challenging.  This work proposes a hierarchical multi-horizon optimization framework implemented via a neural network. The neural network learns a full-route value function to account for the variability in route information and is then used to approximate the terminal cost in a receding horizon optimization. Simulations over real-world routes demonstrate that the proposed approach achieves comparable performance to a stochastic optimization solution obtained via reinforcement learning, while requiring no sophisticated training paradigm and negligible on-board memory.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#30740;&#31350;&#22522;&#20110;&#19990;&#30028;&#20215;&#20540;&#35266;&#35843;&#26597;&#25968;&#25454;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#23447;&#25945;&#20449;&#20208;&#12289;&#20215;&#20540;&#35266;&#21644;&#34892;&#20026;&#30340;&#21464;&#21270;&#36235;&#21183;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#22269;&#23478;&#20013;&#65292;&#24180;&#40836;&#21644;&#25910;&#20837;&#26159;&#24433;&#21709;&#23447;&#25945;&#24615;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.10874</link><description>&lt;p&gt;
&#20108;&#21313;&#19968;&#19990;&#32426;&#30340;&#23447;&#25945;&#24402;&#23646;&#65306;&#22522;&#20110;&#19990;&#30028;&#20215;&#20540;&#35266;&#35843;&#26597;&#30340;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey. (arXiv:2310.10874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10874
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30740;&#31350;&#22522;&#20110;&#19990;&#30028;&#20215;&#20540;&#35266;&#35843;&#26597;&#25968;&#25454;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#23447;&#25945;&#20449;&#20208;&#12289;&#20215;&#20540;&#35266;&#21644;&#34892;&#20026;&#30340;&#21464;&#21270;&#36235;&#21183;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#22269;&#23478;&#20013;&#65292;&#24180;&#40836;&#21644;&#25910;&#20837;&#26159;&#24433;&#21709;&#23447;&#25945;&#24615;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#19990;&#30028;&#20215;&#20540;&#35266;&#35843;&#26597;&#20840;&#29699;&#25968;&#25454;&#30340;&#23450;&#37327;&#20998;&#26512;&#30740;&#31350;&#12290;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#65292;&#25105;&#20204;&#26088;&#22312;&#35782;&#21035;&#23447;&#25945;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#20351;&#29992;&#22269;&#23478;&#32423;&#25968;&#25454;&#23558;&#35843;&#26597;&#20013;&#30340;&#21463;&#35775;&#32773;&#20998;&#20026;&#23447;&#25945;&#21644;&#38750;&#23447;&#25945;&#12290;&#25105;&#20204;&#20351;&#29992;&#37325;&#37319;&#26679;&#25216;&#26415;&#24179;&#34913;&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#19981;&#24179;&#34913;&#23398;&#20064;&#24615;&#33021;&#25351;&#26631;&#12290;&#21464;&#37327;&#37325;&#35201;&#24615;&#20998;&#26512;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#22269;&#23478;&#20013;&#65292;&#24180;&#40836;&#21644;&#25910;&#20837;&#26159;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#12290;&#36890;&#36807;&#19982;&#23447;&#25945;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#22522;&#26412;&#31038;&#20250;&#23398;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#35752;&#35770;&#20102;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#26426;&#22120;&#23398;&#20064;&#22312;&#21442;&#19982;&#19990;&#30028;&#20215;&#20540;&#35266;&#35843;&#26597;&#30340;30&#20010;&#22269;&#23478;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#22312;&#27169;&#24335;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is a quantitative analysis of the data collected globally by the World Value Survey. The data is used to study the trajectories of change in individuals' religious beliefs, values, and behaviors in societies. Utilizing random forest, we aim to identify the key factors of religiosity and classify respondents of the survey as religious and non religious using country level data. We use resampling techniques to balance the data and improve imbalanced learning performance metrics. The results of the variable importance analysis suggest that Age and Income are the most important variables in the majority of countries. The results are discussed with fundamental sociological theories regarding religion and human behavior. This study is an application of machine learning in identifying the underlying patterns in the data of 30 countries participating in the World Value Survey. The results from variable importance analysis and classification of imbalanced data provide valuable insigh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20449;&#21495;&#21270;&#36947;&#36335;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32852;&#21512;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#30456;&#20851;&#24615;&#21644;&#20849;&#20139;&#35266;&#27979;&#21644;&#22870;&#21169;&#26469;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#21512;&#20316;&#65292;&#20174;&#32780;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10856</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20449;&#21495;&#21270;&#36947;&#36335;&#32593;&#32476;&#20013;&#32852;&#21512;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning. (arXiv:2310.10856v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20449;&#21495;&#21270;&#36947;&#36335;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32852;&#21512;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#30456;&#20851;&#24615;&#21644;&#20849;&#20139;&#35266;&#27979;&#21644;&#22870;&#21169;&#26469;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#21512;&#20316;&#65292;&#20174;&#32780;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20132;&#36890;&#25317;&#22581;&#26159;&#22256;&#25200;&#29616;&#20195;&#36947;&#36335;&#32593;&#32476;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#65292;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#25514;&#26045;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20449;&#21495;&#21270;&#36947;&#36335;&#32593;&#32476;&#20013;&#32852;&#21512;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#21516;&#26102;&#25511;&#21046;&#20449;&#21495;&#26102;&#24207;&#21644;&#36335;&#24452;&#36873;&#25321;&#26469;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;&#36890;&#36807;&#24314;&#31435;&#20195;&#29702;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24182;&#20351;&#20854;&#20849;&#20139;&#35266;&#27979;&#21644;&#22870;&#21169;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#21512;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#20010;&#20307;&#30340;&#35757;&#32451;&#12290;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;&#31639;&#27861;&#26469;&#22788;&#29702;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;
&lt;/p&gt;
&lt;p&gt;
Urban traffic congestion is a critical predicament that plagues modern road networks. To alleviate this issue and enhance traffic efficiency, traffic signal control and vehicle routing have proven to be effective measures. In this paper, we propose a joint optimization approach for traffic signal control and vehicle routing in signalized road networks. The objective is to enhance network performance by simultaneously controlling signal timings and route choices using Multi-Agent Deep Reinforcement Learning (MADRL). Signal control agents (SAs) are employed to establish signal timings at intersections, whereas vehicle routing agents (RAs) are responsible for selecting vehicle routes. By establishing relevance between agents and enabling them to share observations and rewards, interaction and cooperation among agents are fostered, which enhances individual training. The Multi-Agent Advantage Actor-Critic algorithm is used to handle multi-agent environments, and Deep Neural Network (DNN) s
&lt;/p&gt;</description></item><item><title>CoTFormer&#26159;&#19968;&#31181;transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21547;&#30340;&#38142;&#24605;&#32771;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#20013;&#26174;&#33879;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;</title><link>http://arxiv.org/abs/2310.10845</link><description>&lt;p&gt;
CoTFormer&#65306;&#26356;&#22810;&#30340;&#20851;&#27880;&#20196;&#29260;&#24357;&#34917;&#20102;&#26356;&#23569;&#30340;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
CoTFormer: More Tokens With Attention Make Up For Less Depth. (arXiv:2310.10845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10845
&lt;/p&gt;
&lt;p&gt;
CoTFormer&#26159;&#19968;&#31181;transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21547;&#30340;&#38142;&#24605;&#32771;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#20013;&#26174;&#33879;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#21457;&#23637;&#36234;&#26469;&#36234;&#22823;&#21644;&#26356;&#28145;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#31454;&#36187;&#27491;&#22312;&#36827;&#34892;&#20013;&#12290;&#28982;&#32780;&#65292;&#20687;&#38142;&#24605;&#32771;&#65288;CoT&#65289;&#26041;&#27861;&#36825;&#26679;&#30340;&#25216;&#26415;&#22312;&#23454;&#29616;&#26368;&#20339;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20173;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#38142;&#24605;&#32771;&#21644;&#20351;&#29992;&#26356;&#28145;&#30340;transformer&#20043;&#38388;&#30340;&#36817;&#20284;&#24179;&#34892;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoTFormer&#65292;&#19968;&#31181;&#20351;&#29992;&#38544;&#21547;&#38142;&#24605;&#32771;&#26426;&#21046;&#26469;&#23454;&#29616;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#23481;&#37327;&#30340;transformer&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#35777;&#26126;&#20102;CoTFormer&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26126;&#26174;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;
&lt;/p&gt;
&lt;p&gt;
The race to continually develop ever larger and deeper foundational models is underway. However, techniques like the Chain-of-Thought (CoT) method continue to play a pivotal role in achieving optimal downstream performance. In this work, we establish an approximate parallel between using chain-of-thought and employing a deeper transformer. Building on this insight, we introduce CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to achieve capacity comparable to a deeper model. Our empirical findings demonstrate the effectiveness of CoTFormers, as they significantly outperform larger standard transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#32463;&#36807;&#23433;&#20840;&#35843;&#25972;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#21033;&#29992;&#24369;&#28857;&#24182;&#35823;&#23548;AI&#31995;&#32479;&#65292;&#23545;&#20110;&#22797;&#26434;&#31995;&#32479;&#30340;&#25915;&#20987;&#23588;&#20026;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2310.10844</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24694;&#24847;&#25915;&#20987;&#25152;&#25581;&#31034;&#30340;&#28431;&#27934;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. (arXiv:2310.10844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#32463;&#36807;&#23433;&#20840;&#35843;&#25972;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#21033;&#29992;&#24369;&#28857;&#24182;&#35823;&#23548;AI&#31995;&#32479;&#65292;&#23545;&#20110;&#22797;&#26434;&#31995;&#32479;&#30340;&#25915;&#20987;&#23588;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20307;&#31995;&#32467;&#26500;&#21644;&#33021;&#21147;&#26041;&#38754;&#36805;&#36895;&#21457;&#23637;&#65292;&#38543;&#30528;&#23427;&#20204;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#30340;&#28145;&#20837;&#25972;&#21512;&#65292;&#23457;&#26597;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#23545;LLMs&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#30340;&#26032;&#20852;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#35813;&#39046;&#22495;&#26159;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#23433;&#20840;&#24615;&#30340;&#35266;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#32463;&#36807;&#23433;&#20840;&#35843;&#25972;&#30340;LLMs&#65288;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#21644;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#21152;&#24378;&#23398;&#20064;&#65289;&#20063;&#21487;&#33021;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#21033;&#29992;&#24369;&#28857;&#24182;&#35823;&#23548;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#22914;ChatGPT&#21644;Bard&#31561;&#27169;&#22411;&#30340;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#30340;&#26222;&#36941;&#23384;&#22312;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#23433;&#20840;&#23545;&#40784;&#65292;&#24182;&#26681;&#25454;&#21508;&#31181;&#23398;&#20064;&#32467;&#26500;&#23545;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20998;&#31867;&#65306;&#20165;&#25991;&#26412;&#25915;&#20987;&#12289;&#22810;&#27169;&#24577;&#25915;&#20987;&#20197;&#21450;&#19987;&#38376;&#38024;&#23545;&#22797;&#26434;&#31995;&#32479;&#30340;&#20854;&#20182;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#27010;&#29575;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#33945;&#29305;&#21345;&#27931;&#33258;&#22238;&#24402;&#27969;&#23545;&#25968;&#25454;&#30340;&#20284;&#28982;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#22522;&#20110;&#32852;&#21512;&#23494;&#24230;&#20272;&#35745;&#30340;&#20854;&#20182;&#27010;&#29575;&#20998;&#31867;&#22120;&#30340;&#25552;&#20986;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.10843</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#33945;&#29305;&#21345;&#27931;&#33258;&#22238;&#24402;&#27969;&#36827;&#34892;&#27010;&#29575;&#20998;&#31867;&#30340;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow. (arXiv:2310.10843v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#27010;&#29575;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#33945;&#29305;&#21345;&#27931;&#33258;&#22238;&#24402;&#27969;&#23545;&#25968;&#25454;&#30340;&#20284;&#28982;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#22522;&#20110;&#32852;&#21512;&#23494;&#24230;&#20272;&#35745;&#30340;&#20854;&#20182;&#27010;&#29575;&#20998;&#31867;&#22120;&#30340;&#25552;&#20986;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#20272;&#35745;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#23427;&#29992;&#20110;&#20272;&#35745;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#20854;&#20013;&#19968;&#31867;&#23494;&#24230;&#20272;&#35745;&#22120;&#26159;&#28151;&#21512;&#27169;&#22411;&#65292;&#22914;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#24471;&#21040;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#21478;&#19968;&#31867;&#23494;&#24230;&#20272;&#35745;&#22120;&#26159;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20204;&#20174;&#36755;&#20837;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#25968;&#25454;&#12290;&#20854;&#20013;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26159;&#33945;&#29305;&#21345;&#27931;&#33258;&#22238;&#24402;&#27969;&#65288;MAF&#65289;&#65292;&#23427;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#21644;&#33258;&#22238;&#24402;&#32593;&#32476;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23494;&#24230;&#20272;&#35745;&#22120;&#29992;&#20110;&#20998;&#31867;&#65292;&#23613;&#31649;&#23427;&#20204;&#36890;&#24120;&#29992;&#20110;&#20272;&#35745;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#22120;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;GMM&#21644;MAF&#65289;&#23545;&#25968;&#25454;&#30340;&#31867;&#21035;&#30340;&#20284;&#28982;&#36827;&#34892;&#24314;&#27169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#31867;&#22120;&#20248;&#20110;&#20165;&#20351;&#29992;&#21333;&#20010;&#39640;&#26031;&#20998;&#24067;&#23545;&#20284;&#28982;&#36827;&#34892;&#24314;&#27169;&#30340;&#36739;&#31616;&#21333;&#30340;&#20998;&#31867;&#22120;&#65292;&#22914;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25552;&#20986;&#22522;&#20110;&#32852;&#21512;&#23494;&#24230;&#20272;&#35745;&#30340;&#20854;&#20182;&#27010;&#29575;&#20998;&#31867;&#22120;&#24320;&#36767;&#20102;&#30740;&#31350;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density estimation, which estimates the distribution of data, is an important category of probabilistic machine learning. A family of density estimators is mixture models, such as Gaussian Mixture Model (GMM) by expectation maximization. Another family of density estimators is the generative models which generate data from input latent variables. One of the generative models is the Masked Autoregressive Flow (MAF) which makes use of normalizing flows and autoregressive networks. In this paper, we use the density estimators for classification, although they are often used for estimating the distribution of data. We model the likelihood of classes of data by density estimation, specifically using GMM and MAF. The proposed classifiers outperform simpler classifiers such as linear discriminant analysis which model the likelihood using only a single Gaussian distribution. This work opens the research door for proposing other probabilistic classifiers based on joint density estimation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#33258;&#21160;&#26816;&#27979;&#22522;&#20110;&#39057;&#29575;&#30340;&#20107;&#20214;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#21040;&#26102;&#38388;-&#39057;&#29575;&#22495;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20107;&#20214;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10841</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#29992;&#20110;&#26816;&#27979;&#20256;&#24863;&#22120;&#25968;&#25454;&#26102;&#38388;&#24207;&#21015;&#20013;&#22522;&#20110;&#39057;&#29575;&#30340;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data. (arXiv:2310.10841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#33258;&#21160;&#26816;&#27979;&#22522;&#20110;&#39057;&#29575;&#30340;&#20107;&#20214;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#21040;&#26102;&#38388;-&#39057;&#29575;&#22495;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20107;&#20214;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20107;&#20214;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#36890;&#36807;&#20256;&#24863;&#22120;&#25968;&#25454;&#30417;&#25511;&#25216;&#26415;&#31995;&#32479;&#34892;&#20026;&#30340;&#22522;&#26412;&#23454;&#36341;&#20043;&#19968;&#12290;&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36861;&#36394;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20107;&#20214;&#26041;&#38754;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#20026;&#20102;&#35780;&#20272;&#20027;&#21160;&#36710;&#36742;&#23433;&#20840;&#31995;&#32479;&#65292;&#36827;&#34892;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#36825;&#20123;&#22330;&#26223;&#28041;&#21450;&#20351;&#29992;&#22806;&#37096;&#20256;&#24863;&#22120;&#35760;&#24405;&#36710;&#36742;&#34892;&#20026;&#65292;&#20197;&#35780;&#20272;&#23454;&#38469;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#21270;&#26816;&#27979;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;&#35780;&#20272;&#36895;&#24230;&#65292;&#36824;&#36890;&#36807;&#36991;&#20813;&#25968;&#25454;&#26816;&#26597;&#20013;&#20027;&#35266;&#30340;&#12289;&#22522;&#20110;&#20154;&#30340;&#35780;&#20215;&#26469;&#26631;&#20934;&#21270;&#21644;&#23458;&#35266;&#21270;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35782;&#21035;&#22522;&#20110;&#39057;&#29575;&#30340;&#20107;&#20214;&#12290;&#20026;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34987;&#26144;&#23556;&#21040;&#26102;&#38388;-&#39057;&#29575;&#22495;&#20013;&#30340;&#34920;&#31034;&#65292;&#21363;scalogram&#12290;&#22312;&#36807;&#28388;scalogram&#20197;&#22686;&#24378;&#20449;&#21495;&#30340;&#30456;&#20851;&#37096;&#20998;&#20043;&#21518;&#65292;&#20351;&#29992;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26816;&#27979;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated event detection has emerged as one of the fundamental practices to monitor the behavior of technical systems by means of sensor data. In the automotive industry, these methods are in high demand for tracing events in time series data. For assessing the active vehicle safety systems, a diverse range of driving scenarios is conducted. These scenarios involve the recording of the vehicle's behavior using external sensors, enabling the evaluation of operational performance. In such setting, automated detection methods not only accelerate but also standardize and objectify the evaluation by avoiding subjective, human-based appraisals in the data inspection. This work proposes a novel event detection method that allows to identify frequency-based events in time series data. To this aim, the time series data is mapped to representations in the time-frequency domain, known as scalograms. After filtering scalograms to enhance relevant parts of the signal, an object detection model is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#21644;&#20135;&#21697;-&#38190;&#23384;&#20648;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#21442;&#25968;&#30456;&#31561;&#30340;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.10837</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Approximating Two-Layer Feedforward Networks for Efficient Transformers. (arXiv:2310.10837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#21644;&#20135;&#21697;-&#38190;&#23384;&#20648;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#21442;&#25968;&#30456;&#31561;&#30340;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65311;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MoEs)&#26500;&#24314;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;MoEs&#30340;&#20960;&#20010;&#26032;&#39062;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#21508;&#31181;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#20197;&#36817;&#20284;&#20004;&#23618;NNs(&#20363;&#22914;Transformer&#30340;&#21069;&#39304;&#22359;)&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20135;&#21697;-&#38190;&#23384;&#20648;(PKMs)&#12290;&#20511;&#21161;&#36825;&#20010;&#26694;&#26550;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;MoEs&#21644;PKMs&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#22312;&#35745;&#31639;&#30456;&#31561;&#26465;&#20214;&#19979;&#27604;&#36739;MoEs&#19982;&#23494;&#38598;&#22522;&#20934;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26465;&#20214;&#26159;&#21442;&#25968;&#30456;&#31561;&#65292;&#36825;&#23545;&#20110;&#27491;&#30830;&#35780;&#20272;LMs&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;MoEs&#22312;WikiText-103&#21644;enwiki8&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#19981;&#21516;&#35268;&#27169;&#19978;&#19982;&#23494;&#38598;&#30340;Transformer-XL&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#36164;&#28304;&#25928;&#29575;&#26356;&#39640;&#12290;&#36825;&#35777;&#26126;MoEs&#19981;&#20165;&#36866;&#29992;&#20110;&#36229;&#22823;&#22411;LMs&#65292;&#20063;&#36866;&#29992;&#20110;&#20219;&#20309;&#35268;&#27169;&#30340;&#36164;&#28304;-
&lt;/p&gt;
&lt;p&gt;
How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#26399;&#26395;&#31614;&#21517;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#20219;&#21153;&#23398;&#20064;&#21040;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.10836</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#26399;&#26395;&#31614;&#21517;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes based data augmentation and expected signature for time series classification. (arXiv:2310.10836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#26399;&#26395;&#31614;&#21517;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#20219;&#21153;&#23398;&#20064;&#21040;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31614;&#21517;&#26159;&#25551;&#36848;&#36335;&#24452;&#65288;&#21363;&#20174;&#21306;&#38388;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#36830;&#32493;&#20989;&#25968;&#65289;&#30340;&#22522;&#26412;&#23545;&#35937;&#12290;&#21516;&#26679;&#65292;&#26399;&#26395;&#31614;&#21517;&#25552;&#20379;&#20102;&#38543;&#26426;&#36807;&#31243;&#30340;&#32479;&#35745;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26399;&#26395;&#31614;&#21517;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#25968;&#25454;&#22686;&#24378;&#35745;&#31639;&#24471;&#21040;&#12290;&#19968;&#20010;&#20027;&#35201;&#29305;&#28857;&#26159;&#36890;&#36807;&#20351;&#29992;&#35813;&#27169;&#22411;&#30340;&#26377;&#30417;&#30563;&#20219;&#21153;&#23398;&#20064;&#21040;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The signature is a fundamental object that describes paths (that is, continuous functions from an interval to a Euclidean space). Likewise, the expected signature provides a statistical description of the law of stochastic processes. We propose a feature extraction model for time series built upon the expected signature. This is computed through a Gaussian processes based data augmentation. One of the main features is that an optimal feature extraction is learnt through the supervised task that uses the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.10835</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#21487;&#35777;&#26126;&#30340;&#27010;&#29575;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Provable Probabilistic Imaging using Score-Based Generative Priors. (arXiv:2310.10835v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26102;&#65292;&#20272;&#35745;&#39640;&#36136;&#37327;&#22270;&#20687;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#26159;&#22270;&#20687;&#37325;&#24314;&#31639;&#27861;&#20013;&#30340;&#20004;&#20010;&#29702;&#24819;&#29305;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#65288;PMC&#65289;&#20316;&#20026;&#19968;&#31181;&#23545;&#19968;&#33324;&#21453;&#38382;&#39064;&#21487;&#33021;&#35299;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;PMC&#33021;&#22815;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#32467;&#21512;&#20016;&#23500;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#36827;&#34892;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;PMC&#31639;&#27861;&#65292;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#25554;&#20837;&#24335;&#20808;&#39564;&#65288;PnP&#65289;&#21644;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;RED&#65289;&#31639;&#27861;&#30340;&#37319;&#26679;&#27169;&#25311;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#23545;PMC&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#20004;&#31181;&#31639;&#27861;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#38750;&#23545;&#25968;&#20985;&#20284;&#28982;&#21644;&#19981;&#23436;&#32654;&#24471;&#20998;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating high-quality images while also quantifying their uncertainty are two desired features in an image reconstruction algorithm for solving ill-posed inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as a principled framework for characterizing the space of possible solutions to a general inverse problem. PMC is able to incorporate expressive score-based generative priors for high-quality image reconstruction while also performing uncertainty quantification via posterior sampling. In particular, we introduce two PMC algorithms which can be viewed as the sampling analogues of the traditional plug-and-play priors (PnP) and regularization by denoising (RED) algorithms. We also establish a theoretical analysis for characterizing the convergence of the PMC algorithms. Our analysis provides non-asymptotic stationarity guarantees for both algorithms, even in the presence of non-log-concave likelihoods and imperfect score networks. We demonstrate the performance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10833</link><description>&lt;p&gt;
&#36866;&#24403;&#30340;Laplacian&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proper Laplacian Representation Learning. (arXiv:2310.10833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#23398;&#20064;&#29366;&#24577;&#30340;&#33391;&#22909;&#34920;&#31034;&#23545;&#20110;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;Laplacian&#34920;&#31034;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20869;&#22312;&#22870;&#21169;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26102;&#38388;&#24310;&#38271;&#30340;&#21160;&#20316;&#21457;&#29616;&#21644;&#22870;&#21169;&#22609;&#36896;&#65292;&#20197;&#21450;&#20449;&#24687;&#20016;&#23500;&#30340;&#29366;&#24577;&#32534;&#30721;&#12290;&#20026;&#20102;&#33719;&#24471;Laplacian&#34920;&#31034;&#65292;&#38656;&#35201;&#35745;&#31639;&#22270;Laplacian&#30340;&#29305;&#24449;&#31995;&#32479;&#65292;&#36825;&#36890;&#24120;&#36890;&#36807;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#30340;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36817;&#20284;&#26041;&#27861;&#20381;&#36182;&#20110;&#26080;&#27861;&#39640;&#25928;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#25910;&#25947;&#21040;&#25152;&#38656;&#29305;&#24449;&#21521;&#37327;&#30340;&#20219;&#24847;&#26059;&#36716;&#65292;&#24182;&#19988;&#26080;&#27861;&#31934;&#30830;&#22320;&#24674;&#22797;&#30456;&#24212;&#30340;&#29305;&#24449;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20934;&#30830;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#31995;&#32479;&#20195;&#29702;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#36873;&#21462;&#36817;&#20284;&#24212;&#29992;&#20110;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#26469;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23567;&#36817;&#20284;&#31995;&#32479;&#36712;&#36857;&#21644;&#29366;&#24577;&#21464;&#37327;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.10831</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#31995;&#32479;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#27491;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Accurate Data-Driven Surrogates of Dynamical Systems for Forward Propagation of Uncertainty. (arXiv:2310.10831v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20934;&#30830;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#31995;&#32479;&#20195;&#29702;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#36873;&#21462;&#36817;&#20284;&#24212;&#29992;&#20110;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#26469;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23567;&#36817;&#20284;&#31995;&#32479;&#36712;&#36857;&#21644;&#29366;&#24577;&#21464;&#37327;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36873;&#21462;&#65288;SC&#65289;&#26159;&#19968;&#31181;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#12290;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;SC&#29305;&#21035;&#36866;&#29992;&#20110;&#23436;&#20840;&#22330;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#65292;&#20854;&#25551;&#32472;&#20102;&#20855;&#26377;&#38543;&#26426;&#36755;&#20837;&#21442;&#25968;&#30340;&#27169;&#22411;&#20027;&#35201;&#35299;&#22330;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21363;&#20351;&#22312;&#26368;&#31616;&#21333;&#30340;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;&#21442;&#25968;&#21040;&#35299;&#30340;&#26144;&#23556;&#20855;&#26377;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#29305;&#24615;&#65292;&#26500;&#24314;&#30340;SC&#20195;&#29702;&#27169;&#22411;&#36890;&#24120;&#19981;&#20934;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#23558;SC&#36817;&#20284;&#24212;&#29992;&#20110;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#32780;&#38750;&#35299;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#39537;&#21160;&#30340;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;SINDy&#65289;&#26694;&#26550;&#19982;SC&#32467;&#21512;&#65292;&#25105;&#20204;&#26500;&#24314;&#21160;&#21147;&#23398;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26102;&#38388;&#36827;&#34892;&#31215;&#20998;&#20197;&#26500;&#24314;&#20195;&#29702;&#35299;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SC-over-dynamics&#26694;&#26550;&#22312;&#36817;&#20284;&#31995;&#32479;&#36712;&#36857;&#20197;&#21450;&#29366;&#24577;&#21464;&#37327;&#35823;&#24046;&#26041;&#38754;&#37117;&#20855;&#26377;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic collocation (SC) is a well-known non-intrusive method of constructing surrogate models for uncertainty quantification. In dynamical systems, SC is especially suited for full-field uncertainty propagation that characterizes the distributions of the high-dimensional primary solution fields of a model with stochastic input parameters. However, due to the highly nonlinear nature of the parameter-to-solution map in even the simplest dynamical systems, the constructed SC surrogates are often inaccurate. This work presents an alternative approach, where we apply the SC approximation over the dynamics of the model, rather than the solution. By combining the data-driven sparse identification of nonlinear dynamics (SINDy) framework with SC, we construct dynamics surrogates and integrate them through time to construct the surrogate solutions. We demonstrate that the SC-over-dynamics framework leads to smaller errors, both in terms of the approximated system trajectories as well as the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2310.10818</link><description>&lt;p&gt;
&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#36328;&#20219;&#21153;&#20256;&#36882;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning. (arXiv:2310.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22797;&#26434;&#21644;&#22823;&#35268;&#27169;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#26679;&#26412;&#25928;&#29575;&#23545;&#20110;&#24320;&#21457;&#23454;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#26469;&#33258;&#20808;&#21069;&#32463;&#39564;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21518;&#32487;&#29305;&#24449;&#65288;SF&#65289;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#20294;&#30456;&#21516;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#30693;&#35782;&#27867;&#21270;&#12290;&#26368;&#36817;&#25552;&#20986;&#32467;&#21512;&#27169;&#22411;&#22522;&#20110;&#65288;MB&#65289;&#26041;&#27861;&#21644;SF&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22266;&#23450;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21478;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#23558;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#21518;&#32487;&#29305;&#24449;&#65288;MB-SF&#65289;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#20004;&#20010;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#20219;&#21153;&#26679;&#26412;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30693;&#35782;&#20256;&#36882;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;ERNIE&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35268;&#33539;&#21270;&#20419;&#36827;&#31574;&#30053;&#30340;Lipschitz&#36830;&#32493;&#24615;&#65292;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#22122;&#22768;&#35266;&#23519;&#12289;&#36716;&#25442;&#21160;&#24577;&#30340;&#21464;&#21270;&#21644;&#26234;&#33021;&#20307;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.10810</link><description>&lt;p&gt;
&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;: &#23545;&#25239;&#24615;&#35268;&#33539;&#21270;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#31283;&#23450;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms. (arXiv:2310.10810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;ERNIE&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35268;&#33539;&#21270;&#20419;&#36827;&#31574;&#30053;&#30340;Lipschitz&#36830;&#32493;&#24615;&#65292;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#22122;&#22768;&#35266;&#23519;&#12289;&#36716;&#25442;&#21160;&#24577;&#30340;&#21464;&#21270;&#21644;&#26234;&#33021;&#20307;&#30340;&#24694;&#24847;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#37117;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#29615;&#22659;&#30340;&#24494;&#23567;&#21464;&#21270;&#25935;&#24863;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#24448;&#24448;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#36825;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26500;&#25104;&#20102;&#20005;&#37325;&#38382;&#39064;&#65292;&#22240;&#20026;&#27979;&#35797;&#29615;&#22659;&#21487;&#33021;&#19982;&#35757;&#32451;&#29615;&#22659;&#30053;&#26377;&#19981;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#25511;&#21046;&#31574;&#30053;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;Lipschitz&#19988;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#30340;&#23384;&#22312;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;ERNIE&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35268;&#33539;&#21270;&#20419;&#36827;&#31574;&#30053;&#23545;&#20110;&#29366;&#24577;&#35266;&#23519;&#21644;&#21160;&#20316;&#30340;Lipschitz&#36830;&#32493;&#24615;&#12290;ERNIE&#26694;&#26550;&#23545;&#20110;&#22122;&#22768;&#35266;&#23519;&#12289;&#36716;&#25442;&#21160;&#24577;&#30340;&#21464;&#21270;&#21644;&#26234;&#33021;&#20307;&#30340;&#24694;&#24847;&#34892;&#20026;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;ERNIE&#30340;&#23545;&#25239;&#24615;&#35268;&#33539;&#21270;&#21487;&#33021;&#24341;&#20837;&#19968;&#20123;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#23545;&#25239;&#35757;&#32451;&#32447;&#24615;&#22238;&#24402;&#30340;&#27491;&#21017;&#21270;&#24615;&#36136;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#35299;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#29702;&#35299;&#23545;&#25239;&#35757;&#32451;&#30340;&#25928;&#26524;&#21644;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.10807</link><description>&lt;p&gt;
&#23545;&#23545;&#25239;&#35757;&#32451;&#32447;&#24615;&#22238;&#24402;&#30340;&#27491;&#21017;&#21270;&#24615;&#36136;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regularization properties of adversarially-trained linear regression. (arXiv:2310.10807v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#23545;&#25239;&#35757;&#32451;&#32447;&#24615;&#22238;&#24402;&#30340;&#27491;&#21017;&#21270;&#24615;&#36136;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#35299;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#29702;&#35299;&#23545;&#25239;&#35757;&#32451;&#30340;&#25928;&#26524;&#21644;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#30001;&#23545;&#25163;&#26500;&#36896;&#30340;&#38750;&#24120;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#12290;&#23545;&#25239;&#35757;&#32451;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#23427;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21463;&#21040;&#26368;&#22351;&#24773;&#20917;&#25915;&#20987;&#26102;&#23547;&#25214;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#32447;&#24615;&#27169;&#22411;&#26159;&#21487;&#20197;&#35266;&#23519;&#21040;&#28431;&#27934;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#20063;&#26159;&#25105;&#20204;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#19968;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#26377;&#38480;&#21644;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#23545;&#32447;&#24615;&#22238;&#24402;&#20013;&#23545;&#25239;&#35757;&#32451;&#30340;&#35299;&#19982;&#20854;&#20182;&#27491;&#21017;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;&#65288;A&#65289;&#21482;&#35201;&#26368;&#22823;&#25200;&#21160;&#21322;&#24452;&#23567;&#20110;&#38408;&#20540;&#65292;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65288;&#21442;&#25968;&#25968;&#30446;&#22823;&#20110;&#25968;&#25454;&#25968;&#30446;&#65289;&#30340;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#35299;&#65307;&#30456;&#21453;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#22120;&#23601;&#26159;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#24471;&#21040;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35782;&#21035;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#65292;&#27169;&#22411;&#26356;&#26131;&#35299;&#37322;&#21644;&#25239;&#36807;&#25311;&#21512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;71%&#12290;&#25512;&#21160;&#20102;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#26816;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.10806</link><description>&lt;p&gt;
&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification. (arXiv:2310.10806v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10806
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35782;&#21035;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#35786;&#26029;&#32467;&#26524;&#65292;&#27169;&#22411;&#26356;&#26131;&#35299;&#37322;&#21644;&#25239;&#36807;&#25311;&#21512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;71%&#12290;&#25512;&#21160;&#20102;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#26816;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#24066;&#22330;&#19978;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#22312;&#20687;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36825;&#26679;&#30340;&#24708;&#26080;&#22768;&#24687;&#30340;&#30142;&#30149;&#30340;&#21450;&#26102;&#35786;&#26029;&#26041;&#38754;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;&#20026;&#20102;&#35786;&#26029;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#65292;&#30524;&#31185;&#21307;&#29983;&#20351;&#29992;&#35270;&#32593;&#33180;&#21518;&#38754;&#30340;&#24425;&#33394;&#24213;&#29255;&#29031;&#29255;&#26469;&#36890;&#36807;&#19968;&#20010;&#22256;&#38590;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#35782;&#21035;&#23567;&#30340;&#40092;&#26126;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24213;&#29255;&#22270;&#20687;&#36755;&#20837;&#26469;&#35782;&#21035;DR&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#21367;&#31215;&#23618;&#23545;4&#31181;&#24050;&#30693;&#30340;DR&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#24494;&#23567;&#21160;&#33033;&#30244;&#12289;&#26825;&#19997;&#26001;&#12289;&#28183;&#20986;&#29289;&#21644;&#20986;&#34880;&#65292;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26356;&#26131;&#35299;&#37322;&#21644;&#25239;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#25935;&#24863;&#24615;&#20026;97%&#65292;&#20934;&#30830;&#29575;&#20026;71%&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#36739;&#24378;&#19988;&#20934;&#30830;&#29575;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#24403;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25512;&#21160;&#20102;DR&#26816;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Artificial Intelligence in the medical market brings up increasing concerns but aids in more timely diagnosis of silent progressing diseases like Diabetic Retinopathy. In order to diagnose Diabetic Retinopathy (DR), ophthalmologists use color fundus images, or pictures of the back of the retina, to identify small distinct features through a difficult and time-consuming process. Our work creates a novel CNN model and identifies the severity of DR through fundus image input. We classified 4 known DR features, including micro-aneurysms, cotton wools, exudates, and hemorrhages, through convolutional layers and were able to provide an accurate diagnostic without additional user input. The proposed model is more interpretable and robust to overfitting. We present initial results with a sensitivity of 97% and an accuracy of 71%. Our contribution is an interpretable model with similar accuracy to more complex models. With that, our model advances the field of DR detection an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#20989;&#25968;&#65288;NTKs&#65289;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20248;&#21270;&#23545;&#40784;&#31561;&#20215;&#20110;&#20248;&#21270;GNN&#20013;&#30340;&#22270;&#34920;&#31034;&#25110;&#22270;&#31227;&#20301;&#36816;&#31639;&#31526;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20110;&#20004;&#23618;GNN&#23545;&#40784;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.10791</link><description>&lt;p&gt;
&#31070;&#32463;&#20999;&#21521;&#26680;&#20989;&#25968;&#20026;&#20855;&#26377;&#20132;&#21449;&#21327;&#26041;&#24046;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs. (arXiv:2310.10791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#20989;&#25968;&#65288;NTKs&#65289;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20248;&#21270;&#23545;&#40784;&#31561;&#20215;&#20110;&#20248;&#21270;GNN&#20013;&#30340;&#22270;&#34920;&#31034;&#25110;&#22270;&#31227;&#20301;&#36816;&#31639;&#31526;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20110;&#20004;&#23618;GNN&#23545;&#40784;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20999;&#21521;&#26680;&#20989;&#25968;&#65288;NTKs&#65289;&#25552;&#20379;&#20102;&#20998;&#26512;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21644;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23545;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;NTK&#26680;&#20989;&#25968;&#30340;&#29305;&#24449;&#21521;&#37327;&#19982;&#32473;&#23450;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#65288;&#22312;&#26412;&#25991;&#20013;&#31216;&#20026;&#23545;&#40784;&#65289;&#21487;&#20197;&#25511;&#21046;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#21450;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#27010;&#24565;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;NTKs&#21644;&#23545;&#40784;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20248;&#21270;&#23545;&#40784;&#31561;&#20215;&#20110;&#20248;&#21270;GNN&#20013;&#30340;&#22270;&#34920;&#31034;&#25110;&#22270;&#31227;&#20301;&#36816;&#31639;&#31526;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#23545;&#20110;&#20004;&#23618;GNN&#23545;&#40784;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#30001;&#22270;&#31227;&#20301;&#36816;&#31639;&#31526;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#20989;&#25968;&#25152;&#20915;&#23450;&#12290;&#36890;&#36807;&#23545;NTKs&#30340;&#20998;&#26512;&#24471;&#20986;&#30340;&#29702;&#35770;&#27934;&#23519;&#21147;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK kernel and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept, we investigate NTKs and alignment in the context of graph neural networks (GNNs), where our analysis reveals that optimizing alignment translates to optimizing the graph representation or the graph shift operator in a GNN. Our results further establish the theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experime
&lt;/p&gt;</description></item><item><title>&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2310.10780</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;
&lt;/p&gt;
&lt;p&gt;
Demystifying Poisoning Backdoor Attacks from a Statistical Perspective. (arXiv:2310.10780v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10780
&lt;/p&gt;
&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#20381;&#36182;&#26085;&#30410;&#22686;&#38271;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#21644;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30001;&#20110;&#20854;&#38544;&#34109;&#24615;&#21644;&#28508;&#22312;&#30340;&#20005;&#37325;&#21518;&#26524;&#32780;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31867;&#25915;&#20987;&#28041;&#21450;&#23558;&#35302;&#21457;&#22120;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20197;&#22312;&#23384;&#22312;&#27963;&#21160;&#35302;&#21457;&#22120;&#26102;&#24341;&#36215;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#22312;&#27809;&#26377;&#35302;&#21457;&#22120;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#27491;&#24120;&#21151;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#20026;&#21463;&#25439;&#27169;&#22411;&#22312;&#28165;&#27905;&#21644;&#21518;&#38376;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#24314;&#31435;&#20005;&#26684;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#35780;&#20272;&#20102;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#24320;&#21457;&#30340;&#29702;&#35770;&#22238;&#31572;&#20102;&#19968;&#31995;&#21015;&#22522;&#26412;&#20294;&#20197;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;1&#65289;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#26159;&#20160;&#20040;&#65292;&#65288;2&#65289;&#26368;&#26377;&#25928;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#21521;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#29702;&#35299;...
&lt;/p&gt;
&lt;p&gt;
The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understandin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29616;&#20195;&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#24050;&#23384;&#22312;&#39640;&#20934;&#30830;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#32570;&#20047;&#29983;&#20135;&#32423;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10778</link><description>&lt;p&gt;
&#26159;&#21542;&#26377;&#29305;&#27931;&#20234;&#26408;&#39532;&#65281;: &#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29616;&#20195;&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#26368;&#26032;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25991;&#29486;&#35843;&#26597;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is there a Trojan! : Literature survey and critical evaluation of the latest ML based modern intrusion detection systems in IoT environments. (arXiv:2310.10778v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29616;&#20195;&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#24050;&#23384;&#22312;&#39640;&#20934;&#30830;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#32570;&#20047;&#29983;&#20135;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29289;&#32852;&#32593;&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#24050;&#32463;&#22312;&#25968;&#25454;&#37327;&#21644;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#26041;&#38754;&#19982;&#31227;&#21160;&#32593;&#32476;&#29615;&#22659;&#30456;&#23218;&#32654;&#12290;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#26426;&#23494;&#24615;&#21644;&#38544;&#31169;&#24050;&#32463;&#25104;&#20026;&#23433;&#20840;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#23433;&#20840;&#19987;&#23478;&#23545;&#35774;&#35745;&#20581;&#22766;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20197;&#20445;&#25252;&#29289;&#32852;&#32593;&#29615;&#22659;&#34920;&#31034;&#20852;&#36259;&#65292;&#20316;&#20026;&#20256;&#32479;&#23433;&#20840;&#26041;&#27861;&#30340;&#34917;&#20805;&#12290;&#30001;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#36164;&#28304;&#21463;&#38480;&#65292;&#24182;&#19988;&#20855;&#26377;&#24322;&#26500;&#30340;&#21327;&#35758;&#26632;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#22312;&#36825;&#20123;&#26550;&#26500;&#38480;&#21046;&#19979;&#25928;&#26524;&#19981;&#20339;&#12290;&#36825;&#20419;&#20351;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21019;&#26032;&#22320;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#38750;&#23398;&#20064;&#22411;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#22312;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#28857;&#12290;&#23613;&#31649;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#22312;&#29289;&#32852;&#32593;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#36824;&#32570;&#20047;&#36275;&#22815;&#30340;&#29983;&#20135;&#32423;&#27169;&#22411;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT as a domain has grown so much in the last few years that it rivals that of the mobile network environments in terms of data volumes as well as cybersecurity threats. The confidentiality and privacy of data within IoT environments have become very important areas of security research within the last few years. More and more security experts are interested in designing robust IDS systems to protect IoT environments as a supplement to the more traditional security methods. Given that IoT devices are resource-constrained and have a heterogeneous protocol stack, most traditional intrusion detection approaches don't work well within these schematic boundaries. This has led security researchers to innovate at the intersection of Machine Learning and IDS to solve the shortcomings of non-learning based IDS systems in the IoT ecosystem.  Despite various ML algorithms already having high accuracy with IoT datasets, we can see a lack of sufficient production grade models. This survey paper det
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20013;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#21644;/&#25110;&#22122;&#22768;&#25968;&#25454;&#23454;&#29616;&#20102;&#26657;&#27491;&#12290;</title><link>http://arxiv.org/abs/2310.10776</link><description>&lt;p&gt;
&#32416;&#27491;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20013;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Correcting model misspecification in physics-informed neural networks (PINNs). (arXiv:2310.10776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20013;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#21644;/&#25110;&#22122;&#22768;&#25968;&#25454;&#23454;&#29616;&#20102;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31185;&#23398;&#20013;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#24050;&#32463;&#25104;&#20026;&#33719;&#24471;&#20934;&#30830;&#29289;&#29702;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#24182;&#21487;&#33021;&#25104;&#20026;&#29702;&#35770;&#25512;&#23548;&#30340;&#21487;&#33021;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#32463;&#34987;&#29992;&#20110;&#23398;&#20064;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#23613;&#31649;PINNs&#22312;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;PINNs&#20013;&#32534;&#30721;&#30340;&#29289;&#29702;&#27169;&#22411;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#38169;&#35823;&#35268;&#33539;&#65292;&#22240;&#20026;&#26576;&#20123;&#29289;&#29702;&#36807;&#31243;&#21487;&#33021;&#24182;&#27809;&#26377;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#23548;&#33268;PINN&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;PINNs&#20013;&#38169;&#35823;&#35268;&#33539;&#30340;&#29289;&#29702;&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20026;&#20102;&#36890;&#36807;&#19968;&#20123;&#31232;&#30095;&#21644;/&#25110;&#22122;&#22768;&#25968;&#25454;&#26469;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#20551;&#35774;&#30340;&#29289;&#29702;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26469;&#24314;&#27169;&#19981;&#23436;&#32654;&#27169;&#22411;&#19982;&#35266;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven discovery of governing equations in computational science has emerged as a new paradigm for obtaining accurate physical models and as a possible alternative to theoretical derivations. The recently developed physics-informed neural networks (PINNs) have also been employed to learn governing equations given data across diverse scientific disciplines. Despite the effectiveness of PINNs for discovering governing equations, the physical models encoded in PINNs may be misspecified in complex systems as some of the physical processes may not be fully understood, leading to the poor accuracy of PINN predictions. In this work, we present a general approach to correct the misspecified physical models in PINNs for discovering governing equations, given some sparse and/or noisy data. Specifically, we first encode the assumed physical models, which may be misspecified, then employ other deep neural networks (DNNs) to model the discrepancy between the imperfect models and the observatio
&lt;/p&gt;</description></item><item><title>SAFE is a novel line notation for chemical structures that reimagines SMILES strings as an unordered sequence of interconnected fragment blocks, streamlining complex generative tasks and facilitating fragment-constrained design without the need for intricate decoding or graph-based models. It has been demonstrated to be effective through extensive experimentation.</title><link>http://arxiv.org/abs/2310.10773</link><description>&lt;p&gt;
&#12298;&#24517;&#39035;&#23433;&#20840;&#65306;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#35774;&#35745;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Gotta be SAFE: A New Framework for Molecular Design. (arXiv:2310.10773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10773
&lt;/p&gt;
&lt;p&gt;
SAFE is a novel line notation for chemical structures that reimagines SMILES strings as an unordered sequence of interconnected fragment blocks, streamlining complex generative tasks and facilitating fragment-constrained design without the need for intricate decoding or graph-based models. It has been demonstrated to be effective through extensive experimentation.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#23376;&#23383;&#31526;&#20018;&#34920;&#31034;&#65292;&#22914;SMILES&#65292;&#22312;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#23376;&#35774;&#35745;&#20013;&#32463;&#24120;&#38754;&#20020;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20197;&#38750;&#36830;&#32493;&#30340;&#26041;&#24335;&#25551;&#36848;&#20102;&#20998;&#23376;&#30340;&#20122;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#39034;&#24207;&#36830;&#25509;&#30340;&#29255;&#27573;&#23884;&#20837;&#65288;SAFE&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#21270;&#23398;&#32467;&#26500;&#32447;&#24615;&#31526;&#21495;&#34920;&#31034;&#27861;&#12290;SAFE&#23558;SMILES&#23383;&#31526;&#20018;&#37325;&#26032;&#26500;&#24819;&#20026;&#19968;&#20010;&#26080;&#24207;&#30340;&#20114;&#36830;&#29255;&#27573;&#22359;&#24207;&#21015;&#65292;&#21516;&#26102;&#19982;&#29616;&#26377;&#30340;SMILES&#35299;&#26512;&#22120;&#23436;&#20840;&#20860;&#23481;&#12290;&#23427;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#39592;&#26550;&#35013;&#39280;&#12289;&#29255;&#27573;&#36830;&#25509;&#12289;&#32858;&#21512;&#29289;&#29983;&#25104;&#21644;&#39592;&#26550;&#36339;&#36291;&#65292;&#21516;&#26102;&#20419;&#36827;&#20102;&#21463;&#29255;&#27573;&#32422;&#26463;&#30340;&#35774;&#35745;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#32321;&#29712;&#30340;&#35299;&#30721;&#25110;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21253;&#21547;11&#20159;&#20010;SAFE&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;8700&#19975;&#21442;&#25968;&#30340;&#31867;GPT2&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;SAFE&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;SAFE-GPT&#27169;&#22411;&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Traditional molecular string representations, such as SMILES, often pose challenges for AI-driven molecular design due to their non-sequential depiction of molecular substructures. To address this issue, we introduce Sequential Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical structures. SAFE reimagines SMILES strings as an unordered sequence of interconnected fragment blocks while maintaining full compatibility with existing SMILES parsers. It streamlines complex generative tasks, including scaffold decoration, fragment linking, polymer generation, and scaffold hopping, while facilitating autoregressive generation for fragment-constrained design, thereby eliminating the need for intricate decoding or graph-based models. We demonstrate the effectiveness of SAFE by training an 87-million-parameter GPT2-like model on a dataset containing 1.1 billion SAFE representations. Through extensive experimentation, we show that our SAFE-GPT model exhibits versatile an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#20027;&#26059;&#24459;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#21387;&#32553;&#23558;&#23436;&#25972;&#20998;&#35889;&#36716;&#25442;&#20026;&#20027;&#26059;&#24459;&#35889;&#21333;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;Lead-AE&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.10772</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#21387;&#32553;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#20027;&#26059;&#24459;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Lead Sheet Generation via Semantic Compression. (arXiv:2310.10772v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10772
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#20027;&#26059;&#24459;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#21387;&#32553;&#23558;&#23436;&#25972;&#20998;&#35889;&#36716;&#25442;&#20026;&#20027;&#26059;&#24459;&#35889;&#21333;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;Lead-AE&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#26059;&#24459;&#35889;&#21333;&#24050;&#32463;&#22312;&#29983;&#25104;&#38899;&#20048;&#30740;&#31350;&#20013;&#21464;&#24471;&#24120;&#35265;&#65292;&#34987;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#22810;&#36712;&#38899;&#20048;&#29983;&#25104;&#21644;&#33258;&#21160;&#32534;&#26354;&#65289;&#30340;&#21021;&#27493;&#21387;&#32553;&#34920;&#31034;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#23547;&#25214;&#37197;&#23545;&#30340;&#20027;&#26059;&#24459;&#35889;&#21333;&#21644;&#23436;&#25972;&#20998;&#35889;&#26102;&#36890;&#24120;&#20250;&#22238;&#21040;&#30830;&#23450;&#24615;&#30340;&#38477;&#32500;&#26041;&#27861;&#65288;&#22914;&#22825;&#38469;&#32447;&#31639;&#27861;&#65289;&#26469;&#29983;&#25104;&#20027;&#26059;&#24459;&#35889;&#21333;&#65292;&#24456;&#23569;&#20851;&#27880;&#20027;&#26059;&#24459;&#35889;&#21333;&#26412;&#36523;&#30340;&#36136;&#37327;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#20934;&#30830;&#21453;&#26144;&#32534;&#26354;&#21518;&#30340;&#23545;&#24212;&#29256;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#20027;&#26059;&#24459;&#35889;&#21333;&#29983;&#25104;&#30340;&#38382;&#39064;&#65288;&#21363;&#22312;&#32473;&#23450;&#20854;&#23436;&#25972;&#20998;&#35889;&#29256;&#26412;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20027;&#26059;&#24459;&#35889;&#21333;&#65289;&#65292;&#24182;&#23637;&#31034;&#36825;&#19968;&#20219;&#21153;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#26080;&#30417;&#30563;&#30340;&#38899;&#20048;&#21387;&#32553;&#20219;&#21153;&#65292;&#20854;&#20013;&#20027;&#26059;&#24459;&#35889;&#21333;&#20195;&#34920;&#20102;&#20998;&#35889;&#30340;&#21387;&#32553;&#28508;&#22312;&#29256;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;Lead-AE&#65292;&#23427;&#23558;&#20027;&#26059;&#24459;&#35889;&#21333;&#24314;&#27169;&#20026;&#21407;&#22987;&#24207;&#21015;&#30340;&#31163;&#25955;&#23376;&#36873;&#25321;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;top-k&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lead sheets have become commonplace in generative music research, being used as an initial compressed representation for downstream tasks like multitrack music generation and automatic arrangement. Despite this, researchers have often fallen back on deterministic reduction methods (such as the skyline algorithm) to generate lead sheets when seeking paired lead sheets and full scores, with little attention being paid toward the quality of the lead sheets themselves and how they accurately reflect their orchestrated counterparts. To address these issues, we propose the problem of conditional lead sheet generation (i.e. generating a lead sheet given its full score version), and show that this task can be formulated as an unsupervised music compression task, where the lead sheet represents a compressed latent version of the score. We introduce a novel model, called Lead-AE, that models the lead sheets as a discrete subselection of the original sequence, using a differentiable top-k operato
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#21457;&#29616;&#20855;&#26377;&#26080;&#38480;&#28145;&#24230;&#23618;&#24182;&#19988;&#23485;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;&#25581;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.10767</link><description>&lt;p&gt;
&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39640;&#26031;&#36807;&#31243;&#65306;&#26469;&#33258;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models. (arXiv:2310.10767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#21457;&#29616;&#20855;&#26377;&#26080;&#38480;&#28145;&#24230;&#23618;&#24182;&#19988;&#23485;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;&#25581;&#31034;&#20102;&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23485;&#24230;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#31561;&#20215;&#24615;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#65292;&#22312;&#20445;&#25345;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#23436;&#32654;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#34987;&#31216;&#20026;&#33391;&#24615;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32467;&#26524;&#20027;&#35201;&#38598;&#20013;&#22312;&#27973;&#23618;&#25110;&#26377;&#38480;&#28145;&#24230;&#30340;&#32593;&#32476;&#19978;&#65292;&#38656;&#35201;&#23545;&#20855;&#26377;&#26080;&#38480;&#28145;&#24230;&#23618;&#30340;&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#20363;&#22914;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#21644;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;(DEQ)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;(DEQ)&#65292;&#23427;&#26159;&#19968;&#20010;&#20855;&#26377;&#20849;&#20139;&#26435;&#37325;&#30697;&#38453;&#30340;&#26080;&#38480;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24403;DEQ&#23618;&#30340;&#23485;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#25910;&#25947;&#21040;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#25152;&#35859;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#39640;&#26031;&#36807;&#31243;(NNGP)&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#26497;&#38480;&#20114;&#25442;&#65292;&#22312;&#20856;&#22411;&#30340;&#26080;&#38480;&#28145;&#24230;&#22810;&#23618;&#32593;&#32476;&#20013;&#20063;&#19981;&#20250;&#35266;&#23519;&#21040;&#36825;&#31181;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks with wide layers have attracted significant attention due to their equivalence to Gaussian processes, enabling perfect fitting of training data while maintaining generalization performance, known as benign overfitting. However, existing results mainly focus on shallow or finite-depth networks, necessitating a comprehensive analysis of wide neural networks with infinite-depth layers, such as neural ordinary differential equations (ODEs) and deep equilibrium models (DEQs). In this paper, we specifically investigate the deep equilibrium model (DEQ), an infinite-depth neural network with shared weight matrices across layers. Our analysis reveals that as the width of DEQ layers approaches infinity, it converges to a Gaussian process, establishing what is known as the Neural Network and Gaussian Process (NNGP) correspondence. Remarkably, this convergence holds even when the limits of depth and width are interchanged, which is not observed in typical infinite-depth Multilayer 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20803;&#22238;&#24402;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#20154;&#33041;&#32452;&#32455;&#30340;&#26368;&#20339;&#26412;&#26500;&#26448;&#26009;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#20934;&#30830;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10762</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#36229;&#24377;&#24615;&#26448;&#26009;&#27169;&#22411;&#30340;&#20154;&#33041;&#30382;&#23618;&#30340;&#21457;&#29616;&#65306;&#22810;&#20803;&#20998;&#26512;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches. (arXiv:2310.10762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20803;&#22238;&#24402;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#20154;&#33041;&#32452;&#32455;&#30340;&#26368;&#20339;&#26412;&#26500;&#26448;&#26009;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#20934;&#30830;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#22914;&#26377;&#38480;&#20803;&#20998;&#26512;&#65292;&#20026;&#25581;&#31034;&#33041;&#37096;&#29289;&#29702;&#34892;&#20026;&#30340;&#28508;&#22312;&#26426;&#21046;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#39044;&#27979;&#33041;&#37096;&#29289;&#29702;&#24615;&#36136;&#38656;&#35201;&#26377;&#25928;&#30340;&#26412;&#26500;&#27169;&#22411;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#33041;&#32452;&#32455;&#21147;&#23398;&#29305;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#26368;&#36866;&#21512;&#20154;&#33041;&#32452;&#32455;&#30340;&#26412;&#26500;&#26448;&#26009;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20803;&#22238;&#24402;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#33324;&#21270;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#32463;&#20856;&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#24471;&#21040;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#65292;&#38500;&#20102;&#38450;&#27490;&#28508;&#22312;&#30340;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#22806;&#65292;&#20004;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#35774;&#32622;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20174;&#32473;&#23450;&#30340;&#21487;&#25509;&#21463;&#30340;&#20272;&#35745;&#22120;&#20013;&#33258;&#21160;&#35782;&#21035;&#20934;&#30830;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20116;&#39033;&#21644;&#20004;&#39033;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Traditional computational methods, such as the finite element analysis, have provided valuable insights into uncovering the underlying mechanisms of brain physical behaviors. However, precise predictions of brain physics require effective constitutive models to represent the intricate mechanical properties of brain tissue. In this study, we aimed to identify the most favorable constitutive material model for human brain tissue. To achieve this, we applied artificial neural network and multiple regression methods to a generalization of widely accepted classic models, and compared the results obtained from these two approaches. To evaluate the applicability and efficacy of the model, all setups were kept consistent across both methods, except for the approach to prevent potential overfitting. Our results demonstrate that artificial neural networks are capable of automatically identifying accurate constitutive models from given admissible estimators. Nonetheless, the five-term and two-ter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23545;&#20110;&#40065;&#26834;&#22343;&#20540;&#20272;&#35745;&#30340;&#20223;&#23556;&#31561;&#21464;&#24615;&#20272;&#35745;&#22120;&#30340;&#25968;&#37327;&#21270;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20223;&#23556;&#31561;&#21464;&#24615;&#20250;&#23548;&#33268;&#24674;&#22797;&#35823;&#24046;&#20005;&#37325;&#24694;&#21270;&#65292;&#36895;&#29575;&#38477;&#20302;&#19968;&#20010;&#22240;&#23376;$\sqrt{d}$&#12290;&#20256;&#32479;&#20272;&#35745;&#22120;&#19981;&#26159;&#26368;&#20248;&#30340;&#25110;&#32570;&#20047;&#37327;&#21270;&#20445;&#35777;&#65292;&#32780;&#20855;&#26377;&#37327;&#21270;&#20445;&#35777;&#30340;&#26368;&#26032;&#20272;&#35745;&#22120;&#19981;&#20855;&#26377;&#20223;&#23556;&#31561;&#21464;&#24615;&#25110;&#38656;&#35201;&#39069;&#22806;&#30340;&#20998;&#24067;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.10758</link><description>&lt;p&gt;
&#32479;&#35745;&#20108;&#20803;&#20272;&#35745;&#30340;&#20223;&#23556;&#31561;&#21464;&#24615;&#30340;&#32479;&#35745;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Statistical Barriers to Affine-equivariant Estimation. (arXiv:2310.10758v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23545;&#20110;&#40065;&#26834;&#22343;&#20540;&#20272;&#35745;&#30340;&#20223;&#23556;&#31561;&#21464;&#24615;&#20272;&#35745;&#22120;&#30340;&#25968;&#37327;&#21270;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20223;&#23556;&#31561;&#21464;&#24615;&#20250;&#23548;&#33268;&#24674;&#22797;&#35823;&#24046;&#20005;&#37325;&#24694;&#21270;&#65292;&#36895;&#29575;&#38477;&#20302;&#19968;&#20010;&#22240;&#23376;$\sqrt{d}$&#12290;&#20256;&#32479;&#20272;&#35745;&#22120;&#19981;&#26159;&#26368;&#20248;&#30340;&#25110;&#32570;&#20047;&#37327;&#21270;&#20445;&#35777;&#65292;&#32780;&#20855;&#26377;&#37327;&#21270;&#20445;&#35777;&#30340;&#26368;&#26032;&#20272;&#35745;&#22120;&#19981;&#20855;&#26377;&#20223;&#23556;&#31561;&#21464;&#24615;&#25110;&#38656;&#35201;&#39069;&#22806;&#30340;&#20998;&#24067;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#20110;&#40065;&#26834;&#22343;&#20540;&#20272;&#35745;&#32780;&#35328;&#20223;&#23556;&#31561;&#21464;&#24615;&#20272;&#35745;&#22120;&#30340;&#25968;&#37327;&#21270;&#24615;&#33021;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#28982;&#30340;&#31283;&#23450;&#24615;&#35201;&#27714;&#65292;&#36825;&#31867;&#20223;&#23556;&#31561;&#21464;&#24615;&#20272;&#35745;&#22120;&#30340;&#26500;&#24314;&#24050;&#32463;&#22312;&#32479;&#35745;&#25991;&#29486;&#20013;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#26368;&#36817;&#24191;&#27867;&#30740;&#31350;&#30340;&#24322;&#24120;&#20540;&#27169;&#22411;&#19979;&#23450;&#37327;&#35780;&#20272;&#20102;&#36825;&#20123;&#20272;&#35745;&#22120;&#65306;&#37325;&#23614;&#21644;&#23545;&#25239;&#24615;&#30772;&#22351;&#35774;&#32622;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19979;&#30028;&#65292;&#26174;&#31034;&#20223;&#23556;&#31561;&#21464;&#24615;&#20250;&#23548;&#33268;&#24674;&#22797;&#35823;&#24046;&#20005;&#37325;&#24694;&#21270;&#65292;&#23450;&#37327;&#36895;&#29575;&#38477;&#20302;&#19968;&#20010;&#22240;&#23376;$\sqrt{d}$&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20223;&#23556;&#31561;&#21464;&#24615;&#20272;&#35745;&#22120;&#30340;&#31867;&#21035;&#20013;&#65292;&#32463;&#20856;&#20272;&#35745;&#22120;&#22914;Tukey&#20013;&#20301;&#25968;&#65288;Tukey '75&#65289;&#21644;Stahel-Donoho&#20272;&#35745;&#22120;&#65288;Stahel '81&#21644;Donoho '82&#65289;&#22312;&#25968;&#37327;&#21270;&#26041;&#38754;&#35201;&#20040;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#20219;&#20309;&#25968;&#37327;&#21270;&#30340;&#20445;&#35777;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20855;&#26377;&#24378;&#22823;&#37327;&#21270;&#20445;&#35777;&#30340;&#26368;&#26032;&#20272;&#35745;&#22120;&#35201;&#20040;&#19981;&#20855;&#26377;&#20223;&#23556;&#31561;&#21464;&#24615;&#65292;&#35201;&#20040;&#38656;&#35201;&#39069;&#22806;&#30340;&#20998;&#24067;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the quantitative performance of affine-equivariant estimators for robust mean estimation. As a natural stability requirement, the construction of such affine-equivariant estimators has been extensively studied in the statistics literature. We quantitatively evaluate these estimators under two outlier models which have been the subject of much recent work: the heavy-tailed and adversarial corruption settings. We establish lower bounds which show that affine-equivariance induces a strict degradation in recovery error with quantitative rates degrading by a factor of $\sqrt{d}$ in both settings. We find that classical estimators such as the Tukey median (Tukey '75) and Stahel-Donoho estimator (Stahel '81 and Donoho '82) are either quantitatively sub-optimal even within the class of affine-equivariant estimators or lack any quantitative guarantees. On the other hand, recent estimators with strong quantitative guarantees are not affine-equivariant or require additional distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#26465;&#20214;&#24418;&#29366;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;3D&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#12290;&#36890;&#36807;&#23398;&#20064;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#24418;&#29366;&#27169;&#22411;&#21644;&#27169;&#24577;&#30456;&#20851;&#30340;&#32454;&#21270;&#32593;&#32476;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#33719;&#24471;&#24515;&#33039;&#24038;&#24515;&#23460;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.10756</link><description>&lt;p&gt;
3D&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#23618;&#26465;&#20214;&#24418;&#29366;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Conditional Shape Models for 3D cardiac image segmentation. (arXiv:2310.10756v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#26465;&#20214;&#24418;&#29366;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;3D&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#12290;&#36890;&#36807;&#23398;&#20064;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#24418;&#29366;&#27169;&#22411;&#21644;&#27169;&#24577;&#30456;&#20851;&#30340;&#32454;&#21270;&#32593;&#32476;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#33719;&#24471;&#24515;&#33039;&#24038;&#24515;&#23460;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#32467;&#26500;&#30340;&#30830;&#23450;&#36890;&#24120;&#26159;&#35768;&#22810;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#24037;&#20316;&#27969;&#30340;&#31532;&#19968;&#27493;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#32593;&#32476;&#27809;&#26377;&#32467;&#21512;&#35299;&#21078;&#24418;&#29366;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#28145;&#23618;&#26465;&#20214;&#24418;&#29366;&#27169;&#22411;&#65288;DCSMs&#65289;&#20316;&#20026;&#26680;&#24515;&#32452;&#20214;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#23618;&#38544;&#24335;&#24418;&#29366;&#34920;&#31034;&#65292;&#35813;&#31639;&#27861;&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#19982;&#27169;&#24577;&#26080;&#20851;&#30340;&#24418;&#29366;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#29305;&#24449;&#29983;&#25104;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#12290;&#20026;&#20102;&#20351;&#29983;&#25104;&#30340;&#24418;&#29366;&#25311;&#21512;&#22270;&#20687;&#65292;&#24418;&#29366;&#27169;&#22411;&#26159;&#26681;&#25454;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21040;&#25110;&#30001;&#29992;&#25143;&#25552;&#20379;&#30340;&#35299;&#21078;&#26631;&#35760;&#26465;&#20214;&#21270;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#19968;&#20010;&#27169;&#24577;&#30456;&#20851;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#32454;&#21270;&#32593;&#32476;&#65292;&#29992;&#20110;&#25429;&#25417;&#38544;&#24335;&#20989;&#25968;&#26410;&#34920;&#31034;&#30340;&#20219;&#20309;&#32454;&#33410;&#12290;&#25152;&#25552;&#20986;&#30340;DCSM&#26694;&#26550;&#22312;&#22810;&#20010;3D&#27169;&#24577;&#65288;&#22686;&#24378;CT&#65292;&#38750;&#22686;&#24378;CT&#65292;3D&#36229;&#22768;&#24515;&#21160;&#22270;&#65289;&#30340;&#24515;&#33039;&#24038;&#24515;&#23460;&#65288;LV&#65289;&#20998;&#21106;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delineation of anatomical structures is often the first step of many medical image analysis workflows. While convolutional neural networks achieve high performance, these do not incorporate anatomical shape information. We introduce a novel segmentation algorithm that uses Deep Conditional Shape models (DCSMs) as a core component. Using deep implicit shape representations, the algorithm learns a modality-agnostic shape model that can generate the signed distance functions for any anatomy of interest. To fit the generated shape to the image, the shape model is conditioned on anatomic landmarks that can be automatically detected or provided by the user. Finally, we add a modality-dependent, lightweight refinement network to capture any fine details not represented by the implicit function. The proposed DCSM framework is evaluated on the problem of cardiac left ventricle (LV) segmentation from multiple 3D modalities (contrast-enhanced CT, non-contrasted CT, 3D echocardiography-3DE). We de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10745</link><description>&lt;p&gt;
Mori-Zwanzig&#28508;&#21464;&#31354;&#38388;Koopman&#38381;&#21253;&#29992;&#20110;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder. (arXiv:2310.10745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#31616;&#21270;&#22797;&#26434;&#21160;&#21147;&#23398;&#29702;&#35299;&#30340;&#23453;&#36149;&#26041;&#27861;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#36924;&#36817;&#26377;&#38480;Koopman&#31639;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#36873;&#25321;&#21512;&#36866;&#30340;&#21487;&#35266;&#23519;&#37327;&#12289;&#38477;&#32500;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#20851;&#38190;&#21487;&#35266;&#23519;&#37327;&#26469;&#36924;&#36817;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#38598;&#25104;&#38750;&#39532;&#23572;&#21487;&#22827;&#26657;&#27491;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#21464;&#27969;&#24418;&#20013;&#20135;&#29983;&#20102;&#21160;&#21147;&#23398;&#30340;&#23553;&#38381;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31934;&#30830;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#24555;&#36895;&#23545;&#25239;&#24615;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#31181;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#35823;&#23548;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;FALFA&#12290;</title><link>http://arxiv.org/abs/2310.10744</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#24555;&#36895;&#23545;&#25239;&#24615;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fast Adversarial Label-Flipping Attack on Tabular Data. (arXiv:2310.10744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#24555;&#36895;&#23545;&#25239;&#24615;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#31181;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#35823;&#23548;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;FALFA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#38656;&#35201;&#39640;&#21487;&#38752;&#24615;&#30340;&#39046;&#22495;&#65292;&#22914;&#32593;&#32476;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#22312;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#24694;&#24847;&#22320;&#32763;&#36716;&#19968;&#37096;&#20998;&#35757;&#32451;&#26631;&#31614;&#20197;&#30772;&#22351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#37325;&#22823;&#25285;&#24551;&#65292;&#22240;&#20026;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#23558;&#39640;&#24230;&#20559;&#26012;&#30340;&#25968;&#25454;&#38598;&#20266;&#35013;&#25104;&#26131;&#20110;&#35299;&#20915;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#24448;&#24448;&#20250;&#35823;&#23548;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#38477;&#20302;&#38450;&#24481;&#25514;&#26045;&#24182;&#38169;&#35823;&#20272;&#35745;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#34920;&#26684;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#25285;&#24551;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#35782;&#21035;&#30495;&#23454;&#26631;&#31614;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#20351;&#24471;&#24694;&#24847;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#24456;&#23481;&#26131;&#36867;&#33073;&#26816;&#27979;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#39118;&#38505;&#20307;&#29616;&#22312;&#23545;&#25163;&#30340;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FALFA&#65288;&#24555;&#36895;&#23545;&#25239;&#24615;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#26631;&#31614;&#29983;&#25104;&#25915;&#20987;&#26041;&#27861;&#12290;FALFA&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Machine learning models are increasingly used in fields that require high reliability such as cybersecurity. However, these models remain vulnerable to various attacks, among which the adversarial label-flipping attack poses significant threats. In label-flipping attacks, the adversary maliciously flips a portion of training labels to compromise the machine learning model. This paper raises significant concerns as these attacks can camouflage a highly skewed dataset as an easily solvable classification problem, often misleading machine learning practitioners into lower defenses and miscalculations of potential risks. This concern amplifies in tabular data settings, where identifying true labels requires expertise, allowing malicious label-flipping attacks to easily slip under the radar. To demonstrate this risk is inherited in the adversary's objective, we propose FALFA (Fast Adversarial Label-Flipping Attack), a novel efficient attack for crafting adversarial labels. FALFA is based on
&lt;/p&gt;</description></item><item><title>MOFDiff&#26159;&#19968;&#31181;&#31895;&#31890;&#24230;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;MOF&#32467;&#26500;&#65292;&#24182;&#33021;&#26377;&#25928;&#29983;&#25104;&#26377;&#25928;&#21644;&#26032;&#39062;&#30340;MOF&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.10732</link><description>&lt;p&gt;
MOFDiff: &#38024;&#23545;&#37329;&#23646;&#26377;&#26426;&#39592;&#26550;&#35774;&#35745;&#30340;&#31895;&#31890;&#24230;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design. (arXiv:2310.10732v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10732
&lt;/p&gt;
&lt;p&gt;
MOFDiff&#26159;&#19968;&#31181;&#31895;&#31890;&#24230;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;MOF&#32467;&#26500;&#65292;&#24182;&#33021;&#26377;&#25928;&#29983;&#25104;&#26377;&#25928;&#21644;&#26032;&#39062;&#30340;MOF&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#23646;&#26377;&#26426;&#39592;&#26550; (MOFs) &#22240;&#20854;&#24322;&#24120;&#22810;&#23380;&#24615;&#21644;&#21487;&#35843;&#25511;&#30340;&#21270;&#23398;&#24615;&#36136;&#32780;&#22312;&#27668;&#20307;&#20648;&#23384;&#21644;&#25429;&#33719;&#30899;&#31561;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#23427;&#20204;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;&#26041;&#27861;&#26681;&#25454;&#24050;&#30693;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#32452;&#21512;&#20998;&#23376;&#26500;&#24314;&#22359;&#26469;&#29983;&#25104;&#20551;&#35774;&#24615;&#30340;MOFs&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37492;&#21035;&#39640;&#24615;&#33021;MOFs&#30340;&#33021;&#21147;&#24120;&#24120;&#21463;&#21040;&#30001;&#32467;&#26524;&#21270;&#23398;&#31354;&#38388;&#30340;&#26377;&#38480;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MOFDiff: &#36890;&#36807;&#23545;&#26500;&#24314;&#22359;&#30340;&#22352;&#26631;&#21644;&#26631;&#35782;&#36827;&#34892;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;MOF&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#35013;&#31639;&#27861;&#30830;&#23450;&#23436;&#25972;&#30340;&#21407;&#23376;&#32423;MOF&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20316;&#20026;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23562;&#37325;&#32622;&#25442;&#23545;&#31216;&#24615;&#21644;&#26059;&#36716;&#24179;&#31227;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#26377;&#25928;&#21644;&#26032;&#39062;MOF&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metal-organic frameworks (MOFs) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry. Their modular nature has enabled the use of template-based methods to generate hypothetical MOFs by combining molecular building blocks in accordance with known network topologies. However, the ability of these methods to identify top-performing MOFs is often hindered by the limited diversity of the resulting chemical space. In this work, we propose MOFDiff: a coarse-grained (CG) diffusion model that generates CG MOF structures through a denoising diffusion process over the coordinates and identities of the building blocks. The all-atom MOF structure is then determined through a novel assembly algorithm. Equivariant graph neural networks are used for the diffusion model to respect the permutational and roto-translational symmetries. We comprehensively evaluate our model's capability to generate valid and novel MOF struct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35266;&#27979;&#30740;&#31350;&#20013;&#25506;&#27979;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#20013;&#30340;&#21160;&#24577;&#26263;&#33021;&#37327;&#27169;&#22411;&#12290;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#20010;&#28508;&#22312;&#21442;&#25968;&#65292;&#21487;&#20197;&#39044;&#27979;&#21040;95%&#65288;99%&#65289;&#30340;DE&#21151;&#29575;&#35889;&#65292;&#22312;&#32771;&#34385;&#23431;&#23449;&#26041;&#24046;&#30340;&#39640;&#26031;&#35823;&#24046;&#33539;&#22260;&#20869;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10717</link><description>&lt;p&gt;
&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25506;&#27979;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#20013;&#30340;&#21160;&#24577;&#26263;&#33021;&#37327;
&lt;/p&gt;
&lt;p&gt;
A representation learning approach to probe for dynamical dark energy in matter power spectra. (arXiv:2310.10717v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35266;&#27979;&#30740;&#31350;&#20013;&#25506;&#27979;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#20013;&#30340;&#21160;&#24577;&#26263;&#33021;&#37327;&#27169;&#22411;&#12290;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#20010;&#28508;&#22312;&#21442;&#25968;&#65292;&#21487;&#20197;&#39044;&#27979;&#21040;95%&#65288;99%&#65289;&#30340;DE&#21151;&#29575;&#35889;&#65292;&#22312;&#32771;&#34385;&#23431;&#23449;&#26041;&#24046;&#30340;&#39640;&#26031;&#35823;&#24046;&#33539;&#22260;&#20869;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DE-VAE&#65292;&#19968;&#31181;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#35266;&#27979;&#30740;&#31350;&#20013;&#23547;&#25214;&#21160;&#24577;&#26263;&#33021;&#37327;&#65288;DE&#65289;&#27169;&#22411;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;DE-VAE&#22312;$.01-2.5 \ h/\rm{Mpc}$&#30340;&#27874;&#25968;$k$&#21644;&#22235;&#20010;&#32418;&#31227;&#20540;$z \in (0.1,0.48,0.78,1.5)$&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#26368;&#20856;&#22411;&#30340;&#21160;&#24577;DE&#21442;&#25968;&#21270;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#24182;&#19988;&#20351;&#29992;&#20004;&#20010;&#39069;&#22806;&#21442;&#25968;&#26469;&#25551;&#36848;&#28436;&#21270;&#30340;DE&#24577;&#26041;&#31243;&#12290;&#36825;&#20123;&#21387;&#32553;&#34920;&#31034;&#19982;&#26631;&#20934;&#30340;&#20919;&#26263;&#29289;&#36136;&#65288;CDM&#65289;&#21442;&#25968;&#36830;&#25509;&#65292;&#24182;&#26144;&#23556;&#22238;&#37325;&#26500;&#30340;&#25552;&#21319;&#65307;&#21387;&#32553;&#21644;&#37325;&#26500;&#32452;&#20214;&#37117;&#34987;&#21442;&#25968;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#21333;&#19968;&#30340;&#28508;&#22312;&#21442;&#25968;&#36275;&#20197;&#22312;&#24191;&#27867;&#30340;&#23431;&#23449;&#23398;&#21442;&#25968;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#21040;95%&#65288;99%&#65289;&#30340;DE&#21151;&#29575;&#35889;&#65292;&#24182;&#19988;&#22312;&#21253;&#25324;&#23431;&#23449;&#26041;&#24046;&#30340;&#39640;&#26031;&#35823;&#24046;&#30340;1$ \sigma$&#65288;2$ \sigma$&#65289;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DE-VAE, a variational autoencoder (VAE) architecture to search for a compressed representation of dynamical dark energy (DE) models in observational studies of the cosmic large-scale structure. DE-VAE is trained on matter power spectra boosts generated at wavenumbers $k\in(0.01-2.5) \ h/\rm{Mpc}$ and at four redshift values $z\in(0.1,0.48,0.78,1.5)$ for the most typical dynamical DE parametrization with two extra parameters describing an evolving DE equation of state. The boosts are compressed to a lower-dimensional representation, which is concatenated with standard cold dark matter (CDM) parameters and then mapped back to reconstructed boosts; both the compression and the reconstruction components are parametrized as neural networks. Remarkably, we find that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra generated over a broad range of cosmological parameters within $1\sigma$ ($2\sigma$) of a Gaussian error which includes cosmic variance, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10705</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#21322;&#23548;&#20307;&#26230;&#22278;&#22320;&#22270;&#20013;&#32570;&#38519;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#35782;&#21035;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#23398;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;ML&#22312;&#26230;&#22278;&#32570;&#38519;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#32570;&#20047;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#35797;&#22270;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#25991;&#29486;&#65292;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;ML&#31639;&#27861;&#22312;&#26230;&#22278;&#32570;&#38519;&#26816;&#27979;&#39046;&#22495;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23398;&#20998;&#31867;&#20307;&#31995;&#65292;&#35814;&#32454;&#20998;&#31867;&#20102;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#23376;&#25216;&#26415;&#21010;&#20998;&#12290;&#36825;&#20010;&#20998;&#31867;&#20307;&#31995;&#20174;&#24191;&#27867;&#30340;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#21040;&#20855;&#20307;&#30340;&#23376;&#25216;&#26415;&#32467;&#26463;&#12290;&#23427;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#39564;&#35777;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#30340;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;ACE&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#24322;&#24120;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#36827;&#24322;&#24120;&#26816;&#27979;&#30340;&#36879;&#26126;&#24230;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#35201;&#20040;&#26356;&#39640;&#65292;&#35201;&#20040;&#19982;&#40657;&#30418;&#19981;&#21487;&#35299;&#37322;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.10702</link><description>&lt;p&gt;
&#36879;&#26126;&#30340;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transparent Anomaly Detection via Concept-based Explanations. (arXiv:2310.10702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#30340;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;ACE&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#24322;&#24120;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#36827;&#24322;&#24120;&#26816;&#27979;&#30340;&#36879;&#26126;&#24230;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#35201;&#20040;&#26356;&#39640;&#65292;&#35201;&#20040;&#19982;&#40657;&#30418;&#19981;&#21487;&#35299;&#37322;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#25552;&#21319;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#38656;&#35201;&#36229;&#20986;&#20934;&#30830;&#24615;&#30340;&#36879;&#26126;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#24322;&#24120;&#26816;&#27979;&#30340;&#20219;&#21153;&#38598;&#20013;&#22312;&#25214;&#20986;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#36981;&#24490;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#23545;&#20854;&#32467;&#26524;&#36827;&#34892;&#28165;&#26224;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36879;&#26126;&#30340;&#24322;&#24120;&#26816;&#27979;&#27010;&#24565;&#35299;&#37322;&#65288;ACE&#65289;&#26041;&#27861;&#12290;ACE&#33021;&#22815;&#20197;&#27010;&#24565;&#30340;&#24418;&#24335;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#24322;&#24120;&#39044;&#27979;&#12290;&#25454;&#25105;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#25552;&#20986;&#35774;&#35745;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#30340;&#35770;&#25991;&#12290;&#38500;&#20102;&#20419;&#36827;&#24322;&#24120;&#26816;&#27979;&#30340;&#36879;&#26126;&#24230;&#65292;&#23427;&#36824;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#26524;&#35201;&#20040;&#26356;&#39640;&#65292;&#35201;&#20040;&#19982;&#40657;&#30418;&#19981;&#21487;&#35299;&#37322;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;ACE&#22312;&#19977;&#20010;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#25152;&#38656;&#36164;&#28304;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#30456;&#20851;&#26469;&#22686;&#24378;&#21152;&#36895;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10699</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Reusing Pretrained Models by Multi-linear Operators for Efficient Training. (arXiv:2310.10699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#25152;&#38656;&#36164;&#28304;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#30456;&#20851;&#26469;&#22686;&#24378;&#21152;&#36895;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914;bert2BERT&#21644;LiGO&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#27169;&#22411;&#21021;&#22987;&#21270;&#22823;&#22411;&#27169;&#22411;&#65288;&#31216;&#20026;&#8220;&#30446;&#26631;&#27169;&#22411;&#8221;&#65289;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#23613;&#31649;&#36825;&#20123;&#20808;&#21069;&#30740;&#31350;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#21482;&#26144;&#23556;&#37096;&#20998;&#26435;&#37325;&#25104;&#38271;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24573;&#30053;&#20102;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#30456;&#20851;&#24615;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37096;&#20998;&#26144;&#23556;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;&#23436;&#25972;&#30340;&#20449;&#24687;&#65292;&#24182;&#23548;&#33268;&#25104;&#38271;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#37325;&#36827;&#34892;&#32447;&#24615;&#30456;&#20851;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#21152;&#36895;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the ``target model''), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#27969;&#34892;&#24230;&#20559;&#24046;&#23548;&#33268;&#30340;&#27867;&#21270;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#21644;&#20943;&#23569;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#65292;&#20197;&#21450;&#19981;&#38656;&#20107;&#20808;&#20102;&#35299;&#27979;&#35797;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#21435;&#20559;&#35265;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;OOD&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10696</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#21327;&#21516;&#36807;&#28388;&#19982;&#27969;&#34892;&#24230;&#20998;&#24067;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Collaborative Filtering to Popularity Distribution Shift. (arXiv:2310.10696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#27969;&#34892;&#24230;&#20559;&#24046;&#23548;&#33268;&#30340;&#27867;&#21270;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#21644;&#20943;&#23569;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#65292;&#20197;&#21450;&#19981;&#38656;&#20107;&#20808;&#20102;&#35299;&#27979;&#35797;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#21435;&#20559;&#35265;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;OOD&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39046;&#20808;&#30340;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#27169;&#22411;&#20013;&#65292;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#34920;&#31034;&#24448;&#24448;&#20250;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27969;&#34892;&#24230;&#20559;&#24046;&#20316;&#20026;&#24555;&#25463;&#26041;&#24335;&#12290;&#27969;&#34892;&#24230;&#24555;&#25463;&#26041;&#24335;&#23545;&#20110;&#22312;&#20998;&#24067;&#65288;ID&#65289;&#24615;&#33021;&#19978;&#26159;&#22909;&#30340;&#65292;&#20294;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#65288;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#27969;&#34892;&#24230;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#26102;&#65289;&#65292;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#21435;&#20559;&#35265;&#31574;&#30053;&#23581;&#35797;&#35780;&#20272;&#34920;&#31034;&#20013;&#30340;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20943;&#23569;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#19981;&#36275;&#20043;&#22788;&#65306;&#65288;1&#65289;&#22312;&#27979;&#37327;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#26102;&#65292;&#22823;&#22810;&#25968;&#31574;&#30053;&#21482;&#20351;&#29992;&#21333;&#19968;&#26041;&#38754;&#30340;&#32479;&#35745;&#25351;&#26631;&#65288;&#21363;&#39033;&#30446;&#39057;&#29575;&#23545;&#39033;&#30446;&#21644;&#29992;&#25143;&#39057;&#29575;&#23545;&#29992;&#25143;&#26041;&#38754;&#65289;&#65292;&#19981;&#33021;&#36866;&#24212;&#29992;&#25143;-&#39033;&#30446;&#23545;&#30340;&#32452;&#21512;&#31243;&#24230;&#65307;&#65288;2&#65289;&#22312;&#20943;&#23569;&#24555;&#25463;&#26041;&#24335;&#26102;&#65292;&#35768;&#22810;&#31574;&#30053;&#20551;&#35774;&#27979;&#35797;&#20998;&#24067;&#20107;&#20808;&#24050;&#30693;&#12290;&#36825;&#23548;&#33268;&#36136;&#37327;&#36739;&#20302;&#30340;&#21435;&#20559;&#35265;&#34920;&#31034;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#20197;&#29306;&#29298;OOD&#27867;&#21270;&#24615;&#33021;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
In leading collaborative filtering (CF) models, representations of users and items are prone to learn popularity bias in the training data as shortcuts. The popularity shortcut tricks are good for in-distribution (ID) performance but poorly generalized to out-of-distribution (OOD) data, i.e., when popularity distribution of test data shifts w.r.t. the training one. To close the gap, debiasing strategies try to assess the shortcut degrees and mitigate them from the representations. However, there exist two deficiencies: (1) when measuring the shortcut degrees, most strategies only use statistical metrics on a single aspect (i.e., item frequency on item and user frequency on user aspect), failing to accommodate the compositional degree of a user-item pair; (2) when mitigating shortcuts, many strategies assume that the test distribution is known in advance. This results in low-quality debiased representations. Worse still, these strategies achieve OOD generalizability with a sacrifice on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35780;&#20998;&#27169;&#22411;&#26469;&#29983;&#25104;&#20855;&#26377;&#33258;&#36866;&#24212;&#26230;&#32990;&#30340;&#31283;&#23450;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21487;&#29992;&#25968;&#25454;&#20013;&#30340;&#26230;&#26684;&#65292;&#20351;&#29992;&#20004;&#20010;&#21435;&#22122;&#36807;&#31243;&#24182;&#34892;&#29983;&#25104;&#26230;&#26684;&#65292;&#20197;&#23454;&#29616;&#23545;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#26230;&#20307;&#32467;&#26500;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10695</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35780;&#20998;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#33258;&#36866;&#24212;&#26230;&#32990;&#30340;&#31283;&#23450;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Score-Based Models for Generating Stable Structures with Adaptive Crystal Cells. (arXiv:2310.10695v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35780;&#20998;&#27169;&#22411;&#26469;&#29983;&#25104;&#20855;&#26377;&#33258;&#36866;&#24212;&#26230;&#32990;&#30340;&#31283;&#23450;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21487;&#29992;&#25968;&#25454;&#20013;&#30340;&#26230;&#26684;&#65292;&#20351;&#29992;&#20004;&#20010;&#21435;&#22122;&#36807;&#31243;&#24182;&#34892;&#29983;&#25104;&#26230;&#26684;&#65292;&#20197;&#23454;&#29616;&#23545;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#26230;&#20307;&#32467;&#26500;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#65292;&#21457;&#29616;&#26032;&#30340;&#21151;&#33021;&#24615;&#21644;&#31283;&#23450;&#30340;&#26448;&#26009;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#65288;&#20363;&#22914;&#21270;&#23398;&#31283;&#23450;&#24615;&#21644;&#25351;&#23450;&#30340;&#21270;&#23398;&#25104;&#20998;&#65289;&#30340;&#26032;&#26230;&#20307;&#32467;&#26500;&#12290;&#19982;&#29983;&#25104;&#20998;&#23376;&#30456;&#27604;&#65292;&#26230;&#20307;&#32467;&#26500;&#30001;&#20110;&#26230;&#20307;&#30340;&#21608;&#26399;&#24615;&#20197;&#21450;&#19982;&#31354;&#38388;&#32676;&#30456;&#20851;&#30340;&#29305;&#23450;&#23545;&#31216;&#24615;&#32422;&#26463;&#32780;&#24102;&#26469;&#26032;&#30340;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#36864;&#28779;Langevin&#21160;&#21147;&#23398;&#30340;&#35780;&#20998;&#27010;&#29575;&#27169;&#22411;&#26469;&#36866;&#24212;&#26230;&#20307;&#29983;&#25104;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#26230;&#32990;&#30340;&#26230;&#26684;&#19981;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26230;&#26684;&#20174;&#21487;&#29992;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#32780;&#22312;&#29983;&#25104;&#26032;&#30340;&#21270;&#23398;&#32467;&#26500;&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#20004;&#20010;&#21435;&#22122;&#36807;&#31243;&#24182;&#34892;&#29983;&#25104;&#26230;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of new functional and stable materials is a big challenge due to its complexity. This work aims at the generation of new crystal structures with desired properties, such as chemical stability and specified chemical composition, by using machine learning generative models. Compared to the generation of molecules, crystal structures pose new difficulties arising from the periodic nature of the crystal and from the specific symmetry constraints related to the space group. In this work, score-based probabilistic models based on annealed Langevin dynamics, which have shown excellent performance in various applications, are adapted to the task of crystal generation. The novelty of the presented approach resides in the fact that the lattice of the crystal cell is not fixed. During the training of the model, the lattice is learned from the available data, whereas during the sampling of a new chemical structure, two denoising processes are used in parallel to generate the lattice 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;iNaturalist&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#32467;&#26500;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.10693</link><description>&lt;p&gt;
iNaturalist&#20844;&#27665;&#31185;&#23398;&#31038;&#21306;&#30340;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Network Analysis of the iNaturalist Citizen Science Community. (arXiv:2310.10693v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;iNaturalist&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#32467;&#26500;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20844;&#27665;&#31185;&#23398;&#24050;&#25104;&#20026;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20854;&#33021;&#22815;&#20174;&#25968;&#21315;&#21517;&#20844;&#27665;&#31185;&#23398;&#23478;&#37027;&#37324;&#33719;&#21462;&#25968;&#25454;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#20351;&#20854;&#20855;&#26377;&#26080;&#21487;&#26367;&#20195;&#30340;&#20215;&#20540;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#20132;&#20114;&#21644;&#32467;&#26500;&#20173;&#28982;&#34987;&#20102;&#35299;&#24456;&#23569;&#24182;&#19988;&#34987;&#23569;&#20998;&#26512;&#12290;&#25105;&#20204;&#20197;iNaturalist&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#26512;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;iNaturalist&#20013;&#30340;&#25968;&#25454;&#26500;&#24314;&#20026;&#19968;&#20010;&#20108;&#20998;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21487;&#35270;&#21270;&#21644;&#24050;&#24314;&#31435;&#30340;&#32593;&#32476;&#31185;&#23398;&#25216;&#26415;&#26469;&#20102;&#35299;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#32467;&#26500;&#21644;&#20132;&#20114;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;iNaturalist&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#30456;&#23545;&#20110;&#20854;&#20182;&#24120;&#35265;&#30340;&#22522;&#20934;&#32593;&#32476;&#20855;&#26377;&#29420;&#29305;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#35777;&#26126;&#20102;&#35813;&#32593;&#32476;&#21487;&#20197;&#29992;&#20110;&#33719;&#24471;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, citizen science has become a larger and larger part of the scientific community. Its ability to crowd source data and expertise from thousands of citizen scientists makes it invaluable. Despite the field's growing popularity, the interactions and structure of citizen science projects are still poorly understood and under analyzed. We use the iNaturalist citizen science platform as a case study to analyze the structure of citizen science projects. We frame the data from iNaturalist as a bipartite network and use visualizations as well as established network science techniques to gain insights into the structure and interactions between users in citizen science projects. Finally, we propose a novel unique benchmark for network science research by using the iNaturalist data to create a network which has an unusual structure relative to other common benchmark networks. We demonstrate using a link prediction task that this network can be used to gain novel insights into a v
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#23383;VLSI&#30005;&#36335;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10691</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#39640;&#25968;&#23383;VLSI&#30005;&#36335;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#65306;&#20851;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation. (arXiv:2310.10691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#25968;&#23383;VLSI&#30005;&#36335;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#30005;&#23376;&#30005;&#36335;&#30340;&#20154;&#24037;&#25968;&#25454;&#29983;&#25104;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25552;&#39640;&#21518;&#32493;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#29992;&#20110;&#24615;&#33021;&#35780;&#20272;&#12289;&#35774;&#35745;&#21644;&#27979;&#35797;&#31561;&#20219;&#21153;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;HSPICE&#35774;&#35745;&#29615;&#22659;&#20013;&#30340;22&#32435;&#31859;CMOS&#25216;&#26415;&#33410;&#28857;&#27169;&#25311;&#24471;&#21040;&#20102;&#20195;&#34920;&#24615;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#22312;&#25968;&#23383;&#30005;&#36335;&#30340;VLSI&#35774;&#35745;&#30340;&#39044;&#27979;&#20998;&#26512;&#20013;&#20855;&#26377;&#26126;&#26174;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has seen remarkable growth over the past few years, with diffusion models being state-of-the-art for image generation. This study investigates the use of diffusion models in generating artificial data generation for electronic circuits for enhancing the accuracy of subsequent machine learning models in tasks such as performance assessment, design, and testing when training data is usually known to be very limited. We utilize simulations in the HSPICE design environment with 22nm CMOS technology nodes to obtain representative real training data for our proposed diffusion model. Our results demonstrate the close resemblance of synthetic data using diffusion model to real data. We validate the quality of generated data, and demonstrate that data augmentation certainly effective in predictive analysis of VLSI design for digital circuits.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PS-AAS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#20013;&#12290;&#36890;&#36807;&#21019;&#24314;&#31639;&#27861;&#34892;&#20026;&#20803;&#34920;&#31034;&#65292;&#26500;&#24314;&#31639;&#27861;&#20043;&#38388;&#30340;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31639;&#27861;&#36873;&#25321;&#22810;&#26679;&#21270;&#12289;&#20195;&#34920;&#24615;&#21644;&#38750;&#20887;&#20313;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31639;&#27861;&#36873;&#25321;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10685</link><description>&lt;p&gt;
PS-AAS: &#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#22312;&#40657;&#30418;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization. (arXiv:2310.10685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10685
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PS-AAS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#20013;&#12290;&#36890;&#36807;&#21019;&#24314;&#31639;&#27861;&#34892;&#20026;&#20803;&#34920;&#31034;&#65292;&#26500;&#24314;&#31639;&#27861;&#20043;&#38388;&#30340;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31639;&#27861;&#36873;&#25321;&#22810;&#26679;&#21270;&#12289;&#20195;&#34920;&#24615;&#21644;&#38750;&#20887;&#20313;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31639;&#27861;&#36873;&#25321;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#65288;AAS&#65289;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35201;&#36873;&#25321;&#30340;&#31639;&#27861;&#25237;&#36164;&#32452;&#21512;&#12290;&#36873;&#25321;&#25237;&#36164;&#32452;&#21512;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#30340;&#39640;&#28789;&#27963;&#24615;&#21644;AAS&#20219;&#21153;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#23454;&#38469;&#20013;&#65292;&#36873;&#25321;&#25237;&#36164;&#32452;&#21512;&#30340;&#26368;&#24120;&#35265;&#26041;&#27861;&#21487;&#33021;&#26159;&#22312;&#19968;&#20123;&#24863;&#20852;&#36259;&#30340;&#21442;&#32771;&#20219;&#21153;&#20013;&#36873;&#25321;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#26367;&#20195;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21019;&#24314;&#31639;&#27861;&#34892;&#20026;&#20803;&#34920;&#31034;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#20803;&#34920;&#31034;&#30456;&#20284;&#24615;&#20174;&#19968;&#32452;&#31639;&#27861;&#20013;&#26500;&#24314;&#22270;&#24418;&#65292;&#24182;&#24212;&#29992;&#22270;&#24418;&#31639;&#27861;&#26469;&#36873;&#25321;&#22810;&#26679;&#21270;&#12289;&#20195;&#34920;&#24615;&#21644;&#38750;&#20887;&#20313;&#30340;&#26368;&#32456;&#25237;&#36164;&#32452;&#21512;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20803;&#34920;&#31034;&#25216;&#26415;&#65288;SHAP&#21644;performance2vec&#65289;&#26469;&#36873;&#25321;&#38468;&#21152;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20849;&#35745;324&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of automated algorithm selection (AAS) strongly depends on the portfolio of algorithms to choose from. Selecting the portfolio is a non-trivial task that requires balancing the trade-off between the higher flexibility of large portfolios with the increased complexity of the AAS task. In practice, probably the most common way to choose the algorithms for the portfolio is a greedy selection of the algorithms that perform well in some reference tasks of interest.  We set out in this work to investigate alternative, data-driven portfolio selection techniques. Our proposed method creates algorithm behavior meta-representations, constructs a graph from a set of algorithms based on their meta-representation similarity, and applies a graph algorithm to select a final portfolio of diverse, representative, and non-redundant algorithms. We evaluate two distinct meta-representation techniques (SHAP and performance2vec) for selecting complementary portfolios from a total of 324 diff
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.10683</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10683
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#23398;&#20064;&#65292;&#21363;&#24536;&#35760;&#19981;&#21463;&#27426;&#36814;&#30340;&#65288;&#38750;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33267;&#23569;&#19977;&#31181;&#24773;&#22659;&#21487;&#20197;&#20174;&#21435;&#23398;&#20064;&#20013;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65306;&#65288;1&#65289;&#21024;&#38500;&#26377;&#23475;&#22238;&#22797;&#65292;&#65288;2&#65289;&#25353;&#35201;&#27714;&#21024;&#38500;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#65288;3&#65289;&#28040;&#38500;&#24187;&#35273;&#12290;&#20316;&#20026;&#23545;&#40784;&#25216;&#26415;&#30340;&#19968;&#31181;&#65292;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#21482;&#38656;&#35201;&#36127;&#38754;&#65288;&#20363;&#22914;&#26377;&#23475;&#65289;&#31034;&#20363;&#65292;&#36825;&#27604;&#22312;RLHF&#65288;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#25152;&#38656;&#30340;&#27491;&#38754;&#65288;&#20363;&#22914;&#26377;&#24110;&#21161;&#19988;&#36890;&#24120;&#30001;&#20154;&#31867;&#32534;&#20889;&#65289;&#31034;&#20363;&#26356;&#23481;&#26131;&#21644;&#26356;&#20415;&#23452;&#22320;&#25910;&#38598;&#65288;&#20363;&#22914;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#25110;&#29992;&#25143;&#25253;&#21578;&#65289;&#65307;&#65288;2&#65289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65307;&#65288;3&#65289;&#24403;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#35757;&#32451;&#26679;&#26412;&#23548;&#33268;&#20102;&#19981;&#33391;&#34892;&#20026;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#25928;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;LLM&#21435;&#23398;&#20064;&#30340;&#24037;&#20316;&#20043;&#19968;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#22312;LLM&#21435;&#23398;&#20064;&#20013;&#21046;&#23450;&#20102;&#35774;&#32622;&#12289;&#30446;&#26631;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20174;&#19994;&#32773;&#21482;&#26377;&#26377;&#38480;&#30340;
&lt;/p&gt;
&lt;p&gt;
We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#30340;&#38477;&#32500;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#34920;&#36798;&#30340;&#20154;&#31867;&#24773;&#24863;&#21644;&#35266;&#28857;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10672</link><description>&lt;p&gt;
&#34701;&#21512;&#37327;&#23376;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum-Classical Machine Learning for Sentiment Analysis. (arXiv:2310.10672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#30340;&#38477;&#32500;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#34920;&#36798;&#30340;&#20154;&#31867;&#24773;&#24863;&#21644;&#35266;&#28857;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#21512;&#20316;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#34920;&#36798;&#30340;&#20154;&#31867;&#24773;&#24863;&#21644;&#35266;&#28857;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#32463;&#20856;&#30340;&#38477;&#32500;&#25216;&#26415;&#65288;&#22914;PCA&#21644;Haar&#23567;&#27874;&#21464;&#25442;&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#33521;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#30340;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#38477;&#32500;&#22788;&#29702;&#21518;&#65292;&#22522;&#20110;&#37327;&#23376;&#30340;&#28151;&#21512;&#31639;&#27861;&#30340;&#24615;&#33021;&#19968;&#33268;&#19988;&#20248;&#20110;&#32463;&#20856;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collaboration between quantum computing and classical machine learning offers potential advantages in natural language processing, particularly in the sentiment analysis of human emotions and opinions expressed in large-scale datasets. In this work, we propose a methodology for sentiment analysis using hybrid quantum-classical machine learning algorithms. We investigate quantum kernel approaches and variational quantum circuit-based classifiers and integrate them with classical dimension reduction techniques such as PCA and Haar wavelet transform. The proposed methodology is evaluated using two distinct datasets, based on English and Bengali languages. Experimental results show that after dimensionality reduction of the data, performance of the quantum-based hybrid algorithms were consistent and better than classical methods.
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;OMVI&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;OMD&#65292;&#36890;&#36807;&#35782;&#21035;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#26469;&#24212;&#23545;&#24694;&#24847;&#36719;&#20214;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.10670</link><description>&lt;p&gt;
&#26234;&#33021;OMVI&#65306;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#35782;&#21035;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
Smart OMVI: Obfuscated Malware Variant Identification using a novel dataset. (arXiv:2310.10670v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10670
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;OMVI&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;OMD&#65292;&#36890;&#36807;&#35782;&#21035;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#26469;&#24212;&#23545;&#24694;&#24847;&#36719;&#20214;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#32593;&#32476;&#23433;&#20840;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#38543;&#30528;&#27599;&#22825;&#35745;&#31639;&#26426;&#20351;&#29992;&#30340;&#22686;&#38271;&#12290;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29616;&#22312;&#36827;&#34892;&#30340;&#19981;&#20165;&#20165;&#26159;&#30149;&#27602;&#20256;&#25773;&#21644;&#35745;&#31639;&#26426;&#40657;&#23458;&#34892;&#20026;&#12290;&#30001;&#20110;&#23041;&#32961;&#21040;&#19968;&#20010;&#22269;&#23478;&#30340;&#29983;&#23384;&#65292;&#32593;&#32476;&#25112;&#20105;&#24050;&#32463;&#21457;&#23637;&#36215;&#26469;&#12290;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#30340;&#31532;&#19968;&#36947;&#38450;&#32447;&#65292;&#24182;&#19988;&#26159;&#32593;&#32476;&#29359;&#32618;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#22825;&#65292;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#30446;&#26631;&#20247;&#22810;&#30340;&#35745;&#31639;&#26426;&#29992;&#25143;&#12289;&#20225;&#19994;&#21644;&#25919;&#24220;&#26426;&#26500;&#65292;&#36896;&#25104;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#25439;&#22833;&#12290;&#23613;&#31649;&#23433;&#20840;&#19987;&#23478;&#25317;&#26377;&#21508;&#31181;&#24037;&#20855;&#26469;&#35782;&#21035;&#24694;&#24847;&#36719;&#20214;&#65292;&#20294;&#24694;&#24847;&#36719;&#20214;&#21487;&#20197;&#36890;&#36807;&#20854;&#35774;&#35745;&#24072;&#36827;&#34892;&#24494;&#23567;&#24039;&#22937;&#30340;&#35843;&#25972;&#26469;&#35268;&#36991;&#22810;&#20010;&#26432;&#27602;&#36719;&#20214;&#30340;&#26816;&#27979;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;(OMD)&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;40&#20010;&#19981;&#21516;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#65292;&#26377;21924&#20010;&#26679;&#26412;&#65292;&#24182;&#19988;&#37319;&#29992;&#20102;&#27169;&#25311;&#25915;&#20987;&#32773;&#31574;&#30053;&#30340;&#28151;&#28102;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity has become a significant issue in the digital era as a result of the growth in everyday computer use. Cybercriminals now engage in more than virus distribution and computer hacking. Cyberwarfare has developed as a result because it has become a threat to a nation's survival. Malware analysis serves as the first line of defence against an attack and is a significant component of cybercrime. Every day, malware attacks target a large number of computer users, businesses, and governmental agencies, causing billions of dollars in losses. Malware may evade multiple AV software with a very minor, cunning tweak made by its designers, despite the fact that security experts have a variety of tools at their disposal to identify it. To address this challenge, a new dataset called the Obfuscated Malware Dataset (OMD) has been developed. This dataset comprises 40 distinct malware families having 21924 samples, and it incorporates obfuscation techniques that mimic the strategies employe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#22270;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#32593;&#32476;&#38887;&#24615;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#20449;&#24687;&#20256;&#25773;&#31561;&#39046;&#22495;&#20855;&#26377;&#36739;&#39640;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10667</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#22270;&#32452;&#21512;&#20248;&#21270;&#22686;&#24378;&#32593;&#32476;&#38887;&#24615;&#65306;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#20449;&#24687;&#20256;&#25773;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Resilience through Machine Learning-powered Graph Combinatorial Optimization: Applications in Cyber Defense and Information Diffusion. (arXiv:2310.10667v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#22270;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#26469;&#22686;&#24378;&#32593;&#32476;&#38887;&#24615;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#21644;&#20449;&#24687;&#20256;&#25773;&#31561;&#39046;&#22495;&#20855;&#26377;&#36739;&#39640;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#21644;&#32593;&#32476;&#36890;&#20449;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#21450;&#20854;&#24212;&#29992;&#29615;&#22659;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#30001;&#20110;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#26356;&#23481;&#26131;&#21457;&#29983;&#30828;&#20214;&#25925;&#38556;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#20013;&#24515;&#21270;&#24212;&#29992;&#32780;&#35328;&#65292;&#32593;&#32476;&#38887;&#24615;&#23545;&#20110;&#20943;&#23569;&#25915;&#20987;&#30340;&#24433;&#21709;&#24182;&#30830;&#20445;&#32593;&#32476;&#22312;&#25915;&#20987;&#12289;&#25925;&#38556;&#25110;&#20013;&#26029;&#26399;&#38388;&#25552;&#20379;&#21487;&#25509;&#21463;&#30340;&#26381;&#21153;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#32593;&#32476;&#38887;&#24615;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#32593;&#32476;&#38887;&#24615;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#30830;&#23450;&#32593;&#32476;&#20013;&#30340;&#29942;&#39048;&#33410;&#28857;&#21644;&#36793;&#32536;&#65292;&#24182;&#35774;&#35745;&#31215;&#26497;&#30340;&#21709;&#24212;&#25514;&#26045;&#26469;&#20445;&#25252;&#32593;&#32476;&#20813;&#21463;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#32771;&#34385;&#26356;&#23485;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#32593;&#32476;&#23433;&#20840;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the burgeoning advancements of computing and network communication technologies, network infrastructures and their application environments have become increasingly complex. Due to the increased complexity, networks are more prone to hardware faults and highly susceptible to cyber-attacks. Therefore, for rapidly growing network-centric applications, network resilience is essential to minimize the impact of attacks and to ensure that the network provides an acceptable level of services during attacks, faults or disruptions. In this regard, this thesis focuses on developing effective approaches for enhancing network resilience. Existing approaches for enhancing network resilience emphasize on determining bottleneck nodes and edges in the network and designing proactive responses to safeguard the network against attacks. However, existing solutions generally consider broader application domains and possess limited applicability when applied to specific application areas such as cyber
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#32852;&#21512;FDIA&#26816;&#27979;&#21644;&#23450;&#20301;&#26694;&#26550;&#65292;&#20197;&#25552;&#21462;&#29289;&#29702;&#22240;&#26524;&#20851;&#31995;&#26469;&#26816;&#27979;&#21644;&#23450;&#20301;&#34394;&#20551;&#25968;&#25454;&#25554;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.10666</link><description>&lt;p&gt;
&#20174;&#27979;&#37327;&#20013;&#25552;&#21462;&#29289;&#29702;&#22240;&#26524;&#20851;&#31995;&#20197;&#26816;&#27979;&#21644;&#23450;&#20301;&#34394;&#20551;&#25968;&#25454;&#25554;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Extracting Physical Causality from Measurements to Detect and Localize False Data Injection Attacks. (arXiv:2310.10666v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#32852;&#21512;FDIA&#26816;&#27979;&#21644;&#23450;&#20301;&#26694;&#26550;&#65292;&#20197;&#25552;&#21462;&#29289;&#29702;&#22240;&#26524;&#20851;&#31995;&#26469;&#26816;&#27979;&#21644;&#23450;&#20301;&#34394;&#20551;&#25968;&#25454;&#25554;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#25968;&#25454;&#25554;&#20837;&#25915;&#20987;&#65288;FDIA&#65289;&#24050;&#25104;&#20026;&#29616;&#20195;&#32593;&#32476;&#29289;&#29702;&#30005;&#21147;&#31995;&#32479;&#20013;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FDIA&#26816;&#27979;&#25216;&#26415;&#23558;&#21407;&#22987;&#27979;&#37327;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20197;&#21306;&#20998;&#27491;&#24120;&#26679;&#26412;&#21644;&#25915;&#20987;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#26356;&#22810;&#22320;&#20851;&#27880;&#25968;&#25454;&#20540;&#30340;&#32479;&#35745;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#31995;&#32479;&#25805;&#20316;&#28857;&#21464;&#21270;&#25110;FDIA&#31867;&#22411;&#21644;&#24378;&#24230;&#21464;&#21270;&#24341;&#36215;&#30340;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;FDIA&#23450;&#20301;&#20219;&#21153;&#26469;&#35828;&#12290;&#32780;&#22240;&#26524;&#25512;&#26029;&#21017;&#25552;&#21462;&#20102;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#21327;&#21516;&#27874;&#21160;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#26524;&#24615;&#27169;&#24335;&#30001;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#65288;&#22914;&#27431;&#22982;&#23450;&#24459;&#21644;&#22522;&#23572;&#38669;&#22827;&#23450;&#24459;&#65289;&#30830;&#23450;&#12290;&#23427;&#20204;&#23545;FDIA&#24341;&#36215;&#30340;&#29289;&#29702;&#23450;&#24459;&#36829;&#21453;&#25935;&#24863;&#65292;&#20294;&#22312;&#31995;&#32479;&#25805;&#20316;&#28857;&#28418;&#31227;&#26102;&#36235;&#20110;&#31283;&#23450;&#12290;&#20511;&#21161;&#36825;&#20010;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#32852;&#21512;FDIA&#26816;&#27979;&#21644;&#23450;&#20301;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
False Data Injection Attack (FDIA) has become a growing concern in modern cyber-physical power systems. Most existing FDIA detection techniques project the raw measurement data into a high-dimensional latent space to separate normal and attacked samples. These approaches focus more on the statistical correlations of data values and are therefore susceptible to data distribution drifts induced by changes in system operating points or changes in FDIA types and strengths, especially for FDIA localization tasks. Causal inference, on the other hand, extracts the causality behind the coordinated fluctuations of different measurements. The causality patterns are determined by fundamental physical laws such as Ohm's Law and Kirchhoff's Law. They are sensitive to the violation of physical laws caused by FDIA, but tend to remain stable with the drift of system operating points. Leveraging this advantage, this paper proposes a joint FDIA detection and localization framework based on causal infere
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#35752;&#35770;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#25193;&#23637;&#29616;&#23454;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#20803;&#23431;&#23449;&#30340;&#38544;&#31169;&#38382;&#39064;&#20196;&#20154;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#27785;&#28024;&#24335;&#34394;&#25311;&#20307;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#20803;&#23431;&#23449;&#23558;&#21033;&#29992;&#22810;&#31181;&#25216;&#26415;&#36827;&#34892;&#21457;&#23637;&#65292;&#24182;&#25910;&#38598;&#29992;&#25143;&#25968;&#25454;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#27785;&#28024;&#24335;&#30340;&#26381;&#21153;&#65292;&#20294;&#36825;&#20063;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.10665</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#25193;&#23637;&#29616;&#23454;&#65288;AI-XR&#65289;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Privacy Preservation in Artificial Intelligence and Extended Reality (AI-XR) Metaverses: A Survey. (arXiv:2310.10665v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#35752;&#35770;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#25193;&#23637;&#29616;&#23454;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#20803;&#23431;&#23449;&#30340;&#38544;&#31169;&#38382;&#39064;&#20196;&#20154;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#27785;&#28024;&#24335;&#34394;&#25311;&#20307;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#20803;&#23431;&#23449;&#23558;&#21033;&#29992;&#22810;&#31181;&#25216;&#26415;&#36827;&#34892;&#21457;&#23637;&#65292;&#24182;&#25910;&#38598;&#29992;&#25143;&#25968;&#25454;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#27785;&#28024;&#24335;&#30340;&#26381;&#21153;&#65292;&#20294;&#36825;&#20063;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#26159;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#23427;&#35774;&#24819;&#20102;&#19968;&#20010;&#34394;&#25311;&#23431;&#23449;&#65292;&#19968;&#20010;&#21327;&#20316;&#31354;&#38388;&#65292;&#20010;&#20307;&#21487;&#20197;&#22312;&#20854;&#20013;&#20114;&#21160;&#12289;&#21019;&#36896;&#21644;&#21442;&#19982;&#21508;&#31181;&#27963;&#21160;&#12290;&#38543;&#30528;&#36825;&#20010;&#27010;&#24565;&#30340;&#21457;&#23637;&#21644;&#27785;&#28024;&#24335;&#34394;&#25311;&#20307;&#39564;&#30340;&#26222;&#21450;&#65292;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20803;&#23431;&#23449;&#38544;&#31169;&#38382;&#39064;&#25351;&#30340;&#26159;&#22312;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#20013;&#30340;&#20010;&#20154;&#20449;&#24687;&#21644;&#25968;&#25454;&#38544;&#31169;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20851;&#20999;&#65292;&#22240;&#20026;&#20849;&#20139;&#30340;VR&#31354;&#38388;&#27010;&#24565;&#21464;&#24471;&#26356;&#21152;&#21487;&#25509;&#36817;&#12290;&#20803;&#23431;&#23449;&#23558;&#20511;&#21161;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#12289;&#28151;&#21512;&#29616;&#23454;&#65288;MR&#65289;&#21644;&#22522;&#20110;5G/6G&#36890;&#20449;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#27785;&#28024;&#24335;&#26381;&#21153;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20803;&#23431;&#23449;&#20381;&#36182;&#20110;&#25910;&#38598;&#31934;&#32454;&#21270;&#30340;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#23548;&#33268;&#20102;&#21508;&#31181;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#23436;&#20840;&#23454;&#29616;&#20803;&#23431;&#23449;&#30340;&#28508;&#21147;&#20043;&#21069;&#65292;&#38544;&#31169;&#20445;&#25252;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The metaverse is a nascent concept that envisions a virtual universe, a collaborative space where individuals can interact, create, and participate in a wide range of activities. Privacy in the metaverse is a critical concern as the concept evolves and immersive virtual experiences become more prevalent. The metaverse privacy problem refers to the challenges and concerns surrounding the privacy of personal information and data within Virtual Reality (VR) environments as the concept of a shared VR space becomes more accessible. Metaverse will harness advancements from various technologies such as Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and 5G/6G-based communication to provide personalized and immersive services to its users. Moreover, to enable more personalized experiences, the metaverse relies on the collection of fine-grained user data that leads to various privacy issues. Therefore, before the potential of the metaverse can be fully realized, privacy
&lt;/p&gt;</description></item><item><title>Nebula&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21160;&#24577;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12290;&#23427;&#33021;&#22815;&#27010;&#25324;&#19981;&#21516;&#30340;&#34892;&#20026;&#34920;&#31034;&#21644;&#26684;&#24335;&#65292;&#24182;&#32467;&#21512;&#21160;&#24577;&#26085;&#24535;&#25253;&#21578;&#20013;&#30340;&#24322;&#26500;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Nebula&#22312;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.10664</link><description>&lt;p&gt;
Nebula:&#29992;&#20110;&#21160;&#24577;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Nebula: Self-Attention for Dynamic Malware Analysis. (arXiv:2310.10664v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10664
&lt;/p&gt;
&lt;p&gt;
Nebula&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21160;&#24577;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12290;&#23427;&#33021;&#22815;&#27010;&#25324;&#19981;&#21516;&#30340;&#34892;&#20026;&#34920;&#31034;&#21644;&#26684;&#24335;&#65292;&#24182;&#32467;&#21512;&#21160;&#24577;&#26085;&#24535;&#25253;&#21578;&#20013;&#30340;&#24322;&#26500;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Nebula&#22312;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#20998;&#26512;&#36890;&#36807;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#25191;&#34892;&#31243;&#24207;&#24182;&#23558;&#20854;&#34892;&#20026;&#23384;&#20648;&#22312;&#26085;&#24535;&#25253;&#21578;&#20013;&#65292;&#21487;&#20197;&#26816;&#27979;Windows&#24694;&#24847;&#36719;&#20214;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#22987;&#22312;&#36825;&#20123;&#25253;&#21578;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25110;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#32771;&#34385;&#20102;&#21367;&#31215;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65292;&#21482;&#20851;&#27880;&#36816;&#34892;&#26102;&#35843;&#29992;&#30340;API&#65292;&#24182;&#26410;&#32771;&#34385;&#20854;&#20182;&#30456;&#20851;&#30340;&#24322;&#26500;&#20449;&#24687;&#26469;&#28304;&#65292;&#22914;&#32593;&#32476;&#21644;&#25991;&#20214;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24456;&#38590;&#33719;&#21462;&#65292;&#36825;&#38480;&#21046;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#20013;&#32467;&#26524;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Nebula&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#12289;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#65292;&#21487;&#20197;&#27010;&#25324;&#19981;&#21516;&#30340;&#34892;&#20026;&#34920;&#31034;&#21644;&#26684;&#24335;&#65292;&#32467;&#21512;&#21160;&#24577;&#26085;&#24535;&#25253;&#21578;&#20013;&#30340;&#24322;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Nebula&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#22312;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic analysis enables detecting Windows malware by executing programs in a controlled environment, and storing their actions in log reports. Previous work has started training machine learning models on such reports to perform either malware detection or malware classification. However, most of the approaches (i) have only considered convolutional and long-short term memory networks, (ii) they have been built focusing only on APIs called at runtime, without considering other relevant though heterogeneous sources of information like network and file operations, and (iii) the code and pretrained models are hardly available, hindering reproducibility of results in this research area. In this work, we overcome these limitations by presenting Nebula, a versatile, self-attention transformer-based neural architecture that can generalize across different behavior representations and formats, combining heterogeneous information from dynamic log reports. We show the efficacy of Nebula on thre
&lt;/p&gt;</description></item><item><title>TII-SSRC-23&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#32508;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20837;&#20405;&#26816;&#27979;&#20219;&#21153;&#20851;&#38190;&#29305;&#24449;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.10661</link><description>&lt;p&gt;
TII-SSRC-23&#25968;&#25454;&#38598;&#65306;&#23545;&#20837;&#20405;&#26816;&#27979;&#20013;&#19981;&#21516;&#27969;&#37327;&#27169;&#24335;&#30340;&#31867;&#22411;&#23398;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
TII-SSRC-23 Dataset: Typological Exploration of Diverse Traffic Patterns for Intrusion Detection. (arXiv:2310.10661v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10661
&lt;/p&gt;
&lt;p&gt;
TII-SSRC-23&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#32508;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20837;&#20405;&#26816;&#27979;&#20219;&#21153;&#20851;&#38190;&#29305;&#24449;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#20854;&#25928;&#26524;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#30830;&#20445;&#36825;&#20123;&#25968;&#25454;&#38598;&#20934;&#30830;&#21453;&#26144;&#33391;&#24615;&#21644;&#24694;&#24847;&#27969;&#37327;&#30340;&#22810;&#26041;&#38754;&#29305;&#24615;&#23545;&#20110;&#21019;&#24314;&#33021;&#22815;&#35782;&#21035;&#21644;&#24212;&#23545;&#21508;&#31181;&#20837;&#20405;&#27169;&#24335;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#24448;&#24448;&#19981;&#36275;&#65292;&#32570;&#20047;&#24517;&#35201;&#30340;&#22810;&#26679;&#24615;&#21644;&#19982;&#24403;&#20195;&#32593;&#32476;&#29615;&#22659;&#30340;&#21305;&#37197;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20837;&#20405;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TII-SSRC-23&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#32508;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#21644;&#23376;&#31867;&#22411;&#30340;&#27969;&#37327;&#65292;&#26159;&#30740;&#31350;&#31038;&#21306;&#30340;&#24378;&#22823;&#32780;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#20026;&#20837;&#20405;&#26816;&#27979;&#20219;&#21153;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of network intrusion detection systems, predominantly based on machine learning, are highly influenced by the dataset they are trained on. Ensuring an accurate reflection of the multifaceted nature of benign and malicious traffic in these datasets is essential for creating models capable of recognizing and responding to a wide array of intrusion patterns. However, existing datasets often fall short, lacking the necessary diversity and alignment with the contemporary network environment, thereby limiting the effectiveness of intrusion detection. This paper introduces TII-SSRC-23, a novel and comprehensive dataset designed to overcome these challenges. Comprising a diverse range of traffic types and subtypes, our dataset is a robust and versatile tool for the research community. Additionally, we conduct a feature importance analysis, providing vital insights into critical features for intrusion detection tasks. Through extensive experimentation, we also establish firm b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#25915;&#20987;&#20013;&#23384;&#22312;&#34892;&#20026;&#23646;&#24615;&#37325;&#21472;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26631;&#31614;&#26816;&#27979;&#27169;&#22411; MLG-Model &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10660</link><description>&lt;p&gt;
&#34892;&#20026;&#23646;&#24615;&#37325;&#21472;&#29616;&#35937;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#20998;&#26512;&#19982;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detection against Network Attacks in the Overlapping Phenomenon of Behavior Attribute. (arXiv:2310.10660v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#32593;&#32476;&#25915;&#20987;&#20013;&#23384;&#22312;&#34892;&#20026;&#23646;&#24615;&#37325;&#21472;&#30340;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26631;&#31614;&#26816;&#27979;&#27169;&#22411; MLG-Model &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#25915;&#20987;&#30340;&#22686;&#21152;&#32473;&#23433;&#20840;&#36896;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#29992;&#20110;&#25903;&#25345;&#30456;&#20851;&#39046;&#22495;&#30740;&#31350;&#30340;&#32593;&#32476;&#25915;&#20987;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25915;&#20987;&#20043;&#38388;&#23384;&#22312;&#30528;&#34892;&#20026;&#23646;&#24615;&#37325;&#21472;&#30340;&#26174;&#33879;&#29616;&#35937;&#65292;&#21363;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#22810;&#20010;&#20855;&#26377;&#30456;&#21516;&#29305;&#24449;&#20294;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#19968;&#29616;&#35937;&#24182;&#37325;&#26032;&#26631;&#27880;&#20102;&#19968;&#20123;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;&#22914;UNSW-NB15&#12289;CCCS-CIC-AndMal-2020&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#22810;&#26631;&#31614;&#26041;&#24335;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#21487;&#20197;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#65292;&#20026;&#36861;&#36394;&#25915;&#20987;&#28304;&#21644;&#26500;&#24314;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26631;&#31614;&#26816;&#27979;&#27169;&#22411;MLD-Model&#65292;&#20854;&#20013;&#37319;&#29992;Wassers&#20540;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of network attacks poses a significant threat. Researchers propose datasets for network attacks to support research in related fields. Then, many attack detection methods based on these datasets are proposed. These detection methods, whether two-classification or multi-classification, belong to single-label learning, i.e., only one label is given to each sample. However, we discover that there is a noteworthy phenomenon of behavior attribute overlap between attacks, The presentation of this phenomenon in a dataset is that there are multiple samples with the same features but different labels. In this paper, we verify the phenomenon in well-known datasets(UNSW-NB15, CCCS-CIC-AndMal-2020) and re-label these data. In addition, detecting network attacks in a multi-label manner can obtain more information, providing support for tracing the attack source and building IDS. Therefore, we propose a multi-label detection model based on deep learning, MLD-Model, in which Wassers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#36951;&#24536;&#30340;&#26032;&#22411;&#40657;&#30418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#38544;&#34255;&#21518;&#38376;&#26469;&#36755;&#20986;&#24694;&#24847;&#39044;&#27979;&#65292;&#20174;&#32780;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.10659</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#36951;&#24536;&#23454;&#26045;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack through Machine Unlearning. (arXiv:2310.10659v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#36951;&#24536;&#30340;&#26032;&#22411;&#40657;&#30418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#38544;&#34255;&#21518;&#38376;&#26469;&#36755;&#20986;&#24694;&#24847;&#39044;&#27979;&#65292;&#20174;&#32780;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#23884;&#20837;&#30340;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20174;&#32780;&#36755;&#20986;&#21487;&#33021;&#19982;&#32473;&#23450;&#36755;&#20837;&#30340;&#39044;&#26399;&#36755;&#20986;&#19981;&#31526;&#30340;&#24694;&#24847;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#36951;&#24536;&#30340;&#26032;&#22411;&#40657;&#30418;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#39318;&#20808;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#26679;&#26412;&#65288;&#21253;&#25324;&#27602;&#25968;&#25454;&#21644;&#32531;&#35299;&#25968;&#25454;&#65289;&#25193;&#20805;&#35757;&#32451;&#38598;&#65292;&#35757;&#32451;&#19968;&#20010;&#8220;&#21892;&#24847;&#8221;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25915;&#20987;&#32773;&#25552;&#20132;&#36951;&#24536;&#35831;&#27714;&#65292;&#20197;&#31227;&#38500;&#32531;&#35299;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36880;&#27493;&#28608;&#27963;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#30001;&#20110;&#21518;&#38376;&#26159;&#22312;&#36845;&#20195;&#30340;&#36951;&#24536;&#36807;&#31243;&#20013;&#26893;&#20837;&#30340;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#29616;&#26377;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the security issues of artificial intelligence have become increasingly prominent due to the rapid development of deep learning research and applications. Backdoor attack is an attack targeting the vulnerability of deep learning models, where hidden backdoors are activated by triggers embedded by the attacker, thereby outputting malicious predictions that may not align with the intended output for a given input. In this work, we propose a novel black-box backdoor attack based on machine unlearning. The attacker first augments the training set with carefully designed samples, including poison and mitigation data, to train a 'benign' model. Then, the attacker posts unlearning requests for the mitigation samples to remove the impact of relevant data on the model, gradually activating the hidden backdoor. Since backdoors are implanted during the iterative unlearning process, it significantly increases the computational overhead of existing defense methods for backdoor dete
&lt;/p&gt;</description></item><item><title>VeriDIP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25152;&#26377;&#26435;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#21644;&#20351;&#29992;&#36739;&#23569;&#31169;&#26377;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;</title><link>http://arxiv.org/abs/2310.10656</link><description>&lt;p&gt;
VeriDIP: &#36890;&#36807;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#26435;
&lt;/p&gt;
&lt;p&gt;
VeriDIP: Verifying Ownership of Deep Neural Networks through Privacy Leakage Fingerprints. (arXiv:2310.10656v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10656
&lt;/p&gt;
&lt;p&gt;
VeriDIP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25152;&#26377;&#26435;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#21644;&#20351;&#29992;&#36739;&#23569;&#31169;&#26377;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#39033;&#26381;&#21153;&#20250;&#23548;&#33268;&#27169;&#22411;&#25220;&#34989;&#65292;&#20174;&#32780;&#23548;&#33268;&#29256;&#26435;&#20405;&#26435;&#12290;&#25152;&#26377;&#26435;&#27979;&#35797;&#25216;&#26415;&#26088;&#22312;&#35782;&#21035;&#27169;&#22411;&#25351;&#32441;&#20197;&#39564;&#35777;&#25220;&#34989;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24448;&#24448;&#20381;&#36182;&#20110;&#36807;&#24230;&#25311;&#21512;&#25110;&#40065;&#26834;&#24615;&#29305;&#24449;&#20316;&#20026;&#25351;&#32441;&#65292;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#24191;&#20041;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#19981;&#36275;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VeriDIP&#30340;&#26032;&#22411;&#25152;&#26377;&#26435;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;DNN&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;VeriDIP&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;(1)&#23427;&#21033;&#29992;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#26469;&#20272;&#35745;&#38544;&#31169;&#27844;&#38706;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#21453;&#26144;&#20102;&#32473;&#23450;&#27169;&#22411;&#30340;&#25351;&#32441;&#12290;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#31361;&#20986;&#20102;&#27169;&#22411;&#35760;&#24518;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;(2)&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36739;&#23569;&#31169;&#26377;&#26679;&#26412;&#22686;&#24378;&#25152;&#26377;&#26435;&#27979;&#35797;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;VeriDIP&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Machine Learning as a Service gives rise to model plagiarism, leading to copyright infringement. Ownership testing techniques are designed to identify model fingerprints for verifying plagiarism. However, previous works often rely on overfitting or robustness features as fingerprints, lacking theoretical guarantees and exhibiting under-performance on generalized models. In this paper, we propose a novel ownership testing method called VeriDIP, which verifies a DNN model's intellectual property. VeriDIP makes two major contributions. (1) It utilizes membership inference attacks to estimate the lower bound of privacy leakage, which reflects the fingerprint of a given model. The privacy leakage fingerprints highlight the unique patterns through which the models memorize sensitive training datasets. (2) We introduce a novel approach using less private samples to enhance the performance of ownership testing.  Extensive experimental results confirm that VeriDIP is effective and eff
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#22686;&#24378;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.10655</link><description>&lt;p&gt;
&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22686;&#24378;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trustworthiness in ML-Based Network Intrusion Detection with Uncertainty Quantification. (arXiv:2310.10655v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10655
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#22686;&#24378;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#21644;&#30456;&#20851;&#36890;&#20449;&#25216;&#26415;&#30340;&#21457;&#23637;&#19981;&#26029;&#22686;&#21152;&#20102;&#32593;&#32476;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#35782;&#21035;&#21644;&#32531;&#35299;&#23545;&#29616;&#20195;&#32593;&#32476;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#35774;&#22791;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#29992;&#20110;&#25191;&#34892;IDS&#25152;&#38656;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36866;&#24212;&#36825;&#20010;&#30446;&#30340;&#32780;&#37319;&#29992;&#30340;&#20856;&#22411;&#30340;ML&#27169;&#22411;&#27809;&#26377;&#36866;&#24403;&#22320;&#32771;&#34385;&#21040;&#19982;&#20854;&#33258;&#36523;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20250;&#20026;&#35823;&#20998;&#31867;&#30340;&#36755;&#20837;&#21644;&#23646;&#20110;&#26410;&#30693;&#31867;&#21035;&#65288;&#20363;&#22914;&#26032;&#22411;&#25915;&#20987;&#65289;&#30340;&#36755;&#20837;&#20135;&#29983;&#35823;&#23548;&#24615;&#36739;&#39640;&#30340;&#20998;&#31867;&#24471;&#20998;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#22522;&#20110;ML&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;ML&#30340;IDS&#24212;&#35813;&#22987;&#32456;&#25552;&#20379;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20197;&#36991;&#20813;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#20107;&#23454;&#19978;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21487;&#20197;&#20026;&#36827;&#19968;&#27493;&#30340;&#20915;&#31574;&#25552;&#20379;&#37325;&#35201;&#30340;&#21442;&#32771;&#65292;&#20174;&#32780;&#22686;&#24378;ML-based IDS&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of Internet and its related communication technologies have consistently increased the risk of cyber-attacks. In this context, a crucial role is played by Intrusion Detection Systems (IDSs), which are security devices designed to identify and mitigate attacks to modern networks. In the last decade, data-driven approaches based on Machine Learning (ML) have gained more and more popularity for executing the classification tasks required by IDSs. However, typical ML models adopted for this purpose do not properly take into account the uncertainty associated with their own prediction. This poses significant challenges, as they tend to produce misleadingly high classification scores for both misclassified inputs and inputs belonging to unknown classes (e.g. novel attacks), limiting the trustworthiness of existing ML-based solutions. In this paper we argue that ML-based IDSs should always provide accurate uncertainty quantification to avoid overconfident predictions. In fact, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10649</link><description>&lt;p&gt;
&#29992;&#20110;&#27714;&#35299;Wasserstein Lagrangian&#27969;&#30340;&#35745;&#31639;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Computational Framework for Solving Wasserstein Lagrangian Flows. (arXiv:2310.10649v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#30340;&#22522;&#30784;&#20960;&#20309;&#65288;&#21160;&#33021;&#65289;&#21644;&#23494;&#24230;&#36335;&#24452;&#30340;&#27491;&#21017;&#21270;&#65288;&#21183;&#33021;&#65289;&#65292;&#21487;&#20197;&#23545;&#26368;&#20248;&#36755;&#36816;&#30340;&#21160;&#21147;&#23398;&#24418;&#24335;&#36827;&#34892;&#25512;&#24191;&#12290;&#36825;&#20123;&#32452;&#21512;&#20135;&#29983;&#19981;&#21516;&#30340;&#21464;&#20998;&#38382;&#39064;&#65288;Lagrangians&#65289;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22914;Schr&#246;dinger&#26725;&#12289;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21644;&#24102;&#26377;&#29289;&#29702;&#32422;&#26463;&#30340;&#26368;&#20248;&#36755;&#36816;&#31561;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#26368;&#20248;&#23494;&#24230;&#36335;&#24452;&#26159;&#26410;&#30693;&#30340;&#65292;&#35299;&#20915;&#36825;&#20123;&#21464;&#20998;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20511;&#21161;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#25311;&#25110;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#36712;&#36857;&#65292;&#20063;&#19981;&#38656;&#35201;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperformin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TacticAI&#65292;&#19968;&#31181;&#19982;&#21033;&#29289;&#28006;&#36275;&#29699;&#20465;&#20048;&#37096;&#30340;&#39046;&#22495;&#19987;&#23478;&#23494;&#20999;&#21512;&#20316;&#24320;&#21457;&#21644;&#35780;&#20215;&#30340;AI&#36275;&#29699;&#25112;&#26415;&#21161;&#25163;&#12290;TacticAI&#33021;&#22815;&#36890;&#36807;&#39044;&#27979;&#21644;&#29983;&#25104;&#30340;&#26041;&#24335;&#24110;&#21161;&#25945;&#32451;&#20204;&#20998;&#26512;&#35282;&#29699;&#24773;&#20917;&#65292;&#24182;&#20026;&#27599;&#20010;&#35282;&#29699;&#24815;&#20363;&#36873;&#25321;&#25104;&#21151;&#21487;&#33021;&#24615;&#26368;&#39640;&#30340;&#29699;&#21592;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.10553</link><description>&lt;p&gt;
TacticAI:&#19968;&#31181;&#36275;&#29699;&#25112;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
TacticAI: an AI assistant for football tactics. (arXiv:2310.10553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TacticAI&#65292;&#19968;&#31181;&#19982;&#21033;&#29289;&#28006;&#36275;&#29699;&#20465;&#20048;&#37096;&#30340;&#39046;&#22495;&#19987;&#23478;&#23494;&#20999;&#21512;&#20316;&#24320;&#21457;&#21644;&#35780;&#20215;&#30340;AI&#36275;&#29699;&#25112;&#26415;&#21161;&#25163;&#12290;TacticAI&#33021;&#22815;&#36890;&#36807;&#39044;&#27979;&#21644;&#29983;&#25104;&#30340;&#26041;&#24335;&#24110;&#21161;&#25945;&#32451;&#20204;&#20998;&#26512;&#35282;&#29699;&#24773;&#20917;&#65292;&#24182;&#20026;&#27599;&#20010;&#35282;&#29699;&#24815;&#20363;&#36873;&#25321;&#25104;&#21151;&#21487;&#33021;&#24615;&#26368;&#39640;&#30340;&#29699;&#21592;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36776;&#21035;&#23545;&#25163;&#22242;&#38431;&#23454;&#26045;&#30340;&#25112;&#26415;&#20851;&#38190;&#27169;&#24335;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#24212;&#23545;&#26041;&#27861;&#26159;&#29616;&#20195;&#36275;&#29699;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20197;&#31639;&#27861;&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TacticAI&#65292;&#19968;&#31181;&#19982;&#21033;&#29289;&#28006;&#36275;&#29699;&#20465;&#20048;&#37096;&#30340;&#39046;&#22495;&#19987;&#23478;&#23494;&#20999;&#21512;&#20316;&#24320;&#21457;&#21644;&#35780;&#20215;&#30340;AI&#36275;&#29699;&#25112;&#26415;&#21161;&#25163;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#26512;&#35282;&#29699;&#65292;&#22240;&#20026;&#23427;&#20204;&#32473;&#25945;&#32451;&#20204;&#25552;&#20379;&#20102;&#30452;&#25509;&#30340;&#24178;&#39044;&#21644;&#25913;&#36827;&#26426;&#20250;&#12290;TacticAI&#21253;&#21547;&#20102;&#19968;&#20010;&#39044;&#27979;&#21644;&#29983;&#25104;&#30340;&#32452;&#20214;&#65292;&#20351;&#25945;&#32451;&#33021;&#22815;&#26377;&#25928;&#22320;&#37319;&#26679;&#21644;&#25506;&#32034;&#27599;&#20010;&#35282;&#29699;&#24815;&#20363;&#30340;&#26367;&#20195;&#29699;&#21592;&#37197;&#32622;&#65292;&#24182;&#36873;&#25321;&#37027;&#20123;&#39044;&#27979;&#25104;&#21151;&#21487;&#33021;&#24615;&#26368;&#39640;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#30456;&#20851;&#30340;&#22522;&#20934;&#20219;&#21153;&#23545;TacticAI&#36827;&#34892;&#20102;&#39564;&#35777;&#65306;&#39044;&#27979;&#25509;&#25910;&#29699;&#21592;&#21644;&#23556;&#38376;&#23581;&#35797;&#20197;&#21450;&#25512;&#33616;&#29699;&#21592;&#20301;&#32622;&#35843;&#25972;&#12290;TacticAI&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#19982;&#21033;&#29289;&#28006;&#36275;&#29699;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#30340;&#23450;&#24615;&#30740;&#31350;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying key patterns of tactics implemented by rival teams, and developing effective responses, lies at the heart of modern football. However, doing so algorithmically remains an open research challenge. To address this unmet need, we propose TacticAI, an AI football tactics assistant developed and evaluated in close collaboration with domain experts from Liverpool FC. We focus on analysing corner kicks, as they offer coaches the most direct opportunities for interventions and improvements. TacticAI incorporates both a predictive and a generative component, allowing the coaches to effectively sample and explore alternative player setups for each corner kick routine and to select those with the highest predicted likelihood of success. We validate TacticAI on a number of relevant benchmark tasks: predicting receivers and shot attempts and recommending player position adjustments. The utility of TacticAI is validated by a qualitative study conducted with football domain experts at Liv
&lt;/p&gt;</description></item><item><title>ReMax&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;PPO&#65292;ReMax&#31616;&#21270;&#20102;&#23454;&#29616;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;fine-tuning&#26102;&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10505</link><description>&lt;p&gt;
ReMax:&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. (arXiv:2310.10505v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10505
&lt;/p&gt;
&lt;p&gt;
ReMax&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;PPO&#65292;ReMax&#31616;&#21270;&#20102;&#23454;&#29616;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;fine-tuning&#26102;&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#35201;&#31574;&#30053;&#26159;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;PPO&#26159;&#20107;&#23454;&#19978;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;PPO&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#26159;&#26412;&#35770;&#25991;&#35797;&#22270;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;RLHF&#20219;&#21153;&#20013;&#30830;&#23450;&#20102;&#19977;&#20010;&#37325;&#35201;&#29305;&#24615;&#65306;&#24555;&#36895;&#27169;&#25311;&#12289;&#30830;&#23450;&#24615;&#36716;&#25442;&#21644;&#36712;&#36857;&#32423;&#22870;&#21169;&#65292;&#36825;&#20123;&#29305;&#24615;&#22312;PPO&#20013;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;RLHF&#30340;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;ReMax&#12290;ReMax&#30340;&#31639;&#27861;&#35774;&#35745;&#26159;&#22522;&#20110;&#19968;&#31181;&#24191;&#20026;&#20351;&#29992;&#30340;&#31639;&#27861;REINFORCE&#65292;&#20294;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;PPO&#20855;&#26377;&#19977;&#37325;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;ReMax&#23454;&#29616;&#31616;&#21333;&#65292;&#28040;&#38500;&#20102;PPO&#20013;&#30340;&#35768;&#22810;&#19982;&#35268;&#27169;&#30456;&#20851;&#19988;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;ReMax&#21407;&#21017;&#19978;&#21487;&#20197;&#33410;&#32422;&#32422;50%&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;&#32467;&#26524;&#23548;&#33268;PPO&#22312;&#36827;&#34892;fine-tuning&#26102;&#20986;&#29616;&#20869;&#23384;&#28322;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alignment is of critical importance for training large language models (LLMs). The predominant strategy to address this is through Reinforcement Learning from Human Feedback (RLHF), where PPO serves as the de-facto algorithm. Yet, PPO is known to suffer from computational inefficiency, which is a challenge that this paper aims to address. We identify three important properties in RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on such observations, we develop a new algorithm tailored for RLHF, called ReMax. The algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is equipped with a new variance-reduction technique.  Our method has three-fold advantages over PPO: first, ReMax is simple to implement and removes many hyper-parameters in PPO, which are scale-sensitive and laborious to tune. Second, ReMax saves about 50% memory usage in principle. As a result, PPO runs out-of-memory when fine-t
&lt;/p&gt;</description></item><item><title>BOSS&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#26469;&#33258;&#21160;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21021;&#22987;&#25216;&#33021;&#38598;&#20043;&#22806;&#30340;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#19981;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;BOSS&#33021;&#22815;&#20174;&#22522;&#26412;&#30340;&#21407;&#22987;&#25216;&#33021;&#20013;&#26500;&#24314;&#20986;&#21508;&#31181;&#22797;&#26434;&#26377;&#29992;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.10021</link><description>&lt;p&gt;
&#33258;&#20027;&#23398;&#20064;&#25216;&#33021;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance. (arXiv:2310.10021v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10021
&lt;/p&gt;
&lt;p&gt;
BOSS&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#26469;&#33258;&#21160;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21021;&#22987;&#25216;&#33021;&#38598;&#20043;&#22806;&#30340;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#19981;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;BOSS&#33021;&#22815;&#20174;&#22522;&#26412;&#30340;&#21407;&#22987;&#25216;&#33021;&#20013;&#26500;&#24314;&#20986;&#21508;&#31181;&#22797;&#26434;&#26377;&#29992;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BOSS&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#26469;&#33258;&#21160;&#23398;&#20064;&#35299;&#20915;&#38271;&#26102;&#31243;&#12289;&#22797;&#26434;&#19988;&#26377;&#24847;&#20041;&#30340;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#19987;&#23478;&#30340;&#30417;&#30563;&#65292;&#20197;&#31034;&#33539;&#25110;&#23500;&#21547;&#22870;&#21169;&#20989;&#25968;&#30340;&#24418;&#24335;&#26469;&#23398;&#20064;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;BOSS&#26041;&#27861;&#36890;&#36807;&#25191;&#34892;&#8220;&#25216;&#33021;&#24341;&#23548;&#8221;&#26469;&#23398;&#20064;&#23436;&#25104;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#20855;&#22791;&#19968;&#32452;&#21407;&#22987;&#25216;&#33021;&#30340;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#22312;&#21021;&#22987;&#25216;&#33021;&#38598;&#20043;&#22806;&#30340;&#20219;&#21153;&#20013;&#19981;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#12290;&#36825;&#31181;&#24341;&#23548;&#38454;&#27573;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25351;&#23548;&#65292;&#21521;&#20195;&#29702;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25216;&#33021;&#32452;&#21512;&#12290;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#65292;BOSS&#33021;&#22815;&#20174;&#22522;&#26412;&#30340;&#21407;&#22987;&#25216;&#33021;&#20013;&#26500;&#24314;&#20986;&#21508;&#31181;&#22797;&#26434;&#26377;&#29992;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#36924;&#30495;&#30340;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;LLM&#24341;&#23548;&#24341;&#23548;&#30340;BOSS&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#24182;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36710;&#36742;&#36335;&#24452;&#20248;&#21270;&#20013;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30456;&#21305;&#37197;&#25110;&#26377;&#25152;&#25913;&#36827;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.09986</link><description>&lt;p&gt;
&#20851;&#20110;&#36710;&#36742;&#36335;&#24452;&#20248;&#21270;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#65288;arXiv&#65306;2310.09986v2 [cs.LG] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
On Statistical Learning of Branch and Bound for Vehicle Routing Optimization. (arXiv:2310.09986v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36710;&#36742;&#36335;&#24452;&#20248;&#21270;&#20013;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30456;&#21305;&#37197;&#25110;&#26377;&#25152;&#25913;&#36827;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#36817;&#20284;&#27714;&#35299;NP&#22256;&#38590;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24182;&#20840;&#38754;&#27604;&#36739;&#20102;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524; - &#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#65292;GraphSAGE&#21644;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;GAT&#65289; - &#26469;&#35299;&#20915;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20197;&#27169;&#25311;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24378;&#20998;&#25903;&#31574;&#30053;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;CVRLIB&#30340;&#20845;&#20010;&#20855;&#26377;&#19981;&#21516;&#25299;&#25169;&#32467;&#26500;&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#20010;&#23454;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27714;&#35299;CVRP&#23454;&#20363;&#25152;&#38656;&#30340;&#26368;&#23567;&#36710;&#36742;&#25968;&#20943;&#23569;&#21040;&#20102;&#19968;&#20010;&#35013;&#31665;&#38382;&#39064;&#20013;&#65292;&#24182;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#20351;&#29992;&#24378;&#20998;&#25903;&#31574;&#30053;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#65292;&#21516;&#26102;&#38656;&#35201;&#30340;&#35745;&#31639;&#36164;&#28304; significantly less comp.
&lt;/p&gt;
&lt;p&gt;
Recently, machine learning of the branch and bound algorithm has shown promise in approximating competent solutions to NP-hard problems. In this paper, we utilize and comprehensively compare the outcomes of three neural networks--graph convolutional neural network (GCNN), GraphSAGE, and graph attention network (GAT)--to solve the capacitated vehicle routing problem. We train these neural networks to emulate the decision-making process of the computationally expensive Strong Branching strategy. The neural networks are trained on six instances with distinct topologies from the CVRPLIB and evaluated on eight additional instances. Moreover, we reduced the minimum number of vehicles required to solve a CVRP instance to a bin-packing problem, which was addressed in a similar manner. Through rigorous experimentation, we found that this approach can match or improve upon the performance of the branch and bound algorithm with the Strong Branching strategy while requiring significantly less comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#30340;&#25216;&#26415;&#25351;&#26631;&#32452;&#21512;&#26469;&#23454;&#29616;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.09903</link><description>&lt;p&gt;
&#35780;&#20272;&#29305;&#24449;&#36873;&#25321;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction. (arXiv:2310.09903v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#30340;&#25216;&#26415;&#25351;&#26631;&#32452;&#21512;&#26469;&#23454;&#29616;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25216;&#26415;&#25351;&#26631;&#23545;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#29305;&#24449;&#36873;&#25321;&#23545;&#36873;&#25321;&#26368;&#20339;&#25351;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#32771;&#34385;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#27169;&#22411;&#24615;&#33021;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26159;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#37492;&#23450;&#20986;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#32929;&#24066;&#20215;&#26684;&#30340;&#26368;&#20339;&#32929;&#24066;&#25351;&#26631;&#32452;&#21512;&#12290;&#20026;&#35780;&#20272;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#23545;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#22312;&#36807;&#21435;10&#24180;&#33529;&#26524;&#20844;&#21496;&#30340;&#25968;&#25454;&#19978;&#20351;&#29992;&#20102;10&#20010;&#35780;&#20272;&#22120;&#21644;123&#20010;&#25216;&#26415;&#25351;&#26631;&#36827;&#34892;&#20102;SFS&#21644;SBS&#30340;&#32771;&#23519;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23558;&#30001;3&#22825;&#26102;&#38388;&#31383;&#21475;&#21019;&#24314;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#29992;&#20110;&#22238;&#24402;&#26041;&#27861;&#30340;&#36755;&#20837;&#12290;&#20174;&#35266;&#23519;&#32467;&#26524;&#21487;&#20197;&#24471;&#20986;&#65306;&#65288;1&#65289;&#27599;&#31181;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the influence of many factors, including technical indicators on stock market prediction, feature selection is important to choose the best indicators. One of the feature selection methods that consider the performance of models during feature selection is the wrapper feature selection method. The aim of this research is to identify a combination of the best stock market indicators through feature selection to predict the stock market price with the least error. In order to evaluate the impact of wrapper feature selection techniques on stock market prediction, in this paper SFS and SBS with 10 estimators and 123 technical indicators have been examined on the last 10 years of Apple Company. Also, by the proposed method, the data created by the 3-day time window were converted to the appropriate input for regression methods. Based on the results observed: (1) Each wrapper feature selection method has different results with different machine learning methods, and each method is mor
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08754</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#20998;&#35789;&#36873;&#25321;&#65306;&#24494;&#19981;&#36275;&#36947;&#36824;&#26159;&#33267;&#20851;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08754
&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;LLM&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#31574;&#21010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36827;&#27493;&#65292;&#32780;&#20998;&#35789;&#22120;&#30340;&#24433;&#21709;&#21017;&#26159;&#19968;&#20010;&#30450;&#28857;&#12290;&#36890;&#36807;&#23545;24&#20010;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#31639;&#27861;&#21644;&#21442;&#25968;&#36827;&#34892;&#22823;&#33539;&#22260;&#23454;&#39564;&#65292;&#25105;&#20204;&#23545;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;LLM&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;&#20016;&#23500;&#24230;&#21644;&#24179;&#31561;&#24615;&#65289;&#24182;&#19981;&#24635;&#26159;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#25351;&#26631;&#25104;&#20026;&#23545;&#20998;&#35789;&#22120;&#35780;&#20272;&#30340;&#21487;&#30097;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38024;&#23545;&#20116;&#31181;&#26368;&#24120;&#35265;&#30340;&#27431;&#27954;&#35821;&#35328;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#38656;&#35201;&#35789;&#27719;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#21033;&#29992;&#36873;&#25321;&#24615;&#21098;&#25509;&#21644;&#22522;&#22240;&#22797;&#21046;&#20135;&#29983;&#30340;&#24207;&#21015;&#20043;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#65292;&#23398;&#20064;&#21040;&#24191;&#20041;RNA&#21516;&#20301;&#32032;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;RNA&#21322;&#34928;&#26399;&#21644;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#39044;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#30382;&#23572;&#36874;&#30456;&#20851;&#24615;&#22686;&#21152;&#20102;&#22810;&#36798;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.08738</link><description>&lt;p&gt;
&#29992;RNA&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Splicing Up Your Predictions with RNA Contrastive Learning. (arXiv:2310.08738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#21033;&#29992;&#36873;&#25321;&#24615;&#21098;&#25509;&#21644;&#22522;&#22240;&#22797;&#21046;&#20135;&#29983;&#30340;&#24207;&#21015;&#20043;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#65292;&#23398;&#20064;&#21040;&#24191;&#20041;RNA&#21516;&#20301;&#32032;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;RNA&#21322;&#34928;&#26399;&#21644;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#39044;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#30382;&#23572;&#36874;&#30456;&#20851;&#24615;&#22686;&#21152;&#20102;&#22810;&#36798;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#36805;&#36895;&#31215;&#32047;&#65292;&#25105;&#20204;&#23545;RNA&#35843;&#25511;&#20195;&#30721;&#30340;&#29702;&#35299;&#23578;&#19981;&#23436;&#25972;&#12290;&#36817;&#26399;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#35268;&#21017;&#65288;&#20363;&#22914;&#35821;&#35328;&#20013;&#30340;&#21477;&#23376;&#32467;&#26500;&#65289;&#30340;&#33021;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#36873;&#25321;&#24615;&#21098;&#25509;&#21644;&#22522;&#22240;&#22797;&#21046;&#20135;&#29983;&#30340;&#24207;&#21015;&#20043;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#22522;&#22240;&#32452;&#25968;&#25454;&#19978;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#21644;&#23545;&#27604;&#30446;&#26631;&#20351;&#24471;&#23398;&#20064;&#21040;&#24191;&#20041;RNA&#21516;&#20301;&#32032;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;RNA&#21322;&#34928;&#26399;&#21644;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#20004;&#39033;&#20219;&#21153;&#30340;&#32447;&#24615;&#25506;&#32034;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#30382;&#23572;&#36874;&#30456;&#20851;&#24615;&#22686;&#21152;&#20102;&#22810;&#36798;&#20004;&#20493;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23545;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#25506;&#32034;&#25581;&#31034;&#20102;&#25105;&#20204;&#23545;&#27604;&#30446;&#26631;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Recent self-supervised methods in other domains have demonstrated the ability to learn rules underlying the data-generating process such as sentence structure in language. Inspired by this, we extend contrastive learning techniques to genomic data by utilizing functional similarities between sequences generated through alternative splicing and gene duplication. Our novel dataset and contrastive objective enable the learning of generalized RNA isoform representations. We validate their utility on downstream tasks such as RNA half-life and mean ribosome load prediction. Our pre-training strategy yields competitive results using linear probing on both tasks, along with up to a two-fold increase in Pearson correlation in low-data conditions. Importantly, our exploration of the learned latent space reveals that our contrastive objective yields semantically meaningful representa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>NeuroInspect&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#21487;&#35299;&#37322;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#30830;&#23450;&#32593;&#32476;&#20013;&#23548;&#33268;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#24182;&#21487;&#35270;&#21270;&#23884;&#20837;&#20854;&#20013;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#24341;&#20837;&#20102;CLIP-Illusion&#26469;&#29983;&#25104;&#29305;&#24449;&#22270;&#20687;&#65292;&#24182;&#20197;&#31867;&#20026;&#26465;&#20214;&#26469;&#32771;&#23519;&#31070;&#32463;&#20803;&#19982;&#20915;&#31574;&#23618;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07184</link><description>&lt;p&gt;
NeuroInspect&#65306;&#36890;&#36807;&#31867;&#26465;&#20214;&#21487;&#35270;&#21270;&#23454;&#29616;&#30340;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#35843;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations. (arXiv:2310.07184v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07184
&lt;/p&gt;
&lt;p&gt;
NeuroInspect&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#21487;&#35299;&#37322;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#30830;&#23450;&#32593;&#32476;&#20013;&#23548;&#33268;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#24182;&#21487;&#35270;&#21270;&#23884;&#20837;&#20854;&#20013;&#30340;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#24341;&#20837;&#20102;CLIP-Illusion&#26469;&#29983;&#25104;&#29305;&#24449;&#22270;&#20687;&#65292;&#24182;&#20197;&#31867;&#20026;&#26465;&#20214;&#26469;&#32771;&#23519;&#31070;&#32463;&#20803;&#19982;&#20915;&#31574;&#23618;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#20986;&#38169;&#12290;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#28145;&#24230;&#23398;&#20064;&#20174;&#19994;&#32773;&#20351;&#29992;&#26377;&#25928;&#30340;&#35843;&#35797;&#24037;&#20855;&#26469;&#35299;&#37322;&#32593;&#32476;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35843;&#35797;&#26041;&#27861;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#35843;&#25972;&#20915;&#31574;&#36807;&#31243;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuroInspect&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#21487;&#35299;&#37322;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;&#21453;&#20107;&#23454;&#35299;&#37322;&#12289;&#29305;&#24449;&#21487;&#35270;&#21270;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#21066;&#20943;&#12290;&#25105;&#20204;&#30340;&#35843;&#35797;&#26694;&#26550;&#39318;&#20808;&#30830;&#23450;&#32593;&#32476;&#20013;&#23548;&#33268;&#38169;&#35823;&#30340;&#31070;&#32463;&#20803;&#65292;&#28982;&#21518;&#21487;&#35270;&#21270;&#23884;&#20837;&#22312;&#36825;&#20123;&#31070;&#32463;&#20803;&#20013;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#20154;&#31867;&#35299;&#37322;&#12290;&#20026;&#20102;&#25552;&#20379;&#36825;&#20123;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLIP-Illusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20195;&#34920;&#29305;&#24449;&#30340;&#22270;&#20687;&#65292;&#24182;&#20197;&#31867;&#20026;&#26465;&#20214;&#26469;&#32771;&#23519;&#31070;&#32463;&#20803;&#19982;&#20915;&#31574;&#23618;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. W
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OILCA&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#35782;&#21035;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;"&#23545;&#25239;&#24615;"&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#29615;&#22659;&#21464;&#21270;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04706</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#19982;&#21464;&#20998;&#36870;&#21521;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation Learning with Variational Counterfactual Reasoning. (arXiv:2310.04706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OILCA&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#35782;&#21035;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;"&#23545;&#25239;&#24615;"&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#29615;&#22659;&#21464;&#21270;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#26088;&#22312;&#23398;&#20064;&#19968;&#31181;&#26368;&#20248;&#30340;&#19987;&#23478;&#34892;&#20026;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22312;&#32447;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#65292;&#31163;&#32447;&#25968;&#25454;&#38598;&#26159;&#20174;&#27809;&#26377;&#22870;&#21169;&#30340;&#27425;&#20248;&#34892;&#20026;&#20013;&#25910;&#38598;&#26469;&#30340;&#12290;&#30001;&#20110;&#19987;&#23478;&#25968;&#25454;&#31232;&#32570;&#65292;&#26234;&#33021;&#20307;&#36890;&#24120;&#21482;&#33021;&#31616;&#21333;&#22320;&#35760;&#20303;&#36139;&#20047;&#30340;&#36712;&#36857;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#32570;&#20047;&#23545;&#26032;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#28040;&#38500;&#20250;&#23545;&#26234;&#33021;&#20307;&#36896;&#25104;&#20559;&#24046;&#24182;&#38459;&#30861;&#27867;&#21270;&#30340;&#20266;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OILCA&#30340;&#26694;&#26550;&#65292;&#21363;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#19982;&#23545;&#25239;&#25968;&#25454;&#22686;&#24378;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#35782;&#21035;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;"&#23545;&#25239;&#24615;"&#26679;&#26412;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#23545;&#25239;&#24615;&#35782;&#21035;&#21644;&#27867;&#21270;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In offline Imitation Learning (IL), an agent aims to learn an optimal expert behavior policy without additional online environment interactions. However, in many real-world scenarios, such as robotics manipulation, the offline dataset is collected from suboptimal behaviors without rewards. Due to the scarce expert data, the agents usually suffer from simply memorizing poor trajectories and are vulnerable to the variations in the environments, lacking the capability of generalizing to new environments. To effectively remove spurious features that would otherwise bias the agent and hinder generalization, we propose a framework named \underline{O}ffline \underline{I}mitation \underline{L}earning with \underline{C}ounterfactual data \underline{A}ugmentation (OILCA). In particular, we leverage the identifiable variational autoencoder to generate \textit{counterfactual} samples. We theoretically analyze the counterfactual identification and the improvement of generalization. Moreover, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.16705</link><description>&lt;p&gt;
&#35299;&#30721;&#22270;&#20687;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16705
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Google Bard&#36825;&#20010;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;Bard&#22312;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#30456;&#32467;&#21512;&#26041;&#38754;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#19978;&#26377;&#25361;&#25112;&#30340;&#38382;&#39064;&#20294;&#26080;&#27861;&#20462;&#25913;&#21407;&#22987;&#35270;&#35273;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#25361;&#25112;-&#21709;&#24212;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;Google Bard&#36827;&#34892;&#20102;64&#20010;&#35270;&#35273;&#25361;&#25112;&#65292;&#26088;&#22312;&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#25361;&#25112;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#8220;&#35270;&#35273;&#24773;&#22659;&#25512;&#29702;&#8221;&#65292;&#8220;&#35270;&#35273;&#25991;&#26412;&#25512;&#29702;&#8221;&#21644;&#8220;&#19979;&#19968;&#22330;&#26223;&#39044;&#27979;&#8221;&#31561;&#65292;&#20197;&#30830;&#23450;Bard&#22312;&#34701;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20998;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Bard&#20542;&#21521;&#20110;&#26681;&#25454;&#22270;&#29255;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#30830;&#23450;&#22270;&#29255;&#20013;&#30340;&#32447;&#32034;&#26102;&#12290;&#19982;GPT4&#31561;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;Bard&#20284;&#20046;&#19981;&#20381;&#36182;&#20110;&#20687;Tesseract&#36825;&#26679;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#24211;&#65292;&#32780;&#26159;&#20687;Google Lens&#21644;Visual API&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#35782;&#21035;&#22797;&#26434;&#22270;&#29255;&#20013;&#30340;&#25991;&#26412;&#12290;&#26174;&#30528;&#30340;&#26159;&#65292;Bard&#21487;&#20197;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#35299;&#20915;ChatGPT&#26080;&#27861;&#29702;&#35299;&#30340;&#39564;&#35777;&#30721;&#65292;&#25512;&#33616;&#20351;&#29992;Tesseract&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;Bard&#27169;&#22411;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#26080;&#27861;&#37325;&#24314;&#25110;&#20462;&#25913;&#21407;&#22987;&#30340;&#35270;&#35273;&#23545;&#35937;&#26469;&#25903;&#25345;&#20854;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;Transformer&#35757;&#32451;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#23545;&#24212;&#30340;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#26469;&#22797;&#29616;&#21644;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#20004;&#20010;&#28304;&#22836;&#65292;&#24182;&#34920;&#26126;&#20808;&#21069;&#20351;&#29992;&#30340;&#32531;&#35299;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#35757;&#32451;&#20013;&#21516;&#26679;&#26377;&#25928;&#12290;&#36825;&#20010;&#21457;&#29616;&#26377;&#21161;&#20110;&#23558;&#32531;&#35299;&#26041;&#27861;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.14322</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;Transformer&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#23567;&#35268;&#27169;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Small-scale proxies for large-scale Transformer training instabilities. (arXiv:2309.14322v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;Transformer&#35757;&#32451;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#23545;&#24212;&#30340;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#26469;&#22797;&#29616;&#21644;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#20004;&#20010;&#28304;&#22836;&#65292;&#24182;&#34920;&#26126;&#20808;&#21069;&#20351;&#29992;&#30340;&#32531;&#35299;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#35757;&#32451;&#20013;&#21516;&#26679;&#26377;&#25928;&#12290;&#36825;&#20010;&#21457;&#29616;&#26377;&#21161;&#20110;&#23558;&#32531;&#35299;&#26041;&#27861;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#30740;&#31350;&#22242;&#38431;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#26102;&#25253;&#21578;&#20102;&#22312;&#23567;&#35268;&#27169;&#35757;&#32451;&#20013;&#26410;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#30340;&#21407;&#22240;&#20855;&#26377;&#31185;&#23398;&#24847;&#20041;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36827;&#34892;&#22797;&#29616;&#65292;&#20351;&#24471;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#22312;&#23567;&#35268;&#27169;&#19978;&#22797;&#29616;&#21644;&#30740;&#31350;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20851;&#27880;&#20808;&#21069;&#24037;&#20316;&#20013;&#25551;&#36848;&#30340;&#20004;&#20010;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#28304;&#22836;&#65306;&#27880;&#24847;&#21147;&#23618;&#20013;logits&#30340;&#22686;&#38271;&#65288;Dehghani&#31561;&#20154;&#65292;2023&#65289;&#21644;&#36755;&#20986;logits&#19982;&#23545;&#25968;&#27010;&#29575;&#20043;&#38388;&#30340;&#21457;&#25955;&#65288;Chowdhery&#31561;&#20154;&#65292;2022&#65289;&#12290;&#36890;&#36807;&#22312;&#21508;&#20010;&#23610;&#24230;&#19978;&#27979;&#37327;&#23398;&#20064;&#29575;&#21644;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#20197;&#39640;&#23398;&#20064;&#29575;&#35757;&#32451;&#23567;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#20063;&#20250;&#20986;&#29616;&#65292;&#24182;&#19988;&#20808;&#21069;&#22312;&#22823;&#35268;&#27169;&#19978;&#20351;&#29992;&#30340;&#32531;&#35299;&#26041;&#27861;&#22312;&#36825;&#20010;&#24773;&#26223;&#19979;&#21516;&#26679;&#26377;&#25928;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#31243;&#24230;&#19978;&#26159;&#21542;&#21487;&#20197;&#23558;&#36825;&#20123;&#32531;&#35299;&#26041;&#27861;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training stability and instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;wav2vec&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35328;&#35821;&#22833;&#35821;&#30151;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.14107</link><description>&lt;p&gt;
&#22522;&#20110;Wav2vec&#30340;&#35328;&#35821;&#22833;&#35821;&#30151;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech. (arXiv:2309.14107v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;wav2vec&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35328;&#35821;&#22833;&#35821;&#30151;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22768;&#23398;&#35821;&#38899;&#20449;&#21495;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#65292;&#21487;&#20197;&#20316;&#20026;&#21307;&#23398;&#35786;&#26029;&#20013;&#30340;&#24037;&#20855;&#29992;&#20110;&#35328;&#35821;&#22833;&#35821;&#30151;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;wav2vec 2.0&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26500;&#24314;&#20102;&#35328;&#35821;&#22833;&#35821;&#30151;&#30340;&#26816;&#27979;&#21644;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#31995;&#32479;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;UA-speech&#25968;&#25454;&#24211;&#12290;&#22312;&#26816;&#27979;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;wav2vec&#27169;&#22411;&#30340;&#31532;&#19968;&#23618;&#23884;&#20837;&#25928;&#26524;&#26368;&#22909;&#65292;&#30456;&#27604;&#20110;&#26368;&#20339;&#22522;&#20934;&#29305;&#24449;&#65288;&#22768;&#35889;&#22270;&#65289;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.23%&#12290;&#22312;&#30740;&#31350;&#30340;&#20005;&#37325;&#31243;&#24230;&#32423;&#21035;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#26368;&#32456;&#23618;&#30340;&#23884;&#20837;&#30456;&#27604;&#20110;&#26368;&#20339;&#22522;&#20934;&#29305;&#24449;&#65288;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65289;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;10.62%&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22768;&#38376;&#28304;&#29305;&#24449;&#30340;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22768;&#38899;&#30149;&#29702;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22768;&#38376;&#28304;&#21253;&#21547;&#30340;&#20449;&#24687;&#23545;&#20110;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#21028;&#21035;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.14080</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#38376;&#28304;&#29305;&#24449;&#36827;&#34892;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#20998;&#26512;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detection of Pathological Voice using Glottal Source Features. (arXiv:2309.14080v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22768;&#38376;&#28304;&#29305;&#24449;&#30340;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22768;&#38899;&#30149;&#29702;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22768;&#38376;&#28304;&#21253;&#21547;&#30340;&#20449;&#24687;&#23545;&#20110;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#21028;&#21035;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#22768;&#38899;&#30149;&#29702;&#33021;&#22815;&#23454;&#29616;&#23458;&#35266;&#35780;&#20272;&#21644;&#26089;&#26399;&#24178;&#39044;&#35786;&#26029;&#12290;&#26412;&#30740;&#31350;&#23545;&#22768;&#38376;&#28304;&#29305;&#24449;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22768;&#38899;&#30149;&#29702;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#20934;&#23553;&#38381;&#30456;&#20301;&#65288;QCP&#65289;&#30340;&#22768;&#38376;&#36870;&#28388;&#27874;&#26041;&#27861;&#20272;&#35745;&#24471;&#21040;&#30340;&#22768;&#38376;&#27969;&#21160;&#12289;&#20351;&#29992;&#38646;&#39057;&#29575;&#28388;&#27874;&#65288;ZFF&#65289;&#26041;&#27861;&#35745;&#31639;&#24471;&#21040;&#30340;&#36817;&#20284;&#22768;&#38376;&#28304;&#20449;&#21495;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;&#22768;&#23398;&#22768;&#38899;&#20449;&#21495;&#26469;&#25552;&#21462;&#22768;&#38376;&#28304;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#30001;QCP&#21644;ZFF&#35745;&#31639;&#20986;&#30340;&#22768;&#38376;&#28304;&#27874;&#24418;&#20013;&#25512;&#23548;&#20986;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCCs&#65289;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#30149;&#29702;&#24615;&#22768;&#38899;&#30340;&#22768;&#38376;&#28304;&#39057;&#35889;&#21464;&#21270;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#24211;&#65292;&#21363;&#21307;&#38498;Universitario Principe de Asturias&#65288;HUPA&#65289;&#25968;&#25454;&#24211;&#21644;Saarbrucken Voice Disorders&#65288;SVD&#65289;&#25968;&#25454;&#24211;&#12290;&#29305;&#24449;&#20998;&#26512;&#25581;&#31034;&#20102;&#22768;&#38376;&#28304;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#30340;&#21028;&#21035;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of voice pathology enables objective assessment and earlier intervention for the diagnosis. This study provides a systematic analysis of glottal source features and investigates their effectiveness in voice pathology detection. Glottal source features are extracted using glottal flows estimated with the quasi-closed phase (QCP) glottal inverse filtering method, using approximate glottal source signals computed with the zero frequency filtering (ZFF) method, and using acoustic voice signals directly. In addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from the glottal source waveforms computed by QCP and ZFF to effectively capture the variations in glottal source spectra of pathological voice. Experiments were carried out using two databases, the Hospital Universitario Principe de Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database. Analysis of features revealed that the glottal source contains information that discri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#36830;&#25509;&#29702;&#35770;&#21644;&#23454;&#36341;&#65292;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23545;&#20110;&#20004;&#23618;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#35813;&#32593;&#32476;&#23558;&#20174;&#25105;&#20204;&#30340;&#20445;&#35777;&#20013;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2309.12128</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems. (arXiv:2309.12128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#36830;&#25509;&#29702;&#35770;&#21644;&#23454;&#36341;&#65292;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23545;&#20110;&#20004;&#23618;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#35813;&#32593;&#32476;&#23558;&#20174;&#25105;&#20204;&#30340;&#20445;&#35777;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#36825;&#26679;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#32463;&#39564;&#24615;&#22320;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#26126;&#30830;&#29702;&#35770;&#20445;&#35777;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#26469;&#25511;&#21046;&#31070;&#32463;&#20999;&#21521;&#26680;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19979;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#36830;&#25509;&#36825;&#20004;&#20010;&#39046;&#22495;&#65292;&#24182;&#20026;&#26080;&#30417;&#30563;&#21069;&#39304;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#30830;&#23450;&#24615;&#30340;&#25910;&#25947;&#21644;&#24674;&#22797;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#36229;&#21442;&#25968;&#21270;&#30028;&#38480;&#65292;&#22312;&#36825;&#20123;&#30028;&#38480;&#19979;&#65292;&#20855;&#26377;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#28145;&#24230;&#36870;&#20808;&#39564;&#32593;&#32476;&#23558;&#21463;&#30410;&#20110;&#25105;&#20204;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
&lt;/p&gt;</description></item><item><title>Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2309.10150</link><description>&lt;p&gt;
Q-Transformer&#65306;&#36890;&#36807;&#33258;&#22238;&#24402;Q-&#20989;&#25968;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10150
&lt;/p&gt;
&lt;p&gt;
Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#21644;&#33258;&#20027;&#37319;&#38598;&#25968;&#25454;&#30340;&#22823;&#22411;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Transformer&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;Q&#20989;&#25968;&#34920;&#31034;&#65292;&#36890;&#36807;&#31163;&#32447;&#26102;&#24046;&#22791;&#20221;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#31216;&#20026;Q-Transformer&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#30340;Q&#20540;&#34920;&#31034;&#20026;&#21333;&#29420;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#21487;&#20197;&#24212;&#29992;&#39640;&#23481;&#37327;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;Q&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#35774;&#35745;&#20915;&#31574;&#65292;&#20351;&#20854;&#22312;&#31163;&#32447;RL&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;Q-Transformer&#22312;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#22871;&#20214;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#39033;&#30446;&#30340;&#32593;&#31449;&#21644;&#35270;&#39057;&#21487;&#20197;&#22312;https://q-transformer.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
&lt;/p&gt;</description></item><item><title>&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#65292;&#29992;ReLU&#26367;&#25442;softmax&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#22312;&#35745;&#31639;&#24615;&#33021;&#19978;&#25509;&#36817;&#25110;&#21305;&#37197;softmax&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#38500;&#27861;&#21487;&#20197;&#32531;&#35299;&#31934;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08586</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#29992;ReLU&#26367;&#25442;softmax
&lt;/p&gt;
&lt;p&gt;
Replacing softmax with ReLU in Vision Transformers. (arXiv:2309.08586v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08586
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#65292;&#29992;ReLU&#26367;&#25442;softmax&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#22312;&#35745;&#31639;&#24615;&#33021;&#19978;&#25509;&#36817;&#25110;&#21305;&#37197;softmax&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#38500;&#27861;&#21487;&#20197;&#32531;&#35299;&#31934;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24403;&#23558;&#27880;&#24847;&#21147;softmax&#26367;&#25442;&#20026;ReLU&#36825;&#26679;&#30340;&#36880;&#28857;&#28608;&#27963;&#26102;&#65292;&#31934;&#24230;&#20250;&#19979;&#38477;&#12290;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#36890;&#36807;&#24207;&#21015;&#38271;&#24230;&#38500;&#20197;&#27880;&#24847;&#21147;&#19979;&#38477;&#34987;&#32531;&#35299;&#12290;&#25105;&#20204;&#22312;ImageNet-21k&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23567;&#22411;&#21040;&#22823;&#22411;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#26041;&#38754;&#65292;ReLU&#27880;&#24847;&#21147;&#21487;&#20197;&#36798;&#21040;&#25110;&#21305;&#37197;softmax&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research observed accuracy degradation when replacing the attention softmax with a point-wise activation such as ReLU. In the context of vision transformers, we find that this degradation is mitigated when dividing by sequence length. Our experiments training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65288;FOT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#36807;&#21435;&#25968;&#25454;&#30340;&#19981;&#20999;&#23454;&#38469;&#20551;&#35774;&#21644;&#38544;&#31169;&#21407;&#21017;&#30340;&#36829;&#21453;&#12290;</title><link>http://arxiv.org/abs/2309.01289</link><description>&lt;p&gt;
&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65306;&#20943;&#36731;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning. (arXiv:2309.01289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65288;FOT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#36807;&#21435;&#25968;&#25454;&#30340;&#19981;&#20999;&#23454;&#38469;&#20551;&#35774;&#21644;&#38544;&#31169;&#21407;&#21017;&#30340;&#36829;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#33021;&#22815;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#19978;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#19978;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#20986;&#29616;&#26032;&#30340;&#20219;&#21153;&#65292;&#20840;&#23616;&#27169;&#22411;&#24212;&#35813;&#22312;&#19981;&#36951;&#24536;&#20043;&#21069;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#12290;&#36825;&#31181;&#30495;&#23454;&#22330;&#26223;&#34987;&#31216;&#20026;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#12290;CFL&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24403;&#20840;&#23616;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#20854;&#22312;&#26087;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36817;&#26399;&#26377;&#19968;&#20123;&#20851;&#20110;CFL&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#20915;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#23545;&#36807;&#21435;&#25968;&#25454;&#26679;&#26412;&#30340;&#21487;&#29992;&#24615;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#36829;&#21453;&#20102;FL&#30340;&#38544;&#31169;&#21407;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32852;&#37030;&#27491;&#20132;&#35757;&#32451;&#65288;FOT&#65289;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#24182;&#35299;&#20915;&#20840;&#23616;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(DM-GNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#33410;&#28857;&#20998;&#31867;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21452;&#29305;&#24449;&#25552;&#21462;&#22120;&#26500;&#24314;GNN&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#33410;&#28857;&#20998;&#31867;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30340;&#20256;&#25773;&#26041;&#26696;&#26469;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16470</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain-adaptive Message Passing Graph Neural Network. (arXiv:2308.16470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(DM-GNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#33410;&#28857;&#20998;&#31867;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21452;&#29305;&#24449;&#25552;&#21462;&#22120;&#26500;&#24314;GNN&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#33410;&#28857;&#20998;&#31867;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30340;&#20256;&#25773;&#26041;&#26696;&#26469;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;(CNNC)&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#20855;&#26377;&#20016;&#23500;&#26631;&#31614;&#30340;&#28304;&#32593;&#32476;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#23545;&#26631;&#31614;&#19981;&#20805;&#20998;&#30340;&#30446;&#26631;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;CNNC&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#21644;&#26465;&#20214;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;(DM-GNN)&#65292;&#33021;&#22815;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#33410;&#28857;&#20998;&#31867;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21452;&#29305;&#24449;&#25552;&#21462;&#22120;&#26500;&#24314;&#20102;GNN&#32534;&#30721;&#22120;&#65292;&#23558;&#33258;&#25105;&#23884;&#20837;&#23398;&#20064;&#19982;&#37051;&#23621;&#23884;&#20837;&#23398;&#20064;&#20998;&#31163;&#65292;&#20197;&#20849;&#21516;&#25429;&#25417;&#36830;&#25509;&#33410;&#28857;&#20043;&#38388;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#20256;&#25773;&#33410;&#28857;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#33258;&#36523;&#39044;&#27979;&#21644;&#37051;&#23621;&#39044;&#27979;&#30456;&#32467;&#21512;&#26469;&#32454;&#21270;&#27599;&#20010;&#33410;&#28857;&#30340;&#26631;&#31614;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26377;&#26631;&#31614;&#28304;&#32593;&#32476;&#35774;&#35745;&#20102;&#26631;&#31614;&#24863;&#30693;&#30340;&#20256;&#25773;&#26041;&#26696;&#65292;&#20197;&#20419;&#36827;&#26631;&#31614;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-network node classification (CNNC), which aims to classify nodes in a label-deficient target network by transferring the knowledge from a source network with abundant labels, draws increasing attention recently. To address CNNC, we propose a domain-adaptive message passing graph neural network (DM-GNN), which integrates graph neural network (GNN) with conditional adversarial domain adaptation. DM-GNN is capable of learning informative representations for node classification that are also transferrable across networks. Firstly, a GNN encoder is constructed by dual feature extractors to separate ego-embedding learning from neighbor-embedding learning so as to jointly capture commonality and discrimination between connected nodes. Secondly, a label propagation node classifier is proposed to refine each node's label prediction by combining its own prediction and its neighbors' prediction. In addition, a label-aware propagation scheme is devised for the labeled source network to promo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#20840;&#38754;&#35299;&#37322;&#21644;&#24212;&#29992;&#30340;&#29616;&#29366;&#21450;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13877</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#65306;&#25552;&#39640;&#25928;&#29575;&#21644;&#33539;&#22260;&#30340;&#20808;&#36827;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Applications of machine Learning to improve the efficiency and range of microbial biosynthesis: a review of state-of-art techniques. (arXiv:2308.13877v1 [q-bio.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#20840;&#38754;&#35299;&#37322;&#21644;&#24212;&#29992;&#30340;&#29616;&#29366;&#21450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#65292;&#25216;&#26415;&#36798;&#21040;&#20102;&#39030;&#23792;&#12290;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#32534;&#31243;&#21644;&#25216;&#26415;&#36884;&#24452;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#12289;&#33258;&#21160;&#21270;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20248;&#21270;&#25968;&#25454;&#20998;&#26512;&#12289;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#21644;&#21152;&#36895;/&#25913;&#36827;&#29616;&#26377;&#21151;&#33021;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#27491;&#22312;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#19988;&#27491;&#22312;&#25506;&#32034;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#39046;&#22495;&#23601;&#26159;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#39046;&#22495;&#20998;&#21035;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;&#35813;&#20449;&#24687;&#21253;&#25324;&#36807;&#21435;&#30340;&#36235;&#21183;&#12289;&#29616;&#20195;&#30340;&#21457;&#23637;&#12289;&#26410;&#26469;&#30340;&#25913;&#36827;&#12289;&#27969;&#31243;&#30340;&#35299;&#37322;&#20197;&#21450;&#24403;&#21069;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26803;&#29702;&#20102;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#24212;&#29992;&#30340;&#20840;&#38754;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern world, technology is at its peak. Different avenues in programming and technology have been explored for data analysis, automation, and robotics. Machine learning is key to optimize data analysis, make accurate predictions, and hasten/improve existing functions. Thus, presently, the field of machine learning in artificial intelligence is being developed and its uses in varying fields are being explored. One field in which its uses stand out is that of microbial biosynthesis. In this paper, a comprehensive overview of the differing machine learning programs used in biosynthesis is provided, alongside brief descriptions of the fields of machine learning and microbial biosynthesis separately. This information includes past trends, modern developments, future improvements, explanations of processes, and current problems they face. Thus, this paper's main contribution is to distill developments in, and provide a holistic explanation of, 2 key fields and their applicability to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#25104;&#21151;&#29983;&#25104;&#20102;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2308.11890</link><description>&lt;p&gt;
&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;3D&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#25104;&#21151;&#29983;&#25104;&#20102;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#20307;&#22522;&#33647;&#29289;&#35774;&#35745;&#26088;&#22312;&#35782;&#21035;&#19982;&#24050;&#30693;&#27963;&#24615;&#20998;&#23376;&#24418;&#29366;&#30456;&#20284;&#30340;&#26032;&#22411;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#20998;&#23376;&#30340;&#24418;&#29366;&#26465;&#20214;&#19979;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31561;&#21464;&#24418;&#29366;&#24341;&#23548;&#30340;&#29983;&#25104;&#27169;&#22411;ShapeMol&#12290;ShapeMol&#30001;&#19968;&#20010;&#31561;&#21464;&#24418;&#29366;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#36825;&#20123;&#32534;&#30721;&#29983;&#25104;3D&#20998;&#23376;&#30340;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#32452;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ShapeMol&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#12289;&#22810;&#26679;&#19988;&#31867;&#20284;&#32473;&#23450;&#24418;&#29366;&#26465;&#20214;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;ShapeMol&#22312;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;3D&#24418;&#29366;&#24182;&#19982;&#34507;&#30333;&#38774;&#28857;&#32467;&#21512;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10875</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21450;&#20854;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#29983;&#29289;&#32479;&#35745;&#23398;&#12289;&#29983;&#24577;&#23398;&#21644;&#21046;&#36896;&#19994;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries. (arXiv:2308.10875v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#31185;&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#31361;&#21464;&#20195;&#29702;&#30340;&#31454;&#20105;&#24615;&#32676;&#20307;&#20248;&#21270;&#22120;(CSO-MA)&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;&#31454;&#20105;&#23545;&#25163;&#22312;&#32479;&#35745;&#31185;&#23398;&#20013;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#28789;&#27963;&#24615;&#21644;&#36229;&#36234;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#39640;&#25928;&#19988;&#21487;&#20197;&#25972;&#21512;&#21508;&#31181;&#25104;&#26412;&#32467;&#26500;&#25110;&#22810;&#20010;&#29992;&#25143;&#25351;&#23450;&#30340;&#38750;&#32447;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#21253;&#25324;(i)&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#36890;&#36807;&#21333;&#32454;&#32990;&#24191;&#20041;&#36235;&#21183;&#27169;&#22411;&#25214;&#21040;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20197;&#30740;&#31350;&#20266;&#26102;&#24577;&#65292;(ii) &#20272;&#35745;&#25945;&#32946;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;Rasch&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;(iii) &#22312;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#27169;&#22411;&#20013;&#20026;Cox&#22238;&#24402;&#25214;&#21040;M-&#20272;&#35745;&#65292;(iv) &#30697;&#38453;&#34917;&#20840;&#20197;&#22635;&#34917;&#20004;&#20010;&#36830;&#36830;&#19981;&#36890;&#22270;&#20013;&#30340;&#32570;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nature-inspired metaheuristic algorithms are important components of artificial intelligence, and are increasingly used across disciplines to tackle various types of challenging optimization problems. We apply a newly proposed nature-inspired metaheuristic algorithm called competitive swarm optimizer with mutated agents (CSO-MA) and demonstrate its flexibility and out-performance relative to its competitors in a variety of optimization problems in the statistical sciences. In particular, we show the algorithm is efficient and can incorporate various cost structures or multiple user-specified nonlinear constraints. Our applications include (i) finding maximum likelihood estimates of parameters in a single cell generalized trend model to study pseudotime in bioinformatics, (ii) estimating parameters in a commonly used Rasch model in education research, (iii) finding M-estimates for a Cox regression in a Markov renewal model and (iv) matrix completion to impute missing values in a two com
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16189</link><description>&lt;p&gt;
&#29992;&#20110;16&#20301;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;16&#20301;&#35745;&#31639;&#20013;&#20351;&#29992;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;RMSProp&#21644;Adam&#65289;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20986;&#29616;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#24178;&#25200;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#19968;&#36229;&#21442;&#25968;epsilon&#26159;&#36825;&#31181;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23545;16&#20301;&#35745;&#31639;&#20013;&#36825;&#20123;&#20248;&#21270;&#22120;&#20013;epsilon&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#21457;&#29616;&#24494;&#35843;&#20854;&#20540;&#21487;&#20197;&#24674;&#22797;RMSProp&#21644;Adam&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;16&#20301;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34987;&#21457;&#29616;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Adam&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02932</link><description>&lt;p&gt;
&#24403;&#25298;&#32477;&#23398;&#20064;&#23545;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#26368;&#20248;&#26102;
&lt;/p&gt;
&lt;p&gt;
When No-Rejection Learning is Optimal for Regression with Rejection. (arXiv:2307.02932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25298;&#32477;&#23398;&#20064;&#26159;&#30740;&#31350;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30456;&#20114;&#20316;&#29992;&#30340;&#20856;&#22411;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25298;&#32477;&#22120;&#12290;&#22312;&#26679;&#26412;&#21040;&#36798;&#26102;&#65292;&#25298;&#32477;&#22120;&#39318;&#20808;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#23427;&#65307;&#22914;&#26524;&#25509;&#21463;&#65292;&#39044;&#27979;&#22120;&#23436;&#25104;&#39044;&#27979;&#20219;&#21153;&#65307;&#22914;&#26524;&#34987;&#25298;&#32477;&#65292;&#21017;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#20154;&#31867;&#12290;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#39044;&#27979;&#22120;&#21644;&#25298;&#32477;&#22120;&#12290;&#36825;&#25913;&#21464;&#20102;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#38750;&#20984;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#24102;&#26377;&#25298;&#32477;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#19968;&#20123;&#30740;&#31350;&#24320;&#21457;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#24182;&#30740;&#31350;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish tha
&lt;/p&gt;</description></item><item><title>&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#31934;&#30830;&#24230;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24182;&#22312;&#30828;&#20214;&#24863;&#30693;&#21644;&#30828;&#20214;&#26080;&#24863;&#30693;&#30340;NAS&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01998</link><description>&lt;p&gt;
&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65306;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities. (arXiv:2307.01998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01998
&lt;/p&gt;
&lt;p&gt;
&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#39044;&#27979;&#32593;&#32476;&#31934;&#30830;&#24230;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#38646;&#20195;&#20215;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#24182;&#22312;&#30828;&#20214;&#24863;&#30693;&#21644;&#30828;&#20214;&#26080;&#24863;&#30693;&#30340;NAS&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38646;&#20195;&#20215;&#65288;&#25110;&#26080;&#38656;&#35757;&#32451;&#65289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#23558;NAS&#20174;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#35299;&#25918;&#20986;&#26469;&#12290;&#38646;&#20195;&#20215;NAS&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#39044;&#27979;&#26576;&#20123;&#32473;&#23450;&#32593;&#32476;&#31934;&#30830;&#24230;&#30340;&#20195;&#29702;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#32593;&#32476;&#21442;&#25968;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24050;&#32463;&#25552;&#20986;&#30340;&#20195;&#29702;&#36890;&#24120;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#24182;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#21644;NAS&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#23457;&#26597;&#21644;&#27604;&#36739;&#26368;&#20808;&#36827;&#30340;&#38646;&#20195;&#20215;NAS&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#30828;&#20214;&#30340;&#24847;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20027;&#27969;&#30340;&#38646;&#20195;&#20215;&#20195;&#29702;&#24182;&#35752;&#35770;&#23427;&#20204;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#27604;&#36739;&#36825;&#20123;&#38646;&#20195;&#20215;&#20195;&#29702;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#30828;&#20214;&#24863;&#30693;&#21644;&#30828;&#20214;&#26080;&#24863;&#30693;&#30340;NAS&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#22909;&#30340;&#20195;&#29702;&#30340;&#20960;&#20010;&#26377;&#21069;&#36884;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, zero-shot (or training-free) Neural Architecture Search (NAS) approaches have been proposed to liberate NAS from the expensive training process. The key idea behind zero-shot NAS approaches is to design proxies that can predict the accuracy of some given networks without training the network parameters. The proxies proposed so far are usually inspired by recent progress in theoretical understanding of deep learning and have shown great potential on several datasets and NAS benchmarks. This paper aims to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness. To this end, we first review the mainstream zero-shot proxies and discuss their theoretical underpinnings. We then compare these zero-shot proxies through large-scale experiments and demonstrate their effectiveness in both hardware-aware and hardware-oblivious NAS scenarios. Finally, we point out several promising ideas to design better proxies
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14872</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#24179;&#34913;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#20960;&#20309;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits. (arXiv:2306.14872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#23454;&#35777;&#24615;&#33021;&#19982;&#24754;&#35266;&#29702;&#35770;&#21518;&#24724;&#30028;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#21253;&#25324;&#36138;&#24515;&#12289;OFUL&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#22312;&#20869;&#30340;&#24191;&#27867;&#31639;&#27861;&#31867;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#22312;&#20445;&#30041;&#22522;&#26412;&#31639;&#27861;&#22823;&#37096;&#20998;&#20248;&#33391;&#29305;&#24615;&#30340;&#21516;&#26102;&#8220;&#26657;&#27491;&#8221;&#22522;&#26412;&#31639;&#27861;&#22312;&#26576;&#20123;&#23454;&#20363;&#20013;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28176;&#36817;&#26368;&#20248;&#21518;&#24724;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is motivated by recent developments in the linear bandit literature, which have revealed a discrepancy between the promising empirical performance of algorithms such as Thompson sampling and Greedy, when compared to their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometry of the uncertainty ellipsoid, enabling us to establish an instance-dependent frequentist regret bound for a broad class of algorithms, including Greedy, OFUL, and Thompson sampling. This result empowers us to identify and ``course-correct" instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\tilde{\mathcal{O}}(d\sqrt{T})$, while retaining most of the desirable properties of the base algorithms. We present sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14079</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#65306;&#36890;&#36807;&#25193;&#25955;&#20998;&#25968;&#21305;&#37197;&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching. (arXiv:2306.14079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20248;&#21270;&#33539;&#24335;&#65292;&#20363;&#22914;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25110;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#20801;&#35768;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#20197;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#20854;&#22312;&#39640;&#32500;&#24230;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#20180;&#32454;&#22320;&#32771;&#34385;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20114;&#24433;&#21709;&#12290;&#25105;&#20204;&#22768;&#31216;&#65292;&#20026;&#20102;&#35753;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#23427;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19981;&#20165;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;Lipschitz&#24120;&#25968;&#26469;&#20998;&#26512;&#27169;&#22411;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#21644;&#25968;&#25454;&#20284;&#28982;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, 
&lt;/p&gt;</description></item><item><title>ALP&#26159;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21160;&#20316;&#20449;&#24687;&#34701;&#20837;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#23398;&#20064;&#21487;&#26222;&#36941;&#24212;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#31215;&#26497;&#25506;&#32034;&#21644;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.10190</link><description>&lt;p&gt;
ALP: &#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#29992;&#20110;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
ALP: Action-Aware Embodied Learning for Perception. (arXiv:2306.10190v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10190
&lt;/p&gt;
&lt;p&gt;
ALP&#26159;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21160;&#20316;&#20449;&#24687;&#34701;&#20837;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#23398;&#20064;&#21487;&#26222;&#36941;&#24212;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#31215;&#26497;&#25506;&#32034;&#21644;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22312;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#34987;&#21160;&#30340;&#12289;&#31574;&#21010;&#22909;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22522;&#26412;&#19978;&#26080;&#27861;&#36866;&#24212;&#19968;&#20010;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#65292;&#22240;&#20026;&#36755;&#20837;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20197;&#26356;&#20154;&#31867;&#20013;&#24515;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26469;&#36827;&#34892;&#23398;&#20064;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#26694;&#26550;&#65288;ALP&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21644;&#36870;&#21160;&#21147;&#23398;&#39044;&#27979;&#30446;&#26631;&#30340;&#32467;&#21512;&#65292;&#23558;&#21160;&#20316;&#20449;&#24687;&#34701;&#20837;&#21040;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#31215;&#26497;&#25506;&#32034;&#65292;&#26082;&#23398;&#20064;&#21487;&#26222;&#36941;&#24212;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#21448;&#25910;&#38598;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ALP&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods in training and benchmarking vision models exhibit an over-reliance on passive, curated datasets. Although models trained on these datasets have shown strong performance in a wide variety of tasks such as classification, detection, and segmentation, they fundamentally are unable to generalize to an ever-evolving world due to constant out-of-distribution shifts of input data. Therefore, instead of training on fixed datasets, can we approach learning in a more human-centric and adaptive manner? In this paper, we introduce Action-Aware Embodied Learning for Perception (ALP), an embodied learning framework that incorporates action information into representation learning through a combination of optimizing a reinforcement learning policy and an inverse dynamics prediction objective. Our method actively explores in complex 3D environments to both learn generalizable task-agnostic visual representations as well as collect downstream training data. We show that ALP outperforms
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;(OIC)&#30340;&#36890;&#29992;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26159;&#22312;&#20915;&#31574;&#36873;&#25321;&#26041;&#38754;&#30340;&#19968;&#20010;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.10081</link><description>&lt;p&gt;
&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;&#65306;&#21078;&#26512;&#21644;&#32416;&#27491;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Optimizer's Information Criterion: Dissecting and Correcting Bias in Data-Driven Optimization. (arXiv:2306.10081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;(OIC)&#30340;&#36890;&#29992;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26159;&#22312;&#20915;&#31574;&#36873;&#25321;&#26041;&#38754;&#30340;&#19968;&#20010;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#20013;&#65292;&#25152;&#24471;&#20915;&#31574;&#30340;&#26679;&#26412;&#34920;&#29616;&#36890;&#24120;&#23384;&#22312;&#30528;&#23545;&#30495;&#23454;&#34920;&#29616;&#30340;&#20048;&#35266;&#20559;&#24046;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#35781;&#21650;&#65292;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#23494;&#20999;&#30456;&#20851;&#12290;&#20256;&#32479;&#30340;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#22914;&#20132;&#21449;&#39564;&#35777;&#65292;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#25105;&#20204;&#31216;&#20043;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;&#65288;OIC&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#19981;&#38656;&#35201;&#35299;&#20915;&#20219;&#20309;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;OIC&#23558;&#33879;&#21517;&#30340;&#36196;&#27744;&#20449;&#24687;&#20934;&#21017;&#25512;&#24191;&#21040;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#65292;&#20851;&#38190;&#26159;&#35780;&#20272;&#23458;&#35266;&#34920;&#29616;&#65292;&#19981;&#20165;&#28041;&#21450;&#27169;&#22411;&#25311;&#21512;&#65292;&#36824;&#28041;&#21450;&#20854;&#19982;&#19979;&#28216;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20915;&#31574;&#36873;&#25321;&#32780;&#19981;&#20165;&#20165;&#26159;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#38382;&#39064;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven optimization, the sample performance of the obtained decision typically incurs an optimistic bias against the true performance, a phenomenon commonly known as the Optimizer's Curse and intimately related to overfitting in machine learning. Common techniques to correct this bias, such as cross-validation, require repeatedly solving additional optimization problems and are therefore computationally expensive. We develop a general bias correction approach, building on what we call Optimizer's Information Criterion (OIC), that directly approximates the first-order bias and does not require solving any additional optimization problems. Our OIC generalizes the celebrated Akaike Information Criterion to evaluate the objective performance in data-driven optimization, which crucially involves not only model fitting but also its interplay with the downstream optimization. As such it can be used for decision selection instead of only model selection. We apply our approach to a rang
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;-ExpTanh&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#36718;&#32974;&#21147;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#29616;&#26377;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26694;&#26550;&#20013;&#35299;&#26512;&#21047;&#24335;&#36718;&#32974;&#27169;&#22411;&#30340;&#26367;&#20195;&#21697;,&#25104;&#21151;&#23454;&#29616;&#20102;&#20165;&#21033;&#29992;&#23569;&#20110;&#19977;&#20998;&#38047;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33258;&#20027;&#28418;&#31227;</title><link>http://arxiv.org/abs/2306.06330</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#36718;&#32974;&#27169;&#22411;&#30340;&#19977;&#20998;&#38047;&#25968;&#25454;&#33258;&#20027;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Autonomous Drifting with 3 Minutes of Data via Learned Tire Models. (arXiv:2306.06330v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;-ExpTanh&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#36718;&#32974;&#21147;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#29616;&#26377;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26694;&#26550;&#20013;&#35299;&#26512;&#21047;&#24335;&#36718;&#32974;&#27169;&#22411;&#30340;&#26367;&#20195;&#21697;,&#25104;&#21151;&#23454;&#29616;&#20102;&#20165;&#21033;&#29992;&#23569;&#20110;&#19977;&#20998;&#38047;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33258;&#20027;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38468;&#30528;&#26497;&#38480;&#38468;&#36817;&#65292;&#36718;&#32974;&#25152;&#20135;&#29983;&#30340;&#21147;&#26159;&#38750;&#32447;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#12290;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#39640;&#25928;&#12289;&#20934;&#30830;&#30340;&#24314;&#27169;&#21487;&#20197;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#21147;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;-ExpTanh&#21442;&#25968;&#21270;&#30340;&#26032;&#22411;&#36718;&#32974;&#21147;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#28385;&#36275;&#29289;&#29702;&#27934;&#23519;&#21147;&#20551;&#35774;&#65292;&#21516;&#26102;&#20855;&#26377;&#36275;&#22815;&#30340;&#20445;&#30495;&#24230;&#65292;&#20197;&#30452;&#25509;&#20174;&#36710;&#36742;&#29366;&#24577;&#27979;&#37327;&#20013;&#25429;&#25417;&#39640;&#38454;&#25928;&#24212;&#12290;&#23427;&#20204;&#34987;&#29992;&#20316;&#29616;&#26377;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26694;&#26550;&#20013;&#35299;&#26512;&#21047;&#24335;&#36718;&#32974;&#27169;&#22411;&#30340;&#26367;&#20195;&#21697;&#12290;&#36890;&#36807;&#20351;&#29992;Toyota Supra&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23569;&#20110;&#19977;&#20998;&#38047;&#30340;&#39550;&#39542;&#25968;&#25454;&#36275;&#20197;&#22312;&#21508;&#31181;&#36712;&#36857;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33258;&#20027;&#28418;&#31227;&#65292;&#26368;&#39640;&#36895;&#24230;&#21487;&#36798;45mph&#12290;&#19982;&#22522;&#20934;&#27169;&#22411;&#30340;&#27604;&#36739;&#26174;&#31034;&#36712;&#36857;&#25552;&#39640;&#20102;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Near the limits of adhesion, the forces generated by a tire are nonlinear and intricately coupled. Efficient and accurate modelling in this region could improve safety, especially in emergency situations where high forces are required. To this end, we propose a novel family of tire force models based on neural ordinary differential equations and a neural-ExpTanh parameterization. These models are designed to satisfy physically insightful assumptions while also having sufficient fidelity to capture higher-order effects directly from vehicle state measurements. They are used as drop-in replacements for an analytical brush tire model in an existing nonlinear model predictive control framework. Experiments with a customized Toyota Supra show that scarce amounts of driving data -- less than three minutes -- is sufficient to achieve high-performance autonomous drifting on various trajectories with speeds up to 45mph. Comparisons with the benchmark model show a $4 \times$ improvement in track
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05726</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Sample Policy Iteration for Offline Reinforcement Learning. (arXiv:2306.05726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#20197;&#21069;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#25512;&#23548;&#20986;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#25968;&#25454;&#35206;&#30422;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#65292;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20559;&#31163;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#24403;&#31163;&#32447;&#25968;&#25454;&#38598;&#30001;&#27425;&#20248;&#31574;&#30053;&#25910;&#38598;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#20339;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#22686;&#24378;&#20102;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#12290;&#26680;&#24515;&#35265;&#35299;&#26159;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#29992;&#20110;&#34892;&#20026;&#35268;&#21017;&#30340;&#31574;&#30053;&#65292;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#36880;&#28176;&#25913;&#36827;&#33258;&#36523;&#65292;&#21516;&#26102;&#38544;&#24335;&#36991;&#20813;&#26597;&#35810;&#26679;&#26412;&#22806;&#30340;&#34892;&#21160;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#30340;&#23398;&#20064;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#23398;&#20064;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#33391;&#22909;&#35206;&#30422;&#30340;&#34892;&#21160;&#23398;&#20064;&#26679;&#26412;&#20869;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) seeks to derive an effective control policy from previously collected data. To circumvent errors due to inadequate data coverage, behavior-regularized methods optimize the control policy while concurrently minimizing deviation from the data collection policy. Nevertheless, these methods often exhibit subpar practical performance, particularly when the offline dataset is collected by sub-optimal policies. In this paper, we propose a novel algorithm employing in-sample policy iteration that substantially enhances behavior-regularized methods in offline RL. The core insight is that by continuously refining the policy used for behavior regularization, in-sample policy iteration gradually improves itself while implicitly avoids querying out-of-sample actions to avert catastrophic learning failures. Our theoretical analysis verifies its ability to learn the in-sample optimal policy, exclusively utilizing actions well-covered by the dataset. Moreover, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.04810</link><description>&lt;p&gt;
&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#65306;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry. (arXiv:2306.04810v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26435;&#37325;&#23545;&#31216;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#21512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#20256;&#25773;&#12290;&#36890;&#36807;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#29983;&#29289;&#21512;&#29702;&#24615;&#21463;&#21040;&#20105;&#35758;&#65292;&#29616;&#22312;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#21363;&#22823;&#33041;&#26159;&#21542;&#37319;&#29992;&#31867;&#20284;&#20110;&#23427;&#30340;&#30417;&#30563;&#23398;&#20064;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23618;&#28608;&#27963;&#20043;&#38388;&#36827;&#34892;&#30456;&#20851;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26367;&#20195;&#35268;&#33539;&#26041;&#27861;&#65292;&#20197;&#25551;&#36848;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#20449;&#21495;&#22312;&#21069;&#21521;&#21644;&#21518;&#21521;&#26041;&#21521;&#19978;&#20256;&#25773;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#26032;&#26694;&#26550;&#35299;&#20915;&#20102;&#26377;&#20851;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#30456;&#24212;&#30446;&#26631;&#30340;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#65292;&#19982;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#27169;&#25311;&#19968;&#31181;&#20855;&#26377;&#26641;&#31361;&#22788;&#29702;&#21644;&#20391;&#25233;&#21046;&#31070;&#32463;&#20803;&#30340;&#26356;&#29983;&#29289;&#30495;&#23454;&#30340;&#22810;&#23460;&#37329;&#23383;&#22612;&#24418;&#31070;&#32463;&#20803;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks, however, its biological-plausibility is disputed, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04265</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#22312;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#30340;&#26412;&#36136;&#19982;&#21516;&#36136;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#34920;&#26126;1-hop&#20197;&#22806;&#30340;&#32858;&#21512;&#26041;&#24335;&#24182;&#24341;&#36215;&#26089;&#26399;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;Haar-type&#22270;&#26694;&#26550;&#65292;&#22312;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#26500;&#24314;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20102;&#22270;&#26694;&#26550;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#12290;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;9&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30456;&#23545;&#36739;&#22823;&#21644;&#26356;&#23494;&#38598;&#30340;&#36830;&#25509;&#30340;&#22823;&#37096;&#20998;&#24322;&#36136;&#25968;&#25454;&#38598;&#65289;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20313;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#29305;&#24449;&#20316;&#20026;&#23558;&#22797;&#26434;&#20540;&#29305;&#24449;&#25512;&#24191;&#21040;&#39640;&#32428;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#31243;&#26469;&#25552;&#21462;&#20998;&#24067;&#24335;&#34920;&#31034;&#20013;&#30340;&#29289;&#20307;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#25193;&#23637;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.00600</link><description>&lt;p&gt;
&#26059;&#36716;&#29305;&#24449;&#29992;&#20110;&#29289;&#20307;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Rotating Features for Object Discovery. (arXiv:2306.00600v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#29305;&#24449;&#20316;&#20026;&#23558;&#22797;&#26434;&#20540;&#29305;&#24449;&#25512;&#24191;&#21040;&#39640;&#32428;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#31243;&#26469;&#25552;&#21462;&#20998;&#24067;&#24335;&#34920;&#31034;&#20013;&#30340;&#29289;&#20307;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#25193;&#23637;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#32465;&#23450;&#38382;&#39064;&#28041;&#21450;&#22823;&#33041;&#22914;&#20309;&#22312;&#22266;&#23450;&#30340;&#31070;&#32463;&#36830;&#25509;&#32593;&#32476;&#20013;&#34920;&#31034;&#21644;&#36830;&#25509;&#29289;&#20307;&#65292;&#20173;&#28982;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35770;&#12290;&#22823;&#22810;&#25968;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22522;&#20110;&#25554;&#27133;&#30340;&#26041;&#27861;&#19978;&#65292;&#30001;&#20110;&#20854;&#31163;&#25955;&#24615;&#21644;&#38590;&#20197;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#28857;&#65292;&#21487;&#33021;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#65292;&#22797;&#26434;&#33258;&#21160;&#32534;&#30721;&#22120;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#23398;&#20064;&#36830;&#32493;&#21644;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20195;&#26367;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#21482;&#36866;&#29992;&#20110;&#31616;&#21333;&#30340;&#29609;&#20855;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26059;&#36716;&#29305;&#24449;&#65292;&#23558;&#22797;&#20540;&#29305;&#24449;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#20998;&#24067;&#24335;&#34920;&#31034;&#20013;&#29289;&#20307;&#30340;&#26032;&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#31616;&#21333;&#30340;&#29609;&#20855;&#25968;&#25454;&#25193;&#23637;&#21040;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19555</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#20316;&#20026;&#25277;&#35937;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#21151;&#30340;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;LLMs&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#25110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36824;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#20063;&#19981;&#30830;&#23450;&#12290;&#25277;&#35937;&#25512;&#29702;&#26159;&#35748;&#30693;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25214;&#21040;&#21644;&#24212;&#29992;&#19968;&#33324;&#27169;&#24335;&#12290;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#22312;&#23616;&#38480;&#24615;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30446;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#25506;&#31350;&#20102;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476;&#27169;&#22359;&#65292;&#21487;&#20351;&#33292;&#37096;&#36229;&#22768;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#19981;&#21516;&#30340;&#29992;&#25143;&#21644;&#20250;&#35805;&#65292;&#19988;&#33021;&#26174;&#33879;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19130</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476;&#30340;&#33292;&#37096;&#36229;&#22768;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks. (arXiv:2305.19130v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19130
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476;&#27169;&#22359;&#65292;&#21487;&#20351;&#33292;&#37096;&#36229;&#22768;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#19981;&#21516;&#30340;&#29992;&#25143;&#21644;&#20250;&#35805;&#65292;&#19988;&#33021;&#26174;&#33879;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#20174;&#21457;&#38899;&#36816;&#21160;&#25968;&#25454;&#20013;&#21512;&#25104;&#21487;&#25026;&#30340;&#35821;&#38899;&#65292;&#36825;&#20123;&#25104;&#26524;&#26469;&#33258;&#20110;&#26080;&#22768;&#35821;&#38899;&#25509;&#21475;&#65288;SSI&#65289;&#12290;&#28982;&#32780;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#24448;&#24448;&#29305;&#23450;&#20110;&#26576;&#19968;&#35828;&#35805;&#20154;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#36827;&#34892;&#24555;&#36895;&#20999;&#25442;&#21464;&#24471;&#40635;&#28902;&#12290;&#21363;&#20351;&#26159;&#21516;&#19968;&#20010;&#35762;&#32773;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#36827;&#34892;&#27169;&#22411;&#24212;&#29992;&#25928;&#26524;&#20063;&#36739;&#24046;&#12290;&#20026;&#20102;&#24110;&#21161;&#33292;&#37096;&#36229;&#22768;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340; SSI &#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#19981;&#21516;&#30340;&#29992;&#25143;&#21644;&#20250;&#35805;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#28145;&#24230;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;&#36716;&#25442;&#32593;&#32476; (STN) &#27169;&#22359;&#65292;&#33021;&#22815;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#12290;&#34429;&#28982; STN &#21482;&#21344;&#32593;&#32476;&#30340;&#32422; 10%&#65292;&#20294;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36866;&#24212; STN &#27169;&#22359;&#23601;&#21487;&#20197;&#23558;&#22343;&#26041;&#35823;&#24046;&#24179;&#22343;&#20943;&#23569; 88%&#65292;&#32780;&#19981;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#32593;&#32476;&#12290;&#24403;&#23558;&#32593;&#32476;&#36866;&#24212;&#21040;&#21516;&#19968;&#35762;&#32773;&#30340;&#19981;&#21516;&#35760;&#24405;&#20250;&#35805;&#26102;&#65292;&#25913;&#36827;&#25928;&#26524;&#26356;&#22823;&#65288;&#22823;&#32422; 92%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88% on the average, compared to retraining the whole network. The improvement is even larger (around 92%) when adapting the network to different recording sessions from the same speaker.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIMUS&#30340;&#26032;&#22411;&#27169;&#20223;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#20223;TAMP&#20195;&#29702;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#31574;&#30053;&#12290;OPTIMUS&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27169;&#20223;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;TAMP&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#24615;&#33021;&#20248;&#36234;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.16309</link><description>&lt;p&gt;
&#29992;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#27169;&#25311;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Imitating Task and Motion Planning with Visuomotor Transformers. (arXiv:2305.16309v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIMUS&#30340;&#26032;&#22411;&#27169;&#20223;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#20223;TAMP&#20195;&#29702;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#31574;&#30053;&#12290;OPTIMUS&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27169;&#20223;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;TAMP&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#24615;&#33021;&#20248;&#36234;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#25163;&#21160;&#32534;&#31243;&#25110;&#35797;&#38169;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#30417;&#30563;&#65292;&#22240;&#20026;&#32791;&#26102;&#21644;&#21171;&#21160;&#23494;&#38598;&#32780;&#38590;&#20197;&#25193;&#23637;&#12290;&#30456;&#21453;&#65292;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#21487;&#20197;&#33258;&#20027;&#22320;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#22810;&#26679;&#21270;&#28436;&#31034;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;TAMP&#30417;&#30563;&#21592;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19982;&#28789;&#27963;&#30340;Transformer&#27169;&#22411;&#30456;&#32467;&#21512;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24378;&#22823;&#33539;&#20363;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIMUS&#30340;&#26032;&#22411;&#27169;&#20223;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#20223;TAMP&#20195;&#29702;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21160;&#20316;&#36716;&#25442;&#22120;&#31574;&#30053;&#12290;OPTIMUS&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27169;&#20223;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;TAMP&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#24615;&#33021;&#20248;&#36234;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#35774;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However, common methods of data collection, such as human supervision, scale poorly, as they are time-consuming and labor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomously generate large-scale datasets of diverse demonstrations. In this work, we show that the combination of large-scale datasets generated by TAMP supervisors and flexible Transformer models to fit them is a powerful paradigm for robot manipulation. To that end, we present a novel imitation learning system called OPTIMUS that trains large-scale visuomotor Transformer policies by imitating a TAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that is specifically curated for imitation learning and can be used to train performant transformer-based policies. In this paper, we present a thorough study of the design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16014</link><description>&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22810;&#23569;&#26679;&#26412;&#25165;&#33021;&#21033;&#29992;&#24179;&#28369;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#30340;&#26680;&#24515;&#21407;&#21017;&#20043;&#19968;&#26159;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21487;&#20197;&#25171;&#30772;&#32500;&#24230;&#28798;&#38590;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#27888;&#21202;&#23637;&#24320;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#38656;&#35201;&#36275;&#22815;&#25509;&#36817;&#19968;&#36215;&#30340;&#26679;&#26412;&#26469;&#33719;&#24471;&#39640;&#38454;&#23548;&#25968;&#30340;&#26377;&#24847;&#20041;&#20272;&#35745;&#65292;&#36825;&#22312;&#25968;&#25454;&#37327;&#30456;&#23545;&#36739;&#23567;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#24191;&#20041;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#30340;&#19979;&#30028;&#65292;&#30740;&#31350;&#20102;&#24120;&#25968;&#21644;&#30636;&#24577;&#21306;&#22495;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#21364;&#21457;&#25381;&#20102;&#20027;&#23548;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
&lt;/p&gt;</description></item><item><title>Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.14342</link><description>&lt;p&gt;
Sophia&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14342
&lt;/p&gt;
&lt;p&gt;
Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20248;&#21270;&#31639;&#27861;&#30340;&#24494;&#23567;&#25913;&#36827;&#23558;&#20250;&#22823;&#22823;&#38477;&#20302;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;Adam&#21450;&#20854;&#21464;&#31181;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#20108;&#38454;&#65288;&#22522;&#20110;Hessian&#30340;&#65289;&#20248;&#21270;&#22120;&#24448;&#24448;&#20250;&#24102;&#26469;&#22826;&#22810;&#30340;&#27599;&#27493;&#24320;&#38144;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sophia&#65292;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20272;&#35745;&#30340;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#12290;&#26356;&#26032;&#27493;&#39588;&#26159;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#38500;&#20197;&#20272;&#35745;Hessian&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#65292;&#28982;&#21518;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#12290;&#35009;&#21098;&#25511;&#21046;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26356;&#26032;&#22823;&#23567;&#65292;&#24182;&#25511;&#21046;&#20102;Hessian&#22312;&#36712;&#36857;&#19978;&#30340;&#38750;&#20984;&#24615;&#21644;&#24555;&#36895;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;Sophia&#21482;&#22312;&#27599;&#20960;&#27425;&#36845;&#20195;&#20013;&#20272;&#35745;&#23545;&#35282;Hessian&#65292;&#36825;&#20960;&#20046;&#27809;&#26377;&#24179;&#22343;&#27599;&#27493;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;&#22312;&#20351;&#29992;GPT m&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13035</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#20351;ViT&#25104;&#24418;&#65306;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#35774;&#35745;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32553;&#25918;&#23450;&#24459;&#34987;&#29992;&#26469;&#25512;&#23548;&#22312;&#32473;&#23450;&#35745;&#31639;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#65288;&#21442;&#25968;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#21457;&#23637;&#24182;&#25913;&#36827;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20197;&#25512;&#27979;&#22914;&#23485;&#24230;&#21644;&#28145;&#24230;&#31561;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#24182;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#25104;&#21151;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#32463;&#36807;&#24418;&#29366;&#20248;&#21270;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#22312;&#20165;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#35745;&#31639;&#37327;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;SoViT-400m/14&#22312;ILSRCV2012&#19978;&#21462;&#24471;&#20102;90.3%&#30340;&#24494;&#35843;&#20934;&#30830;&#24230;&#65292;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;ViT-g/14&#65292;&#22312;&#30456;&#21516;&#35774;&#32622;&#19979;&#25509;&#36817;ViT-G/14&#65292;&#21516;&#26102;&#25512;&#26029;&#25104;&#26412;&#20063;&#19981;&#21040;&#19968;&#21322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#23383;&#24149;&#12289;VQA&#21644;&#38646;-shot&#36716;&#31227;&#65292;&#22312;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24182;&#30830;&#23450;&#20102;&#20854;&#38480;&#21046;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#25361;&#25112;&#20102;&#30450;&#30446;&#25193;&#22823;&#35270;&#35273;&#27169;&#22411;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#22686;&#24378;&#20803;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#65288;EMLC&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#20803;&#23398;&#20064;&#36807;&#31243;&#21644;&#24341;&#20837;&#26356;&#20934;&#30830;&#30340;&#20803;&#26799;&#24230;&#25512;&#23548;&#65292;&#20197;&#21450;&#20351;&#29992;&#26032;&#39062;&#30340;&#25945;&#24072;&#26550;&#26500;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12961</link><description>&lt;p&gt;
&#29992;&#20110;&#24212;&#23545;&#26631;&#31614;&#38169;&#35823;&#30340;&#22686;&#24378;&#20803;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhanced Meta Label Correction for Coping with Label Corruption. (arXiv:2305.12961v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12961
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#22686;&#24378;&#20803;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#65288;EMLC&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#20803;&#23398;&#20064;&#36807;&#31243;&#21644;&#24341;&#20837;&#26356;&#20934;&#30830;&#30340;&#20803;&#26799;&#24230;&#25512;&#23548;&#65292;&#20197;&#21450;&#20351;&#29992;&#26032;&#39062;&#30340;&#25945;&#24072;&#26550;&#26500;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22788;&#29702;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22788;&#29702;&#20102;&#20154;&#20026;&#27880;&#20837;&#22122;&#22768;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#20173;&#28982;&#19981;&#33021;&#20805;&#20998;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#12290;&#38543;&#30528;&#20803;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#36741;&#21161;&#30340;&#24178;&#20928;&#23567;&#25968;&#25454;&#38598;&#26469;&#20803;&#26657;&#27491;&#35757;&#32451;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20803;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#22686;&#24378;&#20803;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#65288;&#31616;&#31216;EMLC&#65289;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#20803;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#24555;&#36895;&#21644;&#26356;&#20934;&#30830;&#30340;&#20803;&#26799;&#24230;&#25512;&#23548;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#30340;&#25945;&#24072;&#26550;&#26500;&#65292;&#37197;&#22791;&#20102;&#26032;&#39062;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;EMLC&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#25152;&#26377;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;EMLC&#25552;&#21319;&#20102;&#23545;&#22122;&#22768;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20808;&#21069;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods for learning with the presence of noisy labels have successfully handled datasets with artificially injected noise but still fall short of adequately handling real-world noise. With the increasing use of meta-learning in the diverse fields of machine learning, researchers leveraged auxiliary small clean datasets to meta-correct the training labels. Nonetheless, existing meta-label correction approaches are not fully exploiting their potential. In this study, we propose an Enhanced Meta Label Correction approach abbreviated as EMLC for the learning with noisy labels (LNL) problem. We re-examine the meta-learning process and introduce faster and more accurate meta-gradient derivations. We propose a novel teacher architecture tailored explicitly to the LNL problem, equipped with novel training objectives. EMLC outperforms prior approaches and achieves state-of-the-art results in all standard benchmarks. Notably, EMLC enhances the previous art on the noisy real-world da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.10664</link><description>&lt;p&gt;
&#26435;&#37325;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26080;&#38480;&#23485;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance. (arXiv:2305.10664v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;Neal&#65288;1996&#65289;&#30340;&#32463;&#20856;&#32780;&#26377;&#24433;&#21709;&#21147;&#30340;&#20316;&#21697;&#24050;&#30693;&#65292;&#20855;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#38480;&#23485;&#24230;&#26631;&#24230;&#26497;&#38480;&#26159;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#65292;&#24403;&#32593;&#32476;&#26435;&#37325;&#20855;&#26377;&#26377;&#30028;&#20808;&#39564;&#26041;&#24046;&#26102;&#12290;Neal&#30340;&#32467;&#26524;&#24050;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#38544;&#34255;&#23618;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#65292;&#20063;&#20855;&#26377;&#39640;&#26031;&#36807;&#31243;&#26631;&#24230;&#26497;&#38480;&#12290;&#39640;&#26031;&#36807;&#31243;&#30340;&#26131;&#22788;&#29702;&#23646;&#24615;&#20801;&#35768;&#30452;&#25509;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#30456;&#27604;&#26377;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#26497;&#38480;&#36807;&#31243;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#22833;&#25928;&#65292;&#25454;&#36866;&#24403;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;$\alpha$&#36807;&#31243;&#30340;&#26631;&#24230;&#26497;&#38480;&#30340;&#25991;&#29486;&#36739;&#22810;&#30340;&#26159;&#21069;&#21521;&#27169;&#25311;&#65292;&#32780;&#22312;&#36825;&#20123;&#26435;&#37325;&#19979;&#30340;&#21518;&#39564;&#25512;&#26029;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#25512;&#26029;&#30340;&#26032;&#29702;&#35770;&#27934;&#23519;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#39564;&#25910;&#32553;&#36895;&#29575;&#32467;&#26524;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, \emph{when the network weights have bounded prior variance}. Neal's result has been extended to networks with multiple hidden layers and to convolutional neural networks, also with Gaussian process scaling limits. The tractable properties of Gaussian processes then allow straightforward posterior inference and uncertainty quantification, considerably simplifying the study of the limit process compared to a network of finite width. Neural network weights with unbounded variance, however, pose unique challenges. In this case, the classical central limit theorem breaks down and it is well known that the scaling limit is an $\alpha$-stable process under suitable conditions. However, current literature is primarily limited to forward simulations under these processes and the problem of posterior inference under s
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.10120</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10120
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#12289;&#35823;&#23548;&#25110;&#19981;&#24403;&#20869;&#23481;&#30340;&#25285;&#24551;&#12290;&#21463;&#27492;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25345;&#32493;&#23398;&#20064;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#20197;&#25351;&#23450;&#35813;&#22914;&#20309;&#36951;&#24536;&#19968;&#20010;&#27010;&#24565;&#12290;&#36873;&#25321;&#24615;&#36951;&#24536;&#21487;&#24212;&#29992;&#20110;&#21464;&#20998;&#20284;&#28982;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;&#19981;&#21516;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35825;&#23548;&#36951;&#24536;&#21508;&#31181;&#27010;&#24565;&#65292;&#20174;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#25972;&#20010;&#31867;&#21035;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21517;&#20154;&#21644;&#35064;&#20307;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;https://github.com/clear-nus/selective-amnesia&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07440</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#20869;&#23384;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Optimizing Memory Mapping Using Deep Reinforcement Learning. (arXiv:2305.07440v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#35843;&#24230;&#21644;&#20998;&#37197;&#26159;&#35768;&#22810;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#28085;&#30422;&#25317;&#22622;&#25511;&#21046;&#21040;&#20113;&#35745;&#31639;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35843;&#24230;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#21363;&#32534;&#35793;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#26399;&#38388;&#20986;&#29616;&#30340;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#65306;&#21363;&#23558;&#24352;&#37327;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#20869;&#23384;&#23618;&#20197;&#20248;&#21270;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#21644;&#39640;&#32500;&#25968;&#25454;&#36755;&#20837;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time.  We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memo
&lt;/p&gt;</description></item><item><title>HINT&#26159;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#22411;&#27169;&#22411;&#26063;&#65292;&#33021;&#22815;&#26377;&#25928;&#12289;&#20934;&#30830;&#22320;&#36827;&#34892;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;Bootstrap&#26041;&#27861;&#24182;&#20026;&#32593;&#32476;&#21152;&#20837;&#35268;&#33539;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#36755;&#20986;&#35268;&#33539;&#21270;&#26469;&#20445;&#35777;&#20854;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#31934;&#24230;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.07089</link><description>&lt;p&gt;
HINT:&#23618;&#27425;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#19968;&#33268;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting. (arXiv:2305.07089v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07089
&lt;/p&gt;
&lt;p&gt;
HINT&#26159;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#22411;&#27169;&#22411;&#26063;&#65292;&#33021;&#22815;&#26377;&#25928;&#12289;&#20934;&#30830;&#22320;&#36827;&#34892;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;Bootstrap&#26041;&#27861;&#24182;&#20026;&#32593;&#32476;&#21152;&#20837;&#35268;&#33539;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#36755;&#20986;&#35268;&#33539;&#21270;&#26469;&#20445;&#35777;&#20854;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#31934;&#24230;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Hierarchical Mixture Networks"&#65288;HINT&#65289;&#30340;&#27169;&#22411;&#26063;&#65292;&#29992;&#20110;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20803;&#28151;&#21512;&#24182;&#20351;&#29992;&#22797;&#21512;&#20284;&#28982;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#26469;&#19987;&#38376;&#38024;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#32593;&#32476;&#29305;&#21270;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;Bootstrap&#26041;&#27861;&#21152;&#20197;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#35268;&#33539;&#21270;&#29305;&#24449;&#25552;&#21462;&#21644;&#36755;&#20986;&#35268;&#33539;&#21270;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#23610;&#24230;&#21464;&#21270;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;8&#65285; sCRPS&#22686;&#24378;&#31934;&#24230;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#37096;&#20214;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#24182;&#24191;&#27867;&#30740;&#31350;&#20102;&#22810;&#20803;&#28151;&#21512;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290; HINT&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/Nixtla/neuralforecast&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Hierarchical Mixture Networks (HINT), a model family for efficient and accurate coherent forecasting. We specialize the networks on the task via a multivariate mixture optimized with composite likelihood and made coherent via bootstrap reconciliation. Additionally, we robustify the networks to stark time series scale variations, incorporating normalized feature extraction and recomposition of output scales within their architecture. We demonstrate 8% sCRPS improved accuracy across five datasets compared to the existing state-of-the-art. We conduct ablation studies on our model's components and extensively investigate the theoretical properties of the multivariate mixture. HINT's code is available at this https://github.com/Nixtla/neuralforecast.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#21051;&#30011;&#21560;&#24341;&#22495;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.06547</link><description>&lt;p&gt;
&#31163;&#25955;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Lyapunov Control for Discrete-Time Systems. (arXiv:2305.06547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#21051;&#30011;&#21560;&#24341;&#22495;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32447;&#24615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#24050;&#32463;&#34987;&#20805;&#20998;&#20102;&#35299;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19968;&#33324;&#26041;&#27861;&#26159;&#21033;&#29992;&#26446;&#38597;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#29702;&#35770;&#26469;&#35745;&#31639;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#20989;&#25968;&#21644;&#30456;&#20851;&#25511;&#21046;&#31574;&#30053;&#30340;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#23547;&#25214;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#36866;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#23398;&#20064;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#30340;&#26041;&#27861;&#12290;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#31532;&#19968;&#20010;&#26159;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#39564;&#35777;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#20013;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#26159;&#35745;&#31639;&#23376;&#27700;&#24179;&#38598;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21051;&#30011;&#20102;&#21560;&#24341;&#22495;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
While ensuring stability for linear systems is well understood, it remains a major challenge for systems with nonlinear dynamics. A general approach in such cases is to leverage Lyapunov stability theory to compute a combination of a Lyapunov control function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been recently proposed that represent Lyapunov functions using neural networks. However, such approaches have been designed exclusively for continuous-time systems. We propose the first approach for learning neural Lyapunov control in discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the stability conditions in discrete-time systems. The second is a novel approach for computing sub-level sets which characterize the region of att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;</title><link>http://arxiv.org/abs/2305.05465</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#21160;&#24577;&#20013;&#30340;&#32858;&#31867;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;Transformer&#35270;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24403;&#26435;&#37325;&#19981;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#34920;token&#30340;&#31890;&#23376;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#32780;&#36235;&#21521;&#20110;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#12290;&#20986;&#29616;&#30340;&#26497;&#38480;&#23545;&#35937;&#31867;&#22411;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#30697;&#38453;&#25910;&#25947;&#20110;&#20302;&#31209;&#24067;&#23572;&#30697;&#38453;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#32452;&#21512;&#22312;&#25968;&#23398;&#19978;&#35777;&#23454;&#20102;Vaswani&#31561;&#20154;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#20250;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13734</link><description>&lt;p&gt;
&#19968;&#20010;LLM&#30693;&#36947;&#33258;&#24049;&#22312;&#25746;&#35854;&#30340;&#20869;&#37096;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#65288;&#21487;&#33021;&#65289;&#26368;&#20026;&#31361;&#20986;&#30340;&#32570;&#28857;&#26159;&#20197;&#33258;&#20449;&#30340;&#35821;&#27668;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#25581;&#31034;&#19968;&#20010;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;LLM&#25152;&#29983;&#25104;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35821;&#21477;&#12290;&#19968;&#20010;&#20998;&#31867;&#22120;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#26681;&#25454;LLM&#30340;&#28608;&#27963;&#20540;&#26469;&#26816;&#27979;&#21738;&#20010;&#35821;&#21477;&#26159;&#30495;&#23454;&#30340;&#25110;&#34394;&#20551;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#22120;&#25509;&#25910;LLM&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#21477;&#29983;&#25104;&#30340;&#28608;&#27963;&#20540;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#26816;&#27979;&#35821;&#21477;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#29978;&#33267;&#27604;&#23569;&#37327;&#25552;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#21487;&#20449;&#24230;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#31283;&#23450;&#21644;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;SwitchBack&#21644;AdamW-Adafacto&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13013</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#23450;&#21644;&#20302;&#31934;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stable and low-precision training for large-scale vision-language models. (arXiv:2304.13013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#31283;&#23450;&#21644;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;SwitchBack&#21644;AdamW-Adafacto&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#31283;&#23450;&#22823;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#20026;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SwitchBack&#65292;&#36825;&#26159;&#19968;&#31181;&#32447;&#24615;&#23618;&#29992;&#20110;int8&#37327;&#21270;&#35757;&#32451;&#65292;&#20854;&#25552;&#20379;&#20102;13-25&#65285;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19982;1B&#21442;&#25968;CLIP ViT-Huge&#30340;bfloat16&#35757;&#32451;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#22312;&#30446;&#21069;&#20026;&#27490;&#26159;&#26368;&#22823;&#30340;int8&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;int8&#65292;&#22240;&#20026;GPU&#25903;&#25345;float8&#24456;&#23569;&#65292;&#34429;&#28982;&#25105;&#20204;&#20063;&#36890;&#36807;&#27169;&#25311;&#20998;&#26512;&#20102;float8&#35757;&#32451;&#12290;&#20026;&#20102;&#31283;&#23450;&#35757;&#32451;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25439;&#22833;&#23792;&#20540;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#20108;&#27425;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;AdamW second moment&#20043;&#21518;1-8&#27425;&#36845;&#20195;&#20013;&#19968;&#33268;&#21457;&#29983;&#20302;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#33616;&#20351;&#29992;AdamW-Adafacto&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) Towards accelerating training, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) Towards stable training, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafacto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#28151;&#21512;&#19987;&#23478;&#65288;GMoE&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20855;&#26377;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#21644;&#21253;&#21547;&#24322;&#26500;&#33410;&#28857;&#21644;&#36793;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#24212;&#22810;&#26679;&#30340;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.02806</link><description>&lt;p&gt;
&#22270;&#28151;&#21512;&#19987;&#23478;&#65306;&#26174;&#24335;&#22810;&#26679;&#24615;&#24314;&#27169;&#19979;&#30340;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling. (arXiv:2304.02806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#28151;&#21512;&#19987;&#23478;&#65288;GMoE&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20855;&#26377;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#21644;&#21253;&#21547;&#24322;&#26500;&#33410;&#28857;&#21644;&#36793;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22686;&#24378;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#24212;&#22810;&#26679;&#30340;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#36890;&#24120;&#20855;&#26377;&#22810;&#26679;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#21253;&#21547;&#24322;&#26500;&#33410;&#28857;&#21644;&#36793;&#12290;&#20026;&#20102;&#22686;&#24378;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#24050;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#31616;&#21333;&#22320;&#22686;&#21152;GNN&#27169;&#22411;&#23481;&#37327;&#23558;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#25512;&#29702;&#25104;&#26412;&#21644;GNN&#38590;&#20197;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#24605;&#24819;&#24341;&#20837;&#21040;GNN&#20013;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#36866;&#24212;&#22810;&#26679;&#30340;&#35757;&#32451;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26032;&#22270;&#28151;&#21512;&#19987;&#23478;&#65288;GMoE&#65289;&#27169;&#22411;&#20351;&#24471;&#22270;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#21487;&#20197;&#21160;&#24577;&#22320;&#36873;&#25321;&#20854;&#33258;&#24049;&#30340;&#26368;&#20339;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been widely applied to learning over graph data. Yet, real-world graphs commonly exhibit diverse graph structures and contain heterogeneous nodes and edges. Moreover, to enhance the generalization ability of GNNs, it has become common practice to further increase the diversity of training graph structures by incorporating graph augmentations and/or performing large-scale pre-training on more graphs. Therefore, it becomes essential for a GNN to simultaneously model diverse graph structures. Yet, naively increasing the GNN model capacity will suffer from both higher inference costs and the notorious trainability issue of GNNs. This paper introduces the Mixture-of-Expert (MoE) idea to GNNs, aiming to enhance their ability to accommodate the diversity of training graph structures, without incurring computational overheads. Our new Graph Mixture of Expert (GMoE) model enables each node in the graph to dynamically select its own optimal \textit{information a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;&#65288;DC Transformer&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35782;&#21035;&#21333;&#19968;&#21644;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#65292;&#32858;&#28966;&#20110;&#20840;&#23616;&#29305;&#24449;&#65292;&#20934;&#30830;&#39044;&#27979;&#32570;&#38519;&#30340;&#25968;&#30446;&#21644;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13827</link><description>&lt;p&gt;
&#39640;&#25928;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#22270;&#26696;&#35782;&#21035;&#20351;&#29992;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact Deformable Convolutional Transformers. (arXiv:2303.13827v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;&#65288;DC Transformer&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35782;&#21035;&#21333;&#19968;&#21644;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#65292;&#32858;&#28966;&#20110;&#20840;&#23616;&#29305;&#24449;&#65292;&#20934;&#30830;&#39044;&#27979;&#32570;&#38519;&#30340;&#25968;&#30446;&#21644;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#22278;&#21046;&#36896;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25968;&#21315;&#20010;&#27493;&#39588;&#12290;&#26230;&#22278;&#32570;&#38519;&#22270;&#26696;&#35782;&#21035;&#65288;DPR&#65289;&#23545;&#20110;&#25214;&#21040;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#26230;&#22278;&#38136;&#36896;&#30340;&#20135;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#21333;&#19968;&#31867;&#22411;DPR&#30456;&#27604;&#65292;&#28151;&#21512;&#31867;&#22411;DPR&#30001;&#20110;&#31354;&#38388;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#32570;&#38519;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23384;&#22312;&#25968;&#30446;&#31561;&#21407;&#22240;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;&#32570;&#38519;&#30340;&#25968;&#30446;&#21644;&#31867;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32039;&#20945;&#22411;&#21487;&#24418;&#21464;&#21367;&#31215;&#21464;&#25442;&#22120;&#65288;DC Transformer&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DC Transformer&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#24418;&#21464;&#20869;&#26680;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#32858;&#28966;&#20110;&#26230;&#22278;&#22270;&#20013;&#23384;&#22312;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31616;&#27905;&#22320;&#27169;&#25311;&#20102;&#26230;&#22278;&#22270;&#21644;&#32570;&#38519;&#20043;&#38388;&#30340;&#20869;&#37096;&#20851;&#31995;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;38&#31181;&#32570;&#38519;&#27169;&#24335;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DC Transformer&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DC Transformer&#22312;&#35782;&#21035;&#21333;&#19968;&#20197;&#21450;&#28151;&#21512;&#22411;&#26230;&#22278;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#24471;&#24322;&#24120;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial to find the root cause of the issue and further improving the yield in the wafer foundry. Mixed-type DPR is much more complicated compared to single-type DPR due to varied spatial features, the uncertainty of defects, and the number of defects present. To accurately predict the number of defects as well as the types of defects, we propose a novel compact deformable convolutional transformer (DC Transformer). Specifically, DC Transformer focuses on the global features present in the wafer map by virtue of learnable deformable kernels and multi-head attention to the global features. The proposed method succinctly models the internal relationship between the wafer maps and the defects. DC Transformer is evaluated on a real dataset containing 38 defect patterns. Experimental results show that DC Transformer performs exceptionally well in recognizing both single 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20174;&#26230;&#22278;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#20854;&#36895;&#24230;&#24555;&#65292;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#65292;&#22312;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11632</link><description>&lt;p&gt;
&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#30340;&#33455;&#29255;&#29305;&#24449;&#25552;&#21462;&#21644;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Embarrassingly Simple Approach for Wafer Feature Extraction and Defect Pattern Recognition. (arXiv:2303.11632v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20174;&#26230;&#22278;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#20854;&#36895;&#24230;&#24555;&#65292;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#65292;&#22312;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#33455;&#29255;&#22270;&#20013;&#30340;&#32570;&#38519;&#27169;&#24335;&#23545;&#20110;&#25214;&#21040;&#28508;&#22312;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#25552;&#39640;&#26230;&#22278;&#30340;&#20135;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#36825;&#20123;&#32570;&#38519;&#27169;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38750;&#24120;&#24222;&#22823;&#65292;&#24182;&#20855;&#26377;&#26174;&#30528;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#23427;&#20204;&#36824;&#38656;&#35201;GPU&#25903;&#25345;&#25165;&#33021;&#26377;&#25928;&#36816;&#20316;&#12290;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#20351;&#36825;&#20123;&#27169;&#22411;&#19981;&#36866;&#21512;&#20110;&#21046;&#36896;&#26230;&#22278;&#26102;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#26230;&#22278;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#26497;&#20854;&#24555;&#36895;&#65292;&#30452;&#35266;&#65292;&#38750;&#21442;&#25968;&#21270;&#19988;&#21487;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#19981;&#38656;&#35201;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25968;&#25454;&#28857;&#30340;&#30456;&#23545;&#24418;&#29366;&#21644;&#20301;&#32622;&#65292;&#22914;&#25105;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#25152;&#25581;&#31034;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying defect patterns in a wafer map during manufacturing is crucial to find the root cause of the underlying issue and provides valuable insights on improving yield in the foundry. Currently used methods use deep neural networks to identify the defects. These methods are generally very huge and have significant inference time. They also require GPU support to efficiently operate. All these issues make these models not fit for on-line prediction in the manufacturing foundry. In this paper, we propose an extremely simple yet effective technique to extract features from wafer images. The proposed method is extremely fast, intuitive, and non-parametric while being explainable. The experiment results show that the proposed pipeline outperforms conventional deep learning models. Our feature extraction requires no training or fine-tuning while preserving the relative shape and location of data points as revealed by our interpretability analysis.
&lt;/p&gt;</description></item><item><title>InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.11435</link><description>&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65306;&#22270;&#20687;&#20462;&#22797;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11435
&lt;/p&gt;
&lt;p&gt;
InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65288;InDI&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#23427;&#36991;&#20813;&#20102;&#25152;&#35859;&#30340;&#8220;&#22343;&#20540;&#22238;&#24402;&#8221;&#25928;&#24212;&#65292;&#24182;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#23454;&#29616;&#65292;&#31867;&#20284;&#20110;&#29983;&#25104;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#20010;&#27424;&#23450;&#38382;&#39064;&#65292;&#22810;&#20010;&#39640;&#36136;&#37327;&#22270;&#20687;&#37117;&#21487;&#33021;&#26159;&#32473;&#23450;&#20302;&#36136;&#37327;&#36755;&#20837;&#30340;&#21487;&#34892;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#21333;&#27493;&#22238;&#24402;&#27169;&#22411;&#30340;&#32467;&#26524;&#36890;&#24120;&#26159;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#32454;&#33410;&#21644;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.10019</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#21450;&#20854;&#22312;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices. (arXiv:2303.10019v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20998;&#20301;&#25968;&#21644;&#21327;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24179;&#28369;&#36807;&#31243;&#20801;&#35768;&#22312;&#32447;&#23398;&#20064;&#12290;&#36890;&#36807;&#32500;&#25968;&#38477;&#20302;&#21644;&#32602;&#20989;&#25968;&#24179;&#28369;&#31561;&#20004;&#31181;&#24179;&#28369;&#26041;&#27861;&#26469;&#23558;&#26631;&#20934;CRPS&#23398;&#20064;&#26694;&#26550;&#25512;&#24191;&#21040;&#22810;&#20803;&#32500;&#24230;&#20013;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#26085;&#21069;&#30005;&#20215;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#65292;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, taking into account dependencies between quantiles and covariates through a smoothing procedure that allows for online learning. Two smoothing methods are discussed: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. The methodology is applied to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26377;&#20851;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.09767</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#30340;&#19968;&#20999;&#65306;&#23545;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (arXiv:2303.09767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26377;&#20851;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#26159;&#25915;&#20987;&#32773;&#26377;&#24847;&#35774;&#35745;&#29992;&#20110;&#28151;&#28102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#20415;&#20854;&#29359;&#38169;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#26679;&#26412;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#29983;&#21629;&#21644;&#23433;&#20840;&#30340;&#39046;&#22495;&#65292;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#39046;&#22495;&#30740;&#31350;&#23545;&#25239;&#25915;&#20987;&#26426;&#21046;&#21644;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#26377;&#20851;&#27169;&#22411;&#20351;&#29992;&#30340;&#25968;&#25454;&#23545;&#20854;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#25991;&#29486;&#12290;&#23427;&#31995;&#32479;&#22320;&#35782;&#21035;&#21644;&#24635;&#32467;&#20102;&#36825;&#20010;&#39046;&#22495;&#20869;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#30693;&#35782;&#30340;&#24046;&#36317;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine-learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews literature that focuses on the effects of data used by a model on the model's adversarial robustness. It systematically identifies and summarizes the state-of-the-art research in this area and further discusses gaps of knowledge and promising future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09447</link><description>&lt;p&gt;
&#20351;&#29992;Prompt-Tuning&#30340;&#21407;&#22411;&#36716;&#21521;&#38024;&#23545;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20316;&#20026;&#31867;&#21035;&#23884;&#20837;&#30340;&#19968;&#31181;&#34920;&#31034;&#65292;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#30340;&#20869;&#23384;&#21344;&#29992;&#25110;&#20943;&#36731;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#23548;&#33268;&#30340;&#24615;&#33021;&#24613;&#21095;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65288;CPP&#65289;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#35843;&#25972;&#65292;&#24403;&#22312;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19978;&#36827;&#34892;&#20248;&#21270;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38556;&#30861;&#24182;&#26174;&#30528;&#25552;&#39640;&#21407;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPP&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;CPP&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#23427;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#19979;&#36830;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#21069;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;/&#25110;&#21518;&#22788;&#29702;&#30340;&#26102;&#38388;&#28857;&#23581;&#35797;&#23558;&#31354;&#38388;&#20449;&#24687;&#32435;&#20837;&#22238;&#24402;&#38543;&#26426;&#26862;&#26519;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#24182;&#20998;&#31867;&#26368;&#26032;&#37319;&#29992;&#30340;&#35843;&#25972;&#22238;&#24402;&#38543;&#26426;&#26862;&#26519;&#20197;&#36866;&#24212;&#31354;&#38388;&#30456;&#20851;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.04693</link><description>&lt;p&gt;
&#22312;&#22238;&#24402;&#38543;&#26426;&#26862;&#26519;&#20013;&#23547;&#25214;&#31354;&#38388;&#20381;&#36182;&#30340;&#36335;&#24452;&#65306;&#20998;&#31867;&#21644;&#31995;&#32479;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A path in regression Random Forest looking for spatial dependence: a taxonomy and a systematic review. (arXiv:2303.04693v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04693
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#21069;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;/&#25110;&#21518;&#22788;&#29702;&#30340;&#26102;&#38388;&#28857;&#23581;&#35797;&#23558;&#31354;&#38388;&#20449;&#24687;&#32435;&#20837;&#22238;&#24402;&#38543;&#26426;&#26862;&#26519;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#24182;&#20998;&#31867;&#26368;&#26032;&#37319;&#29992;&#30340;&#35843;&#25972;&#22238;&#24402;&#38543;&#26426;&#26862;&#26519;&#20197;&#36866;&#24212;&#31354;&#38388;&#30456;&#20851;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#22240;&#20026;&#23427;&#22312;&#24314;&#27169;&#21709;&#24212;&#21464;&#37327;&#21644;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#24378;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#12290;&#22312;&#29615;&#22659;&#24212;&#29992;&#20013;&#65292;&#24120;&#24120;&#20986;&#29616;&#24863;&#20852;&#36259;&#30340;&#29616;&#35937;&#21487;&#33021;&#23384;&#22312;&#31354;&#38388;&#21644;/&#25110;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#36825;&#22312;RF&#30340;&#26631;&#20934;&#29256;&#26412;&#20013;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#23427;&#20204;&#22312;&#20309;&#26102;&#65288;&#21069;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;/&#25110;&#21518;&#22788;&#29702;&#65289;&#23581;&#35797;&#23558;&#31354;&#38388;&#20449;&#24687;&#32435;&#20837;&#22238;&#24402;RF&#20013;&#26469;&#23545;&#31574;&#30053;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#12298;&#31995;&#32479;&#22238;&#39038;&#21644;Meta&#20998;&#26512;&#39318;&#36873;&#25253;&#21578;&#39033;&#30446;&#12299;&#65288;PRISMA&#65289;&#25552;&#20379;&#30340;&#26631;&#20934;&#65292;&#23545;&#26368;&#36817;&#37319;&#29992;&#30340;&#35843;&#25972;&#22238;&#24402;RF&#20197;&#36866;&#24212;&#31354;&#38388;&#30456;&#20851;&#25968;&#25454;&#30340;&#31574;&#30053;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#21644;&#20998;&#31867;&#12290;&#21518;&#32773;&#26159;&#19968;&#31181;&#21487;&#37325;&#22797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25910;&#38598;&#21644;&#22788;&#29702;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#19981;&#21516;&#26469;&#28304;&#30340;&#29616;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Forest (RF) is a well-known data-driven algorithm applied in several fields thanks to its flexibility in modeling the relationship between the response variable and the predictors, also in case of strong non-linearities. In environmental applications, it often occurs that the phenomenon of interest may present spatial and/or temporal dependence that is not taken explicitly into account by RF in its standard version. In this work, we propose a taxonomy to classify strategies according to when (Pre-, In- and/or Post-processing) they try to include the spatial information into regression RF. Moreover, we provide a systematic review and classify the most recent strategies adopted to "adjust" regression RF to spatially dependent data, based on the criteria provided by the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA). The latter consists of a reproducible methodology for collecting and processing existing literature on a specified topic from different so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24102;&#31526;&#21495;&#25490;&#21015;&#34920;&#31034;&#30340;&#23494;&#38598;&#36830;&#25509;$G$-&#19981;&#21464;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;($G$-DNN)&#26550;&#26500;&#65292;&#36890;&#36807;&#32806;&#21512;&#26435;&#37325;&#65292;&#20351;&#24471;&#32593;&#32476;&#30340;&#21069;&#28608;&#27963;&#33021;&#22815;&#36890;&#36807;$G$&#30340;&#24102;&#31526;&#21495;&#25490;&#21015;&#34920;&#31034;&#36827;&#34892;&#21464;&#25442;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#26063;&#26356;&#20016;&#23500;&#30340;$G$-&#19981;&#21464;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.04614</link><description>&lt;p&gt;
&#20855;&#26377;&#24102;&#31526;&#21495;&#25490;&#21015;&#34920;&#31034;&#30340;&#23494;&#38598;&#36830;&#25509;&#30340;$G$-&#19981;&#21464;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Densely Connected $G$-invariant Deep Neural Networks with Signed Permutation Representations. (arXiv:2303.04614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24102;&#31526;&#21495;&#25490;&#21015;&#34920;&#31034;&#30340;&#23494;&#38598;&#36830;&#25509;$G$-&#19981;&#21464;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;($G$-DNN)&#26550;&#26500;&#65292;&#36890;&#36807;&#32806;&#21512;&#26435;&#37325;&#65292;&#20351;&#24471;&#32593;&#32476;&#30340;&#21069;&#28608;&#27963;&#33021;&#22815;&#36890;&#36807;$G$&#30340;&#24102;&#31526;&#21495;&#25490;&#21015;&#34920;&#31034;&#36827;&#34892;&#21464;&#25442;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#26063;&#26356;&#20016;&#23500;&#30340;$G$-&#19981;&#21464;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#23545;&#20110;&#26377;&#38480;&#32676;$G$&#65292;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#23494;&#38598;&#36830;&#25509;$G$-&#19981;&#21464;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;($G$-DNN)&#26550;&#26500;&#12290;&#19982;&#25991;&#29486;&#20013;&#20854;&#20182;$G$-&#19981;&#21464;&#26550;&#26500;&#19981;&#21516;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;$G$-DNN&#30340;&#21069;&#28608;&#27963;&#33021;&#22815;&#36890;&#36807;$G$&#30340;&#24102;&#31526;&#21495;&#25490;&#21015;&#34920;&#31034;(signed perm-reps)&#36827;&#34892;&#21464;&#25442;&#12290;&#27492;&#22806;&#65292;$G$-DNN&#30340;&#21508;&#20010;&#23618;&#19981;&#35201;&#27714;&#26159;$G$-&#31561;&#21464;&#30340;&#65307;&#32780;&#26159;&#36890;&#36807;&#23558;&#36755;&#20837;&#32593;&#32476;&#30340;&#21069;&#28608;&#27963;&#20989;&#25968;&#38480;&#21046;&#20026;$G$-&#31561;&#21464;&#20989;&#25968;&#30340;&#26041;&#24335;&#65292;&#22312;&#25152;&#26377;&#23618;&#20043;&#38388;&#32806;&#21512;&#26435;&#37325;&#12290;&#32467;&#26524;&#26159;&#19968;&#26063;&#26356;&#20016;&#23500;&#30340;$G$-&#19981;&#21464;&#26550;&#26500;&#65292;&#36825;&#22312;&#20197;&#21069;&#20174;&#26410;&#35265;&#36807;&#12290;&#25105;&#20204;&#36890;&#36807;&#26435;&#37325;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25512;&#23548;&#20102;$G$-DNN&#30340;&#39640;&#25928;&#23454;&#29616;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#8220;&#21487;&#25509;&#21463;&#8221;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#8212;&#8212;&#21363;&#38750;&#36864;&#21270;&#19988;&#19982;&#26356;&#23567;&#30340;&#26550;&#26500;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and investigate, for finite groups $G$, $G$-invariant deep neural network ($G$-DNN) architectures with ReLU activation that are densely connected-- i.e., include all possible skip connections. In contrast to other $G$-invariant architectures in the literature, the preactivations of the$G$-DNNs presented here are able to transform by \emph{signed} permutation representations (signed perm-reps) of $G$. Moreover, the individual layers of the $G$-DNNs are not required to be $G$-equivariant; instead, the preactivations are constrained to be $G$-equivariant functions of the network input in a way that couples weights across all layers. The result is a richer family of $G$-invariant architectures never seen previously. We derive an efficient implementation of $G$-DNNs after a reparameterization of weights, as well as necessary and sufficient conditions for an architecture to be ``admissible''-- i.e., nondegenerate and inequivalent to smaller architectures. We include code that al
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;LSTM&#65289;&#21644;&#23454;&#29616;&#27874;&#21160;&#29575;&#27979;&#37327;&#30340;&#26032;&#26694;&#26550;&#33021;&#22815;&#20849;&#21516;&#24314;&#27169;&#25910;&#30410;&#21644;&#23454;&#29616;&#27874;&#21160;&#29575;&#27979;&#37327;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#21644;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#27874;&#21160;&#29575;&#30340;&#23450;&#24335;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2302.08002</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#23454;&#29616;GARCH&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Enhanced Realized GARCH. (arXiv:2302.08002v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08002
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;LSTM&#65289;&#21644;&#23454;&#29616;&#27874;&#21160;&#29575;&#27979;&#37327;&#30340;&#26032;&#26694;&#26550;&#33021;&#22815;&#20849;&#21516;&#24314;&#27169;&#25910;&#30410;&#21644;&#23454;&#29616;&#27874;&#21160;&#29575;&#27979;&#37327;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#21644;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#27874;&#21160;&#29575;&#30340;&#23450;&#24335;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27874;&#21160;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#65288;LSTM&#65289;&#21644;&#23454;&#29616;&#27874;&#21160;&#29575;&#27979;&#37327;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#31181;LSTM&#22686;&#24378;&#30340;&#23454;&#29616;GARCH&#26694;&#26550;&#34701;&#21512;&#20102;&#37329;&#34701;&#35745;&#37327;&#23398;&#12289;&#39640;&#39057;&#20132;&#26131;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24314;&#27169;&#36827;&#23637;&#12290;&#36890;&#36807;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#21644;&#39044;&#27979;&#12290;&#26032;&#30340;&#26694;&#26550;&#33021;&#22815;&#20849;&#21516;&#24314;&#27169;&#25910;&#30410;&#21644;&#23454;&#29616;&#27874;&#21160;&#29575;&#27979;&#37327;&#65292;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#20869;&#26679;&#26412;&#25311;&#21512;&#21644;&#36229;&#36807;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#27874;&#21160;&#29575;&#30340;&#23450;&#24335;&#20107;&#23454;&#12290;&#20351;&#29992;&#24191;&#27867;&#20132;&#26131;&#30340;31&#20010;&#32929;&#31080;&#25351;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;COVID-19&#22823;&#27969;&#34892;&#22312;&#20869;&#30340;&#26102;&#38388;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to volatility modeling by combining deep learning (LSTM) and realized volatility measures. This LSTM-enhanced realized GARCH framework incorporates and distills modeling advances from financial econometrics, high frequency trading data and deep learning. Bayesian inference via the Sequential Monte Carlo method is employed for statistical inference and forecasting. The new framework can jointly model the returns and realized volatility measures, has an excellent in-sample fit and superior predictive performance compared to several benchmark models, while being able to adapt well to the stylized facts in volatility. The performance of the new framework is tested using a wide range of metrics, from marginal likelihood, volatility forecasting, to tail risk forecasting and option pricing. We report on a comprehensive empirical study using 31 widely traded stock indices over a time period that includes COVID-19 pandemic.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#20165;&#20855;&#26377;&#23545;&#27491;&#24120;&#26680;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#26102;&#65292;&#33719;&#24471;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;sa&#65288;s-&#65289;&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#24050;&#30693;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|&#65289;&#65288;&#21709;&#24212;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|^2&#65289;&#65289;&#65292;&#23545;&#20110;&#29305;&#23450;&#31639;&#27861;&#21644;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#12289;KL&#25110;&#21345;&#26041;&#25955;&#24230;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.05372</link><description>&lt;p&gt;
&#36808;&#21521;&#27169;&#22411;&#22522;&#30784;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Minimax Optimality of Model-based Robust Reinforcement Learning. (arXiv:2302.05372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#20165;&#20855;&#26377;&#23545;&#27491;&#24120;&#26680;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#26102;&#65292;&#33719;&#24471;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;sa&#65288;s-&#65289;&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#24050;&#30693;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|&#65289;&#65288;&#21709;&#24212;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|^2&#65289;&#65289;&#65292;&#23545;&#20110;&#29305;&#23450;&#31639;&#27861;&#21644;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#12289;KL&#25110;&#21345;&#26041;&#25955;&#24230;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21482;&#26377;&#23545;&#27491;&#24120;&#26680;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#26102;&#65292;&#33719;&#24471;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#37319;&#26679;&#22797;&#26434;&#24230;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38750;&#40065;&#26834;&#24773;&#20917;&#19979;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#19988;&#24050;&#30693;&#20219;&#20309;&#24212;&#29992;&#20110;&#32463;&#39564;MDP&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#29992;&#949;^2/&#65288;H^3 * |S| * |A|&#65289;&#20010;&#26679;&#26412;&#26469;&#20272;&#35745;&#65292;&#22343;&#21487;&#25552;&#20379;&#949;-&#26368;&#20248;&#31574;&#30053;&#65292;&#20174;&#32780;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#12290;&#40065;&#26834;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#26356;&#21152;&#23569;&#35265;&#12290;&#23545;&#20110;sa&#65288;s-&#65289;&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#24050;&#30693;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|&#65289;&#65288;&#21709;&#24212;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|^2&#65289;&#65289;&#65292;&#23545;&#20110;&#29305;&#23450;&#31639;&#27861;&#21644;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#12289;KL&#25110;&#21345;&#26041;&#25955;&#24230;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#29992;Lp&#29699;&#23450;&#20041;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#65288;&#22238;&#22797;&#21040;TV&#24773;&#20917;&#65289;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of obtaining an $\epsilon$-optimal policy in \emph{Robust} discounted Markov Decision Processes (RMDPs), given only access to a generative model of the nominal kernel. This problem is widely studied in the non-robust case, and it is known that any planning approach applied to an empirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid}{\epsilon^2})$ samples provides an $\epsilon$-optimal policy, which is minimax optimal. Results in the robust case are much more scarce. For $sa$(resp $s$-)rectangular uncertainty sets, the best known sample complexity is $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid}{\epsilon^2})$ (resp. $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid^2}{\epsilon^2})$), for specific algorithms and when the uncertainty set is based on the total variation (TV), the KL or the Chi-square divergences. In this paper, we consider uncertainty sets defined with an $L_p$-ball (recovering the TV case), and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#26469;&#21435;&#38500;&#21253;&#21547;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.05290</link><description>&lt;p&gt;
&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;&#32467;&#26500;&#24615;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Removing Structured Noise with Diffusion Models. (arXiv:2302.05290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#26469;&#21435;&#38500;&#21253;&#21547;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#36866;&#23450;&#21453;&#38382;&#39064;&#38656;&#35201;&#20180;&#32454;&#21046;&#23450;&#26377;&#20851;&#24863;&#20852;&#36259;&#20449;&#21495;&#30340;&#20808;&#39564;&#20449;&#24565;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#26377;&#22122;&#22768;&#27979;&#37327;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#25163;&#24037;&#21046;&#23450;&#20449;&#21495;&#20808;&#39564;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25152;&#21462;&#20195;&#65292;&#24182;&#19988;&#20960;&#20010;&#22242;&#38431;&#26368;&#36817;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#33539;&#24335;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20016;&#23500;&#12289;&#32467;&#26500;&#21270;&#30340;&#22122;&#22768;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#26465;&#20214;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#21547;&#22122;&#22768;&#21644;&#20449;&#21495;&#29983;&#25104;&#20998;&#24067;&#30340;&#23398;&#20064;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23384;&#22312;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#21453;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20248;&#20110;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#21644;&#23545;&#25239;&#32593;&#32476;&#30340;&#31454;&#20105;&#22522;&#32447;&#12290;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#24320;&#36767;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#26356;&#20934;&#30830;&#24314;&#27169;&#30340;&#26032;&#26426;&#20250;&#21644;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving ill-posed inverse problems requires careful formulation of prior beliefs over the signals of interest and an accurate description of their manifestation into noisy measurements. Handcrafted signal priors based on e.g. sparsity are increasingly replaced by data-driven deep generative models, and several groups have recently shown that state-of-the-art score-based diffusion models yield particularly strong performance and flexibility. In this paper, we show that the powerful paradigm of posterior sampling with diffusion models can be extended to include rich, structured, noise models. To that end, we propose a joint conditional reverse diffusion process with learned scores for the noise and signal-generating distribution. We demonstrate strong performance gains across various inverse problems with structured noise, outperforming competitive baselines that use normalizing flows and adversarial networks. This opens up new opportunities and relevant practical applications of diffusi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UniPC&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;(DPMs)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#32479;&#19968;&#20462;&#27491;&#22120;(UniC)&#21644;&#32479;&#19968;&#39044;&#27979;&#22120;(UniP)&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23569;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2302.04867</link><description>&lt;p&gt;
UniPC&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models. (arXiv:2302.04867v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UniPC&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;(DPMs)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#32479;&#19968;&#20462;&#27491;&#22120;(UniC)&#21644;&#32479;&#19968;&#39044;&#27979;&#22120;(UniP)&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23569;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#21435;&#22122;&#32593;&#32476;&#30340;&#22810;&#27425;&#35780;&#20272;&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;DPM&#20013;&#36827;&#34892;&#37319;&#26679;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#27492;&#21152;&#36895;DPM&#37319;&#26679;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#35774;&#35745;&#20986;&#20102;&#24555;&#36895;&#37319;&#26679;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#26080;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#24212;&#29992;&#26356;&#38738;&#30544;&#20110;&#36739;&#23569;&#30340;&#27493;&#39588;(&#22914;&lt;10)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32479;&#19968;&#20462;&#27491;&#22120;(UniC)&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#20309;&#29616;&#26377;&#30340;DPM&#37319;&#26679;&#22120;&#20043;&#21518;&#24212;&#29992;&#65292;&#25552;&#39640;&#31934;&#30830;&#24230;&#30340;&#38454;&#25968;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#20010;&#25903;&#25345;&#20219;&#24847;&#38454;&#25968;&#30340;&#32479;&#19968;&#39044;&#27979;&#22120;(UniP)&#20316;&#20026;&#21103;&#20135;&#21697;&#12290;&#36890;&#36807;&#32467;&#21512;UniP&#21644;UniC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UniPC&#30340;&#32479;&#19968;&#39044;&#27979;-&#20462;&#27491;&#26694;&#26550;&#65292;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;DPMs&#65292;&#23427;&#20855;&#26377;&#20219;&#24847;&#38454;&#25968;&#30340;&#32479;&#19968;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37319;&#26679;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM is time-consuming due to the multiple evaluations of the denoising network, making it more and more important to accelerate the sampling of DPMs. Despite recent progress in designing fast samplers, existing methods still cannot generate satisfying images in many applications where fewer steps (e.g., $&lt;$10) are favored. In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods, especially in extremel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39057;&#32321;&#26041;&#21521;&#33609;&#31295;&#26469;&#38477;&#20302;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03764</link><description>&lt;p&gt;
Sketchy: &#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#19982;&#39057;&#32321;&#26041;&#21521;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions. (arXiv:2302.03764v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39057;&#32321;&#26041;&#21521;&#33609;&#31295;&#26469;&#38477;&#20302;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;Kronecker&#22240;&#23376;&#26799;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#32858;&#28966;&#22312;&#19968;&#20010;&#21464;&#21270;&#30340;&#23567;&#30340;&#20027;&#29305;&#24449;&#31354;&#38388;&#19978;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#37319;&#29992;&#20302;&#31209;&#30340;&#33609;&#31295;&#26041;&#27861;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#29992;&#39057;&#32321;&#26041;&#21521;&#65288;FD&#65289;&#33609;&#31295;&#26469;&#20943;&#23569;&#32500;&#25252;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#24212;&#29992;FD&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#36164;&#28304;&#38656;&#27714;&#21644;&#36951;&#25022;&#20445;&#35777;&#30340;&#36864;&#21270;&#20043;&#38388;&#36827;&#34892;&#39640;&#25928;&#25554;&#20540;: &#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20165;$dk$&#30340;&#20869;&#23384;&#19982;&#23436;&#25972;&#30697;&#38453;$d^2$&#30340;&#20869;&#23384;&#36951;&#25022;&#21305;&#37197;&#65292;&#30452;&#21040;&#22312;&#24213;&#37096;$d-k$&#30340;&#29305;&#24449;&#20540;&#19978;&#28155;&#21152;&#35823;&#24046;&#20026;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive regularization methods that exploit more than the diagonal entries exhibit state of the art performance for many tasks, but can be prohibitive in terms of memory and running time. We find the spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace that changes throughout training, motivating a low-rank sketching approach. We describe a generic method for reducing memory and compute requirements of maintaining a matrix preconditioner using the Frequent Directions (FD) sketch. While previous approaches have explored applying FD for second-order optimization, we present a novel analysis which allows efficient interpolation between resource requirements and the degradation in regret guarantees with rank $k$: in the online convex optimization (OCO) setting over dimension $d$, we match full-matrix $d^2$ memory regret using only $dk$ memory up to additive error in the bottom $d-k$ eigenvalues of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#28382;&#21518;&#21644;&#21363;&#26102;/&#21516;&#26102;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2302.03246</link><description>&lt;p&gt;
CDANs: &#26469;&#33258;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data. (arXiv:2302.03246v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#28382;&#21518;&#21644;&#21363;&#26102;/&#21516;&#26102;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#35768;&#22810;&#26041;&#38754;&#20013;&#34987;&#21457;&#29616;&#65292;&#22914;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#12289;&#29983;&#21629;&#20307;&#24449;&#27979;&#37327;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#12290;&#22240;&#26524;&#21457;&#29616;&#28041;&#21450;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#23545;&#20110;&#25552;&#21462;&#26377;&#20851;&#20154;&#31867;&#20581;&#24247;&#30340;&#34892;&#21160;&#27934;&#23519;&#21147;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#22411;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CDANs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#22914;&#39640;&#32500;&#24230;&#12289;&#26080;&#27861;&#35782;&#21035;&#28382;&#21518;&#22240;&#26524;&#20851;&#31995;&#21644;&#24573;&#35270;&#21464;&#21270;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35782;&#21035;&#20986;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#28382;&#21518;&#21644;&#21363;&#26102;/&#21516;&#26102;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#21464;&#21270;&#30340;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#28382;&#21518;&#29238;&#33410;&#28857;&#26469;&#20248;&#21270;&#32422;&#26463;&#25628;&#32034;&#20013;&#30340;&#26465;&#20214;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series data are found in many areas of healthcare such as medical time series, electronic health records (EHR), measurements of vitals, and wearable devices. Causal discovery, which involves estimating causal relationships from observational data, holds the potential to play a significant role in extracting actionable insights about human health. In this study, we present a novel constraint-based causal discovery approach for autocorrelated and non-stationary time series data (CDANs). Our proposed method addresses several limitations of existing causal discovery methods for autocorrelated and non-stationary time series data, such as high dimensionality, the inability to identify lagged causal relationships, and overlooking changing modules. Our approach identifies lagged and instantaneous/contemporaneous causal relationships along with changing modules that vary over time. The method optimizes the conditioning sets in a constraint-based search by considering lagged parents instead
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22522;&#20934;MuG&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;&#20843;&#20010;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#28216;&#25103;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#27169;&#24577;&#20381;&#36182;&#24615;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02978</link><description>&lt;p&gt;
MuG: &#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22522;&#20934;&#65292;&#29992;&#20110;&#24102;&#26377;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#23383;&#27573;&#30340;&#28216;&#25103;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields. (arXiv:2302.02978v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22522;&#20934;MuG&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;&#20843;&#20010;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#28216;&#25103;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#27169;&#24577;&#20381;&#36182;&#24615;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#25972;&#21512;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#20248;&#21183;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#27169;&#24577;&#25968;&#25454;&#65292;&#24341;&#21457;&#20102;&#35768;&#22810;&#26032;&#30340;&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22522;&#20934;MuG&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20843;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#35753;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#21644;&#25913;&#36827;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#28216;&#25103;&#65292;&#28085;&#30422;&#20102;&#34920;&#26684;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#22522;&#20934;&#30340;&#27934;&#35265;&#65292;&#21253;&#25324;&#26631;&#31614;&#24179;&#34913;&#27604;&#12289;&#32570;&#22833;&#29305;&#24449;&#30340;&#30334;&#20998;&#27604;&#12289;&#27599;&#20010;&#27169;&#24577;&#20013;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#26631;&#31614;&#21644;&#36755;&#20837;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#21333;&#19968;&#27169;&#24577;&#20998;&#31867;&#22120;&#21644;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#26174;&#31034;&#20102;&#22522;&#20934;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#27169;&#24577;&#20381;&#36182;&#24615;&#30340;&#29305;&#28857;&#12290;MuG&#24050;&#32463;&#22312;https://github.com/lujiaying/MUG-Bench&#19978;&#21457;&#24067;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#25454;&#12289;&#25945;&#31243;&#21644;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MuG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights into the benchmark, including label balance ratios, percentages of missing features, distributions of data within each modality, and the correlations between labels and input modalities. We further present experimental results obtained by several state-of-the-art unimodal classifiers and multimodal classifiers, which demonstrate the challenging and multimodal-dependent properties of the benchmark. MuG is released at https://github.com/lujiaying/MUG-Bench with the data, tutorials, and implemented
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01068</link><description>&lt;p&gt;
Fed-GLOSS-DP: &#21033;&#29992;&#20855;&#26377;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#38598;&#36827;&#34892;&#32852;&#37030;&#20840;&#23616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy. (arXiv:2302.01068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fed-GLOSS-DP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#32447;&#24615;&#36880;&#28857;&#26799;&#24230;&#20998;&#20139;&#26041;&#26696;&#65288;&#22914;FedAvg&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#21033;&#29992;&#20174;&#23458;&#25143;&#31471;&#25509;&#25910;&#21040;&#30340;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20102;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#25439;&#22833;&#26367;&#20195;&#29289;&#65292;&#36890;&#36807;&#27169;&#25311;&#26412;&#22320;&#21306;&#22495;&#20869;&#30495;&#23454;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#26469;&#36817;&#20284;&#26412;&#22320;&#25439;&#22833;&#22320;&#24418;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34913;&#37327;&#26377;&#25928;&#36924;&#36817;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#21453;&#26144;&#20102;&#36817;&#20284;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#24674;&#22797;&#20840;&#23616;&#25439;&#22833;&#22320;&#24418;&#24182;&#20840;&#38754;&#20248;&#21270;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#21463;&#26085;&#30410;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26080;&#32541;&#37197;&#21512;&#65292;&#20026;&#23458;&#25143;&#31471;&#19978;&#30340;&#27599;&#20010;&#25968;&#25454;&#35760;&#24405;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#20855;&#26377;&#39640;&#24230;&#20542;&#26012;&#20998;&#24067;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes Fed-GLOSS-DP, a novel privacy-preserving approach for federated learning. Unlike previous linear point-wise gradient-sharing schemes, such as FedAvg, our formulation enables a type of global optimization by leveraging synthetic samples received from clients. These synthetic samples, serving as loss surrogates, approximate local loss landscapes by simulating the utility of real images within a local region. We additionally introduce an approach to measure effective approximation regions reflecting the quality of the approximation. Therefore, the server can recover the global loss landscape and comprehensively optimize the model. Moreover, motivated by the emerging privacy concerns, we demonstrate that our approach seamlessly works with record-level differential privacy (DP), granting theoretical privacy guarantees for every data record on the clients. Extensive results validate the efficacy of our formulation on various datasets with highly skewed distributions. Our m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#26029;&#26102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04900</link><description>&lt;p&gt;
&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics. (arXiv:2301.04900v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34892;&#20026;&#33391;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#26029;&#26102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#36817;&#20284;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#32570;&#20047;&#26126;&#30830;&#21407;&#29702;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#19968;&#31867;&#30001;&#32593;&#32476;&#37051;&#25509;&#30697;&#38453;&#32806;&#21512;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#25551;&#36848;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#37329;&#34701;&#12289;&#31038;&#20132;&#21644;&#31070;&#32463;&#31995;&#32479;&#65292;&#23646;&#20110;&#36825;&#31867;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#36825;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21253;&#25324;&#24517;&#35201;&#30340;&#20559;&#32622;&#21644;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#24378;&#35843;&#19982;&#38745;&#24577;&#30417;&#30563;&#23398;&#20064;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#25552;&#20513;&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#32463;&#20856;&#20551;&#35774;&#20043;&#22806;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#25512;&#26029;&#26102;&#20272;&#35745;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#29992;&#30340;&#31354;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#21508;&#31181;&#22797;&#26434;&#32593;&#32476;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approximations of ordinary differential equations offer a promising alternative to classical methods in discovering a dynamical system model, particularly in complex systems lacking explicit first principles. This paper focuses on a complex system whose dynamics is described with a system of ordinary differential equations, coupled via a network adjacency matrix. Numerous real-world systems, including financial, social, and neural systems, belong to this class of dynamical models. We propose essential elements for approximating such dynamical systems using neural networks, including necessary biases and an appropriate neural architecture. Emphasizing the differences from static supervised learning, we advocate for evaluating generalization beyond classical assumptions of statistical learning theory. To estimate confidence in prediction during inference time, we introduce a dedicated null model. By studying various complex network dynamics, we demonstrate the neural network'
&lt;/p&gt;</description></item><item><title>SMACv2&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#38480;&#21046;&#65292;&#25361;&#25112;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.07489</link><description>&lt;p&gt;
SMACv2&#65306;&#19968;&#31181;&#25913;&#36827;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2212.07489v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07489
&lt;/p&gt;
&lt;p&gt;
SMACv2&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#38480;&#21046;&#65292;&#25361;&#25112;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26143;&#38469;&#20105;&#38712;&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#65288;SMAC&#65289;&#24050;&#25104;&#20026;&#38598;&#20013;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#21463;&#27426;&#36814;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#24180;&#30340;&#25345;&#32493;&#25913;&#36827;&#21518;&#65292;&#31639;&#27861;&#29616;&#22312;&#24050;&#32463;&#23454;&#29616;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;SMAC&#32570;&#20047;&#38656;&#35201;&#22797;&#26434;&#30340;&#8220;&#38381;&#29615;&#8221;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21482;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#30340;&#8220;&#24320;&#29615;&#8221;&#31574;&#30053;&#21487;&#20197;&#22312;&#35768;&#22810;SMAC&#22330;&#26223;&#20013;&#23454;&#29616;&#38750;&#24179;&#20961;&#30340;&#32988;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SMACv2&#65292;&#36825;&#26159;&#22522;&#20934;&#30340;&#19968;&#20010;&#26032;&#29256;&#26412;&#65292;&#20854;&#20013;&#30340;&#22330;&#26223;&#26159;&#31243;&#24207;&#29983;&#25104;&#30340;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#35201;&#27714;&#26234;&#33021;&#20307;&#23545;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#35774;&#32622;&#65288;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#65289;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#25193;&#23637;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#25361;&#25112;&#65288;EPO&#65289;&#65292;&#23427;&#21487;&#20197;&#27979;&#35797;&#26234;&#33021;&#20307;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24577;&#19979;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which au
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#65288;MOPoL&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#20248;&#20915;&#31574;&#26641;&#21644;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.06312</link><description>&lt;p&gt;
&#23545;&#22810;&#20010;&#24863;&#20852;&#36259;&#32467;&#26524;&#30340;&#31574;&#30053;&#23398;&#20064;&#65306;&#23558;&#26368;&#20248;&#31574;&#30053;&#26641;&#19982;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Policy learning for many outcomes of interest: Combining optimal policy trees with multi-objective Bayesian optimisation. (arXiv:2212.06312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#65288;MOPoL&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#20248;&#20915;&#31574;&#26641;&#21644;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#21019;&#24314;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#20197;&#22312;&#19981;&#21516;&#25919;&#31574;&#24178;&#39044;&#20998;&#37197;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#30340;&#25919;&#31574;&#21046;&#23450;&#29615;&#22659;&#20013;&#65292;&#20915;&#31574;&#32773;&#36890;&#24120;&#20851;&#24515;&#19981;&#21516;&#32467;&#26524;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#32431;&#22320;&#26368;&#22823;&#21270;&#19968;&#20010;&#32467;&#26524;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#22810;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#65288;MOPoL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#31574;&#30053;&#23398;&#20064;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#19982;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#25506;&#32034;&#22810;&#20010;&#32467;&#26524;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23427;&#36890;&#36807;&#26500;&#24314;&#38750;&#25903;&#37197;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#25511;&#21046;&#30528;&#32467;&#26524;&#30340;&#26435;&#37325;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#36138;&#24515;&#26641;&#21487;&#20197;&#20316;&#20026;&#38750;&#24120;&#35745;&#31639;&#26114;&#36149;&#30340;&#26368;&#20248;&#26641;&#30340;&#20934;&#30830;&#20195;&#29702;&#65292;&#29992;&#20110;&#20915;&#31574;&#30446;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#21453;&#22797;&#25311;&#21512;&#27169;&#22411;&#26469;&#23398;&#20064;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Methods for learning optimal policies use causal machine learning models to create human-interpretable rules for making choices around the allocation of different policy interventions. However, in realistic policy-making contexts, decision-makers often care about trade-offs between outcomes, not just single-mindedly maximising utility for one outcome. This paper proposes an approach termed Multi-Objective Policy Learning (MOPoL) which combines optimal decision trees for policy learning with a multi-objective Bayesian optimisation approach to explore the trade-off between multiple outcomes. It does this by building a Pareto frontier of non-dominated models for different hyperparameter settings which govern outcome weighting. The key here is that a low-cost greedy tree can be an accurate proxy for the very computationally costly optimal tree for the purposes of making decisions which means models can be repeatedly fit to learn a Pareto frontier. The method is applied to a real-world case
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#30340;&#32467;&#26500;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#21457;&#29616;&#36880;&#32500;&#26799;&#24230;&#36890;&#24120;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#65292;&#32780;&#36880;&#27425;&#36845;&#20195;&#30340;&#26799;&#24230;&#21644;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#36890;&#24120;&#19981;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#12290;</title><link>http://arxiv.org/abs/2212.02083</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#34987;&#24573;&#35270;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
On the Overlooked Structure of Stochastic Gradients. (arXiv:2212.02083v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#30340;&#32467;&#26500;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#21457;&#29616;&#36880;&#32500;&#26799;&#24230;&#36890;&#24120;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#65292;&#32780;&#36880;&#27425;&#36845;&#20195;&#30340;&#26799;&#24230;&#21644;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#36890;&#24120;&#19981;&#21576;&#29616;&#24130;&#24459;&#37325;&#23614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#23494;&#20999;&#30456;&#20851;&#12290;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#26799;&#24230;&#22122;&#22768;&#30340;&#37325;&#23614;&#24615;&#36136;&#26469;&#35299;&#37322;&#38543;&#26426;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#21017;&#25552;&#20986;&#20102;&#23545;&#26799;&#24230;&#22122;&#22768;&#30340;&#37325;&#23614;&#20551;&#35774;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#38543;&#26426;&#26799;&#24230;&#32467;&#26500;&#21644;&#37325;&#23614;&#30340;&#27491;&#24335;&#32479;&#35745;&#26816;&#39564;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20570;&#20986;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#38543;&#26426;&#26799;&#24230;&#21644;&#26799;&#24230;&#22122;&#22768;&#22312;&#21442;&#25968;&#21644;&#36845;&#20195;&#20013;&#30340;&#20998;&#24067;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#35745;&#26816;&#39564;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#26816;&#39564;&#21457;&#29616;&#65292;&#36880;&#32500;&#26799;&#24230;&#36890;&#24120;&#34920;&#29616;&#20986;&#24130;&#24459;&#37325;&#23614;&#65292;&#32780;&#36880;&#27425;&#36845;&#20195;&#30340;&#26799;&#24230;&#21644;&#30001;&#23567;&#25209;&#37327;&#35757;&#32451;&#24341;&#36215;&#30340;&#38543;&#26426;&#26799;&#24230;&#22122;&#22768;&#36890;&#24120;&#19981;&#34920;&#29616;&#20986;&#24130;&#24459;&#37325;&#23614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#21327;&#26041;&#24046;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradients closely relate to both optimization and generalization of deep neural networks (DNNs). Some works attempted to explain the success of stochastic optimization for deep learning by the arguably heavy-tail properties of gradient noise, while other works presented theoretical and empirical evidence against the heavy-tail hypothesis on gradient noise. Unfortunately, formal statistical tests for analyzing the structure and heavy tails of stochastic gradients in deep learning are still under-explored. In this paper, we mainly make two contributions. First, we conduct formal statistical tests on the distribution of stochastic gradients and gradient noise across both parameters and iterations. Our statistical tests reveal that dimension-wise gradients usually exhibit power-law heavy tails, while iteration-wise gradients and stochastic gradient noise caused by minibatch training usually do not exhibit power-law heavy tails. Second, we further discover that the covariance spe
&lt;/p&gt;</description></item><item><title>Photo Rater&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24110;&#21161;&#25668;&#24433;&#24072;&#36873;&#25321;&#26368;&#20339;&#29031;&#29255;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39033;&#30446;&#12290;&#23427;&#36890;&#36807;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12289;&#27169;&#31946;&#20998;&#31867;&#21644;&#23457;&#32654;&#35780;&#20272;&#65292;&#24182;&#26681;&#25454;&#24471;&#20998;&#23545;&#22270;&#20687;&#36827;&#34892;&#25490;&#24207;&#21644;&#21576;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.14420</link><description>&lt;p&gt;
Photo Rater: &#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#36873;&#25321;&#25668;&#24433;&#29031;&#29255;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Photo Rater: Photographs Auto-Selector with Deep Learning. (arXiv:2211.14420v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14420
&lt;/p&gt;
&lt;p&gt;
Photo Rater&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24110;&#21161;&#25668;&#24433;&#24072;&#36873;&#25321;&#26368;&#20339;&#29031;&#29255;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39033;&#30446;&#12290;&#23427;&#36890;&#36807;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12289;&#27169;&#31946;&#20998;&#31867;&#21644;&#23457;&#32654;&#35780;&#20272;&#65292;&#24182;&#26681;&#25454;&#24471;&#20998;&#23545;&#22270;&#20687;&#36827;&#34892;&#25490;&#24207;&#21644;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Photo Rater&#26159;&#19968;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#39033;&#30446;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24110;&#21161;&#25668;&#24433;&#24072;&#22312;&#25293;&#25668;&#21516;&#19968;&#22330;&#26223;&#30340;&#29031;&#29255;&#20013;&#36873;&#25321;&#26368;&#20339;&#29031;&#29255;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#31579;&#36873;&#8221;&#22312;&#25668;&#24433;&#20013;&#65292;&#22914;&#26524;&#25163;&#21160;&#23436;&#25104;&#20250;&#24456;&#32321;&#29712;&#21644;&#32791;&#26102;&#12290;Photo Rater&#21033;&#29992;&#19977;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#23436;&#25104;&#36825;&#26679;&#30340;&#20219;&#21153;&#65306;&#19968;&#20010;&#29992;&#20110;&#19968;&#33324;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65292;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#29031;&#29255;&#26159;&#21542;&#27169;&#31946;&#65288;&#22240;&#20026;&#25163;&#25238;&#25110;&#32773;&#32858;&#28966;&#19981;&#20934;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25972;&#20307;&#23457;&#32654;&#65288;&#21253;&#25324;&#29031;&#29255;&#26500;&#22270;&#31561;&#65289;&#12290;&#22312;&#36890;&#36807;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#20687;&#21518;&#65292;Photo Rater&#20026;&#27599;&#20010;&#22270;&#20687;&#36755;&#20986;&#19968;&#20010;&#26368;&#32456;&#24471;&#20998;&#65292;&#26681;&#25454;&#24471;&#20998;&#25490;&#21517;&#24182;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photo Rater is a computer vision project that uses neural networks to help photographers select the best photo among those that are taken based on the same scene. This process is usually referred to as "culling" in photography, and it can be tedious and time-consuming if done manually. Photo Rater utilizes three separate neural networks to complete such a task: one for general image quality assessment, one for classifying whether the photo is blurry (either due to unsteady hands or out-of-focusness), and one for assessing general aesthetics (including the composition of the photo, among others). After feeding the image through each neural network, Photo Rater outputs a final score for each image, ranking them based on this score and presenting it to the user.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;OTA&#21322;&#20998;&#25955;&#32676;&#38598;&#26080;&#32447;FL&#65288;CWFL&#65289;&#21644;CWFL-Prox&#31639;&#27861;&#65292;&#23427;&#22312;&#36890;&#20449;&#25928;&#29575;&#19978;&#20248;&#20110;&#20998;&#25955;FL&#31574;&#30053;&#65292;&#21516;&#26102;&#21442;&#25968;&#26356;&#26032;&#25910;&#25947;&#21040;&#20840;&#23616;&#26497;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2211.03363</link><description>&lt;p&gt;
&#26080;&#32447;&#32676;&#38598;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31354;&#20013;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Over-The-Air Clustered Wireless Federated Learning. (arXiv:2211.03363v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;OTA&#21322;&#20998;&#25955;&#32676;&#38598;&#26080;&#32447;FL&#65288;CWFL&#65289;&#21644;CWFL-Prox&#31639;&#27861;&#65292;&#23427;&#22312;&#36890;&#20449;&#25928;&#29575;&#19978;&#20248;&#20110;&#20998;&#25955;FL&#31574;&#30053;&#65292;&#21516;&#26102;&#21442;&#25968;&#26356;&#26032;&#25910;&#25947;&#21040;&#20840;&#23616;&#26497;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#24102;&#23485;&#38480;&#21046;&#23548;&#33268;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#21363;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#35757;&#32451;&#12290;&#20351;&#29992;&#24102;&#23485;&#21463;&#38480;&#30340;&#19978;&#34892;&#26080;&#32447;&#20449;&#36947;&#26102;&#65292;&#31354;&#20013;&#65288;OTA&#65289;FL&#26356;&#21463;&#38738;&#30544;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#21487;&#20197;&#21516;&#26102;&#21521;&#26381;&#21153;&#22120;&#20256;&#36755;&#21442;&#25968;&#26356;&#26032;&#12290;&#30001;&#20110;&#24310;&#36831;&#22686;&#21152;&#21644;&#26381;&#21153;&#22120;&#25925;&#38556;&#65292;&#21487;&#33021;&#26080;&#27861;&#20351;&#29992;&#24378;&#22823;&#30340;&#26381;&#21153;&#22120;&#36827;&#34892;&#21442;&#25968;&#32858;&#21512;&#12290;&#22312;&#27809;&#26377;&#24378;&#22823;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#65292;&#21363;&#23458;&#25143;&#31471;&#19982;&#20854;&#37051;&#23621;&#36890;&#20449;&#65292;&#20197;&#33719;&#24471;&#20849;&#35782;ML&#27169;&#22411;&#65292;&#20294;&#36890;&#20449;&#25104;&#26412;&#24040;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OTA&#21322;&#20998;&#25955;&#32676;&#38598;&#26080;&#32447;FL&#65288;CWFL&#65289;&#21644;CWFL-Prox&#31639;&#27861;&#65292;&#19982;&#20998;&#25955;FL&#31574;&#30053;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#21516;&#26102;&#21442;&#25968;&#26356;&#26032;&#25910;&#25947;&#20110;&#20840;&#23616;&#26497;&#23567;&#20540;&#65292;&#27599;&#20010;&#32858;&#31867;&#30340;&#25910;&#25947;&#36895;&#24230;&#20026;O&#65288;1/T&#65289;&#12290;&#20351;&#29992;MNIST&#21644;CIFAR10&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Privacy and bandwidth constraints have led to the use of federated learning (FL) in wireless systems, where training a machine learning (ML) model is accomplished collaboratively without sharing raw data. While using bandwidth-constrained uplink wireless channels, over-the-air (OTA) FL is preferred since the clients can transmit parameter updates simultaneously to a server. A powerful server may not be available for parameter aggregation due to increased latency and server failures. In the absence of a powerful server, decentralised strategy is employed where clients communicate with their neighbors to obtain a consensus ML model while incurring huge communication cost. In this work, we propose the OTA semi-decentralised clustered wireless FL (CWFL) and CWFL-Prox algorithms, which is communication efficient as compared to the decentralised FL strategy, while the parameter updates converge to global minima as O(1/T) for each cluster. Using the MNIST and CIFAR10 datasets, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LSTM-based&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#24494;&#35843;&#25552;&#39640;&#20102;&#29992;&#25143;&#29305;&#23450;&#26410;&#26469;&#27963;&#21160;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.03100</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;LSTM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#39044;&#27979;&#29992;&#25143;&#29305;&#23450;&#30340;&#26410;&#26469;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Predicting User-specific Future Activities using LSTM-based Multi-label Classification. (arXiv:2211.03100v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03100
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LSTM-based&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#24494;&#35843;&#25552;&#39640;&#20102;&#29992;&#25143;&#29305;&#23450;&#26410;&#26469;&#27963;&#21160;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20808;&#21069;&#27963;&#21160;&#30340;&#29992;&#25143;&#29305;&#23450;&#26410;&#26469;&#27963;&#21160;&#39044;&#27979;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#25252;&#22763;&#25552;&#20379;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#36827;&#34892;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#19982;&#20854;&#20182;&#39046;&#22495;&#19981;&#21516;&#65292;&#21307;&#30103;&#39046;&#22495;&#30340;&#27963;&#21160;&#28041;&#21450;&#25252;&#22763;&#21644;&#24739;&#32773;&#65292;&#24182;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#26412;&#25991;&#37319;&#29992;&#21508;&#31181;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#26469;&#32452;&#32455;&#21644;&#20462;&#25913;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;LSTM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65288;&#29992;&#25143;&#19981;&#21487;&#30693;&#30340;&#39044;&#35757;&#32451;&#21644;&#29992;&#25143;&#29305;&#23450;&#30340;&#24494;&#35843;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#39564;&#35777;&#38598;&#19978;&#36798;&#21040;&#20102;31.58%&#30340;&#20934;&#30830;&#29575;&#65292;57.94%&#30340;&#31934;&#30830;&#24230;&#65292;68.31%&#30340;&#21484;&#22238;&#29575;&#21644;60.38%&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#36866;&#24403;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#20010;&#23454;&#39564;&#26159;&#25105;&#20204;&#22242;&#38431;&#8220;&#19981;&#26159;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#31881;&#19997;&#8221;&#21442;&#21152;&#30340;&#8220;&#31532;&#22235;&#20010;&#25252;&#22763;&#25252;&#29702;&#27963;&#21160;&#35782;&#21035;&#25361;&#25112;&#8221;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-specific future activity prediction in the healthcare domain based on previous activities can drastically improve the services provided by the nurses. It is challenging because, unlike other domains, activities in healthcare involve both nurses and patients, and they also vary from hour to hour. In this paper, we employ various data processing techniques to organize and modify the data structure and an LSTM-based multi-label classifier for a novel 2-stage training approach (user-agnostic pre-training and user-specific fine-tuning). Our experiment achieves a validation accuracy of 31.58\%, precision 57.94%, recall 68.31%, and F1 score 60.38%. We concluded that proper data pre-processing and a 2-stage training process resulted in better performance. This experiment is a part of the "Fourth Nurse Care Activity Recognition Challenge" by our team "Not A Fan of Local Minima".
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#30340;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#26041;&#27861;&#27604;&#36739;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01607</link><description>&lt;p&gt;
ImageCAS:&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#30340;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ImageCAS: A Large-Scale Dataset and Benchmark for Coronary Artery Segmentation based on Computed Tomography Angiography Images. (arXiv:2211.01607v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01607
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#30340;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#26041;&#27861;&#27604;&#36739;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#21344;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#36817;&#19968;&#21322;&#12290;&#20896;&#29366;&#21160;&#33033;&#30340;&#34880;&#31649;&#29421;&#31364;&#34987;&#35748;&#20026;&#26159;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20027;&#35201;&#39118;&#38505;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#65288;CTA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20896;&#29366;&#21160;&#33033;&#35786;&#26029;&#30340;&#26080;&#21019;&#25104;&#20687;&#27169;&#24335;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#22270;&#20687;&#20998;&#36776;&#29575;&#12290;&#22312;&#20020;&#24202;&#19978;&#65292;&#20896;&#29366;&#21160;&#33033;&#30340;&#20998;&#21106;&#23545;&#20110;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#37327;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#20381;&#36182;&#20110;&#33258;&#26377;&#25968;&#25454;&#38598;&#65292;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#23558;&#25968;&#25454;&#38598;&#21457;&#34920;&#21040;&#20844;&#20247;&#39046;&#22495;&#65292;&#20854;&#20013;&#21482;&#21253;&#21547;&#25968;&#21313;&#20010;&#22270;&#20687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#28304;&#20195;&#30721;&#23578;&#26410;&#21457;&#24067;&#65292;&#22823;&#22810;&#25968;&#21518;&#32493;&#24037;&#20316;&#20063;&#27809;&#26377;&#19982;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#21028;&#26029;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#38459;&#30861;&#20102;&#31038;&#21306;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#20851;&#38190;&#24615;&#38382;&#39064;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#30340;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular disease (CVD) accounts for about half of non-communicable diseases. Vessel stenosis in the coronary artery is considered to be the major risk of CVD. Computed tomography angiography (CTA) is one of the widely used noninvasive imaging modalities in coronary artery diagnosis due to its superior image resolution. Clinically, segmentation of coronary arteries is essential for the diagnosis and quantification of coronary artery disease. Recently, a variety of works have been proposed to address this problem. However, on one hand, most works rely on in-house datasets, and only a few works published their datasets to the public which only contain tens of images. On the other hand, their source code have not been published, and most follow-up works have not made comparison with existing works, which makes it difficult to judge the effectiveness of the methods and hinders the further exploration of this challenging yet critical problem in the community. In this paper, we propose 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;&#30005;&#23481;&#23618;&#26512;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;320K&#20010;&#21512;&#25104;&#22270;&#20687;&#27979;&#37327;&#23545;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2209.03737</link><description>&lt;p&gt;
CGAN-ECT&#65306;&#20351;&#29992;CGAN&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;&#23618;&#26512;&#25104;&#20687;&#12290;&#65288;arXiv:2209.03737v3 [eess.IV] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
CGAN-ECT: Tomography Image Reconstruction from Electrical Capacitance Measurements Using CGANs. (arXiv:2209.03737v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03737
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;&#30005;&#23481;&#23618;&#26512;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;320K&#20010;&#21512;&#25104;&#22270;&#20687;&#27979;&#37327;&#23545;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30005;&#23481;&#23618;&#26512;&#25104;&#20687;&#65288;ECT&#65289;&#22312;&#22810;&#20010;&#24037;&#19994;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#24212;&#29992;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#39640;&#36136;&#37327;&#19988;&#24555;&#36895;&#30340;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#26469;&#20174;&#21407;&#22987;&#30005;&#23481;&#27979;&#37327;&#20013;&#24471;&#21040;&#22270;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#24037;&#20855;&#65292;&#24050;&#32463;&#22312;&#21253;&#25324;&#30005;&#27668;&#23618;&#26512;&#25104;&#20687;&#22312;&#20869;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;ECT&#22270;&#20687;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#27169;&#22411;&#12290;&#35813;CGAN&#27169;&#22411;&#30340;&#21021;&#22987;&#22270;&#20687;&#26159;&#30001;&#30005;&#23481;&#27979;&#37327;&#26500;&#36896;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23558;&#30005;&#23481;&#27979;&#37327;&#34920;&#31034;&#20026;&#22270;&#20687;&#24418;&#24335;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;320K&#20010;&#21512;&#25104;&#22270;&#20687;&#27979;&#37327;&#23545;&#30340;&#26032;ECT&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#21463;&#27745;&#26579;&#25968;&#25454;&#21644;&#27969;&#21160;&#27169;&#24335;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;CGAN-ECT&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rapid growth of Electrical Capacitance Tomography (ECT) applications in several industrial fields, there is a crucial need for developing high quality, yet fast, methodologies of image reconstruction from raw capacitance measurements. Deep learning, as an effective non-linear mapping tool for complicated functions, has been going viral in many fields including electrical tomography. In this paper, we propose a Conditional Generative Adversarial Network (CGAN) model for reconstructing ECT images from capacitance measurements. The initial image of the CGAN model is constructed from the capacitance measurement. To our knowledge, this is the first time to represent the capacitance measurements in an image form. We have created a new massive ECT dataset of 320K synthetic image measurements pairs for training, and testing the proposed model. The feasibility and generalization ability of the proposed CGAN-ECT model are evaluated using testing dataset, contaminated data and flow pat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#35777;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#35266;&#23519;&#21040;&#31283;&#23450;&#27169;&#24335;&#36328;&#20219;&#21153;&#21644;&#25299;&#25169;&#32467;&#26500;&#25345;&#32493;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#28608;&#21457;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#31185;&#23398;&#30740;&#31350;&#65292;&#20026;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#29702;&#35770;&#21457;&#29616;&#25552;&#20379;&#24517;&#35201;&#30340;&#21160;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.12547</link><description>&lt;p&gt;
BUTTER&#21306;&#22495;&#65306;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21160;&#24577;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The BUTTER Zone: An Empirical Study of Training Dynamics in Fully Connected Neural Networks. (arXiv:2207.12547v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#35777;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#35266;&#23519;&#21040;&#31283;&#23450;&#27169;&#24335;&#36328;&#20219;&#21153;&#21644;&#25299;&#25169;&#32467;&#26500;&#25345;&#32493;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#28608;&#21457;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#31185;&#23398;&#30740;&#31350;&#65292;&#20026;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#29702;&#35770;&#21457;&#29616;&#25552;&#20379;&#24517;&#35201;&#30340;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#35777;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#36830;&#25509;&#21069;&#39304;&#22810;&#23618;&#24863;&#30693;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#29616;&#35937;&#12290;&#35813;&#25968;&#25454;&#38598;&#29616;&#22312;&#21487;&#20197;&#22312;&#32593;&#19978;&#20813;&#36153;&#33719;&#21462;&#65292;&#35760;&#24405;&#20102;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#19979;&#30340;&#26550;&#26500;&#12289;&#20219;&#21153;&#12289;&#28145;&#24230;&#12289;&#32593;&#32476;&#22823;&#23567;&#65288;&#21442;&#25968;&#25968;&#37327;&#65289;&#12289;&#23398;&#20064;&#29575;&#12289;&#25209;&#27425;&#22823;&#23567;&#21644;&#27491;&#21017;&#21270;&#24809;&#32602;&#30340;&#27599;&#20010;&#26102;&#20195;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#24179;&#22343;&#37325;&#22797;&#27599;&#20010;&#23454;&#39564;24&#27425;&#65292;&#20849;&#35760;&#24405;&#20102;1100&#19975;&#20010;&#35757;&#32451;&#36816;&#34892;&#21644;400&#20159;&#20010;&#26102;&#20195;&#12290;&#24635;&#35745;&#20351;&#29992;&#20102;1.7 TB&#30340;&#25968;&#25454;&#38598;&#65292;&#32791;&#36153;&#20102;11000&#20010;CPU&#26680;&#24180;&#12289;72.3&#20010;GPU&#24180;&#21644;163&#20010;&#33410;&#28857;&#24180;&#12290;&#22312;&#35843;&#26597;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#36328;&#20219;&#21153;&#21644;&#25299;&#25169;&#32467;&#26500;&#25345;&#32493;&#23384;&#22312;&#30340;&#31283;&#23450;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24341;&#21457;&#23545;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#31185;&#23398;&#30740;&#31350;&#65292;&#20026;&#25512;&#36827;&#35813;&#39046;&#22495;&#36229;&#36234;&#39640;&#33021;&#32791;&#21644;&#21551;&#21457;&#24335;&#23454;&#36341;&#25152;&#38656;&#30340;&#29702;&#35770;&#21457;&#29616;&#25552;&#20379;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an empirical dataset surveying the deep learning phenomenon on fully-connected feed-forward multilayer perceptron neural networks. The dataset, which is now freely available online, records the per-epoch training and generalization performance of 483 thousand distinct hyperparameter choices of architectures, tasks, depths, network sizes (number of parameters), learning rates, batch sizes, and regularization penalties. Repeating each experiment an average of 24 times resulted in 11 million total training runs and 40 billion epochs recorded. Accumulating this 1.7 TB dataset utilized 11 thousand CPU core-years, 72.3 GPU-years, and 163 node-years. In surveying the dataset, we observe durable patterns persisting across tasks and topologies. We aim to spark scientific study of machine learning techniques as a catalyst for the theoretical discoveries needed to progress the field beyond energy-intensive and heuristic practices.
&lt;/p&gt;</description></item><item><title>&#23548;&#25968;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;DINOs&#65289;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#32500;&#21442;&#25968;&#23548;&#25968;&#23398;&#20064;&#12290;&#36890;&#36807;&#21387;&#32553;&#23548;&#25968;&#20449;&#24687;&#21644;&#39640;&#25928;&#24212;&#29992;&#20110;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#65292;DINOs&#21487;&#20197;&#25552;&#39640;&#31639;&#23376;&#21644;&#23548;&#25968;&#30340;&#20934;&#30830;&#24615;&#65292;&#24212;&#29992;&#20110;&#20247;&#22810;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2206.10745</link><description>&lt;p&gt;
&#23548;&#25968;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#31639;&#23376;&#65306;&#19968;&#20010;&#39640;&#32500;&#21442;&#25968;&#23548;&#25968;&#23398;&#20064;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Derivative-Informed Neural Operator: An Efficient Framework for High-Dimensional Parametric Derivative Learning. (arXiv:2206.10745v4 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10745
&lt;/p&gt;
&lt;p&gt;
&#23548;&#25968;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;DINOs&#65289;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#32500;&#21442;&#25968;&#23548;&#25968;&#23398;&#20064;&#12290;&#36890;&#36807;&#21387;&#32553;&#23548;&#25968;&#20449;&#24687;&#21644;&#39640;&#25928;&#24212;&#29992;&#20110;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#65292;DINOs&#21487;&#20197;&#25552;&#39640;&#31639;&#23376;&#21644;&#23548;&#25968;&#30340;&#20934;&#30830;&#24615;&#65292;&#24212;&#29992;&#20110;&#20247;&#22810;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23548;&#25968;&#20449;&#24687;&#39537;&#21160;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;DINOs&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#65292;&#29992;&#20110;&#36817;&#20284;&#20316;&#20026;&#20174;&#36755;&#20837;&#20989;&#25968;&#31354;&#38388;&#21040;&#36755;&#20986;&#20989;&#25968;&#31354;&#38388;&#25110;&#24863;&#20852;&#36259;&#30340;&#20540;&#30340;&#26080;&#38480;&#32500;&#26144;&#23556;&#30340;&#31639;&#23376;&#12290;&#22312;&#31163;&#25955;&#21270;&#20043;&#21518;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#37117;&#26159;&#39640;&#32500;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#19981;&#20165;&#20197;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#36817;&#20284;&#31639;&#23376;&#65292;&#32780;&#19988;&#36817;&#20284;&#23427;&#20204;&#23545;&#36755;&#20837;&#20989;&#25968;&#20540;&#21442;&#25968;&#30340;&#23548;&#25968;&#65288;&#38597;&#21487;&#27604;&#30697;&#38453;&#65289;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#65292;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20248;&#21270;&#21644;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#12290;&#20027;&#35201;&#30340;&#22256;&#38590;&#21253;&#25324;&#29983;&#25104;&#23548;&#25968;&#35757;&#32451;&#25968;&#25454;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#38382;&#39064;&#30340;&#39640;&#32500;&#24230;&#23548;&#33268;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#23548;&#25968;&#30340;&#22266;&#26377;&#20302;&#32500;&#24615;&#36136;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#21387;&#32553;&#23548;&#25968;&#20449;&#24687;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose derivative-informed neural operators (DINOs), a general family of neural networks to approximate operators as infinite-dimensional mappings from input function spaces to output function spaces or quantities of interest. After discretizations both inputs and outputs are high-dimensional. We aim to approximate not only the operators with improved accuracy but also their derivatives (Jacobians) with respect to the input function-valued parameter to empower derivative-based algorithms in many applications, e.g., Bayesian inverse problems, optimization under parameter uncertainty, and optimal experimental design. The major difficulties include the computational cost of generating derivative training data and the high dimensionality of the problem leading to large training cost. To address these challenges, we exploit the intrinsic low-dimensionality of the derivatives and develop algorithms for compressing derivative information and efficiently imposing it in neural operator trai
&lt;/p&gt;</description></item><item><title>SSM-DTA&#26694;&#26550;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#25171;&#30772;&#20102;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#39044;&#27979;&#20013;&#25968;&#25454;&#31232;&#32570;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2206.09818</link><description>&lt;p&gt;
SSM-DTA: &#25171;&#30772;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction. (arXiv:2206.09818v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09818
&lt;/p&gt;
&lt;p&gt;
SSM-DTA&#26694;&#26550;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#25171;&#30772;&#20102;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#39044;&#27979;&#20013;&#25968;&#25454;&#31232;&#32570;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#65288;DTA&#65289;&#22312;&#26089;&#26399;&#33647;&#29289;&#30740;&#21457;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#21487;&#20197;&#19982;&#29305;&#23450;&#38774;&#28857;&#26377;&#25928;&#30456;&#20114;&#20316;&#29992;&#24182;&#35843;&#33410;&#20854;&#27963;&#24615;&#30340;&#33647;&#29289;&#12290;&#23613;&#31649;&#28287;&#23454;&#39564;&#20173;&#28982;&#26159;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#32791;&#26102;&#32791;&#21147;&#65292;&#23548;&#33268;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#32473;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#29616;&#26377;DTA&#25968;&#25454;&#24320;&#21457;&#25216;&#26415;&#19978;&#65292;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSM-DTA&#26694;&#26550;&#65292;&#23427;&#21253;&#21547;&#19977;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22810;&#20219;&#21153;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;DTA&#39044;&#27979;&#19982;&#33945;&#29256;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;&#25104;&#23545;&#30340;&#33647;&#29289;-&#38774;&#28857;&#25968;&#25454;&#12290;&#65288;2&#65289;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#26080;&#37197;&#23545;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#26469;&#22686;&#24378;&#33647;&#29289;&#21644;&#38774;&#28857;&#30340;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of Drug-Target Affinity (DTA) is of vital importance in early-stage drug discovery, facilitating the identification of drugs that can effectively interact with specific targets and regulate their activities. While wet experiments remain the most reliable method, they are time-consuming and resource-intensive, resulting in limited data availability that poses challenges for deep learning approaches. Existing methods have primarily focused on developing techniques based on the available DTA data, without adequately addressing the data scarcity issue. To overcome this challenge, we present the SSM-DTA framework, which incorporates three simple yet highly effective strategies: (1) A multi-task training approach that combines DTA prediction with masked language modeling (MLM) using paired drug-target data. (2) A semi-supervised training method that leverages large-scale unpaired molecules and proteins to enhance drug and target representations. This approach differs from
&lt;/p&gt;</description></item><item><title>PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.07940</link><description>&lt;p&gt;
PROFHIT: &#38754;&#21521;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#40065;&#26834;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07940
&lt;/p&gt;
&lt;p&gt;
PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24615;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#31181;&#65292;&#20854;&#30446;&#26631;&#26159;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#20998;&#23618;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#19978;&#20063;&#24341;&#20837;&#20102;&#20998;&#23618;&#20851;&#31995;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#40664;&#40664;&#22320;&#20551;&#35774;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#20998;&#23618;&#20851;&#31995;&#19968;&#33268;&#65292;&#24182;&#19988;&#19981;&#36866;&#24212;&#26174;&#31034;&#19982;&#27492;&#20551;&#35774;&#20559;&#31163;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHIT&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#24615;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;PROFHIT&#37319;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gaps and propose PROFHIT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHIT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2206.02346</link><description>&lt;p&gt;
&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#32422;&#26463;MDP&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs. (arXiv:2206.02346v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#22870;&#21169;&#65292;&#21516;&#26102;&#28385;&#36275;&#23545;&#39044;&#26399;&#24635;&#25928;&#29992;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#32422;&#26463;MDP&#65289;&#30340;&#25240;&#25187;&#26080;&#38480;&#26102;&#24207;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#65288;NPG-PD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#65292;&#36890;&#36807;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;&#23613;&#31649;&#24213;&#23618;&#26368;&#22823;&#21270;&#28041;&#21450;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38750;&#20984;&#32422;&#26463;&#38598;&#65292;&#20294;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#35268;&#26041;&#38754;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#65292;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#36895;&#29575;&#12290;&#27492;&#31867;&#25910;&#25947;&#19982;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#21363;&#26080;&#32500;&#24230;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23545;&#25968;&#32447;&#24615;&#21644;&#19968;&#33324;&#24179;&#28369;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#30830;&#31435;&#20102;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sequential decision making problems aimed at maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we esta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.01815</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#19968;&#33268;&#24615;&#21644;&#20844;&#24179;&#20445;&#35777;&#30340;&#25512;&#33616;&#31995;&#32479;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35299;&#20915;&#38750;&#36127;/&#27491;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#19981;&#26159;&#20154;&#20026;&#22320;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20219;&#24847;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#26368;&#23567;&#21270;&#19968;&#20010;&#32467;&#26500;&#37327;&#65292;&#22914;&#31209;&#25110;&#33539;&#25968;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#23646;&#24615;/&#32422;&#26463;&#65306;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#19968;&#33268;&#24615;&#65292;&#20445;&#35777;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#30456;&#23545;&#36739;&#24369;&#30340;&#25903;&#25345;&#20551;&#35774;&#19979;&#20445;&#35777;&#20102;&#35299;&#30340;&#21807;&#19968;&#24615;&#12290;&#35813;&#26694;&#26550;&#21644;&#35299;&#31639;&#27861;&#20063;&#30452;&#25509;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#30340;&#24352;&#37327;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22266;&#23450;&#32500;&#24230; d &#30340;&#38382;&#39064;&#35268;&#27169;&#30340;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#21512;&#29702;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#24212;&#35813;&#36866;&#29992;&#20110;&#20219;&#20309; RS &#38382;&#39064;&#30340;&#35299;&#65292;&#36275;&#20197;&#20801;&#35768;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#24314;&#31435;&#21807;&#19968;&#24615;&#20445;&#35777;&#12290;&#20851;&#38190;&#29702;&#35770;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#19979;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#23558;&#29420;&#31435;&#24615;&#39537;&#21160;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31639;&#27861;&#35299;&#37322;&#20026;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#27867;&#21270;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.02355</link><description>&lt;p&gt;
&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#27867;&#21270;&#30340;&#22522;&#20110;&#29420;&#31435;&#24615;&#39537;&#21160;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis on Independence-driven Importance Weighting for Covariate-shift Generalization. (arXiv:2111.02355v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#23558;&#29420;&#31435;&#24615;&#39537;&#21160;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31639;&#27861;&#35299;&#37322;&#20026;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#27867;&#21270;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21464;&#37327;&#20559;&#31227;&#27867;&#21270;&#26159;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#27867;&#21270;&#20013;&#30340;&#20856;&#22411;&#24773;&#20917;&#65292;&#35201;&#27714;&#22312;&#26410;&#30693;&#30340;&#27979;&#35797;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#35813;&#20998;&#24067;&#19982;&#21487;&#35775;&#38382;&#30340;&#35757;&#32451;&#20998;&#24067;&#20197;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#24418;&#24335;&#26377;&#25152;&#19981;&#21516;&#12290;&#26368;&#36817;&#65292;&#31283;&#23450;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#29420;&#31435;&#24615;&#39537;&#21160;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31639;&#27861;&#22312;&#22788;&#29702;&#21253;&#25324;&#22238;&#24402;&#31639;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#22810;&#20010;&#23398;&#20064;&#27169;&#22411;&#19978;&#26174;&#31034;&#20986;&#20102;&#32463;&#39564;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#23578;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#23427;&#20204;&#35299;&#37322;&#20026;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25351;&#23450;&#20102;&#19968;&#32452;&#21464;&#37327;&#65292;&#31216;&#20026;&#26368;&#23567;&#31283;&#23450;&#21464;&#37327;&#38598;&#65292;&#35813;&#38598;&#21512;&#26159;&#22788;&#29702;&#21327;&#21464;&#37327;&#20559;&#31227;&#27867;&#21270;&#30340;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#22343;&#26041;&#25439;&#22833;&#21644;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30340;&#26368;&#23567;&#26368;&#20248;&#21464;&#37327;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#29702;&#24819;&#26465;&#20214;&#19979;&#65292;&#22312;&#36825;&#20123;&#31639;&#27861;&#19979;&#65292;&#29420;&#31435;&#24615;&#39537;&#21160;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#36825;&#20010;&#26368;&#23567;&#31283;&#23450;&#21464;&#37327;&#38598;&#30340;&#26377;&#25928;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate-shift generalization, a typical case in out-of-distribution (OOD) generalization, requires a good performance on the unknown test distribution, which varies from the accessible training distribution in the form of covariate shift. Recently, independence-driven importance weighting algorithms in stable learning literature have shown empirical effectiveness to deal with covariate-shift generalization on several learning models, including regression algorithms and deep neural networks, while their theoretical analyses are missing. In this paper, we theoretically prove the effectiveness of such algorithms by explaining them as feature selection processes. We first specify a set of variables, named minimal stable variable set, that is the minimal and optimal set of variables to deal with covariate-shift generalization for common loss functions, such as the mean squared loss and binary cross-entropy loss. Afterward, we prove that under ideal conditions, independence-driven importan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#38544;&#31169;&#25935;&#24863;&#24212;&#29992;&#20013;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2108.12978</link><description>&lt;p&gt;
&#31169;&#26377;&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;&#34920;&#36848;&#19982;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Private Multi-Task Learning: Formulation and Applications to Federated Learning. (arXiv:2108.12978v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.12978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#38544;&#31169;&#25935;&#24863;&#24212;&#29992;&#20013;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20381;&#36182;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;MTL&#23545;&#20110;&#38544;&#31169;&#25935;&#24863;&#30340;&#24212;&#29992;&#29305;&#21035;&#37325;&#35201;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#29289;&#32852;&#32593;&#35745;&#31639;&#65292;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#20849;&#20139;&#26469;&#33258;&#22810;&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#25935;&#24863;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#65288;JDP&#65289;&#20026;MTL&#24418;&#24335;&#21270;&#23458;&#25143;&#32423;&#38544;&#31169;&#30340;&#27010;&#24565;&#65292;JDP&#26159;&#24046;&#20998;&#38544;&#31169;&#22312;&#26426;&#21046;&#35774;&#35745;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#19968;&#31181;&#25918;&#26494;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22343;&#20540;&#27491;&#21017;&#21270;MTL&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#28385;&#36275;JDP&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#30446;&#26631;&#21644;&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#12290;&#20174;&#23454;&#35777;&#19978;&#30475;&#65292;&#30456;&#23545;&#20110;&#24120;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;/&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing, where sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of client-level privacy for MTL via joint differential privacy (JDP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method provides improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#40644;&#37329;&#36873;&#25321;&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#24688;&#24403;&#30340;&#35757;&#32451;&#28857;&#26469;&#21152;&#24555;&#27169;&#22411;&#35757;&#32451;&#12290;&#19982;&#20248;&#21270;&#25991;&#29486;&#20013;&#36890;&#24120;&#36873;&#25321;&#30340;&#22256;&#38590;&#28857;&#21644;&#35838;&#31243;&#23398;&#20064;&#20013;&#36890;&#24120;&#20248;&#20808;&#36873;&#25321;&#30340;&#31616;&#21333;&#28857;&#19981;&#21516;&#65292;&#40644;&#37329;&#36873;&#25321;&#36873;&#25321;&#30340;&#28857;&#26082;&#26377;&#36739;&#39640;&#30340;&#20449;&#24687;&#37327;&#21448;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#19988;&#21487;&#36801;&#31227;&#21040;&#20854;&#20182;&#26550;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2107.02565</link><description>&lt;p&gt;
&#20248;&#20808;&#35757;&#32451;&#21487;&#23398;&#20064;&#12289;&#20540;&#24471;&#23398;&#20064;&#19988;&#23578;&#26410;&#23398;&#20064;&#30340;&#28857;&#65288;&#30740;&#35752;&#20250;&#29256;&#26412;&#65289;
&lt;/p&gt;
&lt;p&gt;
Prioritized training on points that are learnable, worth learning, and not yet learned (workshop version). (arXiv:2107.02565v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#40644;&#37329;&#36873;&#25321;&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#24688;&#24403;&#30340;&#35757;&#32451;&#28857;&#26469;&#21152;&#24555;&#27169;&#22411;&#35757;&#32451;&#12290;&#19982;&#20248;&#21270;&#25991;&#29486;&#20013;&#36890;&#24120;&#36873;&#25321;&#30340;&#22256;&#38590;&#28857;&#21644;&#35838;&#31243;&#23398;&#20064;&#20013;&#36890;&#24120;&#20248;&#20808;&#36873;&#25321;&#30340;&#31616;&#21333;&#28857;&#19981;&#21516;&#65292;&#40644;&#37329;&#36873;&#25321;&#36873;&#25321;&#30340;&#28857;&#26082;&#26377;&#36739;&#39640;&#30340;&#20449;&#24687;&#37327;&#21448;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#19988;&#21487;&#36801;&#31227;&#21040;&#20854;&#20182;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#40644;&#37329;&#36873;&#25321;&#8221;&#25216;&#26415;&#65292;&#19968;&#31181;&#29992;&#20110;&#26356;&#24555;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#23427;&#36873;&#25321;&#20102;&#19968;&#31995;&#21015;&#8220;&#24688;&#21040;&#22909;&#22788;&#8221;&#30340;&#35757;&#32451;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#33719;&#21462;&#20989;&#25968;&#8212;&#8212;&#21487;&#32422;&#30340;&#39564;&#35777;&#25439;&#22833;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#8212;&#8212;GoldiProx&#26469;&#39640;&#25928;&#36873;&#25321;&#33021;&#22815;&#26368;&#22823;&#21270;&#39564;&#35777;&#38598;&#20449;&#24687;&#30340;&#35757;&#32451;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#24120;&#22312;&#20248;&#21270;&#25991;&#29486;&#20013;&#36873;&#25321;&#30340;&#8220;&#22256;&#38590;&#8221;&#65288;&#20363;&#22914;&#39640;&#25439;&#22833;&#65289;&#28857;&#24448;&#24448;&#26159;&#26377;&#22122;&#22768;&#30340;&#65292;&#32780;&#36890;&#24120;&#20248;&#20808;&#36873;&#25321;&#29992;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#8220;&#31616;&#21333;&#8221;&#65288;&#20363;&#22914;&#20302;&#22122;&#22768;&#65289;&#26679;&#26412;&#25552;&#20379;&#30340;&#20449;&#24687;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#36890;&#24120;&#34987;&#20027;&#21160;&#23398;&#20064;&#25152;&#38024;&#23545;&#30340;&#24102;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#30340;&#28857;&#24448;&#24448;&#19982;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#36739;&#23567;&#12290;&#30456;&#21453;&#65292;&#40644;&#37329;&#36873;&#25321;&#36873;&#25321;&#30340;&#28857;&#26082;&#36866;&#20013;&#21448;&#20855;&#26377;&#36739;&#22909;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25152;&#36873;&#25321;&#30340;&#24207;&#21015;&#21487;&#20197;&#36801;&#31227;&#21040;&#20854;&#20182;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#23454;&#36341;&#32773;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#21019;&#24314;&#30340;&#24773;&#20917;&#19979;&#20849;&#20139;&#21644;&#22797;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Goldilocks Selection, a technique for faster model training which selects a sequence of training points that are "just right". We propose an information-theoretic acquisition function -- the reducible validation loss -- and compute it with a small proxy model -- GoldiProx -- to efficiently choose training points that maximize information about a validation set. We show that the "hard" (e.g. high loss) points usually selected in the optimization literature are typically noisy, while the "easy" (e.g. low noise) samples often prioritized for curriculum learning confer less information. Further, points with uncertain labels, typically targeted by active learning, tend to be less relevant to the task. In contrast, Goldilocks Selection chooses points that are "just right" and empirically outperforms the above approaches. Moreover, the selected sequence can transfer to other architectures; practitioners can share and reuse it without the need to recreate it.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#25512;&#26029;&#35774;&#32622;&#20013;&#65292;&#38024;&#23545;&#32047;&#35745;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26368;&#20248;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#28176;&#22686;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#36882;&#20943;&#36793;&#38469;&#25910;&#30410;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#35823;&#37197;&#27979;&#35797;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#22870;&#21169;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2106.06483</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#35768;&#22810;&#31867;&#21035;&#30340;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#36890;&#36807;&#31163;&#32447;&#31070;&#35861;&#36827;&#34892;&#26368;&#20248;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles. (arXiv:2106.06483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#25512;&#26029;&#35774;&#32622;&#20013;&#65292;&#38024;&#23545;&#32047;&#35745;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26368;&#20248;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#28176;&#22686;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#36882;&#20943;&#36793;&#38469;&#25910;&#30410;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#35823;&#37197;&#27979;&#35797;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#22870;&#21169;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#25104;&#26412;&#30340;&#20445;&#35777;&#65292;&#23601;&#22909;&#20687;&#26368;&#20248;&#24179;&#34913;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#27169;&#22411;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#19968;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#25512;&#26029;&#35774;&#32622;&#20013;&#23454;&#29616;&#31867;&#20284;&#20445;&#35777;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350; [Marinov and Zimmert, 2021] &#37492;&#21035;&#20986;&#27809;&#26377;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#26080;&#25104;&#26412;&#30340;&#36951;&#25022;&#30028;&#38480;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28176;&#22686;&#31867;&#21035;&#22797;&#26434;&#24615;&#21644;&#38543;&#30528;&#31867;&#21035;&#22797;&#26434;&#24615;&#22686;&#21152;&#26368;&#20339;&#31574;&#30053;&#20215;&#20540;&#36793;&#38469;&#25910;&#30410;&#36882;&#20943;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#26080;&#25104;&#26412;&#27169;&#22411;&#36873;&#25321;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#35823;&#37197;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#23637;&#31034;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#22870;&#21169;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;&#19982;&#20808;&#21069;&#20851;&#20110;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#27169;&#22411;&#36873;&#25321;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25910;&#38598;&#26356;&#22810;&#25968;&#25454;&#26102;&#20250;&#20180;&#32454;&#22320;&#36866;&#24212;&#36880;&#28176;&#28436;&#21464;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#36229;&#36234;&#20102;&#36866;&#24212;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#33539;&#30068;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection in supervised learning provides costless guarantees as if the model that best balances bias and variance was known a priori. We study the feasibility of similar guarantees for cumulative regret minimization in the stochastic contextual bandit setting. Recent work [Marinov and Zimmert, 2021] identifies instances where no algorithm can guarantee costless regret bounds. Nevertheless, we identify benign conditions where costless model selection is feasible: gradually increasing class complexity, and diminishing marginal returns for best-in-class policy value with increasing class complexity. Our algorithm is based on a novel misspecification test, and our analysis demonstrates the benefits of using model selection for reward estimation. Unlike prior work on model selection in contextual bandits, our algorithm carefully adapts to the evolving bias-variance trade-off as more data is collected. In particular, our algorithm and analysis go beyond adapting to the complexity of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;PAC&#20108;&#20998;&#31867;&#12289;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#20197;&#21450;&#20934;&#30830;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#24615;&#37117;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#65292;&#21363;&#26080;&#27861;&#36890;&#36807;&#19968;&#33324;&#36807;&#31243;&#26469;&#30830;&#23450;&#26032;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#24418;&#24335;&#31995;&#32479;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#21644;&#22270;&#28789;&#26426;&#30340;&#20572;&#26426;&#38382;&#39064;&#32534;&#30721;&#20026;&#20989;&#25968;&#31867;&#30340;&#24179;&#20961;&#24615;/&#26377;&#38480;&#24615;&#19982;&#26159;&#21542;&#21487;&#23398;&#20064;&#30456;&#20851;&#32852;&#26469;&#35777;&#26126;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2106.01382</link><description>&lt;p&gt;
&#20174;&#38750;&#24179;&#20961;&#24615;&#21644;&#26377;&#38480;&#24615;&#30340;&#19981;&#21487;&#21028;&#23450;&#21040;&#21487;&#23398;&#20064;&#24615;&#30340;&#19981;&#21487;&#21028;&#23450;
&lt;/p&gt;
&lt;p&gt;
From Undecidability of Non-Triviality and Finiteness to Undecidability of Learnability. (arXiv:2106.01382v3 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;PAC&#20108;&#20998;&#31867;&#12289;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#20197;&#21450;&#20934;&#30830;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#24615;&#37117;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#65292;&#21363;&#26080;&#27861;&#36890;&#36807;&#19968;&#33324;&#36807;&#31243;&#26469;&#30830;&#23450;&#26032;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#24418;&#24335;&#31995;&#32479;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#21644;&#22270;&#28789;&#26426;&#30340;&#20572;&#26426;&#38382;&#39064;&#32534;&#30721;&#20026;&#20989;&#25968;&#31867;&#30340;&#24179;&#20961;&#24615;/&#26377;&#38480;&#24615;&#19982;&#26159;&#21542;&#21487;&#23398;&#20064;&#30456;&#20851;&#32852;&#26469;&#35777;&#26126;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#32773;&#21644;&#23454;&#36341;&#32773;&#19981;&#26029;&#25193;&#22823;&#25104;&#21151;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#20182;&#20204;&#36890;&#36807;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#32463;&#39564;&#24615;&#30340;&#21551;&#21457;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#30693;&#30340;&#36890;&#29992;&#36807;&#31243;&#21487;&#20197;&#20005;&#26684;&#35780;&#20272;&#26032;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25104;&#21151;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#36807;&#31243;&#19981;&#33021;&#23384;&#22312;&#12290;&#23545;&#20110;PAC&#20108;&#20998;&#31867;&#12289;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25945;&#24072;-&#23398;&#20064;&#32773;&#20132;&#20114;&#36827;&#34892;&#20934;&#30830;&#23398;&#20064;&#65292;&#23398;&#20064;&#24615;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#65292;&#21363;&#22312;&#24418;&#24335;&#31995;&#32479;&#20013;&#20844;&#29702;&#30340;&#29420;&#31435;&#24615;&#21644;&#26080;&#27861;&#35745;&#31639;&#24615;&#30340;&#24847;&#20041;&#19978;&#37117;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#36890;&#36807;&#21487;&#35745;&#31639;&#30340;&#26500;&#36896;&#23558;&#24418;&#24335;&#31995;&#32479;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#21644;&#22270;&#28789;&#26426;&#30340;&#20572;&#26426;&#38382;&#39064;&#32534;&#30721;&#20026;&#26576;&#20123;&#20989;&#25968;&#31867;&#26159;&#24179;&#20961;/&#26377;&#38480;&#36824;&#26159;&#39640;&#24230;&#22797;&#26434;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#31867;&#19982;&#36890;&#36807;&#24050;&#24314;&#31435;&#30340;&#23398;&#20064;&#24615;&#29305;&#24449;&#21270;&#26159;&#21542;&#21487;&#23398;&#20064;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning researchers and practitioners steadily enlarge the multitude of successful learning models. They achieve this through in-depth theoretical analyses and experiential heuristics. However, there is no known general-purpose procedure for rigorously evaluating whether newly proposed models indeed successfully learn from data. We show that such a procedure cannot exist. For PAC binary classification, uniform and universal online learning, and exact learning through teacher-learner interactions, learnability is in general undecidable, both in the sense of independence of the axioms in a formal system and in the sense of uncomputability. Our proofs proceed via computable constructions that encode the consistency problem for formal systems and the halting problem for Turing machines into whether certain function classes are trivial/finite or highly complex, which we then relate to whether these classes are learnable via established characterizations of learnability through comp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2105.01331</link><description>&lt;p&gt;
BLM-17m: &#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26435;&#20445;&#25252;&#26159;&#19990;&#30028;&#19978;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#28085;&#30422;&#26368;&#36817;&#20960;&#20010;&#26376;&#20840;&#29699;&#24433;&#21709;&#28145;&#36828;&#30340;&#20154;&#26435;&#30683;&#30462;&#20043;&#19968;&#8212;&#8212;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;17&#30334;&#19975;&#25512;&#25991;&#30340;&#20027;&#39064;&#26816;&#27979;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25512;&#25991;&#26159;&#20174;2020&#24180;5&#26376;25&#26085;&#33267;2020&#24180;8&#26376;21&#26085;&#25910;&#38598;&#30340;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#20107;&#20214;&#24320;&#22987;&#21518;&#30340;89&#22825;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#27979;&#20840;&#29699;&#21644;&#26412;&#22320;&#25253;&#32440;&#30340;&#26368;&#28909;&#38376;&#26032;&#38395;&#20027;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;TF-IDF&#21644;LDA&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;k&#20540;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/MeysamAsgariC/BLMT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#30340;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#22312;&#36882;&#24402;&#26356;&#26032;&#20013;&#38454;&#25968;&#25351;&#25968;&#22686;&#21152;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2002.08410</link><description>&lt;p&gt;
&#29992;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Reduction with Composite Transportation Divergence. (arXiv:2002.08410v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.08410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#30340;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#22312;&#36882;&#24402;&#26356;&#26032;&#20013;&#38454;&#25968;&#25351;&#25968;&#22686;&#21152;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#22312;&#23494;&#24230;&#20272;&#35745;&#12289;&#20449;&#24565;&#20256;&#25773;&#21644;&#36125;&#21494;&#26031;&#28388;&#27874;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#36924;&#36817;&#23494;&#24230;&#20989;&#25968;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#20316;&#20026;&#36882;&#24402;&#26356;&#26032;&#30340;&#21021;&#22987;&#36817;&#20284;&#12290;&#36825;&#20123;&#36882;&#24402;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#28304;&#20110;&#28151;&#21512;&#38454;&#25968;&#30340;&#25351;&#25968;&#22686;&#21152;&#65292;&#23548;&#33268;&#38590;&#20197;&#27714;&#35299;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;&#65288;GMR&#65289;&#23558;&#39640;&#38454;&#39640;&#26031;&#28151;&#21512;&#36817;&#20284;&#20026;&#20302;&#38454;&#28151;&#21512;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#36136;&#21644;&#26368;&#20248;&#30446;&#26631;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#30340;&#26032;&#22411;&#20248;&#21270;GMR&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20027;&#20803;&#26368;&#23567;&#21270;&#31639;&#27861;&#26469;&#35745;&#31639;&#31616;&#21270;&#30340;&#28151;&#21512;&#65292;&#24182;&#22312;g&#20013;&#24314;&#31435;&#20102;&#20854;&#29702;&#35770;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian mixtures are widely used for approximating density functions in various applications such as density estimation, belief propagation, and Bayesian filtering. These applications often utilize Gaussian mixtures as initial approximations that are updated recursively. A key challenge in these recursive processes stems from the exponential increase in the mixture's order, resulting in intractable inference. To overcome the difficulty, the Gaussian mixture reduction (GMR), which approximates a high order Gaussian mixture by one with a lower order, can be used. Although existing clustering-based methods are known for their satisfactory performance and computational efficiency, their convergence properties and optimal targets remain unknown. In this paper, we propose a novel optimization-based GMR method based on composite transportation divergence (CTD). We develop a majorization-minimization algorithm for computing the reduced mixture and establish its theoretical convergence under g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#21644;&#24490;&#29615;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;CTR&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#21407;&#29983;&#24191;&#21578;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/1804.09133</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#21644;&#24490;&#29615;&#32593;&#32476;&#25552;&#39640;&#21407;&#29983;&#24191;&#21578;&#30340;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Native Ads CTR Prediction by Large Scale Event Embedding and Recurrent Networks. (arXiv:1804.09133v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1804.09133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#21644;&#24490;&#29615;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;CTR&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#21407;&#29983;&#24191;&#21578;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#23545;&#20110;&#21407;&#29983;&#24191;&#21578;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#27809;&#26377;&#30452;&#25509;&#30340;&#26597;&#35810;&#24847;&#22270;&#65292;&#22240;&#27492;&#24456;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#26041;&#26696;&#65292;&#36890;&#36807;&#23545;&#29992;&#25143;&#36830;&#32493;&#20107;&#20214;&#36827;&#34892;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#23402;&#29983;&#32593;&#32476;&#26469;&#32534;&#30721;&#27599;&#20010;&#29992;&#25143;&#27983;&#35272;&#20107;&#20214;&#12290;CTR&#39044;&#27979;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#30417;&#30563;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#33258;&#28982;&#22320;&#23558;&#29992;&#25143;&#21382;&#21490;&#24314;&#27169;&#20026;&#20107;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24490;&#29615;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20107;&#20214;&#23884;&#20837;&#21521;&#37327;&#21644;&#27880;&#24847;&#23618;&#23545;&#29992;&#25143;&#21382;&#21490;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#19968;&#20123;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click through rate (CTR) prediction is very important for Native advertisement but also hard as there is no direct query intent. In this paper we propose a large-scale event embedding scheme to encode the each user browsing event by training a Siamese network with weak supervision on the users' consecutive events. The CTR prediction problem is modeled as a supervised recurrent neural network, which naturally model the user history as a sequence of events. Our proposed recurrent models utilizing pretrained event embedding vectors and an attention layer to model the user history. Our experiments demonstrate that our model significantly outperforms the baseline and some variants.
&lt;/p&gt;</description></item></channel></rss>